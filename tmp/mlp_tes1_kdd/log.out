2022-10-24 15:18:15,969 - trainer - INFO - MLP(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=32, bias=True)
  )
  (activation_layers): ModuleList(
    (0): ReLU(inplace=True)
    (1): ReLU(inplace=True)
    (2): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (linear_output): Linear(in_features=32, out_features=1, bias=True)
)
2022-10-24 15:18:15,970 - trainer - INFO -   Total params: 10625
2022-10-24 15:18:15,971 - trainer - INFO -   Trainable params: 10625
2022-10-24 15:18:15,972 - trainer - INFO -   Non-trainable params: 0
2022-10-24 15:18:15,972 - trainer - INFO -   There are 11  training examples
2022-10-24 15:18:15,973 - trainer - INFO -   There are 11 examples for development
2022-10-24 15:18:16,092 - trainer - INFO - start training epoch 1
2022-10-24 15:18:16,093 - trainer - INFO - training using device=cuda
2022-10-24 15:18:16,093 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:16,096 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,238 - trainer - INFO - 
*****************[epoch: 1, global step: 2] eval training set at end of epoch***************
2022-10-24 15:18:17,238 - trainer - INFO - {
  "train_loss": 561360.25
}
2022-10-24 15:18:17,241 - trainer - INFO - start training epoch 2
2022-10-24 15:18:17,242 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,242 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,243 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,250 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval training set based on eval_every=2***************
2022-10-24 15:18:17,251 - trainer - INFO - {
  "train_loss": 551339.78125
}
2022-10-24 15:18:17,258 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval development set based on eval_every=2***************
2022-10-24 15:18:17,259 - trainer - INFO - {
  "dev_loss": 468172.15625,
  "dev_best_score_for_loss": -468172.15625
}
2022-10-24 15:18:17,260 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:17,260 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:18:17,261 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:17,265 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:17,265 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:17,265 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,268 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:18:17,268 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_2
2022-10-24 15:18:17,273 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_2
2022-10-24 15:18:17,274 - trainer - INFO - 
*****************[epoch: 2, global step: 3] eval training set at end of epoch***************
2022-10-24 15:18:17,275 - trainer - INFO - {
  "train_loss": 541319.3125
}
2022-10-24 15:18:17,276 - trainer - INFO - start training epoch 3
2022-10-24 15:18:17,277 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,277 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,278 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,287 - trainer - INFO - 
*****************[epoch: 3, global step: 4] eval training set at end of epoch***************
2022-10-24 15:18:17,287 - trainer - INFO - {
  "train_loss": 468172.1875
}
2022-10-24 15:18:17,288 - trainer - INFO - start training epoch 4
2022-10-24 15:18:17,289 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,289 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,289 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,301 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval training set based on eval_every=2***************
2022-10-24 15:18:17,301 - trainer - INFO - {
  "train_loss": 386919.65625
}
2022-10-24 15:18:17,310 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval development set based on eval_every=2***************
2022-10-24 15:18:17,311 - trainer - INFO - {
  "dev_loss": 75960.296875,
  "dev_best_score_for_loss": -75960.296875
}
2022-10-24 15:18:17,312 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:17,313 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:18:17,313 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:17,317 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:17,318 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:17,318 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,319 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:18:17,321 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_4
2022-10-24 15:18:17,325 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_4
2022-10-24 15:18:17,330 - trainer - INFO - 
*****************[epoch: 4, global step: 5] eval training set at end of epoch***************
2022-10-24 15:18:17,331 - trainer - INFO - {
  "train_loss": 305667.125
}
2022-10-24 15:18:17,332 - trainer - INFO - start training epoch 5
2022-10-24 15:18:17,333 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,334 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,334 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,344 - trainer - INFO - 
*****************[epoch: 5, global step: 6] eval training set at end of epoch***************
2022-10-24 15:18:17,345 - trainer - INFO - {
  "train_loss": 75960.2890625
}
2022-10-24 15:18:17,346 - trainer - INFO - start training epoch 6
2022-10-24 15:18:17,346 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,347 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,347 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,357 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval training set based on eval_every=2***************
2022-10-24 15:18:17,357 - trainer - INFO - {
  "train_loss": 82113.1015625
}
2022-10-24 15:18:17,365 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval development set based on eval_every=2***************
2022-10-24 15:18:17,366 - trainer - INFO - {
  "dev_loss": 132003.96875,
  "dev_best_score_for_loss": -75960.296875
}
2022-10-24 15:18:17,367 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:17,367 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,368 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:17,368 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_6
2022-10-24 15:18:17,373 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_6
2022-10-24 15:18:17,374 - trainer - INFO - 
*****************[epoch: 6, global step: 7] eval training set at end of epoch***************
2022-10-24 15:18:17,375 - trainer - INFO - {
  "train_loss": 88265.9140625
}
2022-10-24 15:18:17,375 - trainer - INFO - start training epoch 7
2022-10-24 15:18:17,376 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,376 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,378 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,388 - trainer - INFO - 
*****************[epoch: 7, global step: 8] eval training set at end of epoch***************
2022-10-24 15:18:17,388 - trainer - INFO - {
  "train_loss": 132003.953125
}
2022-10-24 15:18:17,389 - trainer - INFO - start training epoch 8
2022-10-24 15:18:17,390 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,390 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,391 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,406 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval training set based on eval_every=2***************
2022-10-24 15:18:17,407 - trainer - INFO - {
  "train_loss": 75313.42578125
}
2022-10-24 15:18:17,417 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval development set based on eval_every=2***************
2022-10-24 15:18:17,418 - trainer - INFO - {
  "dev_loss": 19218.228515625,
  "dev_best_score_for_loss": -19218.228515625
}
2022-10-24 15:18:17,419 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:17,421 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,421 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,422 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_2
2022-10-24 15:18:17,423 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:17,427 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:17,428 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:17,428 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,429 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:17,430 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_8
2022-10-24 15:18:17,435 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_8
2022-10-24 15:18:17,436 - trainer - INFO - 
*****************[epoch: 8, global step: 9] eval training set at end of epoch***************
2022-10-24 15:18:17,436 - trainer - INFO - {
  "train_loss": 18622.8984375
}
2022-10-24 15:18:17,437 - trainer - INFO - start training epoch 9
2022-10-24 15:18:17,437 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,438 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,438 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,454 - trainer - INFO - 
*****************[epoch: 9, global step: 10] eval training set at end of epoch***************
2022-10-24 15:18:17,454 - trainer - INFO - {
  "train_loss": 19218.228515625
}
2022-10-24 15:18:17,456 - trainer - INFO - start training epoch 10
2022-10-24 15:18:17,456 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,461 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,462 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,471 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval training set based on eval_every=2***************
2022-10-24 15:18:17,471 - trainer - INFO - {
  "train_loss": 41864.0869140625
}
2022-10-24 15:18:17,481 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval development set based on eval_every=2***************
2022-10-24 15:18:17,481 - trainer - INFO - {
  "dev_loss": 82149.1640625,
  "dev_best_score_for_loss": -19218.228515625
}
2022-10-24 15:18:17,482 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:17,483 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,483 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,484 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,484 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_4
2022-10-24 15:18:17,486 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_10
2022-10-24 15:18:17,490 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_10
2022-10-24 15:18:17,493 - trainer - INFO - 
*****************[epoch: 10, global step: 11] eval training set at end of epoch***************
2022-10-24 15:18:17,495 - trainer - INFO - {
  "train_loss": 64509.9453125
}
2022-10-24 15:18:17,495 - trainer - INFO - start training epoch 11
2022-10-24 15:18:17,496 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,496 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,497 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,510 - trainer - INFO - 
*****************[epoch: 11, global step: 12] eval training set at end of epoch***************
2022-10-24 15:18:17,511 - trainer - INFO - {
  "train_loss": 82149.1640625
}
2022-10-24 15:18:17,512 - trainer - INFO - start training epoch 12
2022-10-24 15:18:17,512 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,513 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,513 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,522 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval training set based on eval_every=2***************
2022-10-24 15:18:17,523 - trainer - INFO - {
  "train_loss": 74031.9296875
}
2022-10-24 15:18:17,532 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval development set based on eval_every=2***************
2022-10-24 15:18:17,532 - trainer - INFO - {
  "dev_loss": 31415.828125,
  "dev_best_score_for_loss": -19218.228515625
}
2022-10-24 15:18:17,534 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:17,536 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,538 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,538 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,539 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_6
2022-10-24 15:18:17,541 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_12
2022-10-24 15:18:17,546 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_12
2022-10-24 15:18:17,547 - trainer - INFO - 
*****************[epoch: 12, global step: 13] eval training set at end of epoch***************
2022-10-24 15:18:17,548 - trainer - INFO - {
  "train_loss": 65914.6953125
}
2022-10-24 15:18:17,548 - trainer - INFO - start training epoch 13
2022-10-24 15:18:17,549 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,550 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,553 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,567 - trainer - INFO - 
*****************[epoch: 13, global step: 14] eval training set at end of epoch***************
2022-10-24 15:18:17,568 - trainer - INFO - {
  "train_loss": 31415.830078125
}
2022-10-24 15:18:17,570 - trainer - INFO - start training epoch 14
2022-10-24 15:18:17,570 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,571 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,571 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,582 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval training set based on eval_every=2***************
2022-10-24 15:18:17,582 - trainer - INFO - {
  "train_loss": 18882.015380859375
}
2022-10-24 15:18:17,589 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval development set based on eval_every=2***************
2022-10-24 15:18:17,590 - trainer - INFO - {
  "dev_loss": 17858.244140625,
  "dev_best_score_for_loss": -17858.244140625
}
2022-10-24 15:18:17,591 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:17,592 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,595 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,595 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_8
2022-10-24 15:18:17,599 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:17,603 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:17,603 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:17,604 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,605 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:17,606 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_14
2022-10-24 15:18:17,612 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_14
2022-10-24 15:18:17,614 - trainer - INFO - 
*****************[epoch: 14, global step: 15] eval training set at end of epoch***************
2022-10-24 15:18:17,614 - trainer - INFO - {
  "train_loss": 6348.20068359375
}
2022-10-24 15:18:17,615 - trainer - INFO - start training epoch 15
2022-10-24 15:18:17,616 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,616 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,617 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,626 - trainer - INFO - 
*****************[epoch: 15, global step: 16] eval training set at end of epoch***************
2022-10-24 15:18:17,628 - trainer - INFO - {
  "train_loss": 17858.244140625
}
2022-10-24 15:18:17,628 - trainer - INFO - start training epoch 16
2022-10-24 15:18:17,629 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,629 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,630 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,638 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval training set based on eval_every=2***************
2022-10-24 15:18:17,639 - trainer - INFO - {
  "train_loss": 31698.1474609375
}
2022-10-24 15:18:17,650 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval development set based on eval_every=2***************
2022-10-24 15:18:17,650 - trainer - INFO - {
  "dev_loss": 36138.7265625,
  "dev_best_score_for_loss": -17858.244140625
}
2022-10-24 15:18:17,651 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:17,652 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,653 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,653 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,654 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_10
2022-10-24 15:18:17,655 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_16
2022-10-24 15:18:17,661 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_16
2022-10-24 15:18:17,665 - trainer - INFO - 
*****************[epoch: 16, global step: 17] eval training set at end of epoch***************
2022-10-24 15:18:17,665 - trainer - INFO - {
  "train_loss": 45538.05078125
}
2022-10-24 15:18:17,666 - trainer - INFO - start training epoch 17
2022-10-24 15:18:17,667 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,668 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,668 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,677 - trainer - INFO - 
*****************[epoch: 17, global step: 18] eval training set at end of epoch***************
2022-10-24 15:18:17,678 - trainer - INFO - {
  "train_loss": 36138.7265625
}
2022-10-24 15:18:17,679 - trainer - INFO - start training epoch 18
2022-10-24 15:18:17,679 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,680 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,680 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,687 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval training set based on eval_every=2***************
2022-10-24 15:18:17,688 - trainer - INFO - {
  "train_loss": 23328.212890625
}
2022-10-24 15:18:17,697 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval development set based on eval_every=2***************
2022-10-24 15:18:17,698 - trainer - INFO - {
  "dev_loss": 6756.6474609375,
  "dev_best_score_for_loss": -6756.6474609375
}
2022-10-24 15:18:17,699 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:17,701 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,701 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,702 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_12
2022-10-24 15:18:17,704 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:17,708 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:17,709 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:17,710 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,711 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:17,712 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_18
2022-10-24 15:18:17,716 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_18
2022-10-24 15:18:17,717 - trainer - INFO - 
*****************[epoch: 18, global step: 19] eval training set at end of epoch***************
2022-10-24 15:18:17,718 - trainer - INFO - {
  "train_loss": 10517.69921875
}
2022-10-24 15:18:17,719 - trainer - INFO - start training epoch 19
2022-10-24 15:18:17,719 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,720 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,724 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,741 - trainer - INFO - 
*****************[epoch: 19, global step: 20] eval training set at end of epoch***************
2022-10-24 15:18:17,741 - trainer - INFO - {
  "train_loss": 6756.6474609375
}
2022-10-24 15:18:17,742 - trainer - INFO - start training epoch 20
2022-10-24 15:18:17,744 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,745 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,746 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,754 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval training set based on eval_every=2***************
2022-10-24 15:18:17,754 - trainer - INFO - {
  "train_loss": 13047.09228515625
}
2022-10-24 15:18:17,761 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval development set based on eval_every=2***************
2022-10-24 15:18:17,761 - trainer - INFO - {
  "dev_loss": 28799.32421875,
  "dev_best_score_for_loss": -6756.6474609375
}
2022-10-24 15:18:17,762 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:17,763 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,764 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,764 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,764 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_14
2022-10-24 15:18:17,766 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_20
2022-10-24 15:18:17,770 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_20
2022-10-24 15:18:17,773 - trainer - INFO - 
*****************[epoch: 20, global step: 21] eval training set at end of epoch***************
2022-10-24 15:18:17,773 - trainer - INFO - {
  "train_loss": 19337.537109375
}
2022-10-24 15:18:17,774 - trainer - INFO - start training epoch 21
2022-10-24 15:18:17,775 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,775 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,776 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,785 - trainer - INFO - 
*****************[epoch: 21, global step: 22] eval training set at end of epoch***************
2022-10-24 15:18:17,786 - trainer - INFO - {
  "train_loss": 28799.32421875
}
2022-10-24 15:18:17,787 - trainer - INFO - start training epoch 22
2022-10-24 15:18:17,787 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,788 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,788 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,797 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval training set based on eval_every=2***************
2022-10-24 15:18:17,798 - trainer - INFO - {
  "train_loss": 27529.0615234375
}
2022-10-24 15:18:17,806 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval development set based on eval_every=2***************
2022-10-24 15:18:17,807 - trainer - INFO - {
  "dev_loss": 14875.236328125,
  "dev_best_score_for_loss": -6756.6474609375
}
2022-10-24 15:18:17,808 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:17,809 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,810 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,811 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,811 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_16
2022-10-24 15:18:17,813 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_22
2022-10-24 15:18:17,818 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_22
2022-10-24 15:18:17,820 - trainer - INFO - 
*****************[epoch: 22, global step: 23] eval training set at end of epoch***************
2022-10-24 15:18:17,820 - trainer - INFO - {
  "train_loss": 26258.798828125
}
2022-10-24 15:18:17,821 - trainer - INFO - start training epoch 23
2022-10-24 15:18:17,821 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,822 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,822 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,836 - trainer - INFO - 
*****************[epoch: 23, global step: 24] eval training set at end of epoch***************
2022-10-24 15:18:17,837 - trainer - INFO - {
  "train_loss": 14875.236328125
}
2022-10-24 15:18:17,837 - trainer - INFO - start training epoch 24
2022-10-24 15:18:17,838 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,838 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,839 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,851 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval training set based on eval_every=2***************
2022-10-24 15:18:17,852 - trainer - INFO - {
  "train_loss": 10249.991943359375
}
2022-10-24 15:18:17,858 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval development set based on eval_every=2***************
2022-10-24 15:18:17,861 - trainer - INFO - {
  "dev_loss": 8492.3798828125,
  "dev_best_score_for_loss": -6756.6474609375
}
2022-10-24 15:18:17,863 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:17,863 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,865 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,866 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,866 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_18
2022-10-24 15:18:17,868 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_24
2022-10-24 15:18:17,873 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_24
2022-10-24 15:18:17,874 - trainer - INFO - 
*****************[epoch: 24, global step: 25] eval training set at end of epoch***************
2022-10-24 15:18:17,880 - trainer - INFO - {
  "train_loss": 5624.74755859375
}
2022-10-24 15:18:17,881 - trainer - INFO - start training epoch 25
2022-10-24 15:18:17,882 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,882 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,883 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,891 - trainer - INFO - 
*****************[epoch: 25, global step: 26] eval training set at end of epoch***************
2022-10-24 15:18:17,892 - trainer - INFO - {
  "train_loss": 8492.3798828125
}
2022-10-24 15:18:17,895 - trainer - INFO - start training epoch 26
2022-10-24 15:18:17,895 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,896 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,896 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,904 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval training set based on eval_every=2***************
2022-10-24 15:18:17,905 - trainer - INFO - {
  "train_loss": 13271.60302734375
}
2022-10-24 15:18:17,917 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval development set based on eval_every=2***************
2022-10-24 15:18:17,918 - trainer - INFO - {
  "dev_loss": 17807.6171875,
  "dev_best_score_for_loss": -6756.6474609375
}
2022-10-24 15:18:17,919 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:17,920 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,921 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,925 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,925 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_20
2022-10-24 15:18:17,932 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_26
2022-10-24 15:18:17,936 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_26
2022-10-24 15:18:17,939 - trainer - INFO - 
*****************[epoch: 26, global step: 27] eval training set at end of epoch***************
2022-10-24 15:18:17,940 - trainer - INFO - {
  "train_loss": 18050.826171875
}
2022-10-24 15:18:17,942 - trainer - INFO - start training epoch 27
2022-10-24 15:18:17,944 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,944 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,945 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,954 - trainer - INFO - 
*****************[epoch: 27, global step: 28] eval training set at end of epoch***************
2022-10-24 15:18:17,954 - trainer - INFO - {
  "train_loss": 17807.6171875
}
2022-10-24 15:18:17,955 - trainer - INFO - start training epoch 28
2022-10-24 15:18:17,955 - trainer - INFO - training using device=cuda
2022-10-24 15:18:17,956 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:17,956 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:17,967 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval training set based on eval_every=2***************
2022-10-24 15:18:17,968 - trainer - INFO - {
  "train_loss": 13294.1474609375
}
2022-10-24 15:18:17,977 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval development set based on eval_every=2***************
2022-10-24 15:18:17,978 - trainer - INFO - {
  "dev_loss": 4947.68701171875,
  "dev_best_score_for_loss": -4947.68701171875
}
2022-10-24 15:18:17,980 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:17,982 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:17,983 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:17,984 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_22
2022-10-24 15:18:17,986 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:17,992 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:17,993 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:17,994 - trainer - INFO -   patience: 200
2022-10-24 15:18:17,995 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:17,995 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_28
2022-10-24 15:18:18,003 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_28
2022-10-24 15:18:18,005 - trainer - INFO - 
*****************[epoch: 28, global step: 29] eval training set at end of epoch***************
2022-10-24 15:18:18,006 - trainer - INFO - {
  "train_loss": 8780.677734375
}
2022-10-24 15:18:18,007 - trainer - INFO - start training epoch 29
2022-10-24 15:18:18,008 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,008 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,009 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,019 - trainer - INFO - 
*****************[epoch: 29, global step: 30] eval training set at end of epoch***************
2022-10-24 15:18:18,019 - trainer - INFO - {
  "train_loss": 4947.68701171875
}
2022-10-24 15:18:18,020 - trainer - INFO - start training epoch 30
2022-10-24 15:18:18,021 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,021 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,022 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,035 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval training set based on eval_every=2***************
2022-10-24 15:18:18,035 - trainer - INFO - {
  "train_loss": 6925.039306640625
}
2022-10-24 15:18:18,044 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval development set based on eval_every=2***************
2022-10-24 15:18:18,045 - trainer - INFO - {
  "dev_loss": 13259.8427734375,
  "dev_best_score_for_loss": -4947.68701171875
}
2022-10-24 15:18:18,046 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:18,047 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,049 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,049 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,049 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_24
2022-10-24 15:18:18,051 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_30
2022-10-24 15:18:18,054 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_30
2022-10-24 15:18:18,055 - trainer - INFO - 
*****************[epoch: 30, global step: 31] eval training set at end of epoch***************
2022-10-24 15:18:18,056 - trainer - INFO - {
  "train_loss": 8902.3916015625
}
2022-10-24 15:18:18,056 - trainer - INFO - start training epoch 31
2022-10-24 15:18:18,057 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,057 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,059 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,074 - trainer - INFO - 
*****************[epoch: 31, global step: 32] eval training set at end of epoch***************
2022-10-24 15:18:18,074 - trainer - INFO - {
  "train_loss": 13259.8427734375
}
2022-10-24 15:18:18,075 - trainer - INFO - start training epoch 32
2022-10-24 15:18:18,076 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,077 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,079 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,087 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval training set based on eval_every=2***************
2022-10-24 15:18:18,087 - trainer - INFO - {
  "train_loss": 12924.76806640625
}
2022-10-24 15:18:18,095 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval development set based on eval_every=2***************
2022-10-24 15:18:18,096 - trainer - INFO - {
  "dev_loss": 7946.14794921875,
  "dev_best_score_for_loss": -4947.68701171875
}
2022-10-24 15:18:18,097 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:18,097 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,099 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,099 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,099 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_26
2022-10-24 15:18:18,101 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_32
2022-10-24 15:18:18,105 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_32
2022-10-24 15:18:18,106 - trainer - INFO - 
*****************[epoch: 32, global step: 33] eval training set at end of epoch***************
2022-10-24 15:18:18,106 - trainer - INFO - {
  "train_loss": 12589.693359375
}
2022-10-24 15:18:18,107 - trainer - INFO - start training epoch 33
2022-10-24 15:18:18,111 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,113 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,114 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,129 - trainer - INFO - 
*****************[epoch: 33, global step: 34] eval training set at end of epoch***************
2022-10-24 15:18:18,130 - trainer - INFO - {
  "train_loss": 7946.14794921875
}
2022-10-24 15:18:18,133 - trainer - INFO - start training epoch 34
2022-10-24 15:18:18,133 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,134 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,135 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,142 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval training set based on eval_every=2***************
2022-10-24 15:18:18,143 - trainer - INFO - {
  "train_loss": 6355.439208984375
}
2022-10-24 15:18:18,149 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval development set based on eval_every=2***************
2022-10-24 15:18:18,149 - trainer - INFO - {
  "dev_loss": 6669.24853515625,
  "dev_best_score_for_loss": -4947.68701171875
}
2022-10-24 15:18:18,150 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:18,151 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,152 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,152 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,152 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_28
2022-10-24 15:18:18,154 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_34
2022-10-24 15:18:18,159 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_34
2022-10-24 15:18:18,160 - trainer - INFO - 
*****************[epoch: 34, global step: 35] eval training set at end of epoch***************
2022-10-24 15:18:18,160 - trainer - INFO - {
  "train_loss": 4764.73046875
}
2022-10-24 15:18:18,161 - trainer - INFO - start training epoch 35
2022-10-24 15:18:18,161 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,162 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,162 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,172 - trainer - INFO - 
*****************[epoch: 35, global step: 36] eval training set at end of epoch***************
2022-10-24 15:18:18,172 - trainer - INFO - {
  "train_loss": 6669.24853515625
}
2022-10-24 15:18:18,174 - trainer - INFO - start training epoch 36
2022-10-24 15:18:18,174 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,175 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,175 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,182 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval training set based on eval_every=2***************
2022-10-24 15:18:18,183 - trainer - INFO - {
  "train_loss": 8311.091064453125
}
2022-10-24 15:18:18,190 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval development set based on eval_every=2***************
2022-10-24 15:18:18,191 - trainer - INFO - {
  "dev_loss": 8884.3623046875,
  "dev_best_score_for_loss": -4947.68701171875
}
2022-10-24 15:18:18,192 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:18,193 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,194 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,194 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,195 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_30
2022-10-24 15:18:18,196 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_36
2022-10-24 15:18:18,201 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_36
2022-10-24 15:18:18,202 - trainer - INFO - 
*****************[epoch: 36, global step: 37] eval training set at end of epoch***************
2022-10-24 15:18:18,205 - trainer - INFO - {
  "train_loss": 9952.93359375
}
2022-10-24 15:18:18,205 - trainer - INFO - start training epoch 37
2022-10-24 15:18:18,206 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,207 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,207 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,217 - trainer - INFO - 
*****************[epoch: 37, global step: 38] eval training set at end of epoch***************
2022-10-24 15:18:18,218 - trainer - INFO - {
  "train_loss": 8884.361328125
}
2022-10-24 15:18:18,218 - trainer - INFO - start training epoch 38
2022-10-24 15:18:18,220 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,220 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,221 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,230 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval training set based on eval_every=2***************
2022-10-24 15:18:18,230 - trainer - INFO - {
  "train_loss": 7161.34765625
}
2022-10-24 15:18:18,239 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval development set based on eval_every=2***************
2022-10-24 15:18:18,240 - trainer - INFO - {
  "dev_loss": 4759.9765625,
  "dev_best_score_for_loss": -4759.9765625
}
2022-10-24 15:18:18,241 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:18,245 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,245 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,246 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_32
2022-10-24 15:18:18,247 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:18,252 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:18,252 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:18,252 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,254 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:18,254 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_38
2022-10-24 15:18:18,259 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_38
2022-10-24 15:18:18,262 - trainer - INFO - 
*****************[epoch: 38, global step: 39] eval training set at end of epoch***************
2022-10-24 15:18:18,263 - trainer - INFO - {
  "train_loss": 5438.333984375
}
2022-10-24 15:18:18,265 - trainer - INFO - start training epoch 39
2022-10-24 15:18:18,265 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,265 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,266 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,274 - trainer - INFO - 
*****************[epoch: 39, global step: 40] eval training set at end of epoch***************
2022-10-24 15:18:18,275 - trainer - INFO - {
  "train_loss": 4759.9765625
}
2022-10-24 15:18:18,275 - trainer - INFO - start training epoch 40
2022-10-24 15:18:18,276 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,276 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,277 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,285 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval training set based on eval_every=2***************
2022-10-24 15:18:18,286 - trainer - INFO - {
  "train_loss": 5752.412109375
}
2022-10-24 15:18:18,295 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval development set based on eval_every=2***************
2022-10-24 15:18:18,296 - trainer - INFO - {
  "dev_loss": 7898.427734375,
  "dev_best_score_for_loss": -4759.9765625
}
2022-10-24 15:18:18,298 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:18,298 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,301 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,302 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,302 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_34
2022-10-24 15:18:18,304 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_40
2022-10-24 15:18:18,311 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_40
2022-10-24 15:18:18,316 - trainer - INFO - 
*****************[epoch: 40, global step: 41] eval training set at end of epoch***************
2022-10-24 15:18:18,318 - trainer - INFO - {
  "train_loss": 6744.84765625
}
2022-10-24 15:18:18,319 - trainer - INFO - start training epoch 41
2022-10-24 15:18:18,319 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,320 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,320 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,329 - trainer - INFO - 
*****************[epoch: 41, global step: 42] eval training set at end of epoch***************
2022-10-24 15:18:18,330 - trainer - INFO - {
  "train_loss": 7898.427734375
}
2022-10-24 15:18:18,330 - trainer - INFO - start training epoch 42
2022-10-24 15:18:18,331 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,331 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,332 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,339 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval training set based on eval_every=2***************
2022-10-24 15:18:18,340 - trainer - INFO - {
  "train_loss": 7264.902099609375
}
2022-10-24 15:18:18,357 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval development set based on eval_every=2***************
2022-10-24 15:18:18,358 - trainer - INFO - {
  "dev_loss": 4709.521484375,
  "dev_best_score_for_loss": -4709.521484375
}
2022-10-24 15:18:18,361 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:18,364 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,364 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,364 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_36
2022-10-24 15:18:18,367 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:18,371 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:18,372 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:18,373 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,374 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:18,375 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_42
2022-10-24 15:18:18,380 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_42
2022-10-24 15:18:18,381 - trainer - INFO - 
*****************[epoch: 42, global step: 43] eval training set at end of epoch***************
2022-10-24 15:18:18,382 - trainer - INFO - {
  "train_loss": 6631.37646484375
}
2022-10-24 15:18:18,383 - trainer - INFO - start training epoch 43
2022-10-24 15:18:18,383 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,384 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,385 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,396 - trainer - INFO - 
*****************[epoch: 43, global step: 44] eval training set at end of epoch***************
2022-10-24 15:18:18,396 - trainer - INFO - {
  "train_loss": 4709.521484375
}
2022-10-24 15:18:18,397 - trainer - INFO - start training epoch 44
2022-10-24 15:18:18,398 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,398 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,399 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,410 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval training set based on eval_every=2***************
2022-10-24 15:18:18,412 - trainer - INFO - {
  "train_loss": 4679.50927734375
}
2022-10-24 15:18:18,425 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval development set based on eval_every=2***************
2022-10-24 15:18:18,429 - trainer - INFO - {
  "dev_loss": 6083.7216796875,
  "dev_best_score_for_loss": -4709.521484375
}
2022-10-24 15:18:18,429 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:18,431 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,432 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,433 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,435 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_38
2022-10-24 15:18:18,436 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_44
2022-10-24 15:18:18,441 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_44
2022-10-24 15:18:18,442 - trainer - INFO - 
*****************[epoch: 44, global step: 45] eval training set at end of epoch***************
2022-10-24 15:18:18,443 - trainer - INFO - {
  "train_loss": 4649.4970703125
}
2022-10-24 15:18:18,443 - trainer - INFO - start training epoch 45
2022-10-24 15:18:18,444 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,445 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,445 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,455 - trainer - INFO - 
*****************[epoch: 45, global step: 46] eval training set at end of epoch***************
2022-10-24 15:18:18,455 - trainer - INFO - {
  "train_loss": 6083.7216796875
}
2022-10-24 15:18:18,456 - trainer - INFO - start training epoch 46
2022-10-24 15:18:18,457 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,457 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,458 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,467 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval training set based on eval_every=2***************
2022-10-24 15:18:18,468 - trainer - INFO - {
  "train_loss": 6192.11376953125
}
2022-10-24 15:18:18,474 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval development set based on eval_every=2***************
2022-10-24 15:18:18,475 - trainer - INFO - {
  "dev_loss": 4922.93115234375,
  "dev_best_score_for_loss": -4709.521484375
}
2022-10-24 15:18:18,476 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:18,476 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,477 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,478 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,478 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_40
2022-10-24 15:18:18,480 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_46
2022-10-24 15:18:18,484 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_46
2022-10-24 15:18:18,485 - trainer - INFO - 
*****************[epoch: 46, global step: 47] eval training set at end of epoch***************
2022-10-24 15:18:18,486 - trainer - INFO - {
  "train_loss": 6300.505859375
}
2022-10-24 15:18:18,487 - trainer - INFO - start training epoch 47
2022-10-24 15:18:18,487 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,488 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,488 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,502 - trainer - INFO - 
*****************[epoch: 47, global step: 48] eval training set at end of epoch***************
2022-10-24 15:18:18,502 - trainer - INFO - {
  "train_loss": 4922.93115234375
}
2022-10-24 15:18:18,503 - trainer - INFO - start training epoch 48
2022-10-24 15:18:18,504 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,504 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,504 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,514 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval training set based on eval_every=2***************
2022-10-24 15:18:18,514 - trainer - INFO - {
  "train_loss": 4565.549560546875
}
2022-10-24 15:18:18,520 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval development set based on eval_every=2***************
2022-10-24 15:18:18,520 - trainer - INFO - {
  "dev_loss": 4890.076171875,
  "dev_best_score_for_loss": -4709.521484375
}
2022-10-24 15:18:18,521 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:18,521 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,522 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,523 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,523 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_42
2022-10-24 15:18:18,525 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_48
2022-10-24 15:18:18,531 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_48
2022-10-24 15:18:18,532 - trainer - INFO - 
*****************[epoch: 48, global step: 49] eval training set at end of epoch***************
2022-10-24 15:18:18,532 - trainer - INFO - {
  "train_loss": 4208.16796875
}
2022-10-24 15:18:18,533 - trainer - INFO - start training epoch 49
2022-10-24 15:18:18,534 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,534 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,535 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,545 - trainer - INFO - 
*****************[epoch: 49, global step: 50] eval training set at end of epoch***************
2022-10-24 15:18:18,545 - trainer - INFO - {
  "train_loss": 4890.07666015625
}
2022-10-24 15:18:18,546 - trainer - INFO - start training epoch 50
2022-10-24 15:18:18,546 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,549 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,550 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,560 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval training set based on eval_every=2***************
2022-10-24 15:18:18,561 - trainer - INFO - {
  "train_loss": 5196.01953125
}
2022-10-24 15:18:18,568 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval development set based on eval_every=2***************
2022-10-24 15:18:18,568 - trainer - INFO - {
  "dev_loss": 5014.3408203125,
  "dev_best_score_for_loss": -4709.521484375
}
2022-10-24 15:18:18,569 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:18,569 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,571 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,571 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,571 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_44
2022-10-24 15:18:18,573 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_50
2022-10-24 15:18:18,580 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_50
2022-10-24 15:18:18,581 - trainer - INFO - 
*****************[epoch: 50, global step: 51] eval training set at end of epoch***************
2022-10-24 15:18:18,582 - trainer - INFO - {
  "train_loss": 5501.96240234375
}
2022-10-24 15:18:18,582 - trainer - INFO - start training epoch 51
2022-10-24 15:18:18,583 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,583 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,584 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,594 - trainer - INFO - 
*****************[epoch: 51, global step: 52] eval training set at end of epoch***************
2022-10-24 15:18:18,594 - trainer - INFO - {
  "train_loss": 5014.3408203125
}
2022-10-24 15:18:18,596 - trainer - INFO - start training epoch 52
2022-10-24 15:18:18,597 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,597 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,597 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,606 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval training set based on eval_every=2***************
2022-10-24 15:18:18,606 - trainer - INFO - {
  "train_loss": 4589.415771484375
}
2022-10-24 15:18:18,615 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval development set based on eval_every=2***************
2022-10-24 15:18:18,616 - trainer - INFO - {
  "dev_loss": 4151.6396484375,
  "dev_best_score_for_loss": -4151.6396484375
}
2022-10-24 15:18:18,616 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:18,618 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,618 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,618 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_46
2022-10-24 15:18:18,620 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:18,625 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:18,625 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:18,626 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,627 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:18,628 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_52
2022-10-24 15:18:18,633 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_52
2022-10-24 15:18:18,634 - trainer - INFO - 
*****************[epoch: 52, global step: 53] eval training set at end of epoch***************
2022-10-24 15:18:18,634 - trainer - INFO - {
  "train_loss": 4164.49072265625
}
2022-10-24 15:18:18,635 - trainer - INFO - start training epoch 53
2022-10-24 15:18:18,636 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,636 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,637 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,646 - trainer - INFO - 
*****************[epoch: 53, global step: 54] eval training set at end of epoch***************
2022-10-24 15:18:18,646 - trainer - INFO - {
  "train_loss": 4151.6396484375
}
2022-10-24 15:18:18,647 - trainer - INFO - start training epoch 54
2022-10-24 15:18:18,647 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,648 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,648 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,657 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval training set based on eval_every=2***************
2022-10-24 15:18:18,657 - trainer - INFO - {
  "train_loss": 4442.408935546875
}
2022-10-24 15:18:18,671 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval development set based on eval_every=2***************
2022-10-24 15:18:18,672 - trainer - INFO - {
  "dev_loss": 4708.89306640625,
  "dev_best_score_for_loss": -4151.6396484375
}
2022-10-24 15:18:18,673 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:18,673 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,674 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,675 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,675 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_48
2022-10-24 15:18:18,678 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_54
2022-10-24 15:18:18,683 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_54
2022-10-24 15:18:18,684 - trainer - INFO - 
*****************[epoch: 54, global step: 55] eval training set at end of epoch***************
2022-10-24 15:18:18,684 - trainer - INFO - {
  "train_loss": 4733.17822265625
}
2022-10-24 15:18:18,685 - trainer - INFO - start training epoch 55
2022-10-24 15:18:18,685 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,686 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,686 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,695 - trainer - INFO - 
*****************[epoch: 55, global step: 56] eval training set at end of epoch***************
2022-10-24 15:18:18,695 - trainer - INFO - {
  "train_loss": 4708.89306640625
}
2022-10-24 15:18:18,696 - trainer - INFO - start training epoch 56
2022-10-24 15:18:18,698 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,698 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,699 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,706 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval training set based on eval_every=2***************
2022-10-24 15:18:18,707 - trainer - INFO - {
  "train_loss": 4396.044189453125
}
2022-10-24 15:18:18,717 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval development set based on eval_every=2***************
2022-10-24 15:18:18,718 - trainer - INFO - {
  "dev_loss": 3870.17626953125,
  "dev_best_score_for_loss": -3870.17626953125
}
2022-10-24 15:18:18,718 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:18,720 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,720 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,721 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_50
2022-10-24 15:18:18,722 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:18,726 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:18,728 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:18,729 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,731 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:18,731 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_56
2022-10-24 15:18:18,735 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_56
2022-10-24 15:18:18,736 - trainer - INFO - 
*****************[epoch: 56, global step: 57] eval training set at end of epoch***************
2022-10-24 15:18:18,737 - trainer - INFO - {
  "train_loss": 4083.1953125
}
2022-10-24 15:18:18,737 - trainer - INFO - start training epoch 57
2022-10-24 15:18:18,738 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,738 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,739 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,748 - trainer - INFO - 
*****************[epoch: 57, global step: 58] eval training set at end of epoch***************
2022-10-24 15:18:18,748 - trainer - INFO - {
  "train_loss": 3870.17626953125
}
2022-10-24 15:18:18,749 - trainer - INFO - start training epoch 58
2022-10-24 15:18:18,750 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,752 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,752 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,761 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval training set based on eval_every=2***************
2022-10-24 15:18:18,764 - trainer - INFO - {
  "train_loss": 4036.822265625
}
2022-10-24 15:18:18,771 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval development set based on eval_every=2***************
2022-10-24 15:18:18,772 - trainer - INFO - {
  "dev_loss": 4331.26513671875,
  "dev_best_score_for_loss": -3870.17626953125
}
2022-10-24 15:18:18,773 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:18,773 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,778 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,778 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,779 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_52
2022-10-24 15:18:18,780 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_58
2022-10-24 15:18:18,785 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_58
2022-10-24 15:18:18,786 - trainer - INFO - 
*****************[epoch: 58, global step: 59] eval training set at end of epoch***************
2022-10-24 15:18:18,786 - trainer - INFO - {
  "train_loss": 4203.46826171875
}
2022-10-24 15:18:18,787 - trainer - INFO - start training epoch 59
2022-10-24 15:18:18,787 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,788 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,788 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,799 - trainer - INFO - 
*****************[epoch: 59, global step: 60] eval training set at end of epoch***************
2022-10-24 15:18:18,800 - trainer - INFO - {
  "train_loss": 4331.2646484375
}
2022-10-24 15:18:18,801 - trainer - INFO - start training epoch 60
2022-10-24 15:18:18,801 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,802 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,802 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,811 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval training set based on eval_every=2***************
2022-10-24 15:18:18,811 - trainer - INFO - {
  "train_loss": 4154.2772216796875
}
2022-10-24 15:18:18,819 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval development set based on eval_every=2***************
2022-10-24 15:18:18,820 - trainer - INFO - {
  "dev_loss": 3681.849609375,
  "dev_best_score_for_loss": -3681.849609375
}
2022-10-24 15:18:18,825 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:18,828 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,831 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,831 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_54
2022-10-24 15:18:18,834 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:18,838 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:18,838 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:18,839 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,845 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:18,845 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_60
2022-10-24 15:18:18,852 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_60
2022-10-24 15:18:18,853 - trainer - INFO - 
*****************[epoch: 60, global step: 61] eval training set at end of epoch***************
2022-10-24 15:18:18,853 - trainer - INFO - {
  "train_loss": 3977.289794921875
}
2022-10-24 15:18:18,854 - trainer - INFO - start training epoch 61
2022-10-24 15:18:18,854 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,855 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,856 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,866 - trainer - INFO - 
*****************[epoch: 61, global step: 62] eval training set at end of epoch***************
2022-10-24 15:18:18,867 - trainer - INFO - {
  "train_loss": 3681.849609375
}
2022-10-24 15:18:18,875 - trainer - INFO - start training epoch 62
2022-10-24 15:18:18,875 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,878 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,878 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,888 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval training set based on eval_every=2***************
2022-10-24 15:18:18,888 - trainer - INFO - {
  "train_loss": 3751.0946044921875
}
2022-10-24 15:18:18,899 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval development set based on eval_every=2***************
2022-10-24 15:18:18,900 - trainer - INFO - {
  "dev_loss": 3980.575439453125,
  "dev_best_score_for_loss": -3681.849609375
}
2022-10-24 15:18:18,901 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:18,902 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,903 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,904 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,904 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_56
2022-10-24 15:18:18,906 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_62
2022-10-24 15:18:18,911 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_62
2022-10-24 15:18:18,912 - trainer - INFO - 
*****************[epoch: 62, global step: 63] eval training set at end of epoch***************
2022-10-24 15:18:18,913 - trainer - INFO - {
  "train_loss": 3820.339599609375
}
2022-10-24 15:18:18,914 - trainer - INFO - start training epoch 63
2022-10-24 15:18:18,915 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,916 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,917 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,930 - trainer - INFO - 
*****************[epoch: 63, global step: 64] eval training set at end of epoch***************
2022-10-24 15:18:18,930 - trainer - INFO - {
  "train_loss": 3980.574951171875
}
2022-10-24 15:18:18,932 - trainer - INFO - start training epoch 64
2022-10-24 15:18:18,932 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,933 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,933 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,941 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval training set based on eval_every=2***************
2022-10-24 15:18:18,942 - trainer - INFO - {
  "train_loss": 3876.8216552734375
}
2022-10-24 15:18:18,953 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval development set based on eval_every=2***************
2022-10-24 15:18:18,954 - trainer - INFO - {
  "dev_loss": 3524.25537109375,
  "dev_best_score_for_loss": -3524.25537109375
}
2022-10-24 15:18:18,955 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:18,956 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:18,956 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:18,957 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_58
2022-10-24 15:18:18,960 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:18,967 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:18,967 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:18,968 - trainer - INFO -   patience: 200
2022-10-24 15:18:18,969 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:18,969 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_64
2022-10-24 15:18:18,975 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_64
2022-10-24 15:18:18,979 - trainer - INFO - 
*****************[epoch: 64, global step: 65] eval training set at end of epoch***************
2022-10-24 15:18:18,980 - trainer - INFO - {
  "train_loss": 3773.068359375
}
2022-10-24 15:18:18,981 - trainer - INFO - start training epoch 65
2022-10-24 15:18:18,981 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,982 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,982 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:18,991 - trainer - INFO - 
*****************[epoch: 65, global step: 66] eval training set at end of epoch***************
2022-10-24 15:18:18,992 - trainer - INFO - {
  "train_loss": 3524.255126953125
}
2022-10-24 15:18:18,994 - trainer - INFO - start training epoch 66
2022-10-24 15:18:18,994 - trainer - INFO - training using device=cuda
2022-10-24 15:18:18,995 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:18,996 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,004 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval training set based on eval_every=2***************
2022-10-24 15:18:19,005 - trainer - INFO - {
  "train_loss": 3548.2298583984375
}
2022-10-24 15:18:19,018 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval development set based on eval_every=2***************
2022-10-24 15:18:19,019 - trainer - INFO - {
  "dev_loss": 3682.4013671875,
  "dev_best_score_for_loss": -3524.25537109375
}
2022-10-24 15:18:19,020 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:19,020 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,021 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,023 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,024 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_60
2022-10-24 15:18:19,026 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_66
2022-10-24 15:18:19,032 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_66
2022-10-24 15:18:19,032 - trainer - INFO - 
*****************[epoch: 66, global step: 67] eval training set at end of epoch***************
2022-10-24 15:18:19,035 - trainer - INFO - {
  "train_loss": 3572.20458984375
}
2022-10-24 15:18:19,036 - trainer - INFO - start training epoch 67
2022-10-24 15:18:19,036 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,037 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,038 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,049 - trainer - INFO - 
*****************[epoch: 67, global step: 68] eval training set at end of epoch***************
2022-10-24 15:18:19,049 - trainer - INFO - {
  "train_loss": 3682.4013671875
}
2022-10-24 15:18:19,050 - trainer - INFO - start training epoch 68
2022-10-24 15:18:19,050 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,051 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,052 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,063 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval training set based on eval_every=2***************
2022-10-24 15:18:19,064 - trainer - INFO - {
  "train_loss": 3623.1334228515625
}
2022-10-24 15:18:19,074 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval development set based on eval_every=2***************
2022-10-24 15:18:19,074 - trainer - INFO - {
  "dev_loss": 3368.3505859375,
  "dev_best_score_for_loss": -3368.3505859375
}
2022-10-24 15:18:19,076 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,079 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,080 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,080 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_62
2022-10-24 15:18:19,082 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,086 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,086 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,087 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,088 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,088 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_68
2022-10-24 15:18:19,093 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_68
2022-10-24 15:18:19,095 - trainer - INFO - 
*****************[epoch: 68, global step: 69] eval training set at end of epoch***************
2022-10-24 15:18:19,096 - trainer - INFO - {
  "train_loss": 3563.865478515625
}
2022-10-24 15:18:19,096 - trainer - INFO - start training epoch 69
2022-10-24 15:18:19,097 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,097 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,098 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,107 - trainer - INFO - 
*****************[epoch: 69, global step: 70] eval training set at end of epoch***************
2022-10-24 15:18:19,108 - trainer - INFO - {
  "train_loss": 3368.350341796875
}
2022-10-24 15:18:19,109 - trainer - INFO - start training epoch 70
2022-10-24 15:18:19,109 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,110 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,110 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,122 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval training set based on eval_every=2***************
2022-10-24 15:18:19,122 - trainer - INFO - {
  "train_loss": 3364.255126953125
}
2022-10-24 15:18:19,131 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval development set based on eval_every=2***************
2022-10-24 15:18:19,132 - trainer - INFO - {
  "dev_loss": 3432.4248046875,
  "dev_best_score_for_loss": -3368.3505859375
}
2022-10-24 15:18:19,133 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:19,134 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,137 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,137 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,138 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_64
2022-10-24 15:18:19,139 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_70
2022-10-24 15:18:19,144 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_70
2022-10-24 15:18:19,148 - trainer - INFO - 
*****************[epoch: 70, global step: 71] eval training set at end of epoch***************
2022-10-24 15:18:19,149 - trainer - INFO - {
  "train_loss": 3360.159912109375
}
2022-10-24 15:18:19,150 - trainer - INFO - start training epoch 71
2022-10-24 15:18:19,151 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,151 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,151 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,160 - trainer - INFO - 
*****************[epoch: 71, global step: 72] eval training set at end of epoch***************
2022-10-24 15:18:19,160 - trainer - INFO - {
  "train_loss": 3432.4248046875
}
2022-10-24 15:18:19,162 - trainer - INFO - start training epoch 72
2022-10-24 15:18:19,163 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,163 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,164 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,171 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval training set based on eval_every=2***************
2022-10-24 15:18:19,171 - trainer - INFO - {
  "train_loss": 3390.611572265625
}
2022-10-24 15:18:19,179 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval development set based on eval_every=2***************
2022-10-24 15:18:19,183 - trainer - INFO - {
  "dev_loss": 3200.26708984375,
  "dev_best_score_for_loss": -3200.26708984375
}
2022-10-24 15:18:19,184 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,185 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,186 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,186 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_66
2022-10-24 15:18:19,188 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,191 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,192 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,192 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,197 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,197 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_72
2022-10-24 15:18:19,203 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_72
2022-10-24 15:18:19,204 - trainer - INFO - 
*****************[epoch: 72, global step: 73] eval training set at end of epoch***************
2022-10-24 15:18:19,204 - trainer - INFO - {
  "train_loss": 3348.79833984375
}
2022-10-24 15:18:19,205 - trainer - INFO - start training epoch 73
2022-10-24 15:18:19,205 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,206 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,206 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,217 - trainer - INFO - 
*****************[epoch: 73, global step: 74] eval training set at end of epoch***************
2022-10-24 15:18:19,217 - trainer - INFO - {
  "train_loss": 3200.26708984375
}
2022-10-24 15:18:19,218 - trainer - INFO - start training epoch 74
2022-10-24 15:18:19,218 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,218 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,219 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,230 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval training set based on eval_every=2***************
2022-10-24 15:18:19,230 - trainer - INFO - {
  "train_loss": 3190.685791015625
}
2022-10-24 15:18:19,238 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval development set based on eval_every=2***************
2022-10-24 15:18:19,238 - trainer - INFO - {
  "dev_loss": 3215.099609375,
  "dev_best_score_for_loss": -3200.26708984375
}
2022-10-24 15:18:19,240 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:19,241 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,243 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,245 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,245 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_68
2022-10-24 15:18:19,247 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_74
2022-10-24 15:18:19,252 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_74
2022-10-24 15:18:19,253 - trainer - INFO - 
*****************[epoch: 74, global step: 75] eval training set at end of epoch***************
2022-10-24 15:18:19,253 - trainer - INFO - {
  "train_loss": 3181.1044921875
}
2022-10-24 15:18:19,254 - trainer - INFO - start training epoch 75
2022-10-24 15:18:19,256 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,258 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,260 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,271 - trainer - INFO - 
*****************[epoch: 75, global step: 76] eval training set at end of epoch***************
2022-10-24 15:18:19,271 - trainer - INFO - {
  "train_loss": 3215.099853515625
}
2022-10-24 15:18:19,273 - trainer - INFO - start training epoch 76
2022-10-24 15:18:19,273 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,277 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,277 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,288 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval training set based on eval_every=2***************
2022-10-24 15:18:19,288 - trainer - INFO - {
  "train_loss": 3180.5989990234375
}
2022-10-24 15:18:19,297 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval development set based on eval_every=2***************
2022-10-24 15:18:19,298 - trainer - INFO - {
  "dev_loss": 3033.048095703125,
  "dev_best_score_for_loss": -3033.048095703125
}
2022-10-24 15:18:19,298 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,300 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,300 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,301 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_70
2022-10-24 15:18:19,304 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,307 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,308 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,308 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,309 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,309 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_76
2022-10-24 15:18:19,314 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_76
2022-10-24 15:18:19,315 - trainer - INFO - 
*****************[epoch: 76, global step: 77] eval training set at end of epoch***************
2022-10-24 15:18:19,316 - trainer - INFO - {
  "train_loss": 3146.09814453125
}
2022-10-24 15:18:19,316 - trainer - INFO - start training epoch 77
2022-10-24 15:18:19,317 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,318 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,319 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,331 - trainer - INFO - 
*****************[epoch: 77, global step: 78] eval training set at end of epoch***************
2022-10-24 15:18:19,331 - trainer - INFO - {
  "train_loss": 3033.048095703125
}
2022-10-24 15:18:19,331 - trainer - INFO - start training epoch 78
2022-10-24 15:18:19,332 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,333 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,333 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,341 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval training set based on eval_every=2***************
2022-10-24 15:18:19,341 - trainer - INFO - {
  "train_loss": 3020.13671875
}
2022-10-24 15:18:19,348 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval development set based on eval_every=2***************
2022-10-24 15:18:19,349 - trainer - INFO - {
  "dev_loss": 3016.830078125,
  "dev_best_score_for_loss": -3016.830078125
}
2022-10-24 15:18:19,349 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,350 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,350 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,351 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_72
2022-10-24 15:18:19,352 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,355 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,355 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,355 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,356 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,356 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_78
2022-10-24 15:18:19,361 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_78
2022-10-24 15:18:19,362 - trainer - INFO - 
*****************[epoch: 78, global step: 79] eval training set at end of epoch***************
2022-10-24 15:18:19,362 - trainer - INFO - {
  "train_loss": 3007.225341796875
}
2022-10-24 15:18:19,362 - trainer - INFO - start training epoch 79
2022-10-24 15:18:19,362 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,363 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,364 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,374 - trainer - INFO - 
*****************[epoch: 79, global step: 80] eval training set at end of epoch***************
2022-10-24 15:18:19,374 - trainer - INFO - {
  "train_loss": 3016.830078125
}
2022-10-24 15:18:19,375 - trainer - INFO - start training epoch 80
2022-10-24 15:18:19,375 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,375 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,376 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,384 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval training set based on eval_every=2***************
2022-10-24 15:18:19,384 - trainer - INFO - {
  "train_loss": 2984.194580078125
}
2022-10-24 15:18:19,389 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval development set based on eval_every=2***************
2022-10-24 15:18:19,390 - trainer - INFO - {
  "dev_loss": 2864.019287109375,
  "dev_best_score_for_loss": -2864.019287109375
}
2022-10-24 15:18:19,390 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,391 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,392 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,392 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_74
2022-10-24 15:18:19,393 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,397 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,400 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,400 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,401 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,401 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_80
2022-10-24 15:18:19,405 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_80
2022-10-24 15:18:19,406 - trainer - INFO - 
*****************[epoch: 80, global step: 81] eval training set at end of epoch***************
2022-10-24 15:18:19,406 - trainer - INFO - {
  "train_loss": 2951.55908203125
}
2022-10-24 15:18:19,406 - trainer - INFO - start training epoch 81
2022-10-24 15:18:19,407 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,407 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,407 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,416 - trainer - INFO - 
*****************[epoch: 81, global step: 82] eval training set at end of epoch***************
2022-10-24 15:18:19,416 - trainer - INFO - {
  "train_loss": 2864.01953125
}
2022-10-24 15:18:19,417 - trainer - INFO - start training epoch 82
2022-10-24 15:18:19,417 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,417 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,417 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,426 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval training set based on eval_every=2***************
2022-10-24 15:18:19,427 - trainer - INFO - {
  "train_loss": 2852.248046875
}
2022-10-24 15:18:19,433 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval development set based on eval_every=2***************
2022-10-24 15:18:19,434 - trainer - INFO - {
  "dev_loss": 2829.945556640625,
  "dev_best_score_for_loss": -2829.945556640625
}
2022-10-24 15:18:19,435 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,436 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,436 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,436 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_76
2022-10-24 15:18:19,437 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,441 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,441 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,442 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,444 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,447 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_82
2022-10-24 15:18:19,451 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_82
2022-10-24 15:18:19,451 - trainer - INFO - 
*****************[epoch: 82, global step: 83] eval training set at end of epoch***************
2022-10-24 15:18:19,452 - trainer - INFO - {
  "train_loss": 2840.4765625
}
2022-10-24 15:18:19,452 - trainer - INFO - start training epoch 83
2022-10-24 15:18:19,452 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,453 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,453 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,463 - trainer - INFO - 
*****************[epoch: 83, global step: 84] eval training set at end of epoch***************
2022-10-24 15:18:19,463 - trainer - INFO - {
  "train_loss": 2829.94580078125
}
2022-10-24 15:18:19,464 - trainer - INFO - start training epoch 84
2022-10-24 15:18:19,464 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,464 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,465 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,471 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval training set based on eval_every=2***************
2022-10-24 15:18:19,472 - trainer - INFO - {
  "train_loss": 2798.326416015625
}
2022-10-24 15:18:19,479 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval development set based on eval_every=2***************
2022-10-24 15:18:19,479 - trainer - INFO - {
  "dev_loss": 2697.77001953125,
  "dev_best_score_for_loss": -2697.77001953125
}
2022-10-24 15:18:19,480 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,481 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,481 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,481 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_78
2022-10-24 15:18:19,483 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,486 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,486 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,486 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,487 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,488 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_84
2022-10-24 15:18:19,493 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_84
2022-10-24 15:18:19,494 - trainer - INFO - 
*****************[epoch: 84, global step: 85] eval training set at end of epoch***************
2022-10-24 15:18:19,494 - trainer - INFO - {
  "train_loss": 2766.70703125
}
2022-10-24 15:18:19,495 - trainer - INFO - start training epoch 85
2022-10-24 15:18:19,495 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,495 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,496 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,504 - trainer - INFO - 
*****************[epoch: 85, global step: 86] eval training set at end of epoch***************
2022-10-24 15:18:19,504 - trainer - INFO - {
  "train_loss": 2697.770263671875
}
2022-10-24 15:18:19,505 - trainer - INFO - start training epoch 86
2022-10-24 15:18:19,505 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,506 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,506 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,513 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval training set based on eval_every=2***************
2022-10-24 15:18:19,513 - trainer - INFO - {
  "train_loss": 2685.7901611328125
}
2022-10-24 15:18:19,521 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval development set based on eval_every=2***************
2022-10-24 15:18:19,522 - trainer - INFO - {
  "dev_loss": 2649.283203125,
  "dev_best_score_for_loss": -2649.283203125
}
2022-10-24 15:18:19,523 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,527 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,527 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,527 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_80
2022-10-24 15:18:19,529 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,533 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,533 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,533 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,535 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,535 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_86
2022-10-24 15:18:19,540 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_86
2022-10-24 15:18:19,540 - trainer - INFO - 
*****************[epoch: 86, global step: 87] eval training set at end of epoch***************
2022-10-24 15:18:19,541 - trainer - INFO - {
  "train_loss": 2673.81005859375
}
2022-10-24 15:18:19,541 - trainer - INFO - start training epoch 87
2022-10-24 15:18:19,541 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,541 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,542 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,548 - trainer - INFO - 
*****************[epoch: 87, global step: 88] eval training set at end of epoch***************
2022-10-24 15:18:19,550 - trainer - INFO - {
  "train_loss": 2649.283447265625
}
2022-10-24 15:18:19,550 - trainer - INFO - start training epoch 88
2022-10-24 15:18:19,551 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,551 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,551 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,558 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval training set based on eval_every=2***************
2022-10-24 15:18:19,558 - trainer - INFO - {
  "train_loss": 2618.5169677734375
}
2022-10-24 15:18:19,564 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval development set based on eval_every=2***************
2022-10-24 15:18:19,565 - trainer - INFO - {
  "dev_loss": 2533.3056640625,
  "dev_best_score_for_loss": -2533.3056640625
}
2022-10-24 15:18:19,566 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,568 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,570 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,570 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_82
2022-10-24 15:18:19,572 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,575 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,576 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,576 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,577 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,577 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_88
2022-10-24 15:18:19,582 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_88
2022-10-24 15:18:19,583 - trainer - INFO - 
*****************[epoch: 88, global step: 89] eval training set at end of epoch***************
2022-10-24 15:18:19,584 - trainer - INFO - {
  "train_loss": 2587.75048828125
}
2022-10-24 15:18:19,584 - trainer - INFO - start training epoch 89
2022-10-24 15:18:19,584 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,585 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,585 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,594 - trainer - INFO - 
*****************[epoch: 89, global step: 90] eval training set at end of epoch***************
2022-10-24 15:18:19,594 - trainer - INFO - {
  "train_loss": 2533.3056640625
}
2022-10-24 15:18:19,595 - trainer - INFO - start training epoch 90
2022-10-24 15:18:19,597 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,598 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,599 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,613 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval training set based on eval_every=2***************
2022-10-24 15:18:19,613 - trainer - INFO - {
  "train_loss": 2520.79150390625
}
2022-10-24 15:18:19,619 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval development set based on eval_every=2***************
2022-10-24 15:18:19,619 - trainer - INFO - {
  "dev_loss": 2472.715576171875,
  "dev_best_score_for_loss": -2472.715576171875
}
2022-10-24 15:18:19,620 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,621 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,621 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,621 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_84
2022-10-24 15:18:19,623 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,627 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,628 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,629 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,629 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,630 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_90
2022-10-24 15:18:19,634 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_90
2022-10-24 15:18:19,634 - trainer - INFO - 
*****************[epoch: 90, global step: 91] eval training set at end of epoch***************
2022-10-24 15:18:19,635 - trainer - INFO - {
  "train_loss": 2508.27734375
}
2022-10-24 15:18:19,635 - trainer - INFO - start training epoch 91
2022-10-24 15:18:19,635 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,635 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,636 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,645 - trainer - INFO - 
*****************[epoch: 91, global step: 92] eval training set at end of epoch***************
2022-10-24 15:18:19,647 - trainer - INFO - {
  "train_loss": 2472.715576171875
}
2022-10-24 15:18:19,648 - trainer - INFO - start training epoch 92
2022-10-24 15:18:19,648 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,648 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,648 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,655 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval training set based on eval_every=2***************
2022-10-24 15:18:19,655 - trainer - INFO - {
  "train_loss": 2443.914306640625
}
2022-10-24 15:18:19,663 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval development set based on eval_every=2***************
2022-10-24 15:18:19,664 - trainer - INFO - {
  "dev_loss": 2370.466064453125,
  "dev_best_score_for_loss": -2370.466064453125
}
2022-10-24 15:18:19,665 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,666 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,666 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,666 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_86
2022-10-24 15:18:19,667 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,670 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,671 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,671 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,672 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,672 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_92
2022-10-24 15:18:19,676 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_92
2022-10-24 15:18:19,677 - trainer - INFO - 
*****************[epoch: 92, global step: 93] eval training set at end of epoch***************
2022-10-24 15:18:19,678 - trainer - INFO - {
  "train_loss": 2415.113037109375
}
2022-10-24 15:18:19,678 - trainer - INFO - start training epoch 93
2022-10-24 15:18:19,678 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,678 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,679 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,686 - trainer - INFO - 
*****************[epoch: 93, global step: 94] eval training set at end of epoch***************
2022-10-24 15:18:19,686 - trainer - INFO - {
  "train_loss": 2370.466064453125
}
2022-10-24 15:18:19,686 - trainer - INFO - start training epoch 94
2022-10-24 15:18:19,686 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,687 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,687 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,695 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval training set based on eval_every=2***************
2022-10-24 15:18:19,695 - trainer - INFO - {
  "train_loss": 2356.12158203125
}
2022-10-24 15:18:19,701 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval development set based on eval_every=2***************
2022-10-24 15:18:19,701 - trainer - INFO - {
  "dev_loss": 2299.009765625,
  "dev_best_score_for_loss": -2299.009765625
}
2022-10-24 15:18:19,702 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,703 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,703 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,703 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_88
2022-10-24 15:18:19,705 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,711 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,711 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,711 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,712 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,713 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_94
2022-10-24 15:18:19,717 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_94
2022-10-24 15:18:19,718 - trainer - INFO - 
*****************[epoch: 94, global step: 95] eval training set at end of epoch***************
2022-10-24 15:18:19,718 - trainer - INFO - {
  "train_loss": 2341.777099609375
}
2022-10-24 15:18:19,719 - trainer - INFO - start training epoch 95
2022-10-24 15:18:19,719 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,721 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,721 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,728 - trainer - INFO - 
*****************[epoch: 95, global step: 96] eval training set at end of epoch***************
2022-10-24 15:18:19,729 - trainer - INFO - {
  "train_loss": 2299.009765625
}
2022-10-24 15:18:19,730 - trainer - INFO - start training epoch 96
2022-10-24 15:18:19,730 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,730 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,731 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,738 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval training set based on eval_every=2***************
2022-10-24 15:18:19,738 - trainer - INFO - {
  "train_loss": 2272.8507080078125
}
2022-10-24 15:18:19,745 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval development set based on eval_every=2***************
2022-10-24 15:18:19,745 - trainer - INFO - {
  "dev_loss": 2208.622314453125,
  "dev_best_score_for_loss": -2208.622314453125
}
2022-10-24 15:18:19,746 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,747 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,748 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,748 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_90
2022-10-24 15:18:19,749 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,756 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,757 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,757 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,758 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,759 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_96
2022-10-24 15:18:19,764 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_96
2022-10-24 15:18:19,765 - trainer - INFO - 
*****************[epoch: 96, global step: 97] eval training set at end of epoch***************
2022-10-24 15:18:19,765 - trainer - INFO - {
  "train_loss": 2246.691650390625
}
2022-10-24 15:18:19,765 - trainer - INFO - start training epoch 97
2022-10-24 15:18:19,767 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,767 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,768 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,777 - trainer - INFO - 
*****************[epoch: 97, global step: 98] eval training set at end of epoch***************
2022-10-24 15:18:19,778 - trainer - INFO - {
  "train_loss": 2208.622314453125
}
2022-10-24 15:18:19,778 - trainer - INFO - start training epoch 98
2022-10-24 15:18:19,778 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,779 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,779 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,786 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval training set based on eval_every=2***************
2022-10-24 15:18:19,787 - trainer - INFO - {
  "train_loss": 2191.8056640625
}
2022-10-24 15:18:19,793 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval development set based on eval_every=2***************
2022-10-24 15:18:19,794 - trainer - INFO - {
  "dev_loss": 2128.728271484375,
  "dev_best_score_for_loss": -2128.728271484375
}
2022-10-24 15:18:19,794 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,795 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,796 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,796 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_92
2022-10-24 15:18:19,797 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,801 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,801 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,801 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,802 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,802 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_98
2022-10-24 15:18:19,806 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_98
2022-10-24 15:18:19,806 - trainer - INFO - 
*****************[epoch: 98, global step: 99] eval training set at end of epoch***************
2022-10-24 15:18:19,807 - trainer - INFO - {
  "train_loss": 2174.989013671875
}
2022-10-24 15:18:19,807 - trainer - INFO - start training epoch 99
2022-10-24 15:18:19,807 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,807 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,808 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,818 - trainer - INFO - 
*****************[epoch: 99, global step: 100] eval training set at end of epoch***************
2022-10-24 15:18:19,818 - trainer - INFO - {
  "train_loss": 2128.728271484375
}
2022-10-24 15:18:19,819 - trainer - INFO - start training epoch 100
2022-10-24 15:18:19,819 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,819 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,820 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,828 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval training set based on eval_every=2***************
2022-10-24 15:18:19,829 - trainer - INFO - {
  "train_loss": 2105.5167236328125
}
2022-10-24 15:18:19,836 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval development set based on eval_every=2***************
2022-10-24 15:18:19,836 - trainer - INFO - {
  "dev_loss": 2046.5396728515625,
  "dev_best_score_for_loss": -2046.5396728515625
}
2022-10-24 15:18:19,837 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,838 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,838 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,838 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_94
2022-10-24 15:18:19,840 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,843 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,843 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,843 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,845 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,846 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_100
2022-10-24 15:18:19,850 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_100
2022-10-24 15:18:19,851 - trainer - INFO - 
*****************[epoch: 100, global step: 101] eval training set at end of epoch***************
2022-10-24 15:18:19,851 - trainer - INFO - {
  "train_loss": 2082.30517578125
}
2022-10-24 15:18:19,852 - trainer - INFO - start training epoch 101
2022-10-24 15:18:19,852 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,853 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,853 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,861 - trainer - INFO - 
*****************[epoch: 101, global step: 102] eval training set at end of epoch***************
2022-10-24 15:18:19,862 - trainer - INFO - {
  "train_loss": 2046.5394287109375
}
2022-10-24 15:18:19,862 - trainer - INFO - start training epoch 102
2022-10-24 15:18:19,862 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,863 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,863 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,870 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval training set based on eval_every=2***************
2022-10-24 15:18:19,870 - trainer - INFO - {
  "train_loss": 2027.24462890625
}
2022-10-24 15:18:19,876 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval development set based on eval_every=2***************
2022-10-24 15:18:19,877 - trainer - INFO - {
  "dev_loss": 1961.690185546875,
  "dev_best_score_for_loss": -1961.690185546875
}
2022-10-24 15:18:19,880 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,881 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,881 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,882 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_96
2022-10-24 15:18:19,883 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,887 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,887 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,887 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,888 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,888 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_102
2022-10-24 15:18:19,893 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_102
2022-10-24 15:18:19,894 - trainer - INFO - 
*****************[epoch: 102, global step: 103] eval training set at end of epoch***************
2022-10-24 15:18:19,895 - trainer - INFO - {
  "train_loss": 2007.9498291015625
}
2022-10-24 15:18:19,895 - trainer - INFO - start training epoch 103
2022-10-24 15:18:19,895 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,895 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,896 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,903 - trainer - INFO - 
*****************[epoch: 103, global step: 104] eval training set at end of epoch***************
2022-10-24 15:18:19,903 - trainer - INFO - {
  "train_loss": 1961.6900634765625
}
2022-10-24 15:18:19,904 - trainer - INFO - start training epoch 104
2022-10-24 15:18:19,904 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,904 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,905 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,914 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval training set based on eval_every=2***************
2022-10-24 15:18:19,914 - trainer - INFO - {
  "train_loss": 1941.1134643554688
}
2022-10-24 15:18:19,924 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval development set based on eval_every=2***************
2022-10-24 15:18:19,924 - trainer - INFO - {
  "dev_loss": 1884.2445068359375,
  "dev_best_score_for_loss": -1884.2445068359375
}
2022-10-24 15:18:19,925 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,927 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,927 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,928 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_98
2022-10-24 15:18:19,929 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,933 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,934 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,934 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,935 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,935 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_104
2022-10-24 15:18:19,942 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_104
2022-10-24 15:18:19,943 - trainer - INFO - 
*****************[epoch: 104, global step: 105] eval training set at end of epoch***************
2022-10-24 15:18:19,943 - trainer - INFO - {
  "train_loss": 1920.536865234375
}
2022-10-24 15:18:19,944 - trainer - INFO - start training epoch 105
2022-10-24 15:18:19,944 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,944 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,945 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,953 - trainer - INFO - 
*****************[epoch: 105, global step: 106] eval training set at end of epoch***************
2022-10-24 15:18:19,954 - trainer - INFO - {
  "train_loss": 1884.244384765625
}
2022-10-24 15:18:19,959 - trainer - INFO - start training epoch 106
2022-10-24 15:18:19,959 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,959 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,960 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:19,969 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval training set based on eval_every=2***************
2022-10-24 15:18:19,970 - trainer - INFO - {
  "train_loss": 1863.2898559570312
}
2022-10-24 15:18:19,976 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval development set based on eval_every=2***************
2022-10-24 15:18:19,977 - trainer - INFO - {
  "dev_loss": 1798.5552978515625,
  "dev_best_score_for_loss": -1798.5552978515625
}
2022-10-24 15:18:19,977 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:19,979 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:19,979 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:19,979 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_100
2022-10-24 15:18:19,981 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:19,984 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:19,985 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:19,985 - trainer - INFO -   patience: 200
2022-10-24 15:18:19,986 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:19,986 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_106
2022-10-24 15:18:19,990 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_106
2022-10-24 15:18:19,991 - trainer - INFO - 
*****************[epoch: 106, global step: 107] eval training set at end of epoch***************
2022-10-24 15:18:19,991 - trainer - INFO - {
  "train_loss": 1842.3353271484375
}
2022-10-24 15:18:19,992 - trainer - INFO - start training epoch 107
2022-10-24 15:18:19,992 - trainer - INFO - training using device=cuda
2022-10-24 15:18:19,992 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:19,992 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,005 - trainer - INFO - 
*****************[epoch: 107, global step: 108] eval training set at end of epoch***************
2022-10-24 15:18:20,006 - trainer - INFO - {
  "train_loss": 1798.5552978515625
}
2022-10-24 15:18:20,006 - trainer - INFO - start training epoch 108
2022-10-24 15:18:20,006 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,007 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,007 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,016 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval training set based on eval_every=2***************
2022-10-24 15:18:20,017 - trainer - INFO - {
  "train_loss": 1779.419921875
}
2022-10-24 15:18:20,024 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval development set based on eval_every=2***************
2022-10-24 15:18:20,024 - trainer - INFO - {
  "dev_loss": 1721.7479248046875,
  "dev_best_score_for_loss": -1721.7479248046875
}
2022-10-24 15:18:20,025 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,026 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,026 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,027 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_102
2022-10-24 15:18:20,028 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,032 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,034 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,035 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,036 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,036 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_108
2022-10-24 15:18:20,041 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_108
2022-10-24 15:18:20,042 - trainer - INFO - 
*****************[epoch: 108, global step: 109] eval training set at end of epoch***************
2022-10-24 15:18:20,042 - trainer - INFO - {
  "train_loss": 1760.2845458984375
}
2022-10-24 15:18:20,043 - trainer - INFO - start training epoch 109
2022-10-24 15:18:20,043 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,043 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,044 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,052 - trainer - INFO - 
*****************[epoch: 109, global step: 110] eval training set at end of epoch***************
2022-10-24 15:18:20,052 - trainer - INFO - {
  "train_loss": 1721.7479248046875
}
2022-10-24 15:18:20,053 - trainer - INFO - start training epoch 110
2022-10-24 15:18:20,053 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,053 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,054 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,060 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval training set based on eval_every=2***************
2022-10-24 15:18:20,061 - trainer - INFO - {
  "train_loss": 1700.418212890625
}
2022-10-24 15:18:20,069 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval development set based on eval_every=2***************
2022-10-24 15:18:20,069 - trainer - INFO - {
  "dev_loss": 1638.6722412109375,
  "dev_best_score_for_loss": -1638.6722412109375
}
2022-10-24 15:18:20,070 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,071 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,071 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,071 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_104
2022-10-24 15:18:20,072 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,076 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,077 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,078 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,079 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,082 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_110
2022-10-24 15:18:20,086 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_110
2022-10-24 15:18:20,087 - trainer - INFO - 
*****************[epoch: 110, global step: 111] eval training set at end of epoch***************
2022-10-24 15:18:20,088 - trainer - INFO - {
  "train_loss": 1679.0885009765625
}
2022-10-24 15:18:20,088 - trainer - INFO - start training epoch 111
2022-10-24 15:18:20,089 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,089 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,089 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,101 - trainer - INFO - 
*****************[epoch: 111, global step: 112] eval training set at end of epoch***************
2022-10-24 15:18:20,102 - trainer - INFO - {
  "train_loss": 1638.6722412109375
}
2022-10-24 15:18:20,102 - trainer - INFO - start training epoch 112
2022-10-24 15:18:20,102 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,102 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,103 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,111 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval training set based on eval_every=2***************
2022-10-24 15:18:20,112 - trainer - INFO - {
  "train_loss": 1619.8765258789062
}
2022-10-24 15:18:20,119 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval development set based on eval_every=2***************
2022-10-24 15:18:20,119 - trainer - INFO - {
  "dev_loss": 1560.7862548828125,
  "dev_best_score_for_loss": -1560.7862548828125
}
2022-10-24 15:18:20,120 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,122 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,122 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,122 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_106
2022-10-24 15:18:20,125 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,132 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,132 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,132 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,133 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,134 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_112
2022-10-24 15:18:20,140 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_112
2022-10-24 15:18:20,141 - trainer - INFO - 
*****************[epoch: 112, global step: 113] eval training set at end of epoch***************
2022-10-24 15:18:20,141 - trainer - INFO - {
  "train_loss": 1601.080810546875
}
2022-10-24 15:18:20,142 - trainer - INFO - start training epoch 113
2022-10-24 15:18:20,142 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,143 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,143 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,152 - trainer - INFO - 
*****************[epoch: 113, global step: 114] eval training set at end of epoch***************
2022-10-24 15:18:20,152 - trainer - INFO - {
  "train_loss": 1560.7861328125
}
2022-10-24 15:18:20,153 - trainer - INFO - start training epoch 114
2022-10-24 15:18:20,153 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,154 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,155 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,163 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval training set based on eval_every=2***************
2022-10-24 15:18:20,163 - trainer - INFO - {
  "train_loss": 1540.20556640625
}
2022-10-24 15:18:20,171 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval development set based on eval_every=2***************
2022-10-24 15:18:20,175 - trainer - INFO - {
  "dev_loss": 1481.4046630859375,
  "dev_best_score_for_loss": -1481.4046630859375
}
2022-10-24 15:18:20,176 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,178 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,178 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,178 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_108
2022-10-24 15:18:20,180 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,183 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,184 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,184 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,185 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,186 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_114
2022-10-24 15:18:20,191 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_114
2022-10-24 15:18:20,192 - trainer - INFO - 
*****************[epoch: 114, global step: 115] eval training set at end of epoch***************
2022-10-24 15:18:20,192 - trainer - INFO - {
  "train_loss": 1519.625
}
2022-10-24 15:18:20,193 - trainer - INFO - start training epoch 115
2022-10-24 15:18:20,193 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,193 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,193 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,202 - trainer - INFO - 
*****************[epoch: 115, global step: 116] eval training set at end of epoch***************
2022-10-24 15:18:20,203 - trainer - INFO - {
  "train_loss": 1481.4046630859375
}
2022-10-24 15:18:20,204 - trainer - INFO - start training epoch 116
2022-10-24 15:18:20,204 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,204 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,205 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,213 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval training set based on eval_every=2***************
2022-10-24 15:18:20,213 - trainer - INFO - {
  "train_loss": 1462.1795043945312
}
2022-10-24 15:18:20,224 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval development set based on eval_every=2***************
2022-10-24 15:18:20,225 - trainer - INFO - {
  "dev_loss": 1402.6663818359375,
  "dev_best_score_for_loss": -1402.6663818359375
}
2022-10-24 15:18:20,225 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,227 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,227 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,227 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_110
2022-10-24 15:18:20,229 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,233 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,233 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,233 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,234 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,234 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_116
2022-10-24 15:18:20,239 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_116
2022-10-24 15:18:20,239 - trainer - INFO - 
*****************[epoch: 116, global step: 117] eval training set at end of epoch***************
2022-10-24 15:18:20,240 - trainer - INFO - {
  "train_loss": 1442.954345703125
}
2022-10-24 15:18:20,240 - trainer - INFO - start training epoch 117
2022-10-24 15:18:20,240 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,241 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,241 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,249 - trainer - INFO - 
*****************[epoch: 117, global step: 118] eval training set at end of epoch***************
2022-10-24 15:18:20,249 - trainer - INFO - {
  "train_loss": 1402.6663818359375
}
2022-10-24 15:18:20,250 - trainer - INFO - start training epoch 118
2022-10-24 15:18:20,250 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,250 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,251 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,257 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval training set based on eval_every=2***************
2022-10-24 15:18:20,258 - trainer - INFO - {
  "train_loss": 1383.3214721679688
}
2022-10-24 15:18:20,269 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval development set based on eval_every=2***************
2022-10-24 15:18:20,269 - trainer - INFO - {
  "dev_loss": 1326.5423583984375,
  "dev_best_score_for_loss": -1326.5423583984375
}
2022-10-24 15:18:20,270 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,271 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,272 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,272 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_112
2022-10-24 15:18:20,274 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,279 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,280 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,280 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,281 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,281 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_118
2022-10-24 15:18:20,286 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_118
2022-10-24 15:18:20,286 - trainer - INFO - 
*****************[epoch: 118, global step: 119] eval training set at end of epoch***************
2022-10-24 15:18:20,287 - trainer - INFO - {
  "train_loss": 1363.9765625
}
2022-10-24 15:18:20,287 - trainer - INFO - start training epoch 119
2022-10-24 15:18:20,288 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,288 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,289 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,299 - trainer - INFO - 
*****************[epoch: 119, global step: 120] eval training set at end of epoch***************
2022-10-24 15:18:20,300 - trainer - INFO - {
  "train_loss": 1326.5423583984375
}
2022-10-24 15:18:20,300 - trainer - INFO - start training epoch 120
2022-10-24 15:18:20,301 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,301 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,301 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,310 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval training set based on eval_every=2***************
2022-10-24 15:18:20,314 - trainer - INFO - {
  "train_loss": 1307.10400390625
}
2022-10-24 15:18:20,321 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval development set based on eval_every=2***************
2022-10-24 15:18:20,321 - trainer - INFO - {
  "dev_loss": 1249.0020751953125,
  "dev_best_score_for_loss": -1249.0020751953125
}
2022-10-24 15:18:20,322 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,323 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,324 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,325 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_114
2022-10-24 15:18:20,327 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,331 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,331 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,331 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,332 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,332 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_120
2022-10-24 15:18:20,336 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_120
2022-10-24 15:18:20,337 - trainer - INFO - 
*****************[epoch: 120, global step: 121] eval training set at end of epoch***************
2022-10-24 15:18:20,337 - trainer - INFO - {
  "train_loss": 1287.6656494140625
}
2022-10-24 15:18:20,338 - trainer - INFO - start training epoch 121
2022-10-24 15:18:20,338 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,338 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,339 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,348 - trainer - INFO - 
*****************[epoch: 121, global step: 122] eval training set at end of epoch***************
2022-10-24 15:18:20,348 - trainer - INFO - {
  "train_loss": 1249.001953125
}
2022-10-24 15:18:20,348 - trainer - INFO - start training epoch 122
2022-10-24 15:18:20,348 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,349 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,349 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,358 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval training set based on eval_every=2***************
2022-10-24 15:18:20,359 - trainer - INFO - {
  "train_loss": 1230.484375
}
2022-10-24 15:18:20,369 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval development set based on eval_every=2***************
2022-10-24 15:18:20,370 - trainer - INFO - {
  "dev_loss": 1174.515625,
  "dev_best_score_for_loss": -1174.515625
}
2022-10-24 15:18:20,371 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,372 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,373 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,373 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_116
2022-10-24 15:18:20,375 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,379 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,379 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,380 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,381 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,381 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_122
2022-10-24 15:18:20,385 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_122
2022-10-24 15:18:20,387 - trainer - INFO - 
*****************[epoch: 122, global step: 123] eval training set at end of epoch***************
2022-10-24 15:18:20,387 - trainer - INFO - {
  "train_loss": 1211.966796875
}
2022-10-24 15:18:20,387 - trainer - INFO - start training epoch 123
2022-10-24 15:18:20,387 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,388 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,388 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,397 - trainer - INFO - 
*****************[epoch: 123, global step: 124] eval training set at end of epoch***************
2022-10-24 15:18:20,397 - trainer - INFO - {
  "train_loss": 1174.5157470703125
}
2022-10-24 15:18:20,397 - trainer - INFO - start training epoch 124
2022-10-24 15:18:20,398 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,398 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,398 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,407 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval training set based on eval_every=2***************
2022-10-24 15:18:20,408 - trainer - INFO - {
  "train_loss": 1155.5955200195312
}
2022-10-24 15:18:20,414 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval development set based on eval_every=2***************
2022-10-24 15:18:20,414 - trainer - INFO - {
  "dev_loss": 1100.170166015625,
  "dev_best_score_for_loss": -1100.170166015625
}
2022-10-24 15:18:20,415 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,416 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,417 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,418 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_118
2022-10-24 15:18:20,419 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,423 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,423 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,423 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,424 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,424 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_124
2022-10-24 15:18:20,428 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_124
2022-10-24 15:18:20,429 - trainer - INFO - 
*****************[epoch: 124, global step: 125] eval training set at end of epoch***************
2022-10-24 15:18:20,429 - trainer - INFO - {
  "train_loss": 1136.67529296875
}
2022-10-24 15:18:20,429 - trainer - INFO - start training epoch 125
2022-10-24 15:18:20,429 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,430 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,430 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,437 - trainer - INFO - 
*****************[epoch: 125, global step: 126] eval training set at end of epoch***************
2022-10-24 15:18:20,437 - trainer - INFO - {
  "train_loss": 1100.170166015625
}
2022-10-24 15:18:20,437 - trainer - INFO - start training epoch 126
2022-10-24 15:18:20,437 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,438 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,438 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,444 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval training set based on eval_every=2***************
2022-10-24 15:18:20,444 - trainer - INFO - {
  "train_loss": 1082.0811157226562
}
2022-10-24 15:18:20,454 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval development set based on eval_every=2***************
2022-10-24 15:18:20,454 - trainer - INFO - {
  "dev_loss": 1027.29931640625,
  "dev_best_score_for_loss": -1027.29931640625
}
2022-10-24 15:18:20,455 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,456 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,457 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,457 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_120
2022-10-24 15:18:20,458 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,462 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,462 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,462 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,463 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,464 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_126
2022-10-24 15:18:20,468 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_126
2022-10-24 15:18:20,469 - trainer - INFO - 
*****************[epoch: 126, global step: 127] eval training set at end of epoch***************
2022-10-24 15:18:20,469 - trainer - INFO - {
  "train_loss": 1063.9920654296875
}
2022-10-24 15:18:20,470 - trainer - INFO - start training epoch 127
2022-10-24 15:18:20,470 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,471 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,471 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,478 - trainer - INFO - 
*****************[epoch: 127, global step: 128] eval training set at end of epoch***************
2022-10-24 15:18:20,478 - trainer - INFO - {
  "train_loss": 1027.2994384765625
}
2022-10-24 15:18:20,478 - trainer - INFO - start training epoch 128
2022-10-24 15:18:20,479 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,480 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,480 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,486 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval training set based on eval_every=2***************
2022-10-24 15:18:20,486 - trainer - INFO - {
  "train_loss": 1009.37548828125
}
2022-10-24 15:18:20,492 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval development set based on eval_every=2***************
2022-10-24 15:18:20,492 - trainer - INFO - {
  "dev_loss": 956.3876342773438,
  "dev_best_score_for_loss": -956.3876342773438
}
2022-10-24 15:18:20,493 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,494 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,495 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,495 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_122
2022-10-24 15:18:20,498 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,502 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,503 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,503 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,504 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,504 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_128
2022-10-24 15:18:20,508 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_128
2022-10-24 15:18:20,509 - trainer - INFO - 
*****************[epoch: 128, global step: 129] eval training set at end of epoch***************
2022-10-24 15:18:20,512 - trainer - INFO - {
  "train_loss": 991.4515380859375
}
2022-10-24 15:18:20,512 - trainer - INFO - start training epoch 129
2022-10-24 15:18:20,512 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,513 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,513 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,521 - trainer - INFO - 
*****************[epoch: 129, global step: 130] eval training set at end of epoch***************
2022-10-24 15:18:20,521 - trainer - INFO - {
  "train_loss": 956.3876342773438
}
2022-10-24 15:18:20,521 - trainer - INFO - start training epoch 130
2022-10-24 15:18:20,521 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,522 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,522 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,530 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval training set based on eval_every=2***************
2022-10-24 15:18:20,530 - trainer - INFO - {
  "train_loss": 938.7485961914062
}
2022-10-24 15:18:20,535 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval development set based on eval_every=2***************
2022-10-24 15:18:20,536 - trainer - INFO - {
  "dev_loss": 886.3834838867188,
  "dev_best_score_for_loss": -886.3834838867188
}
2022-10-24 15:18:20,536 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,537 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,538 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,538 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_124
2022-10-24 15:18:20,539 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,542 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,544 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,544 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,547 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,547 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_130
2022-10-24 15:18:20,552 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_130
2022-10-24 15:18:20,553 - trainer - INFO - 
*****************[epoch: 130, global step: 131] eval training set at end of epoch***************
2022-10-24 15:18:20,553 - trainer - INFO - {
  "train_loss": 921.1095581054688
}
2022-10-24 15:18:20,554 - trainer - INFO - start training epoch 131
2022-10-24 15:18:20,554 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,554 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,555 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,566 - trainer - INFO - 
*****************[epoch: 131, global step: 132] eval training set at end of epoch***************
2022-10-24 15:18:20,566 - trainer - INFO - {
  "train_loss": 886.383544921875
}
2022-10-24 15:18:20,567 - trainer - INFO - start training epoch 132
2022-10-24 15:18:20,567 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,567 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,568 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,575 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval training set based on eval_every=2***************
2022-10-24 15:18:20,576 - trainer - INFO - {
  "train_loss": 869.4779052734375
}
2022-10-24 15:18:20,582 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval development set based on eval_every=2***************
2022-10-24 15:18:20,582 - trainer - INFO - {
  "dev_loss": 818.8114624023438,
  "dev_best_score_for_loss": -818.8114624023438
}
2022-10-24 15:18:20,583 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,584 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,584 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,585 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_126
2022-10-24 15:18:20,586 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,590 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,594 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,594 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,595 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,595 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_132
2022-10-24 15:18:20,601 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_132
2022-10-24 15:18:20,602 - trainer - INFO - 
*****************[epoch: 132, global step: 133] eval training set at end of epoch***************
2022-10-24 15:18:20,603 - trainer - INFO - {
  "train_loss": 852.572265625
}
2022-10-24 15:18:20,605 - trainer - INFO - start training epoch 133
2022-10-24 15:18:20,605 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,605 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,606 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,615 - trainer - INFO - 
*****************[epoch: 133, global step: 134] eval training set at end of epoch***************
2022-10-24 15:18:20,616 - trainer - INFO - {
  "train_loss": 818.8114624023438
}
2022-10-24 15:18:20,616 - trainer - INFO - start training epoch 134
2022-10-24 15:18:20,616 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,616 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,617 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,627 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval training set based on eval_every=2***************
2022-10-24 15:18:20,627 - trainer - INFO - {
  "train_loss": 802.1068725585938
}
2022-10-24 15:18:20,634 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval development set based on eval_every=2***************
2022-10-24 15:18:20,635 - trainer - INFO - {
  "dev_loss": 752.909912109375,
  "dev_best_score_for_loss": -752.909912109375
}
2022-10-24 15:18:20,640 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,642 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,642 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,642 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_128
2022-10-24 15:18:20,644 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,648 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,648 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,648 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,649 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,650 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_134
2022-10-24 15:18:20,655 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_134
2022-10-24 15:18:20,656 - trainer - INFO - 
*****************[epoch: 134, global step: 135] eval training set at end of epoch***************
2022-10-24 15:18:20,656 - trainer - INFO - {
  "train_loss": 785.4022827148438
}
2022-10-24 15:18:20,656 - trainer - INFO - start training epoch 135
2022-10-24 15:18:20,656 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,657 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,657 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,666 - trainer - INFO - 
*****************[epoch: 135, global step: 136] eval training set at end of epoch***************
2022-10-24 15:18:20,667 - trainer - INFO - {
  "train_loss": 752.909912109375
}
2022-10-24 15:18:20,667 - trainer - INFO - start training epoch 136
2022-10-24 15:18:20,667 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,668 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,668 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,675 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval training set based on eval_every=2***************
2022-10-24 15:18:20,676 - trainer - INFO - {
  "train_loss": 736.8434448242188
}
2022-10-24 15:18:20,682 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval development set based on eval_every=2***************
2022-10-24 15:18:20,683 - trainer - INFO - {
  "dev_loss": 689.020263671875,
  "dev_best_score_for_loss": -689.020263671875
}
2022-10-24 15:18:20,684 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,687 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,688 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,688 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_130
2022-10-24 15:18:20,689 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,693 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,693 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,694 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,695 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,695 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_136
2022-10-24 15:18:20,700 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_136
2022-10-24 15:18:20,701 - trainer - INFO - 
*****************[epoch: 136, global step: 137] eval training set at end of epoch***************
2022-10-24 15:18:20,702 - trainer - INFO - {
  "train_loss": 720.7769775390625
}
2022-10-24 15:18:20,702 - trainer - INFO - start training epoch 137
2022-10-24 15:18:20,702 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,702 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,703 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,710 - trainer - INFO - 
*****************[epoch: 137, global step: 138] eval training set at end of epoch***************
2022-10-24 15:18:20,711 - trainer - INFO - {
  "train_loss": 689.0203247070312
}
2022-10-24 15:18:20,712 - trainer - INFO - start training epoch 138
2022-10-24 15:18:20,712 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,713 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,713 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,719 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval training set based on eval_every=2***************
2022-10-24 15:18:20,719 - trainer - INFO - {
  "train_loss": 673.5842895507812
}
2022-10-24 15:18:20,725 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval development set based on eval_every=2***************
2022-10-24 15:18:20,725 - trainer - INFO - {
  "dev_loss": 627.7950439453125,
  "dev_best_score_for_loss": -627.7950439453125
}
2022-10-24 15:18:20,726 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,727 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,728 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,729 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_132
2022-10-24 15:18:20,735 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,738 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,739 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,739 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,740 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,740 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_138
2022-10-24 15:18:20,745 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_138
2022-10-24 15:18:20,746 - trainer - INFO - 
*****************[epoch: 138, global step: 139] eval training set at end of epoch***************
2022-10-24 15:18:20,746 - trainer - INFO - {
  "train_loss": 658.1482543945312
}
2022-10-24 15:18:20,747 - trainer - INFO - start training epoch 139
2022-10-24 15:18:20,747 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,747 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,747 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,754 - trainer - INFO - 
*****************[epoch: 139, global step: 140] eval training set at end of epoch***************
2022-10-24 15:18:20,754 - trainer - INFO - {
  "train_loss": 627.7950439453125
}
2022-10-24 15:18:20,755 - trainer - INFO - start training epoch 140
2022-10-24 15:18:20,755 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,755 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,755 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,764 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval training set based on eval_every=2***************
2022-10-24 15:18:20,765 - trainer - INFO - {
  "train_loss": 612.8165588378906
}
2022-10-24 15:18:20,770 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval development set based on eval_every=2***************
2022-10-24 15:18:20,770 - trainer - INFO - {
  "dev_loss": 568.720458984375,
  "dev_best_score_for_loss": -568.720458984375
}
2022-10-24 15:18:20,771 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,772 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,772 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,772 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_134
2022-10-24 15:18:20,774 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,778 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,778 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,778 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,779 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,779 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_140
2022-10-24 15:18:20,784 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_140
2022-10-24 15:18:20,784 - trainer - INFO - 
*****************[epoch: 140, global step: 141] eval training set at end of epoch***************
2022-10-24 15:18:20,785 - trainer - INFO - {
  "train_loss": 597.8380737304688
}
2022-10-24 15:18:20,785 - trainer - INFO - start training epoch 141
2022-10-24 15:18:20,785 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,786 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,786 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,795 - trainer - INFO - 
*****************[epoch: 141, global step: 142] eval training set at end of epoch***************
2022-10-24 15:18:20,796 - trainer - INFO - {
  "train_loss": 568.720458984375
}
2022-10-24 15:18:20,796 - trainer - INFO - start training epoch 142
2022-10-24 15:18:20,796 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,796 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,797 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,803 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval training set based on eval_every=2***************
2022-10-24 15:18:20,804 - trainer - INFO - {
  "train_loss": 554.4976806640625
}
2022-10-24 15:18:20,813 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval development set based on eval_every=2***************
2022-10-24 15:18:20,814 - trainer - INFO - {
  "dev_loss": 512.83544921875,
  "dev_best_score_for_loss": -512.83544921875
}
2022-10-24 15:18:20,815 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,817 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,817 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,817 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_136
2022-10-24 15:18:20,819 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,823 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,823 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,823 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,824 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,824 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_142
2022-10-24 15:18:20,829 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_142
2022-10-24 15:18:20,830 - trainer - INFO - 
*****************[epoch: 142, global step: 143] eval training set at end of epoch***************
2022-10-24 15:18:20,830 - trainer - INFO - {
  "train_loss": 540.27490234375
}
2022-10-24 15:18:20,831 - trainer - INFO - start training epoch 143
2022-10-24 15:18:20,831 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,831 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,831 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,838 - trainer - INFO - 
*****************[epoch: 143, global step: 144] eval training set at end of epoch***************
2022-10-24 15:18:20,839 - trainer - INFO - {
  "train_loss": 512.83544921875
}
2022-10-24 15:18:20,839 - trainer - INFO - start training epoch 144
2022-10-24 15:18:20,839 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,839 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,840 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,847 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval training set based on eval_every=2***************
2022-10-24 15:18:20,847 - trainer - INFO - {
  "train_loss": 499.83876037597656
}
2022-10-24 15:18:20,855 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval development set based on eval_every=2***************
2022-10-24 15:18:20,856 - trainer - INFO - {
  "dev_loss": 460.9114685058594,
  "dev_best_score_for_loss": -460.9114685058594
}
2022-10-24 15:18:20,856 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,858 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,858 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,858 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_138
2022-10-24 15:18:20,860 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,863 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,864 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,864 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,865 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,865 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_144
2022-10-24 15:18:20,871 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_144
2022-10-24 15:18:20,871 - trainer - INFO - 
*****************[epoch: 144, global step: 145] eval training set at end of epoch***************
2022-10-24 15:18:20,872 - trainer - INFO - {
  "train_loss": 486.8420715332031
}
2022-10-24 15:18:20,872 - trainer - INFO - start training epoch 145
2022-10-24 15:18:20,872 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,872 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,873 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,880 - trainer - INFO - 
*****************[epoch: 145, global step: 146] eval training set at end of epoch***************
2022-10-24 15:18:20,880 - trainer - INFO - {
  "train_loss": 460.9114990234375
}
2022-10-24 15:18:20,881 - trainer - INFO - start training epoch 146
2022-10-24 15:18:20,881 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,881 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,881 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,890 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval training set based on eval_every=2***************
2022-10-24 15:18:20,890 - trainer - INFO - {
  "train_loss": 448.02976989746094
}
2022-10-24 15:18:20,899 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval development set based on eval_every=2***************
2022-10-24 15:18:20,899 - trainer - INFO - {
  "dev_loss": 409.79156494140625,
  "dev_best_score_for_loss": -409.79156494140625
}
2022-10-24 15:18:20,900 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,901 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,902 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,902 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_140
2022-10-24 15:18:20,903 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,906 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,906 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,907 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,907 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,908 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_146
2022-10-24 15:18:20,912 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_146
2022-10-24 15:18:20,914 - trainer - INFO - 
*****************[epoch: 146, global step: 147] eval training set at end of epoch***************
2022-10-24 15:18:20,914 - trainer - INFO - {
  "train_loss": 435.1480407714844
}
2022-10-24 15:18:20,914 - trainer - INFO - start training epoch 147
2022-10-24 15:18:20,915 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,915 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,915 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,923 - trainer - INFO - 
*****************[epoch: 147, global step: 148] eval training set at end of epoch***************
2022-10-24 15:18:20,923 - trainer - INFO - {
  "train_loss": 409.79150390625
}
2022-10-24 15:18:20,923 - trainer - INFO - start training epoch 148
2022-10-24 15:18:20,924 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,924 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,924 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,935 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval training set based on eval_every=2***************
2022-10-24 15:18:20,935 - trainer - INFO - {
  "train_loss": 397.9364776611328
}
2022-10-24 15:18:20,942 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval development set based on eval_every=2***************
2022-10-24 15:18:20,942 - trainer - INFO - {
  "dev_loss": 363.0979309082031,
  "dev_best_score_for_loss": -363.0979309082031
}
2022-10-24 15:18:20,943 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:20,945 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:20,946 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:20,946 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_142
2022-10-24 15:18:20,948 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:20,953 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:20,953 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:20,954 - trainer - INFO -   patience: 200
2022-10-24 15:18:20,956 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:20,956 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_148
2022-10-24 15:18:20,963 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_148
2022-10-24 15:18:20,964 - trainer - INFO - 
*****************[epoch: 148, global step: 149] eval training set at end of epoch***************
2022-10-24 15:18:20,965 - trainer - INFO - {
  "train_loss": 386.0814514160156
}
2022-10-24 15:18:20,966 - trainer - INFO - start training epoch 149
2022-10-24 15:18:20,966 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,966 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,966 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,979 - trainer - INFO - 
*****************[epoch: 149, global step: 150] eval training set at end of epoch***************
2022-10-24 15:18:20,982 - trainer - INFO - {
  "train_loss": 363.0979309082031
}
2022-10-24 15:18:20,983 - trainer - INFO - start training epoch 150
2022-10-24 15:18:20,983 - trainer - INFO - training using device=cuda
2022-10-24 15:18:20,983 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:20,984 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:20,995 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval training set based on eval_every=2***************
2022-10-24 15:18:20,995 - trainer - INFO - {
  "train_loss": 352.0122833251953
}
2022-10-24 15:18:21,002 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval development set based on eval_every=2***************
2022-10-24 15:18:21,003 - trainer - INFO - {
  "dev_loss": 319.4105224609375,
  "dev_best_score_for_loss": -319.4105224609375
}
2022-10-24 15:18:21,003 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,005 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,005 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,005 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_144
2022-10-24 15:18:21,007 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,011 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,011 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,011 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,012 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,012 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_150
2022-10-24 15:18:21,016 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_150
2022-10-24 15:18:21,017 - trainer - INFO - 
*****************[epoch: 150, global step: 151] eval training set at end of epoch***************
2022-10-24 15:18:21,017 - trainer - INFO - {
  "train_loss": 340.9266357421875
}
2022-10-24 15:18:21,018 - trainer - INFO - start training epoch 151
2022-10-24 15:18:21,018 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,018 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,018 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,027 - trainer - INFO - 
*****************[epoch: 151, global step: 152] eval training set at end of epoch***************
2022-10-24 15:18:21,027 - trainer - INFO - {
  "train_loss": 319.4105224609375
}
2022-10-24 15:18:21,027 - trainer - INFO - start training epoch 152
2022-10-24 15:18:21,028 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,028 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,028 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,035 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval training set based on eval_every=2***************
2022-10-24 15:18:21,036 - trainer - INFO - {
  "train_loss": 309.03123474121094
}
2022-10-24 15:18:21,042 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval development set based on eval_every=2***************
2022-10-24 15:18:21,043 - trainer - INFO - {
  "dev_loss": 278.7242431640625,
  "dev_best_score_for_loss": -278.7242431640625
}
2022-10-24 15:18:21,043 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,044 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,044 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,045 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_146
2022-10-24 15:18:21,046 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,049 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,049 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,049 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,050 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,050 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_152
2022-10-24 15:18:21,054 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_152
2022-10-24 15:18:21,055 - trainer - INFO - 
*****************[epoch: 152, global step: 153] eval training set at end of epoch***************
2022-10-24 15:18:21,058 - trainer - INFO - {
  "train_loss": 298.6519470214844
}
2022-10-24 15:18:21,059 - trainer - INFO - start training epoch 153
2022-10-24 15:18:21,059 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,059 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,060 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,069 - trainer - INFO - 
*****************[epoch: 153, global step: 154] eval training set at end of epoch***************
2022-10-24 15:18:21,069 - trainer - INFO - {
  "train_loss": 278.7242431640625
}
2022-10-24 15:18:21,070 - trainer - INFO - start training epoch 154
2022-10-24 15:18:21,070 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,071 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,071 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,077 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval training set based on eval_every=2***************
2022-10-24 15:18:21,078 - trainer - INFO - {
  "train_loss": 269.1228942871094
}
2022-10-24 15:18:21,083 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval development set based on eval_every=2***************
2022-10-24 15:18:21,084 - trainer - INFO - {
  "dev_loss": 241.12069702148438,
  "dev_best_score_for_loss": -241.12069702148438
}
2022-10-24 15:18:21,085 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,086 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,086 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,086 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_148
2022-10-24 15:18:21,087 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,090 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,090 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,091 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,092 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,092 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_154
2022-10-24 15:18:21,097 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_154
2022-10-24 15:18:21,098 - trainer - INFO - 
*****************[epoch: 154, global step: 155] eval training set at end of epoch***************
2022-10-24 15:18:21,098 - trainer - INFO - {
  "train_loss": 259.52154541015625
}
2022-10-24 15:18:21,098 - trainer - INFO - start training epoch 155
2022-10-24 15:18:21,099 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,100 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,102 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,116 - trainer - INFO - 
*****************[epoch: 155, global step: 156] eval training set at end of epoch***************
2022-10-24 15:18:21,117 - trainer - INFO - {
  "train_loss": 241.12069702148438
}
2022-10-24 15:18:21,117 - trainer - INFO - start training epoch 156
2022-10-24 15:18:21,118 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,118 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,119 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,127 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval training set based on eval_every=2***************
2022-10-24 15:18:21,128 - trainer - INFO - {
  "train_loss": 232.33521270751953
}
2022-10-24 15:18:21,134 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval development set based on eval_every=2***************
2022-10-24 15:18:21,135 - trainer - INFO - {
  "dev_loss": 206.94900512695312,
  "dev_best_score_for_loss": -206.94900512695312
}
2022-10-24 15:18:21,135 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,136 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,137 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,137 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_150
2022-10-24 15:18:21,138 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,141 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,142 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,142 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,143 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,143 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_156
2022-10-24 15:18:21,147 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_156
2022-10-24 15:18:21,150 - trainer - INFO - 
*****************[epoch: 156, global step: 157] eval training set at end of epoch***************
2022-10-24 15:18:21,150 - trainer - INFO - {
  "train_loss": 223.5497283935547
}
2022-10-24 15:18:21,151 - trainer - INFO - start training epoch 157
2022-10-24 15:18:21,151 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,151 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,152 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,160 - trainer - INFO - 
*****************[epoch: 157, global step: 158] eval training set at end of epoch***************
2022-10-24 15:18:21,160 - trainer - INFO - {
  "train_loss": 206.94900512695312
}
2022-10-24 15:18:21,161 - trainer - INFO - start training epoch 158
2022-10-24 15:18:21,162 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,163 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,163 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,171 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval training set based on eval_every=2***************
2022-10-24 15:18:21,171 - trainer - INFO - {
  "train_loss": 198.9933624267578
}
2022-10-24 15:18:21,179 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval development set based on eval_every=2***************
2022-10-24 15:18:21,180 - trainer - INFO - {
  "dev_loss": 175.8551788330078,
  "dev_best_score_for_loss": -175.8551788330078
}
2022-10-24 15:18:21,183 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,184 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,184 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,185 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_152
2022-10-24 15:18:21,186 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,190 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,190 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,190 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,192 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,193 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_158
2022-10-24 15:18:21,198 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_158
2022-10-24 15:18:21,198 - trainer - INFO - 
*****************[epoch: 158, global step: 159] eval training set at end of epoch***************
2022-10-24 15:18:21,199 - trainer - INFO - {
  "train_loss": 191.0377197265625
}
2022-10-24 15:18:21,199 - trainer - INFO - start training epoch 159
2022-10-24 15:18:21,200 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,200 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,200 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,207 - trainer - INFO - 
*****************[epoch: 159, global step: 160] eval training set at end of epoch***************
2022-10-24 15:18:21,207 - trainer - INFO - {
  "train_loss": 175.8551788330078
}
2022-10-24 15:18:21,208 - trainer - INFO - start training epoch 160
2022-10-24 15:18:21,209 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,209 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,209 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,215 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval training set based on eval_every=2***************
2022-10-24 15:18:21,215 - trainer - INFO - {
  "train_loss": 168.6692886352539
}
2022-10-24 15:18:21,221 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval development set based on eval_every=2***************
2022-10-24 15:18:21,221 - trainer - INFO - {
  "dev_loss": 147.98269653320312,
  "dev_best_score_for_loss": -147.98269653320312
}
2022-10-24 15:18:21,222 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,223 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,223 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,224 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_154
2022-10-24 15:18:21,227 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,233 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,234 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,234 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,235 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,235 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_160
2022-10-24 15:18:21,240 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_160
2022-10-24 15:18:21,241 - trainer - INFO - 
*****************[epoch: 160, global step: 161] eval training set at end of epoch***************
2022-10-24 15:18:21,241 - trainer - INFO - {
  "train_loss": 161.4833984375
}
2022-10-24 15:18:21,242 - trainer - INFO - start training epoch 161
2022-10-24 15:18:21,242 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,242 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,243 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,252 - trainer - INFO - 
*****************[epoch: 161, global step: 162] eval training set at end of epoch***************
2022-10-24 15:18:21,252 - trainer - INFO - {
  "train_loss": 147.98268127441406
}
2022-10-24 15:18:21,253 - trainer - INFO - start training epoch 162
2022-10-24 15:18:21,253 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,253 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,253 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,264 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval training set based on eval_every=2***************
2022-10-24 15:18:21,264 - trainer - INFO - {
  "train_loss": 141.6208724975586
}
2022-10-24 15:18:21,272 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval development set based on eval_every=2***************
2022-10-24 15:18:21,275 - trainer - INFO - {
  "dev_loss": 123.13914489746094,
  "dev_best_score_for_loss": -123.13914489746094
}
2022-10-24 15:18:21,276 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,278 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,278 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,279 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_156
2022-10-24 15:18:21,280 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,285 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,286 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,286 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,287 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,287 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_162
2022-10-24 15:18:21,292 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_162
2022-10-24 15:18:21,293 - trainer - INFO - 
*****************[epoch: 162, global step: 163] eval training set at end of epoch***************
2022-10-24 15:18:21,293 - trainer - INFO - {
  "train_loss": 135.25906372070312
}
2022-10-24 15:18:21,294 - trainer - INFO - start training epoch 163
2022-10-24 15:18:21,294 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,294 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,295 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,304 - trainer - INFO - 
*****************[epoch: 163, global step: 164] eval training set at end of epoch***************
2022-10-24 15:18:21,304 - trainer - INFO - {
  "train_loss": 123.13914489746094
}
2022-10-24 15:18:21,305 - trainer - INFO - start training epoch 164
2022-10-24 15:18:21,305 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,305 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,306 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,313 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval training set based on eval_every=2***************
2022-10-24 15:18:21,314 - trainer - INFO - {
  "train_loss": 117.49711990356445
}
2022-10-24 15:18:21,323 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval development set based on eval_every=2***************
2022-10-24 15:18:21,324 - trainer - INFO - {
  "dev_loss": 101.465576171875,
  "dev_best_score_for_loss": -101.465576171875
}
2022-10-24 15:18:21,325 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,326 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,327 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,327 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_158
2022-10-24 15:18:21,329 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,334 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,334 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,334 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,335 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,336 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_164
2022-10-24 15:18:21,340 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_164
2022-10-24 15:18:21,341 - trainer - INFO - 
*****************[epoch: 164, global step: 165] eval training set at end of epoch***************
2022-10-24 15:18:21,342 - trainer - INFO - {
  "train_loss": 111.85509490966797
}
2022-10-24 15:18:21,343 - trainer - INFO - start training epoch 165
2022-10-24 15:18:21,343 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,343 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,344 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,353 - trainer - INFO - 
*****************[epoch: 165, global step: 166] eval training set at end of epoch***************
2022-10-24 15:18:21,354 - trainer - INFO - {
  "train_loss": 101.465576171875
}
2022-10-24 15:18:21,354 - trainer - INFO - start training epoch 166
2022-10-24 15:18:21,354 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,355 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,355 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,362 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval training set based on eval_every=2***************
2022-10-24 15:18:21,364 - trainer - INFO - {
  "train_loss": 96.51379013061523
}
2022-10-24 15:18:21,378 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval development set based on eval_every=2***************
2022-10-24 15:18:21,380 - trainer - INFO - {
  "dev_loss": 82.28784942626953,
  "dev_best_score_for_loss": -82.28784942626953
}
2022-10-24 15:18:21,381 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,384 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,385 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,385 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_160
2022-10-24 15:18:21,387 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,391 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,391 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,392 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,392 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,393 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_166
2022-10-24 15:18:21,399 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_166
2022-10-24 15:18:21,400 - trainer - INFO - 
*****************[epoch: 166, global step: 167] eval training set at end of epoch***************
2022-10-24 15:18:21,401 - trainer - INFO - {
  "train_loss": 91.56200408935547
}
2022-10-24 15:18:21,401 - trainer - INFO - start training epoch 167
2022-10-24 15:18:21,401 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,401 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,402 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,413 - trainer - INFO - 
*****************[epoch: 167, global step: 168] eval training set at end of epoch***************
2022-10-24 15:18:21,414 - trainer - INFO - {
  "train_loss": 82.287841796875
}
2022-10-24 15:18:21,414 - trainer - INFO - start training epoch 168
2022-10-24 15:18:21,414 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,418 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,419 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,430 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval training set based on eval_every=2***************
2022-10-24 15:18:21,430 - trainer - INFO - {
  "train_loss": 78.06327819824219
}
2022-10-24 15:18:21,437 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval development set based on eval_every=2***************
2022-10-24 15:18:21,438 - trainer - INFO - {
  "dev_loss": 65.98175048828125,
  "dev_best_score_for_loss": -65.98175048828125
}
2022-10-24 15:18:21,438 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,440 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,441 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,442 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_162
2022-10-24 15:18:21,443 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,447 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,447 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,447 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,448 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,448 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_168
2022-10-24 15:18:21,453 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_168
2022-10-24 15:18:21,454 - trainer - INFO - 
*****************[epoch: 168, global step: 169] eval training set at end of epoch***************
2022-10-24 15:18:21,454 - trainer - INFO - {
  "train_loss": 73.83871459960938
}
2022-10-24 15:18:21,454 - trainer - INFO - start training epoch 169
2022-10-24 15:18:21,455 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,455 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,456 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,472 - trainer - INFO - 
*****************[epoch: 169, global step: 170] eval training set at end of epoch***************
2022-10-24 15:18:21,472 - trainer - INFO - {
  "train_loss": 65.98175048828125
}
2022-10-24 15:18:21,473 - trainer - INFO - start training epoch 170
2022-10-24 15:18:21,473 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,473 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,474 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,481 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval training set based on eval_every=2***************
2022-10-24 15:18:21,481 - trainer - INFO - {
  "train_loss": 62.3620662689209
}
2022-10-24 15:18:21,488 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval development set based on eval_every=2***************
2022-10-24 15:18:21,489 - trainer - INFO - {
  "dev_loss": 52.088680267333984,
  "dev_best_score_for_loss": -52.088680267333984
}
2022-10-24 15:18:21,490 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,491 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,491 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,492 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_164
2022-10-24 15:18:21,493 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,497 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,497 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,497 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,498 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,498 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_170
2022-10-24 15:18:21,502 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_170
2022-10-24 15:18:21,503 - trainer - INFO - 
*****************[epoch: 170, global step: 171] eval training set at end of epoch***************
2022-10-24 15:18:21,504 - trainer - INFO - {
  "train_loss": 58.74238204956055
}
2022-10-24 15:18:21,505 - trainer - INFO - start training epoch 171
2022-10-24 15:18:21,508 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,508 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,509 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,516 - trainer - INFO - 
*****************[epoch: 171, global step: 172] eval training set at end of epoch***************
2022-10-24 15:18:21,517 - trainer - INFO - {
  "train_loss": 52.088680267333984
}
2022-10-24 15:18:21,517 - trainer - INFO - start training epoch 172
2022-10-24 15:18:21,517 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,518 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,519 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,525 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval training set based on eval_every=2***************
2022-10-24 15:18:21,525 - trainer - INFO - {
  "train_loss": 49.03203201293945
}
2022-10-24 15:18:21,531 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval development set based on eval_every=2***************
2022-10-24 15:18:21,532 - trainer - INFO - {
  "dev_loss": 40.41236114501953,
  "dev_best_score_for_loss": -40.41236114501953
}
2022-10-24 15:18:21,532 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,534 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,534 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,534 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_166
2022-10-24 15:18:21,535 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,539 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,539 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,539 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,540 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,540 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_172
2022-10-24 15:18:21,544 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_172
2022-10-24 15:18:21,545 - trainer - INFO - 
*****************[epoch: 172, global step: 173] eval training set at end of epoch***************
2022-10-24 15:18:21,545 - trainer - INFO - {
  "train_loss": 45.97538375854492
}
2022-10-24 15:18:21,546 - trainer - INFO - start training epoch 173
2022-10-24 15:18:21,546 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,546 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,546 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,556 - trainer - INFO - 
*****************[epoch: 173, global step: 174] eval training set at end of epoch***************
2022-10-24 15:18:21,556 - trainer - INFO - {
  "train_loss": 40.41236114501953
}
2022-10-24 15:18:21,557 - trainer - INFO - start training epoch 174
2022-10-24 15:18:21,557 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,557 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,558 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,568 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval training set based on eval_every=2***************
2022-10-24 15:18:21,569 - trainer - INFO - {
  "train_loss": 37.88032341003418
}
2022-10-24 15:18:21,575 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval development set based on eval_every=2***************
2022-10-24 15:18:21,576 - trainer - INFO - {
  "dev_loss": 30.78076171875,
  "dev_best_score_for_loss": -30.78076171875
}
2022-10-24 15:18:21,577 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,578 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,578 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,578 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_168
2022-10-24 15:18:21,580 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,583 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,583 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,583 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,584 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,584 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_174
2022-10-24 15:18:21,588 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_174
2022-10-24 15:18:21,589 - trainer - INFO - 
*****************[epoch: 174, global step: 175] eval training set at end of epoch***************
2022-10-24 15:18:21,589 - trainer - INFO - {
  "train_loss": 35.34828567504883
}
2022-10-24 15:18:21,589 - trainer - INFO - start training epoch 175
2022-10-24 15:18:21,590 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,590 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,590 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,602 - trainer - INFO - 
*****************[epoch: 175, global step: 176] eval training set at end of epoch***************
2022-10-24 15:18:21,603 - trainer - INFO - {
  "train_loss": 30.78076171875
}
2022-10-24 15:18:21,603 - trainer - INFO - start training epoch 176
2022-10-24 15:18:21,604 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,604 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,604 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,613 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval training set based on eval_every=2***************
2022-10-24 15:18:21,613 - trainer - INFO - {
  "train_loss": 28.72829246520996
}
2022-10-24 15:18:21,620 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval development set based on eval_every=2***************
2022-10-24 15:18:21,620 - trainer - INFO - {
  "dev_loss": 22.99690818786621,
  "dev_best_score_for_loss": -22.99690818786621
}
2022-10-24 15:18:21,621 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,622 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,622 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,623 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_170
2022-10-24 15:18:21,624 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,628 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,628 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,628 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,629 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,629 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_176
2022-10-24 15:18:21,633 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_176
2022-10-24 15:18:21,634 - trainer - INFO - 
*****************[epoch: 176, global step: 177] eval training set at end of epoch***************
2022-10-24 15:18:21,634 - trainer - INFO - {
  "train_loss": 26.675823211669922
}
2022-10-24 15:18:21,635 - trainer - INFO - start training epoch 177
2022-10-24 15:18:21,635 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,635 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,636 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,647 - trainer - INFO - 
*****************[epoch: 177, global step: 178] eval training set at end of epoch***************
2022-10-24 15:18:21,648 - trainer - INFO - {
  "train_loss": 22.99690818786621
}
2022-10-24 15:18:21,648 - trainer - INFO - start training epoch 178
2022-10-24 15:18:21,648 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,649 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,649 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,657 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval training set based on eval_every=2***************
2022-10-24 15:18:21,659 - trainer - INFO - {
  "train_loss": 21.342788696289062
}
2022-10-24 15:18:21,667 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval development set based on eval_every=2***************
2022-10-24 15:18:21,668 - trainer - INFO - {
  "dev_loss": 16.77269172668457,
  "dev_best_score_for_loss": -16.77269172668457
}
2022-10-24 15:18:21,668 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,669 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,669 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,670 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_172
2022-10-24 15:18:21,671 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,675 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,675 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,675 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,676 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,676 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_178
2022-10-24 15:18:21,680 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_178
2022-10-24 15:18:21,681 - trainer - INFO - 
*****************[epoch: 178, global step: 179] eval training set at end of epoch***************
2022-10-24 15:18:21,681 - trainer - INFO - {
  "train_loss": 19.688669204711914
}
2022-10-24 15:18:21,682 - trainer - INFO - start training epoch 179
2022-10-24 15:18:21,682 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,682 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,683 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,693 - trainer - INFO - 
*****************[epoch: 179, global step: 180] eval training set at end of epoch***************
2022-10-24 15:18:21,694 - trainer - INFO - {
  "train_loss": 16.77269172668457
}
2022-10-24 15:18:21,694 - trainer - INFO - start training epoch 180
2022-10-24 15:18:21,694 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,695 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,695 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,703 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval training set based on eval_every=2***************
2022-10-24 15:18:21,705 - trainer - INFO - {
  "train_loss": 15.491038799285889
}
2022-10-24 15:18:21,712 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval development set based on eval_every=2***************
2022-10-24 15:18:21,712 - trainer - INFO - {
  "dev_loss": 12.027459144592285,
  "dev_best_score_for_loss": -12.027459144592285
}
2022-10-24 15:18:21,713 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,714 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,714 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,715 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_174
2022-10-24 15:18:21,716 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,719 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,720 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,720 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,722 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,724 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_180
2022-10-24 15:18:21,729 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_180
2022-10-24 15:18:21,733 - trainer - INFO - 
*****************[epoch: 180, global step: 181] eval training set at end of epoch***************
2022-10-24 15:18:21,734 - trainer - INFO - {
  "train_loss": 14.209385871887207
}
2022-10-24 15:18:21,737 - trainer - INFO - start training epoch 181
2022-10-24 15:18:21,737 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,738 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,738 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,757 - trainer - INFO - 
*****************[epoch: 181, global step: 182] eval training set at end of epoch***************
2022-10-24 15:18:21,758 - trainer - INFO - {
  "train_loss": 12.027459144592285
}
2022-10-24 15:18:21,758 - trainer - INFO - start training epoch 182
2022-10-24 15:18:21,758 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,759 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,759 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,768 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval training set based on eval_every=2***************
2022-10-24 15:18:21,768 - trainer - INFO - {
  "train_loss": 11.059932708740234
}
2022-10-24 15:18:21,775 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval development set based on eval_every=2***************
2022-10-24 15:18:21,776 - trainer - INFO - {
  "dev_loss": 8.393757820129395,
  "dev_best_score_for_loss": -8.393757820129395
}
2022-10-24 15:18:21,776 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,778 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,778 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,778 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_176
2022-10-24 15:18:21,780 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,784 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,784 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,785 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,785 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,788 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_182
2022-10-24 15:18:21,794 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_182
2022-10-24 15:18:21,795 - trainer - INFO - 
*****************[epoch: 182, global step: 183] eval training set at end of epoch***************
2022-10-24 15:18:21,795 - trainer - INFO - {
  "train_loss": 10.092406272888184
}
2022-10-24 15:18:21,796 - trainer - INFO - start training epoch 183
2022-10-24 15:18:21,796 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,797 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,798 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,807 - trainer - INFO - 
*****************[epoch: 183, global step: 184] eval training set at end of epoch***************
2022-10-24 15:18:21,808 - trainer - INFO - {
  "train_loss": 8.393757820129395
}
2022-10-24 15:18:21,809 - trainer - INFO - start training epoch 184
2022-10-24 15:18:21,809 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,809 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,810 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,817 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval training set based on eval_every=2***************
2022-10-24 15:18:21,817 - trainer - INFO - {
  "train_loss": 7.652677774429321
}
2022-10-24 15:18:21,823 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval development set based on eval_every=2***************
2022-10-24 15:18:21,823 - trainer - INFO - {
  "dev_loss": 5.626565456390381,
  "dev_best_score_for_loss": -5.626565456390381
}
2022-10-24 15:18:21,824 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,825 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,825 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,825 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_178
2022-10-24 15:18:21,827 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,831 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,833 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,834 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,834 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,835 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_184
2022-10-24 15:18:21,839 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_184
2022-10-24 15:18:21,840 - trainer - INFO - 
*****************[epoch: 184, global step: 185] eval training set at end of epoch***************
2022-10-24 15:18:21,841 - trainer - INFO - {
  "train_loss": 6.911597728729248
}
2022-10-24 15:18:21,841 - trainer - INFO - start training epoch 185
2022-10-24 15:18:21,841 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,841 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,842 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,851 - trainer - INFO - 
*****************[epoch: 185, global step: 186] eval training set at end of epoch***************
2022-10-24 15:18:21,852 - trainer - INFO - {
  "train_loss": 5.626565456390381
}
2022-10-24 15:18:21,852 - trainer - INFO - start training epoch 186
2022-10-24 15:18:21,852 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,852 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,853 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,860 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval training set based on eval_every=2***************
2022-10-24 15:18:21,861 - trainer - INFO - {
  "train_loss": 5.076829195022583
}
2022-10-24 15:18:21,867 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval development set based on eval_every=2***************
2022-10-24 15:18:21,868 - trainer - INFO - {
  "dev_loss": 3.5920448303222656,
  "dev_best_score_for_loss": -3.5920448303222656
}
2022-10-24 15:18:21,868 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,869 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,870 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,870 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_180
2022-10-24 15:18:21,871 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,876 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,876 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,876 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,877 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,881 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_186
2022-10-24 15:18:21,885 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_186
2022-10-24 15:18:21,886 - trainer - INFO - 
*****************[epoch: 186, global step: 187] eval training set at end of epoch***************
2022-10-24 15:18:21,886 - trainer - INFO - {
  "train_loss": 4.527092933654785
}
2022-10-24 15:18:21,887 - trainer - INFO - start training epoch 187
2022-10-24 15:18:21,887 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,888 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,888 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,899 - trainer - INFO - 
*****************[epoch: 187, global step: 188] eval training set at end of epoch***************
2022-10-24 15:18:21,900 - trainer - INFO - {
  "train_loss": 3.5920448303222656
}
2022-10-24 15:18:21,900 - trainer - INFO - start training epoch 188
2022-10-24 15:18:21,900 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,900 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,901 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,908 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval training set based on eval_every=2***************
2022-10-24 15:18:21,908 - trainer - INFO - {
  "train_loss": 3.1996874809265137
}
2022-10-24 15:18:21,914 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval development set based on eval_every=2***************
2022-10-24 15:18:21,914 - trainer - INFO - {
  "dev_loss": 2.1577694416046143,
  "dev_best_score_for_loss": -2.1577694416046143
}
2022-10-24 15:18:21,915 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,916 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,916 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,916 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_182
2022-10-24 15:18:21,918 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,922 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,922 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,923 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,924 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,927 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_188
2022-10-24 15:18:21,931 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_188
2022-10-24 15:18:21,932 - trainer - INFO - 
*****************[epoch: 188, global step: 189] eval training set at end of epoch***************
2022-10-24 15:18:21,932 - trainer - INFO - {
  "train_loss": 2.8073301315307617
}
2022-10-24 15:18:21,933 - trainer - INFO - start training epoch 189
2022-10-24 15:18:21,933 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,933 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,934 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,944 - trainer - INFO - 
*****************[epoch: 189, global step: 190] eval training set at end of epoch***************
2022-10-24 15:18:21,944 - trainer - INFO - {
  "train_loss": 2.1577694416046143
}
2022-10-24 15:18:21,945 - trainer - INFO - start training epoch 190
2022-10-24 15:18:21,945 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,945 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,945 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,953 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval training set based on eval_every=2***************
2022-10-24 15:18:21,953 - trainer - INFO - {
  "train_loss": 1.892749011516571
}
2022-10-24 15:18:21,959 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval development set based on eval_every=2***************
2022-10-24 15:18:21,959 - trainer - INFO - {
  "dev_loss": 1.2270622253417969,
  "dev_best_score_for_loss": -1.2270622253417969
}
2022-10-24 15:18:21,959 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:21,961 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:21,961 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:21,961 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_184
2022-10-24 15:18:21,963 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:21,966 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:21,966 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:21,966 - trainer - INFO -   patience: 200
2022-10-24 15:18:21,967 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:21,968 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_190
2022-10-24 15:18:21,973 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_190
2022-10-24 15:18:21,974 - trainer - INFO - 
*****************[epoch: 190, global step: 191] eval training set at end of epoch***************
2022-10-24 15:18:21,975 - trainer - INFO - {
  "train_loss": 1.6277285814285278
}
2022-10-24 15:18:21,975 - trainer - INFO - start training epoch 191
2022-10-24 15:18:21,975 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,975 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,976 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,985 - trainer - INFO - 
*****************[epoch: 191, global step: 192] eval training set at end of epoch***************
2022-10-24 15:18:21,986 - trainer - INFO - {
  "train_loss": 1.2270621061325073
}
2022-10-24 15:18:21,986 - trainer - INFO - start training epoch 192
2022-10-24 15:18:21,987 - trainer - INFO - training using device=cuda
2022-10-24 15:18:21,987 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:21,987 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:21,995 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval training set based on eval_every=2***************
2022-10-24 15:18:21,995 - trainer - INFO - {
  "train_loss": 1.06606787443161
}
2022-10-24 15:18:22,001 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval development set based on eval_every=2***************
2022-10-24 15:18:22,001 - trainer - INFO - {
  "dev_loss": 0.6569679975509644,
  "dev_best_score_for_loss": -0.6569679975509644
}
2022-10-24 15:18:22,002 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:22,003 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,003 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,003 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_186
2022-10-24 15:18:22,004 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:22,007 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:22,008 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:22,008 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,009 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:22,009 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_192
2022-10-24 15:18:22,013 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_192
2022-10-24 15:18:22,014 - trainer - INFO - 
*****************[epoch: 192, global step: 193] eval training set at end of epoch***************
2022-10-24 15:18:22,015 - trainer - INFO - {
  "train_loss": 0.9050736427307129
}
2022-10-24 15:18:22,018 - trainer - INFO - start training epoch 193
2022-10-24 15:18:22,018 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,019 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,019 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,028 - trainer - INFO - 
*****************[epoch: 193, global step: 194] eval training set at end of epoch***************
2022-10-24 15:18:22,028 - trainer - INFO - {
  "train_loss": 0.6569679379463196
}
2022-10-24 15:18:22,028 - trainer - INFO - start training epoch 194
2022-10-24 15:18:22,029 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,030 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,031 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,039 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval training set based on eval_every=2***************
2022-10-24 15:18:22,040 - trainer - INFO - {
  "train_loss": 0.5639891028404236
}
2022-10-24 15:18:22,047 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval development set based on eval_every=2***************
2022-10-24 15:18:22,047 - trainer - INFO - {
  "dev_loss": 0.33574357628822327,
  "dev_best_score_for_loss": -0.33574357628822327
}
2022-10-24 15:18:22,047 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:22,049 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,049 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,049 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_188
2022-10-24 15:18:22,050 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:22,053 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:22,053 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:22,053 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,054 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:22,054 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_194
2022-10-24 15:18:22,058 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_194
2022-10-24 15:18:22,058 - trainer - INFO - 
*****************[epoch: 194, global step: 195] eval training set at end of epoch***************
2022-10-24 15:18:22,059 - trainer - INFO - {
  "train_loss": 0.4710102677345276
}
2022-10-24 15:18:22,059 - trainer - INFO - start training epoch 195
2022-10-24 15:18:22,060 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,061 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,062 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,075 - trainer - INFO - 
*****************[epoch: 195, global step: 196] eval training set at end of epoch***************
2022-10-24 15:18:22,075 - trainer - INFO - {
  "train_loss": 0.33574360609054565
}
2022-10-24 15:18:22,076 - trainer - INFO - start training epoch 196
2022-10-24 15:18:22,077 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,078 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,079 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,088 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval training set based on eval_every=2***************
2022-10-24 15:18:22,089 - trainer - INFO - {
  "train_loss": 0.28896724432706833
}
2022-10-24 15:18:22,098 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval development set based on eval_every=2***************
2022-10-24 15:18:22,098 - trainer - INFO - {
  "dev_loss": 0.18225295841693878,
  "dev_best_score_for_loss": -0.18225295841693878
}
2022-10-24 15:18:22,099 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:22,100 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,101 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,101 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_190
2022-10-24 15:18:22,102 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:22,106 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:22,106 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:22,108 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,110 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:22,110 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_196
2022-10-24 15:18:22,116 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_196
2022-10-24 15:18:22,116 - trainer - INFO - 
*****************[epoch: 196, global step: 197] eval training set at end of epoch***************
2022-10-24 15:18:22,117 - trainer - INFO - {
  "train_loss": 0.242190882563591
}
2022-10-24 15:18:22,117 - trainer - INFO - start training epoch 197
2022-10-24 15:18:22,118 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,118 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,118 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,128 - trainer - INFO - 
*****************[epoch: 197, global step: 198] eval training set at end of epoch***************
2022-10-24 15:18:22,129 - trainer - INFO - {
  "train_loss": 0.18225297331809998
}
2022-10-24 15:18:22,129 - trainer - INFO - start training epoch 198
2022-10-24 15:18:22,129 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,129 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,130 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,136 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval training set based on eval_every=2***************
2022-10-24 15:18:22,136 - trainer - INFO - {
  "train_loss": 0.16504673659801483
}
2022-10-24 15:18:22,143 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval development set based on eval_every=2***************
2022-10-24 15:18:22,143 - trainer - INFO - {
  "dev_loss": 0.1358659267425537,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,144 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:22,145 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,145 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,145 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_192
2022-10-24 15:18:22,147 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:22,150 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:22,150 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:22,150 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,151 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:22,151 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_198
2022-10-24 15:18:22,155 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_198
2022-10-24 15:18:22,158 - trainer - INFO - 
*****************[epoch: 198, global step: 199] eval training set at end of epoch***************
2022-10-24 15:18:22,159 - trainer - INFO - {
  "train_loss": 0.1478404998779297
}
2022-10-24 15:18:22,159 - trainer - INFO - start training epoch 199
2022-10-24 15:18:22,160 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,160 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,160 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,169 - trainer - INFO - 
*****************[epoch: 199, global step: 200] eval training set at end of epoch***************
2022-10-24 15:18:22,170 - trainer - INFO - {
  "train_loss": 0.1358659267425537
}
2022-10-24 15:18:22,171 - trainer - INFO - start training epoch 200
2022-10-24 15:18:22,171 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,171 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,172 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,178 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval training set based on eval_every=2***************
2022-10-24 15:18:22,179 - trainer - INFO - {
  "train_loss": 0.1373012214899063
}
2022-10-24 15:18:22,184 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval development set based on eval_every=2***************
2022-10-24 15:18:22,185 - trainer - INFO - {
  "dev_loss": 0.15371370315551758,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,185 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:22,186 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,187 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,187 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,187 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_194
2022-10-24 15:18:22,188 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_200
2022-10-24 15:18:22,192 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_200
2022-10-24 15:18:22,193 - trainer - INFO - 
*****************[epoch: 200, global step: 201] eval training set at end of epoch***************
2022-10-24 15:18:22,193 - trainer - INFO - {
  "train_loss": 0.1387365162372589
}
2022-10-24 15:18:22,193 - trainer - INFO - start training epoch 201
2022-10-24 15:18:22,194 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,194 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,194 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,208 - trainer - INFO - 
*****************[epoch: 201, global step: 202] eval training set at end of epoch***************
2022-10-24 15:18:22,208 - trainer - INFO - {
  "train_loss": 0.15371370315551758
}
2022-10-24 15:18:22,209 - trainer - INFO - start training epoch 202
2022-10-24 15:18:22,209 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,209 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,210 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,218 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval training set based on eval_every=2***************
2022-10-24 15:18:22,219 - trainer - INFO - {
  "train_loss": 0.1654055118560791
}
2022-10-24 15:18:22,226 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval development set based on eval_every=2***************
2022-10-24 15:18:22,227 - trainer - INFO - {
  "dev_loss": 0.20471572875976562,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,227 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:22,228 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,229 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,229 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,230 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_196
2022-10-24 15:18:22,231 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_202
2022-10-24 15:18:22,236 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_202
2022-10-24 15:18:22,236 - trainer - INFO - 
*****************[epoch: 202, global step: 203] eval training set at end of epoch***************
2022-10-24 15:18:22,237 - trainer - INFO - {
  "train_loss": 0.17709732055664062
}
2022-10-24 15:18:22,237 - trainer - INFO - start training epoch 203
2022-10-24 15:18:22,237 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,238 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,238 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,247 - trainer - INFO - 
*****************[epoch: 203, global step: 204] eval training set at end of epoch***************
2022-10-24 15:18:22,247 - trainer - INFO - {
  "train_loss": 0.204715758562088
}
2022-10-24 15:18:22,249 - trainer - INFO - start training epoch 204
2022-10-24 15:18:22,253 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,253 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,253 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,261 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval training set based on eval_every=2***************
2022-10-24 15:18:22,263 - trainer - INFO - {
  "train_loss": 0.22024217247962952
}
2022-10-24 15:18:22,270 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval development set based on eval_every=2***************
2022-10-24 15:18:22,270 - trainer - INFO - {
  "dev_loss": 0.26672425866127014,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,271 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:22,272 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,273 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,273 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,273 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_198
2022-10-24 15:18:22,275 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_204
2022-10-24 15:18:22,279 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_204
2022-10-24 15:18:22,280 - trainer - INFO - 
*****************[epoch: 204, global step: 205] eval training set at end of epoch***************
2022-10-24 15:18:22,280 - trainer - INFO - {
  "train_loss": 0.23576858639717102
}
2022-10-24 15:18:22,281 - trainer - INFO - start training epoch 205
2022-10-24 15:18:22,281 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,281 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,282 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,288 - trainer - INFO - 
*****************[epoch: 205, global step: 206] eval training set at end of epoch***************
2022-10-24 15:18:22,288 - trainer - INFO - {
  "train_loss": 0.26672425866127014
}
2022-10-24 15:18:22,289 - trainer - INFO - start training epoch 206
2022-10-24 15:18:22,289 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,289 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,289 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,299 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval training set based on eval_every=2***************
2022-10-24 15:18:22,299 - trainer - INFO - {
  "train_loss": 0.28181053698062897
}
2022-10-24 15:18:22,305 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval development set based on eval_every=2***************
2022-10-24 15:18:22,306 - trainer - INFO - {
  "dev_loss": 0.32510802149772644,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,307 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:22,307 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,308 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,309 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,309 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_200
2022-10-24 15:18:22,310 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_206
2022-10-24 15:18:22,315 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_206
2022-10-24 15:18:22,316 - trainer - INFO - 
*****************[epoch: 206, global step: 207] eval training set at end of epoch***************
2022-10-24 15:18:22,316 - trainer - INFO - {
  "train_loss": 0.2968968152999878
}
2022-10-24 15:18:22,317 - trainer - INFO - start training epoch 207
2022-10-24 15:18:22,317 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,317 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,317 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,326 - trainer - INFO - 
*****************[epoch: 207, global step: 208] eval training set at end of epoch***************
2022-10-24 15:18:22,326 - trainer - INFO - {
  "train_loss": 0.32510799169540405
}
2022-10-24 15:18:22,327 - trainer - INFO - start training epoch 208
2022-10-24 15:18:22,327 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,327 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,328 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,335 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval training set based on eval_every=2***************
2022-10-24 15:18:22,335 - trainer - INFO - {
  "train_loss": 0.3374324142932892
}
2022-10-24 15:18:22,341 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval development set based on eval_every=2***************
2022-10-24 15:18:22,342 - trainer - INFO - {
  "dev_loss": 0.37132373452186584,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,343 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:18:22,346 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,347 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,347 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,348 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_202
2022-10-24 15:18:22,349 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_208
2022-10-24 15:18:22,353 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_208
2022-10-24 15:18:22,355 - trainer - INFO - 
*****************[epoch: 208, global step: 209] eval training set at end of epoch***************
2022-10-24 15:18:22,355 - trainer - INFO - {
  "train_loss": 0.3497568368911743
}
2022-10-24 15:18:22,356 - trainer - INFO - start training epoch 209
2022-10-24 15:18:22,356 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,356 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,357 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,366 - trainer - INFO - 
*****************[epoch: 209, global step: 210] eval training set at end of epoch***************
2022-10-24 15:18:22,366 - trainer - INFO - {
  "train_loss": 0.37132367491722107
}
2022-10-24 15:18:22,366 - trainer - INFO - start training epoch 210
2022-10-24 15:18:22,366 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,367 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,367 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,373 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval training set based on eval_every=2***************
2022-10-24 15:18:22,373 - trainer - INFO - {
  "train_loss": 0.37994441390037537
}
2022-10-24 15:18:22,379 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval development set based on eval_every=2***************
2022-10-24 15:18:22,380 - trainer - INFO - {
  "dev_loss": 0.4019153416156769,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,380 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:18:22,381 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,381 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,382 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,382 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_204
2022-10-24 15:18:22,383 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_210
2022-10-24 15:18:22,387 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_210
2022-10-24 15:18:22,388 - trainer - INFO - 
*****************[epoch: 210, global step: 211] eval training set at end of epoch***************
2022-10-24 15:18:22,391 - trainer - INFO - {
  "train_loss": 0.38856515288352966
}
2022-10-24 15:18:22,392 - trainer - INFO - start training epoch 211
2022-10-24 15:18:22,393 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,393 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,393 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,404 - trainer - INFO - 
*****************[epoch: 211, global step: 212] eval training set at end of epoch***************
2022-10-24 15:18:22,404 - trainer - INFO - {
  "train_loss": 0.4019153416156769
}
2022-10-24 15:18:22,404 - trainer - INFO - start training epoch 212
2022-10-24 15:18:22,405 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,405 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,405 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,413 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval training set based on eval_every=2***************
2022-10-24 15:18:22,413 - trainer - INFO - {
  "train_loss": 0.40658852458000183
}
2022-10-24 15:18:22,419 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval development set based on eval_every=2***************
2022-10-24 15:18:22,419 - trainer - INFO - {
  "dev_loss": 0.41641882061958313,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,420 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:18:22,420 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,421 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,422 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,422 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_206
2022-10-24 15:18:22,423 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_212
2022-10-24 15:18:22,427 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_212
2022-10-24 15:18:22,428 - trainer - INFO - 
*****************[epoch: 212, global step: 213] eval training set at end of epoch***************
2022-10-24 15:18:22,428 - trainer - INFO - {
  "train_loss": 0.4112617075443268
}
2022-10-24 15:18:22,429 - trainer - INFO - start training epoch 213
2022-10-24 15:18:22,429 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,429 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,429 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,438 - trainer - INFO - 
*****************[epoch: 213, global step: 214] eval training set at end of epoch***************
2022-10-24 15:18:22,438 - trainer - INFO - {
  "train_loss": 0.41641882061958313
}
2022-10-24 15:18:22,438 - trainer - INFO - start training epoch 214
2022-10-24 15:18:22,439 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,439 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,439 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,447 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval training set based on eval_every=2***************
2022-10-24 15:18:22,447 - trainer - INFO - {
  "train_loss": 0.41732218861579895
}
2022-10-24 15:18:22,458 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval development set based on eval_every=2***************
2022-10-24 15:18:22,458 - trainer - INFO - {
  "dev_loss": 0.4164281487464905,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,459 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:18:22,460 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,461 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,461 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,461 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_208
2022-10-24 15:18:22,463 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_214
2022-10-24 15:18:22,469 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_214
2022-10-24 15:18:22,469 - trainer - INFO - 
*****************[epoch: 214, global step: 215] eval training set at end of epoch***************
2022-10-24 15:18:22,470 - trainer - INFO - {
  "train_loss": 0.41822555661201477
}
2022-10-24 15:18:22,470 - trainer - INFO - start training epoch 215
2022-10-24 15:18:22,470 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,471 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,471 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,485 - trainer - INFO - 
*****************[epoch: 215, global step: 216] eval training set at end of epoch***************
2022-10-24 15:18:22,486 - trainer - INFO - {
  "train_loss": 0.41642817854881287
}
2022-10-24 15:18:22,486 - trainer - INFO - start training epoch 216
2022-10-24 15:18:22,486 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,487 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,487 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,499 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval training set based on eval_every=2***************
2022-10-24 15:18:22,500 - trainer - INFO - {
  "train_loss": 0.41409868001937866
}
2022-10-24 15:18:22,511 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval development set based on eval_every=2***************
2022-10-24 15:18:22,511 - trainer - INFO - {
  "dev_loss": 0.4045161306858063,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,512 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:18:22,513 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,514 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,514 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,515 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_210
2022-10-24 15:18:22,516 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_216
2022-10-24 15:18:22,521 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_216
2022-10-24 15:18:22,522 - trainer - INFO - 
*****************[epoch: 216, global step: 217] eval training set at end of epoch***************
2022-10-24 15:18:22,522 - trainer - INFO - {
  "train_loss": 0.41176918148994446
}
2022-10-24 15:18:22,523 - trainer - INFO - start training epoch 217
2022-10-24 15:18:22,523 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,524 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,524 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,537 - trainer - INFO - 
*****************[epoch: 217, global step: 218] eval training set at end of epoch***************
2022-10-24 15:18:22,537 - trainer - INFO - {
  "train_loss": 0.4045161306858063
}
2022-10-24 15:18:22,538 - trainer - INFO - start training epoch 218
2022-10-24 15:18:22,538 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,538 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,539 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,548 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval training set based on eval_every=2***************
2022-10-24 15:18:22,549 - trainer - INFO - {
  "train_loss": 0.39971737563610077
}
2022-10-24 15:18:22,555 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval development set based on eval_every=2***************
2022-10-24 15:18:22,556 - trainer - INFO - {
  "dev_loss": 0.38369789719581604,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,557 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:18:22,558 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,559 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,559 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,560 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_212
2022-10-24 15:18:22,562 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_218
2022-10-24 15:18:22,567 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_218
2022-10-24 15:18:22,568 - trainer - INFO - 
*****************[epoch: 218, global step: 219] eval training set at end of epoch***************
2022-10-24 15:18:22,569 - trainer - INFO - {
  "train_loss": 0.39491862058639526
}
2022-10-24 15:18:22,569 - trainer - INFO - start training epoch 219
2022-10-24 15:18:22,569 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,570 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,570 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,581 - trainer - INFO - 
*****************[epoch: 219, global step: 220] eval training set at end of epoch***************
2022-10-24 15:18:22,581 - trainer - INFO - {
  "train_loss": 0.38369789719581604
}
2022-10-24 15:18:22,582 - trainer - INFO - start training epoch 220
2022-10-24 15:18:22,582 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,582 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,583 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,591 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval training set based on eval_every=2***************
2022-10-24 15:18:22,592 - trainer - INFO - {
  "train_loss": 0.3772831857204437
}
2022-10-24 15:18:22,599 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval development set based on eval_every=2***************
2022-10-24 15:18:22,600 - trainer - INFO - {
  "dev_loss": 0.35711169242858887,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,600 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:18:22,601 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,602 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,602 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,602 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_214
2022-10-24 15:18:22,604 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_220
2022-10-24 15:18:22,608 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_220
2022-10-24 15:18:22,609 - trainer - INFO - 
*****************[epoch: 220, global step: 221] eval training set at end of epoch***************
2022-10-24 15:18:22,609 - trainer - INFO - {
  "train_loss": 0.3708684742450714
}
2022-10-24 15:18:22,610 - trainer - INFO - start training epoch 221
2022-10-24 15:18:22,610 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,610 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,610 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,619 - trainer - INFO - 
*****************[epoch: 221, global step: 222] eval training set at end of epoch***************
2022-10-24 15:18:22,620 - trainer - INFO - {
  "train_loss": 0.35711169242858887
}
2022-10-24 15:18:22,621 - trainer - INFO - start training epoch 222
2022-10-24 15:18:22,622 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,625 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,625 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,633 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval training set based on eval_every=2***************
2022-10-24 15:18:22,633 - trainer - INFO - {
  "train_loss": 0.3498498946428299
}
2022-10-24 15:18:22,640 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval development set based on eval_every=2***************
2022-10-24 15:18:22,640 - trainer - INFO - {
  "dev_loss": 0.32761478424072266,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,641 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:18:22,641 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,642 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,643 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,643 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_216
2022-10-24 15:18:22,645 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_222
2022-10-24 15:18:22,650 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_222
2022-10-24 15:18:22,650 - trainer - INFO - 
*****************[epoch: 222, global step: 223] eval training set at end of epoch***************
2022-10-24 15:18:22,651 - trainer - INFO - {
  "train_loss": 0.3425880968570709
}
2022-10-24 15:18:22,651 - trainer - INFO - start training epoch 223
2022-10-24 15:18:22,652 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,652 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,652 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,661 - trainer - INFO - 
*****************[epoch: 223, global step: 224] eval training set at end of epoch***************
2022-10-24 15:18:22,661 - trainer - INFO - {
  "train_loss": 0.32761478424072266
}
2022-10-24 15:18:22,662 - trainer - INFO - start training epoch 224
2022-10-24 15:18:22,662 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,662 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,663 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,672 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval training set based on eval_every=2***************
2022-10-24 15:18:22,673 - trainer - INFO - {
  "train_loss": 0.32010042667388916
}
2022-10-24 15:18:22,680 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval development set based on eval_every=2***************
2022-10-24 15:18:22,681 - trainer - INFO - {
  "dev_loss": 0.2975480854511261,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,682 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:18:22,682 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,683 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,683 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,684 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_218
2022-10-24 15:18:22,685 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_224
2022-10-24 15:18:22,690 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_224
2022-10-24 15:18:22,690 - trainer - INFO - 
*****************[epoch: 224, global step: 225] eval training set at end of epoch***************
2022-10-24 15:18:22,691 - trainer - INFO - {
  "train_loss": 0.31258606910705566
}
2022-10-24 15:18:22,691 - trainer - INFO - start training epoch 225
2022-10-24 15:18:22,691 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,692 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,692 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,700 - trainer - INFO - 
*****************[epoch: 225, global step: 226] eval training set at end of epoch***************
2022-10-24 15:18:22,701 - trainer - INFO - {
  "train_loss": 0.2975480854511261
}
2022-10-24 15:18:22,701 - trainer - INFO - start training epoch 226
2022-10-24 15:18:22,701 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,701 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,702 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,708 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval training set based on eval_every=2***************
2022-10-24 15:18:22,708 - trainer - INFO - {
  "train_loss": 0.2902127653360367
}
2022-10-24 15:18:22,717 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval development set based on eval_every=2***************
2022-10-24 15:18:22,717 - trainer - INFO - {
  "dev_loss": 0.268643856048584,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,718 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:18:22,718 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,719 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,720 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,720 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_220
2022-10-24 15:18:22,721 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_226
2022-10-24 15:18:22,726 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_226
2022-10-24 15:18:22,728 - trainer - INFO - 
*****************[epoch: 226, global step: 227] eval training set at end of epoch***************
2022-10-24 15:18:22,728 - trainer - INFO - {
  "train_loss": 0.28287744522094727
}
2022-10-24 15:18:22,728 - trainer - INFO - start training epoch 227
2022-10-24 15:18:22,729 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,729 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,729 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,736 - trainer - INFO - 
*****************[epoch: 227, global step: 228] eval training set at end of epoch***************
2022-10-24 15:18:22,737 - trainer - INFO - {
  "train_loss": 0.268643856048584
}
2022-10-24 15:18:22,737 - trainer - INFO - start training epoch 228
2022-10-24 15:18:22,737 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,738 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,738 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,748 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval training set based on eval_every=2***************
2022-10-24 15:18:22,748 - trainer - INFO - {
  "train_loss": 0.26182062923908234
}
2022-10-24 15:18:22,754 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval development set based on eval_every=2***************
2022-10-24 15:18:22,754 - trainer - INFO - {
  "dev_loss": 0.24209976196289062,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,755 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:18:22,755 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,756 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,757 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,757 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_222
2022-10-24 15:18:22,759 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_228
2022-10-24 15:18:22,763 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_228
2022-10-24 15:18:22,764 - trainer - INFO - 
*****************[epoch: 228, global step: 229] eval training set at end of epoch***************
2022-10-24 15:18:22,764 - trainer - INFO - {
  "train_loss": 0.2549974024295807
}
2022-10-24 15:18:22,765 - trainer - INFO - start training epoch 229
2022-10-24 15:18:22,765 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,765 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,766 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,774 - trainer - INFO - 
*****************[epoch: 229, global step: 230] eval training set at end of epoch***************
2022-10-24 15:18:22,775 - trainer - INFO - {
  "train_loss": 0.24209976196289062
}
2022-10-24 15:18:22,776 - trainer - INFO - start training epoch 230
2022-10-24 15:18:22,776 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,779 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,779 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,786 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval training set based on eval_every=2***************
2022-10-24 15:18:22,787 - trainer - INFO - {
  "train_loss": 0.2360159084200859
}
2022-10-24 15:18:22,794 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval development set based on eval_every=2***************
2022-10-24 15:18:22,794 - trainer - INFO - {
  "dev_loss": 0.2186867892742157,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,795 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:18:22,795 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,796 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,797 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,797 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_224
2022-10-24 15:18:22,798 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_230
2022-10-24 15:18:22,801 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_230
2022-10-24 15:18:22,802 - trainer - INFO - 
*****************[epoch: 230, global step: 231] eval training set at end of epoch***************
2022-10-24 15:18:22,802 - trainer - INFO - {
  "train_loss": 0.2299320548772812
}
2022-10-24 15:18:22,803 - trainer - INFO - start training epoch 231
2022-10-24 15:18:22,803 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,803 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,803 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,812 - trainer - INFO - 
*****************[epoch: 231, global step: 232] eval training set at end of epoch***************
2022-10-24 15:18:22,812 - trainer - INFO - {
  "train_loss": 0.2186867594718933
}
2022-10-24 15:18:22,812 - trainer - INFO - start training epoch 232
2022-10-24 15:18:22,813 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,813 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,813 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,819 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval training set based on eval_every=2***************
2022-10-24 15:18:22,820 - trainer - INFO - {
  "train_loss": 0.21346090734004974
}
2022-10-24 15:18:22,831 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval development set based on eval_every=2***************
2022-10-24 15:18:22,831 - trainer - INFO - {
  "dev_loss": 0.19870471954345703,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,832 - trainer - INFO -   no_improve_count: 17
2022-10-24 15:18:22,832 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,834 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,834 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,834 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_226
2022-10-24 15:18:22,836 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_232
2022-10-24 15:18:22,841 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_232
2022-10-24 15:18:22,842 - trainer - INFO - 
*****************[epoch: 232, global step: 233] eval training set at end of epoch***************
2022-10-24 15:18:22,843 - trainer - INFO - {
  "train_loss": 0.20823505520820618
}
2022-10-24 15:18:22,844 - trainer - INFO - start training epoch 233
2022-10-24 15:18:22,844 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,844 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,844 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,859 - trainer - INFO - 
*****************[epoch: 233, global step: 234] eval training set at end of epoch***************
2022-10-24 15:18:22,861 - trainer - INFO - {
  "train_loss": 0.19870471954345703
}
2022-10-24 15:18:22,864 - trainer - INFO - start training epoch 234
2022-10-24 15:18:22,864 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,865 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,865 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,875 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval training set based on eval_every=2***************
2022-10-24 15:18:22,876 - trainer - INFO - {
  "train_loss": 0.19437915831804276
}
2022-10-24 15:18:22,884 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval development set based on eval_every=2***************
2022-10-24 15:18:22,887 - trainer - INFO - {
  "dev_loss": 0.18222713470458984,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,887 - trainer - INFO -   no_improve_count: 18
2022-10-24 15:18:22,888 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,889 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,890 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,890 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_228
2022-10-24 15:18:22,891 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_234
2022-10-24 15:18:22,897 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_234
2022-10-24 15:18:22,898 - trainer - INFO - 
*****************[epoch: 234, global step: 235] eval training set at end of epoch***************
2022-10-24 15:18:22,899 - trainer - INFO - {
  "train_loss": 0.19005359709262848
}
2022-10-24 15:18:22,899 - trainer - INFO - start training epoch 235
2022-10-24 15:18:22,900 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,900 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,900 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,908 - trainer - INFO - 
*****************[epoch: 235, global step: 236] eval training set at end of epoch***************
2022-10-24 15:18:22,909 - trainer - INFO - {
  "train_loss": 0.18222713470458984
}
2022-10-24 15:18:22,909 - trainer - INFO - start training epoch 236
2022-10-24 15:18:22,910 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,910 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,910 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,919 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval training set based on eval_every=2***************
2022-10-24 15:18:22,919 - trainer - INFO - {
  "train_loss": 0.17873764783143997
}
2022-10-24 15:18:22,926 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval development set based on eval_every=2***************
2022-10-24 15:18:22,926 - trainer - INFO - {
  "dev_loss": 0.16902343928813934,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,927 - trainer - INFO -   no_improve_count: 19
2022-10-24 15:18:22,927 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,929 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,929 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,929 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_230
2022-10-24 15:18:22,931 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_236
2022-10-24 15:18:22,935 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_236
2022-10-24 15:18:22,936 - trainer - INFO - 
*****************[epoch: 236, global step: 237] eval training set at end of epoch***************
2022-10-24 15:18:22,936 - trainer - INFO - {
  "train_loss": 0.1752481609582901
}
2022-10-24 15:18:22,936 - trainer - INFO - start training epoch 237
2022-10-24 15:18:22,936 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,937 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,937 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,945 - trainer - INFO - 
*****************[epoch: 237, global step: 238] eval training set at end of epoch***************
2022-10-24 15:18:22,946 - trainer - INFO - {
  "train_loss": 0.16902343928813934
}
2022-10-24 15:18:22,948 - trainer - INFO - start training epoch 238
2022-10-24 15:18:22,949 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,949 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,949 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,955 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval training set based on eval_every=2***************
2022-10-24 15:18:22,956 - trainer - INFO - {
  "train_loss": 0.16628019511699677
}
2022-10-24 15:18:22,964 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval development set based on eval_every=2***************
2022-10-24 15:18:22,965 - trainer - INFO - {
  "dev_loss": 0.1587320864200592,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:22,965 - trainer - INFO -   no_improve_count: 20
2022-10-24 15:18:22,966 - trainer - INFO -   patience: 200
2022-10-24 15:18:22,967 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:22,967 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:22,967 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_232
2022-10-24 15:18:22,969 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_238
2022-10-24 15:18:22,972 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_238
2022-10-24 15:18:22,973 - trainer - INFO - 
*****************[epoch: 238, global step: 239] eval training set at end of epoch***************
2022-10-24 15:18:22,973 - trainer - INFO - {
  "train_loss": 0.1635369509458542
}
2022-10-24 15:18:22,974 - trainer - INFO - start training epoch 239
2022-10-24 15:18:22,974 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,974 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,976 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,987 - trainer - INFO - 
*****************[epoch: 239, global step: 240] eval training set at end of epoch***************
2022-10-24 15:18:22,987 - trainer - INFO - {
  "train_loss": 0.1587321013212204
}
2022-10-24 15:18:22,987 - trainer - INFO - start training epoch 240
2022-10-24 15:18:22,987 - trainer - INFO - training using device=cuda
2022-10-24 15:18:22,988 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:22,988 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:22,997 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval training set based on eval_every=2***************
2022-10-24 15:18:22,998 - trainer - INFO - {
  "train_loss": 0.15665000677108765
}
2022-10-24 15:18:23,003 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval development set based on eval_every=2***************
2022-10-24 15:18:23,003 - trainer - INFO - {
  "dev_loss": 0.1509527564048767,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:23,004 - trainer - INFO -   no_improve_count: 21
2022-10-24 15:18:23,004 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,005 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,006 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,006 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_234
2022-10-24 15:18:23,008 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_240
2022-10-24 15:18:23,012 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_240
2022-10-24 15:18:23,013 - trainer - INFO - 
*****************[epoch: 240, global step: 241] eval training set at end of epoch***************
2022-10-24 15:18:23,013 - trainer - INFO - {
  "train_loss": 0.1545679122209549
}
2022-10-24 15:18:23,014 - trainer - INFO - start training epoch 241
2022-10-24 15:18:23,014 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,014 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,015 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,028 - trainer - INFO - 
*****************[epoch: 241, global step: 242] eval training set at end of epoch***************
2022-10-24 15:18:23,028 - trainer - INFO - {
  "train_loss": 0.1509527713060379
}
2022-10-24 15:18:23,028 - trainer - INFO - start training epoch 242
2022-10-24 15:18:23,029 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,029 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,030 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,039 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval training set based on eval_every=2***************
2022-10-24 15:18:23,039 - trainer - INFO - {
  "train_loss": 0.1493857055902481
}
2022-10-24 15:18:23,046 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval development set based on eval_every=2***************
2022-10-24 15:18:23,046 - trainer - INFO - {
  "dev_loss": 0.1451893001794815,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:23,047 - trainer - INFO -   no_improve_count: 22
2022-10-24 15:18:23,047 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,048 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,048 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,049 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_236
2022-10-24 15:18:23,050 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_242
2022-10-24 15:18:23,054 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_242
2022-10-24 15:18:23,054 - trainer - INFO - 
*****************[epoch: 242, global step: 243] eval training set at end of epoch***************
2022-10-24 15:18:23,055 - trainer - INFO - {
  "train_loss": 0.1478186398744583
}
2022-10-24 15:18:23,055 - trainer - INFO - start training epoch 243
2022-10-24 15:18:23,055 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,056 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,056 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,064 - trainer - INFO - 
*****************[epoch: 243, global step: 244] eval training set at end of epoch***************
2022-10-24 15:18:23,064 - trainer - INFO - {
  "train_loss": 0.1451893001794815
}
2022-10-24 15:18:23,065 - trainer - INFO - start training epoch 244
2022-10-24 15:18:23,065 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,065 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,066 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,073 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval training set based on eval_every=2***************
2022-10-24 15:18:23,074 - trainer - INFO - {
  "train_loss": 0.14405548572540283
}
2022-10-24 15:18:23,080 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval development set based on eval_every=2***************
2022-10-24 15:18:23,080 - trainer - INFO - {
  "dev_loss": 0.14100992679595947,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:23,081 - trainer - INFO -   no_improve_count: 23
2022-10-24 15:18:23,081 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,082 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,084 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,085 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_238
2022-10-24 15:18:23,086 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_244
2022-10-24 15:18:23,091 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_244
2022-10-24 15:18:23,092 - trainer - INFO - 
*****************[epoch: 244, global step: 245] eval training set at end of epoch***************
2022-10-24 15:18:23,092 - trainer - INFO - {
  "train_loss": 0.14292167127132416
}
2022-10-24 15:18:23,092 - trainer - INFO - start training epoch 245
2022-10-24 15:18:23,093 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,093 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,093 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,104 - trainer - INFO - 
*****************[epoch: 245, global step: 246] eval training set at end of epoch***************
2022-10-24 15:18:23,104 - trainer - INFO - {
  "train_loss": 0.14100992679595947
}
2022-10-24 15:18:23,104 - trainer - INFO - start training epoch 246
2022-10-24 15:18:23,105 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,105 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,105 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,113 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval training set based on eval_every=2***************
2022-10-24 15:18:23,114 - trainer - INFO - {
  "train_loss": 0.14020878821611404
}
2022-10-24 15:18:23,122 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval development set based on eval_every=2***************
2022-10-24 15:18:23,122 - trainer - INFO - {
  "dev_loss": 0.138030007481575,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:23,123 - trainer - INFO -   no_improve_count: 24
2022-10-24 15:18:23,123 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,124 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,124 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,124 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_240
2022-10-24 15:18:23,126 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_246
2022-10-24 15:18:23,132 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_246
2022-10-24 15:18:23,133 - trainer - INFO - 
*****************[epoch: 246, global step: 247] eval training set at end of epoch***************
2022-10-24 15:18:23,136 - trainer - INFO - {
  "train_loss": 0.13940764963626862
}
2022-10-24 15:18:23,137 - trainer - INFO - start training epoch 247
2022-10-24 15:18:23,137 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,137 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,138 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,147 - trainer - INFO - 
*****************[epoch: 247, global step: 248] eval training set at end of epoch***************
2022-10-24 15:18:23,148 - trainer - INFO - {
  "train_loss": 0.138030007481575
}
2022-10-24 15:18:23,148 - trainer - INFO - start training epoch 248
2022-10-24 15:18:23,148 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,149 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,149 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,155 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval training set based on eval_every=2***************
2022-10-24 15:18:23,156 - trainer - INFO - {
  "train_loss": 0.13745947182178497
}
2022-10-24 15:18:23,163 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval development set based on eval_every=2***************
2022-10-24 15:18:23,163 - trainer - INFO - {
  "dev_loss": 0.1358889937400818,
  "dev_best_score_for_loss": -0.1358659267425537
}
2022-10-24 15:18:23,164 - trainer - INFO -   no_improve_count: 25
2022-10-24 15:18:23,165 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,166 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,166 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,167 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_242
2022-10-24 15:18:23,168 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_248
2022-10-24 15:18:23,172 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_248
2022-10-24 15:18:23,173 - trainer - INFO - 
*****************[epoch: 248, global step: 249] eval training set at end of epoch***************
2022-10-24 15:18:23,173 - trainer - INFO - {
  "train_loss": 0.13688893616199493
}
2022-10-24 15:18:23,173 - trainer - INFO - start training epoch 249
2022-10-24 15:18:23,174 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,174 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,174 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,184 - trainer - INFO - 
*****************[epoch: 249, global step: 250] eval training set at end of epoch***************
2022-10-24 15:18:23,184 - trainer - INFO - {
  "train_loss": 0.1358889937400818
}
2022-10-24 15:18:23,184 - trainer - INFO - start training epoch 250
2022-10-24 15:18:23,184 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,185 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,185 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,193 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval training set based on eval_every=2***************
2022-10-24 15:18:23,194 - trainer - INFO - {
  "train_loss": 0.1354794278740883
}
2022-10-24 15:18:23,201 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval development set based on eval_every=2***************
2022-10-24 15:18:23,201 - trainer - INFO - {
  "dev_loss": 0.13434234261512756,
  "dev_best_score_for_loss": -0.13434234261512756
}
2022-10-24 15:18:23,202 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,203 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,203 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,204 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_244
2022-10-24 15:18:23,205 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,210 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,214 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,214 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,215 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,215 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_250
2022-10-24 15:18:23,221 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_250
2022-10-24 15:18:23,223 - trainer - INFO - 
*****************[epoch: 250, global step: 251] eval training set at end of epoch***************
2022-10-24 15:18:23,224 - trainer - INFO - {
  "train_loss": 0.1350698620080948
}
2022-10-24 15:18:23,224 - trainer - INFO - start training epoch 251
2022-10-24 15:18:23,225 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,225 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,226 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,234 - trainer - INFO - 
*****************[epoch: 251, global step: 252] eval training set at end of epoch***************
2022-10-24 15:18:23,234 - trainer - INFO - {
  "train_loss": 0.13434234261512756
}
2022-10-24 15:18:23,235 - trainer - INFO - start training epoch 252
2022-10-24 15:18:23,235 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,235 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,235 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,242 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval training set based on eval_every=2***************
2022-10-24 15:18:23,242 - trainer - INFO - {
  "train_loss": 0.13402801007032394
}
2022-10-24 15:18:23,248 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval development set based on eval_every=2***************
2022-10-24 15:18:23,249 - trainer - INFO - {
  "dev_loss": 0.13315783441066742,
  "dev_best_score_for_loss": -0.13315783441066742
}
2022-10-24 15:18:23,249 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,250 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,250 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,251 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_246
2022-10-24 15:18:23,252 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,255 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,259 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,259 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,261 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,261 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_252
2022-10-24 15:18:23,266 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_252
2022-10-24 15:18:23,267 - trainer - INFO - 
*****************[epoch: 252, global step: 253] eval training set at end of epoch***************
2022-10-24 15:18:23,268 - trainer - INFO - {
  "train_loss": 0.13371367752552032
}
2022-10-24 15:18:23,268 - trainer - INFO - start training epoch 253
2022-10-24 15:18:23,270 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,270 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,271 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,279 - trainer - INFO - 
*****************[epoch: 253, global step: 254] eval training set at end of epoch***************
2022-10-24 15:18:23,280 - trainer - INFO - {
  "train_loss": 0.13315781950950623
}
2022-10-24 15:18:23,280 - trainer - INFO - start training epoch 254
2022-10-24 15:18:23,281 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,281 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,281 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,289 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval training set based on eval_every=2***************
2022-10-24 15:18:23,289 - trainer - INFO - {
  "train_loss": 0.13289836794137955
}
2022-10-24 15:18:23,298 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval development set based on eval_every=2***************
2022-10-24 15:18:23,298 - trainer - INFO - {
  "dev_loss": 0.13214929401874542,
  "dev_best_score_for_loss": -0.13214929401874542
}
2022-10-24 15:18:23,299 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,300 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,301 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,301 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_248
2022-10-24 15:18:23,303 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,306 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,306 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,307 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,308 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,308 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_254
2022-10-24 15:18:23,313 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_254
2022-10-24 15:18:23,314 - trainer - INFO - 
*****************[epoch: 254, global step: 255] eval training set at end of epoch***************
2022-10-24 15:18:23,314 - trainer - INFO - {
  "train_loss": 0.13263891637325287
}
2022-10-24 15:18:23,314 - trainer - INFO - start training epoch 255
2022-10-24 15:18:23,314 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,315 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,315 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,325 - trainer - INFO - 
*****************[epoch: 255, global step: 256] eval training set at end of epoch***************
2022-10-24 15:18:23,325 - trainer - INFO - {
  "train_loss": 0.13214929401874542
}
2022-10-24 15:18:23,325 - trainer - INFO - start training epoch 256
2022-10-24 15:18:23,326 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,326 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,326 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,333 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval training set based on eval_every=2***************
2022-10-24 15:18:23,334 - trainer - INFO - {
  "train_loss": 0.13191264867782593
}
2022-10-24 15:18:23,338 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval development set based on eval_every=2***************
2022-10-24 15:18:23,339 - trainer - INFO - {
  "dev_loss": 0.13124576210975647,
  "dev_best_score_for_loss": -0.13124576210975647
}
2022-10-24 15:18:23,339 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,341 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,341 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,341 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_250
2022-10-24 15:18:23,342 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,346 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,346 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,346 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,348 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,348 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_256
2022-10-24 15:18:23,352 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_256
2022-10-24 15:18:23,353 - trainer - INFO - 
*****************[epoch: 256, global step: 257] eval training set at end of epoch***************
2022-10-24 15:18:23,353 - trainer - INFO - {
  "train_loss": 0.13167600333690643
}
2022-10-24 15:18:23,353 - trainer - INFO - start training epoch 257
2022-10-24 15:18:23,354 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,354 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,354 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,362 - trainer - INFO - 
*****************[epoch: 257, global step: 258] eval training set at end of epoch***************
2022-10-24 15:18:23,363 - trainer - INFO - {
  "train_loss": 0.13124576210975647
}
2022-10-24 15:18:23,365 - trainer - INFO - start training epoch 258
2022-10-24 15:18:23,368 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,369 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,369 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,379 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval training set based on eval_every=2***************
2022-10-24 15:18:23,379 - trainer - INFO - {
  "train_loss": 0.13101735711097717
}
2022-10-24 15:18:23,386 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval development set based on eval_every=2***************
2022-10-24 15:18:23,386 - trainer - INFO - {
  "dev_loss": 0.13032649457454681,
  "dev_best_score_for_loss": -0.13032649457454681
}
2022-10-24 15:18:23,387 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,388 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,388 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,389 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_252
2022-10-24 15:18:23,390 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,394 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,394 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,395 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,395 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,396 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_258
2022-10-24 15:18:23,400 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_258
2022-10-24 15:18:23,400 - trainer - INFO - 
*****************[epoch: 258, global step: 259] eval training set at end of epoch***************
2022-10-24 15:18:23,401 - trainer - INFO - {
  "train_loss": 0.13078895211219788
}
2022-10-24 15:18:23,401 - trainer - INFO - start training epoch 259
2022-10-24 15:18:23,401 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,402 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,402 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,410 - trainer - INFO - 
*****************[epoch: 259, global step: 260] eval training set at end of epoch***************
2022-10-24 15:18:23,411 - trainer - INFO - {
  "train_loss": 0.13032649457454681
}
2022-10-24 15:18:23,411 - trainer - INFO - start training epoch 260
2022-10-24 15:18:23,412 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,412 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,412 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,423 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval training set based on eval_every=2***************
2022-10-24 15:18:23,423 - trainer - INFO - {
  "train_loss": 0.13011041283607483
}
2022-10-24 15:18:23,433 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval development set based on eval_every=2***************
2022-10-24 15:18:23,433 - trainer - INFO - {
  "dev_loss": 0.12943005561828613,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,434 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,435 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,435 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,436 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_254
2022-10-24 15:18:23,437 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,441 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,441 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,441 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,442 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,442 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_260
2022-10-24 15:18:23,446 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_260
2022-10-24 15:18:23,447 - trainer - INFO - 
*****************[epoch: 260, global step: 261] eval training set at end of epoch***************
2022-10-24 15:18:23,447 - trainer - INFO - {
  "train_loss": 0.12989433109760284
}
2022-10-24 15:18:23,447 - trainer - INFO - start training epoch 261
2022-10-24 15:18:23,448 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,448 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,448 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,455 - trainer - INFO - 
*****************[epoch: 261, global step: 262] eval training set at end of epoch***************
2022-10-24 15:18:23,456 - trainer - INFO - {
  "train_loss": 0.12943005561828613
}
2022-10-24 15:18:23,457 - trainer - INFO - start training epoch 262
2022-10-24 15:18:23,458 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,459 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,462 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,469 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval training set based on eval_every=2***************
2022-10-24 15:18:23,469 - trainer - INFO - {
  "train_loss": 0.12940604239702225
}
2022-10-24 15:18:23,476 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval development set based on eval_every=2***************
2022-10-24 15:18:23,476 - trainer - INFO - {
  "dev_loss": 0.12946216762065887,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,477 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:23,478 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,478 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,479 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,479 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_256
2022-10-24 15:18:23,480 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_262
2022-10-24 15:18:23,484 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_262
2022-10-24 15:18:23,485 - trainer - INFO - 
*****************[epoch: 262, global step: 263] eval training set at end of epoch***************
2022-10-24 15:18:23,485 - trainer - INFO - {
  "train_loss": 0.12938202917575836
}
2022-10-24 15:18:23,486 - trainer - INFO - start training epoch 263
2022-10-24 15:18:23,487 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,487 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,488 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,495 - trainer - INFO - 
*****************[epoch: 263, global step: 264] eval training set at end of epoch***************
2022-10-24 15:18:23,495 - trainer - INFO - {
  "train_loss": 0.12946215271949768
}
2022-10-24 15:18:23,496 - trainer - INFO - start training epoch 264
2022-10-24 15:18:23,496 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,496 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,497 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,508 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval training set based on eval_every=2***************
2022-10-24 15:18:23,509 - trainer - INFO - {
  "train_loss": 0.129495270550251
}
2022-10-24 15:18:23,514 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval development set based on eval_every=2***************
2022-10-24 15:18:23,515 - trainer - INFO - {
  "dev_loss": 0.12959297001361847,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,515 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:23,516 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,517 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,518 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,518 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_258
2022-10-24 15:18:23,521 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_264
2022-10-24 15:18:23,525 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_264
2022-10-24 15:18:23,527 - trainer - INFO - 
*****************[epoch: 264, global step: 265] eval training set at end of epoch***************
2022-10-24 15:18:23,527 - trainer - INFO - {
  "train_loss": 0.12952838838100433
}
2022-10-24 15:18:23,528 - trainer - INFO - start training epoch 265
2022-10-24 15:18:23,528 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,528 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,529 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,537 - trainer - INFO - 
*****************[epoch: 265, global step: 266] eval training set at end of epoch***************
2022-10-24 15:18:23,538 - trainer - INFO - {
  "train_loss": 0.12959295511245728
}
2022-10-24 15:18:23,538 - trainer - INFO - start training epoch 266
2022-10-24 15:18:23,538 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,538 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,539 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,546 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval training set based on eval_every=2***************
2022-10-24 15:18:23,547 - trainer - INFO - {
  "train_loss": 0.12961585074663162
}
2022-10-24 15:18:23,555 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval development set based on eval_every=2***************
2022-10-24 15:18:23,555 - trainer - INFO - {
  "dev_loss": 0.12968093156814575,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,556 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:23,557 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,558 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,558 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,558 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_260
2022-10-24 15:18:23,560 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_266
2022-10-24 15:18:23,566 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_266
2022-10-24 15:18:23,567 - trainer - INFO - 
*****************[epoch: 266, global step: 267] eval training set at end of epoch***************
2022-10-24 15:18:23,567 - trainer - INFO - {
  "train_loss": 0.12963874638080597
}
2022-10-24 15:18:23,568 - trainer - INFO - start training epoch 267
2022-10-24 15:18:23,568 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,568 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,569 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,577 - trainer - INFO - 
*****************[epoch: 267, global step: 268] eval training set at end of epoch***************
2022-10-24 15:18:23,578 - trainer - INFO - {
  "train_loss": 0.12968094646930695
}
2022-10-24 15:18:23,578 - trainer - INFO - start training epoch 268
2022-10-24 15:18:23,578 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,579 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,580 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,587 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval training set based on eval_every=2***************
2022-10-24 15:18:23,587 - trainer - INFO - {
  "train_loss": 0.1296941414475441
}
2022-10-24 15:18:23,596 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval development set based on eval_every=2***************
2022-10-24 15:18:23,597 - trainer - INFO - {
  "dev_loss": 0.12972863018512726,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,599 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:23,602 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,603 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,604 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,604 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_262
2022-10-24 15:18:23,606 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_268
2022-10-24 15:18:23,612 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_268
2022-10-24 15:18:23,613 - trainer - INFO - 
*****************[epoch: 268, global step: 269] eval training set at end of epoch***************
2022-10-24 15:18:23,613 - trainer - INFO - {
  "train_loss": 0.12970733642578125
}
2022-10-24 15:18:23,614 - trainer - INFO - start training epoch 269
2022-10-24 15:18:23,614 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,614 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,615 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,624 - trainer - INFO - 
*****************[epoch: 269, global step: 270] eval training set at end of epoch***************
2022-10-24 15:18:23,625 - trainer - INFO - {
  "train_loss": 0.12972864508628845
}
2022-10-24 15:18:23,625 - trainer - INFO - start training epoch 270
2022-10-24 15:18:23,627 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,627 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,628 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,636 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval training set based on eval_every=2***************
2022-10-24 15:18:23,637 - trainer - INFO - {
  "train_loss": 0.1297273114323616
}
2022-10-24 15:18:23,644 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval development set based on eval_every=2***************
2022-10-24 15:18:23,645 - trainer - INFO - {
  "dev_loss": 0.12971748411655426,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,649 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:18:23,649 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,651 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,651 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,651 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_264
2022-10-24 15:18:23,653 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_270
2022-10-24 15:18:23,659 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_270
2022-10-24 15:18:23,659 - trainer - INFO - 
*****************[epoch: 270, global step: 271] eval training set at end of epoch***************
2022-10-24 15:18:23,660 - trainer - INFO - {
  "train_loss": 0.12972597777843475
}
2022-10-24 15:18:23,661 - trainer - INFO - start training epoch 271
2022-10-24 15:18:23,661 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,661 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,662 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,671 - trainer - INFO - 
*****************[epoch: 271, global step: 272] eval training set at end of epoch***************
2022-10-24 15:18:23,672 - trainer - INFO - {
  "train_loss": 0.12971748411655426
}
2022-10-24 15:18:23,674 - trainer - INFO - start training epoch 272
2022-10-24 15:18:23,675 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,675 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,675 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,683 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval training set based on eval_every=2***************
2022-10-24 15:18:23,684 - trainer - INFO - {
  "train_loss": 0.1297053024172783
}
2022-10-24 15:18:23,695 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval development set based on eval_every=2***************
2022-10-24 15:18:23,696 - trainer - INFO - {
  "dev_loss": 0.12966349720954895,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,697 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:18:23,697 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,698 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,699 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,699 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_266
2022-10-24 15:18:23,700 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_272
2022-10-24 15:18:23,705 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_272
2022-10-24 15:18:23,706 - trainer - INFO - 
*****************[epoch: 272, global step: 273] eval training set at end of epoch***************
2022-10-24 15:18:23,706 - trainer - INFO - {
  "train_loss": 0.12969312071800232
}
2022-10-24 15:18:23,706 - trainer - INFO - start training epoch 273
2022-10-24 15:18:23,707 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,707 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,707 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,715 - trainer - INFO - 
*****************[epoch: 273, global step: 274] eval training set at end of epoch***************
2022-10-24 15:18:23,715 - trainer - INFO - {
  "train_loss": 0.12966351211071014
}
2022-10-24 15:18:23,715 - trainer - INFO - start training epoch 274
2022-10-24 15:18:23,716 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,716 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,716 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,723 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval training set based on eval_every=2***************
2022-10-24 15:18:23,723 - trainer - INFO - {
  "train_loss": 0.12964991480112076
}
2022-10-24 15:18:23,731 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval development set based on eval_every=2***************
2022-10-24 15:18:23,731 - trainer - INFO - {
  "dev_loss": 0.1296052783727646,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,732 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:18:23,733 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,734 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,735 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,736 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_268
2022-10-24 15:18:23,739 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_274
2022-10-24 15:18:23,746 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_274
2022-10-24 15:18:23,747 - trainer - INFO - 
*****************[epoch: 274, global step: 275] eval training set at end of epoch***************
2022-10-24 15:18:23,748 - trainer - INFO - {
  "train_loss": 0.12963631749153137
}
2022-10-24 15:18:23,748 - trainer - INFO - start training epoch 275
2022-10-24 15:18:23,748 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,749 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,749 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,759 - trainer - INFO - 
*****************[epoch: 275, global step: 276] eval training set at end of epoch***************
2022-10-24 15:18:23,759 - trainer - INFO - {
  "train_loss": 0.1296052783727646
}
2022-10-24 15:18:23,760 - trainer - INFO - start training epoch 276
2022-10-24 15:18:23,760 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,760 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,761 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,770 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval training set based on eval_every=2***************
2022-10-24 15:18:23,770 - trainer - INFO - {
  "train_loss": 0.12957770377397537
}
2022-10-24 15:18:23,776 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval development set based on eval_every=2***************
2022-10-24 15:18:23,776 - trainer - INFO - {
  "dev_loss": 0.12950760126113892,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,777 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:18:23,777 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,778 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,778 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,779 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_270
2022-10-24 15:18:23,780 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_276
2022-10-24 15:18:23,785 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_276
2022-10-24 15:18:23,786 - trainer - INFO - 
*****************[epoch: 276, global step: 277] eval training set at end of epoch***************
2022-10-24 15:18:23,786 - trainer - INFO - {
  "train_loss": 0.12955012917518616
}
2022-10-24 15:18:23,787 - trainer - INFO - start training epoch 277
2022-10-24 15:18:23,787 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,787 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,788 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,798 - trainer - INFO - 
*****************[epoch: 277, global step: 278] eval training set at end of epoch***************
2022-10-24 15:18:23,798 - trainer - INFO - {
  "train_loss": 0.12950758635997772
}
2022-10-24 15:18:23,799 - trainer - INFO - start training epoch 278
2022-10-24 15:18:23,799 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,799 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,800 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,805 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval training set based on eval_every=2***************
2022-10-24 15:18:23,806 - trainer - INFO - {
  "train_loss": 0.1294882446527481
}
2022-10-24 15:18:23,812 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval development set based on eval_every=2***************
2022-10-24 15:18:23,813 - trainer - INFO - {
  "dev_loss": 0.12943075597286224,
  "dev_best_score_for_loss": -0.12943005561828613
}
2022-10-24 15:18:23,814 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:18:23,817 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,818 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,818 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,819 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_272
2022-10-24 15:18:23,820 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_278
2022-10-24 15:18:23,824 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_278
2022-10-24 15:18:23,825 - trainer - INFO - 
*****************[epoch: 278, global step: 279] eval training set at end of epoch***************
2022-10-24 15:18:23,826 - trainer - INFO - {
  "train_loss": 0.1294689029455185
}
2022-10-24 15:18:23,826 - trainer - INFO - start training epoch 279
2022-10-24 15:18:23,826 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,827 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,828 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,837 - trainer - INFO - 
*****************[epoch: 279, global step: 280] eval training set at end of epoch***************
2022-10-24 15:18:23,837 - trainer - INFO - {
  "train_loss": 0.12943075597286224
}
2022-10-24 15:18:23,837 - trainer - INFO - start training epoch 280
2022-10-24 15:18:23,838 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,838 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,838 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,846 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval training set based on eval_every=2***************
2022-10-24 15:18:23,849 - trainer - INFO - {
  "train_loss": 0.12940436601638794
}
2022-10-24 15:18:23,854 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval development set based on eval_every=2***************
2022-10-24 15:18:23,855 - trainer - INFO - {
  "dev_loss": 0.12932544946670532,
  "dev_best_score_for_loss": -0.12932544946670532
}
2022-10-24 15:18:23,855 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,856 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,857 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,857 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_274
2022-10-24 15:18:23,859 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,862 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,862 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,863 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,863 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,864 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_280
2022-10-24 15:18:23,867 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_280
2022-10-24 15:18:23,868 - trainer - INFO - 
*****************[epoch: 280, global step: 281] eval training set at end of epoch***************
2022-10-24 15:18:23,868 - trainer - INFO - {
  "train_loss": 0.12937797605991364
}
2022-10-24 15:18:23,868 - trainer - INFO - start training epoch 281
2022-10-24 15:18:23,869 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,869 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,869 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,877 - trainer - INFO - 
*****************[epoch: 281, global step: 282] eval training set at end of epoch***************
2022-10-24 15:18:23,877 - trainer - INFO - {
  "train_loss": 0.12932544946670532
}
2022-10-24 15:18:23,878 - trainer - INFO - start training epoch 282
2022-10-24 15:18:23,878 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,878 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,879 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,885 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval training set based on eval_every=2***************
2022-10-24 15:18:23,885 - trainer - INFO - {
  "train_loss": 0.1293109729886055
}
2022-10-24 15:18:23,894 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval development set based on eval_every=2***************
2022-10-24 15:18:23,894 - trainer - INFO - {
  "dev_loss": 0.12925830483436584,
  "dev_best_score_for_loss": -0.12925830483436584
}
2022-10-24 15:18:23,899 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,901 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,901 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,902 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_276
2022-10-24 15:18:23,903 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,907 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,907 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,908 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,909 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,909 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_282
2022-10-24 15:18:23,914 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_282
2022-10-24 15:18:23,915 - trainer - INFO - 
*****************[epoch: 282, global step: 283] eval training set at end of epoch***************
2022-10-24 15:18:23,915 - trainer - INFO - {
  "train_loss": 0.12929649651050568
}
2022-10-24 15:18:23,916 - trainer - INFO - start training epoch 283
2022-10-24 15:18:23,916 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,916 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,916 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,924 - trainer - INFO - 
*****************[epoch: 283, global step: 284] eval training set at end of epoch***************
2022-10-24 15:18:23,924 - trainer - INFO - {
  "train_loss": 0.12925830483436584
}
2022-10-24 15:18:23,925 - trainer - INFO - start training epoch 284
2022-10-24 15:18:23,925 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,925 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,926 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,933 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval training set based on eval_every=2***************
2022-10-24 15:18:23,933 - trainer - INFO - {
  "train_loss": 0.12923328578472137
}
2022-10-24 15:18:23,941 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval development set based on eval_every=2***************
2022-10-24 15:18:23,941 - trainer - INFO - {
  "dev_loss": 0.12915374338626862,
  "dev_best_score_for_loss": -0.12915374338626862
}
2022-10-24 15:18:23,942 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,943 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,943 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,944 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_278
2022-10-24 15:18:23,945 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,949 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,949 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,949 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,950 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,950 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_284
2022-10-24 15:18:23,955 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_284
2022-10-24 15:18:23,956 - trainer - INFO - 
*****************[epoch: 284, global step: 285] eval training set at end of epoch***************
2022-10-24 15:18:23,956 - trainer - INFO - {
  "train_loss": 0.1292082667350769
}
2022-10-24 15:18:23,956 - trainer - INFO - start training epoch 285
2022-10-24 15:18:23,957 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,957 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,957 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,966 - trainer - INFO - 
*****************[epoch: 285, global step: 286] eval training set at end of epoch***************
2022-10-24 15:18:23,967 - trainer - INFO - {
  "train_loss": 0.12915372848510742
}
2022-10-24 15:18:23,967 - trainer - INFO - start training epoch 286
2022-10-24 15:18:23,968 - trainer - INFO - training using device=cuda
2022-10-24 15:18:23,968 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:23,968 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:23,974 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval training set based on eval_every=2***************
2022-10-24 15:18:23,975 - trainer - INFO - {
  "train_loss": 0.129150390625
}
2022-10-24 15:18:23,980 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval development set based on eval_every=2***************
2022-10-24 15:18:23,980 - trainer - INFO - {
  "dev_loss": 0.12911079823970795,
  "dev_best_score_for_loss": -0.12911079823970795
}
2022-10-24 15:18:23,981 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:23,982 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:23,983 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:23,983 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_280
2022-10-24 15:18:23,986 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:23,993 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:23,993 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:23,993 - trainer - INFO -   patience: 200
2022-10-24 15:18:23,994 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:23,994 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_286
2022-10-24 15:18:24,000 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_286
2022-10-24 15:18:24,000 - trainer - INFO - 
*****************[epoch: 286, global step: 287] eval training set at end of epoch***************
2022-10-24 15:18:24,001 - trainer - INFO - {
  "train_loss": 0.12914705276489258
}
2022-10-24 15:18:24,001 - trainer - INFO - start training epoch 287
2022-10-24 15:18:24,001 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,002 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,002 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,009 - trainer - INFO - 
*****************[epoch: 287, global step: 288] eval training set at end of epoch***************
2022-10-24 15:18:24,010 - trainer - INFO - {
  "train_loss": 0.12911079823970795
}
2022-10-24 15:18:24,010 - trainer - INFO - start training epoch 288
2022-10-24 15:18:24,010 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,010 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,011 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,017 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval training set based on eval_every=2***************
2022-10-24 15:18:24,017 - trainer - INFO - {
  "train_loss": 0.12909384816884995
}
2022-10-24 15:18:24,022 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval development set based on eval_every=2***************
2022-10-24 15:18:24,022 - trainer - INFO - {
  "dev_loss": 0.1290491819381714,
  "dev_best_score_for_loss": -0.1290491819381714
}
2022-10-24 15:18:24,023 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:24,024 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:24,024 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:24,025 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_282
2022-10-24 15:18:24,026 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:24,030 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:24,030 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:24,031 - trainer - INFO -   patience: 200
2022-10-24 15:18:24,033 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:24,035 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_288
2022-10-24 15:18:24,039 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_288
2022-10-24 15:18:24,040 - trainer - INFO - 
*****************[epoch: 288, global step: 289] eval training set at end of epoch***************
2022-10-24 15:18:24,041 - trainer - INFO - {
  "train_loss": 0.12907689809799194
}
2022-10-24 15:18:24,041 - trainer - INFO - start training epoch 289
2022-10-24 15:18:24,042 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,042 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,043 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,053 - trainer - INFO - 
*****************[epoch: 289, global step: 290] eval training set at end of epoch***************
2022-10-24 15:18:24,053 - trainer - INFO - {
  "train_loss": 0.1290491819381714
}
2022-10-24 15:18:24,054 - trainer - INFO - start training epoch 290
2022-10-24 15:18:24,054 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,054 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,055 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,064 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval training set based on eval_every=2***************
2022-10-24 15:18:24,064 - trainer - INFO - {
  "train_loss": 0.12903638929128647
}
2022-10-24 15:18:24,070 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval development set based on eval_every=2***************
2022-10-24 15:18:24,071 - trainer - INFO - {
  "dev_loss": 0.12900598347187042,
  "dev_best_score_for_loss": -0.12900598347187042
}
2022-10-24 15:18:24,071 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:24,073 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:24,073 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:24,073 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_284
2022-10-24 15:18:24,075 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:24,082 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:24,083 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:24,083 - trainer - INFO -   patience: 200
2022-10-24 15:18:24,084 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:24,084 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_290
2022-10-24 15:18:24,088 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_290
2022-10-24 15:18:24,089 - trainer - INFO - 
*****************[epoch: 290, global step: 291] eval training set at end of epoch***************
2022-10-24 15:18:24,090 - trainer - INFO - {
  "train_loss": 0.12902359664440155
}
2022-10-24 15:18:24,090 - trainer - INFO - start training epoch 291
2022-10-24 15:18:24,091 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,091 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,092 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,102 - trainer - INFO - 
*****************[epoch: 291, global step: 292] eval training set at end of epoch***************
2022-10-24 15:18:24,102 - trainer - INFO - {
  "train_loss": 0.12900598347187042
}
2022-10-24 15:18:24,103 - trainer - INFO - start training epoch 292
2022-10-24 15:18:24,103 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,103 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,104 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,111 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval training set based on eval_every=2***************
2022-10-24 15:18:24,111 - trainer - INFO - {
  "train_loss": 0.12899449467658997
}
2022-10-24 15:18:24,117 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval development set based on eval_every=2***************
2022-10-24 15:18:24,117 - trainer - INFO - {
  "dev_loss": 0.1289917528629303,
  "dev_best_score_for_loss": -0.1289917528629303
}
2022-10-24 15:18:24,118 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:24,119 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:24,119 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:24,119 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_286
2022-10-24 15:18:24,120 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:24,124 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:24,125 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:24,126 - trainer - INFO -   patience: 200
2022-10-24 15:18:24,127 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:24,130 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_292
2022-10-24 15:18:24,135 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_292
2022-10-24 15:18:24,135 - trainer - INFO - 
*****************[epoch: 292, global step: 293] eval training set at end of epoch***************
2022-10-24 15:18:24,136 - trainer - INFO - {
  "train_loss": 0.1289830058813095
}
2022-10-24 15:18:24,136 - trainer - INFO - start training epoch 293
2022-10-24 15:18:24,136 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,137 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,138 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,146 - trainer - INFO - 
*****************[epoch: 293, global step: 294] eval training set at end of epoch***************
2022-10-24 15:18:24,146 - trainer - INFO - {
  "train_loss": 0.1289917528629303
}
2022-10-24 15:18:24,147 - trainer - INFO - start training epoch 294
2022-10-24 15:18:24,147 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,147 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,148 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,154 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval training set based on eval_every=2***************
2022-10-24 15:18:24,154 - trainer - INFO - {
  "train_loss": 0.12898017466068268
}
2022-10-24 15:18:24,161 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval development set based on eval_every=2***************
2022-10-24 15:18:24,161 - trainer - INFO - {
  "dev_loss": 0.12894268333911896,
  "dev_best_score_for_loss": -0.12894268333911896
}
2022-10-24 15:18:24,162 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:24,163 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:24,163 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:24,163 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_288
2022-10-24 15:18:24,164 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:24,167 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:24,168 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:24,169 - trainer - INFO -   patience: 200
2022-10-24 15:18:24,170 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:24,171 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_294
2022-10-24 15:18:24,175 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_294
2022-10-24 15:18:24,176 - trainer - INFO - 
*****************[epoch: 294, global step: 295] eval training set at end of epoch***************
2022-10-24 15:18:24,176 - trainer - INFO - {
  "train_loss": 0.12896859645843506
}
2022-10-24 15:18:24,177 - trainer - INFO - start training epoch 295
2022-10-24 15:18:24,177 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,177 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,178 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,186 - trainer - INFO - 
*****************[epoch: 295, global step: 296] eval training set at end of epoch***************
2022-10-24 15:18:24,186 - trainer - INFO - {
  "train_loss": 0.12894268333911896
}
2022-10-24 15:18:24,186 - trainer - INFO - start training epoch 296
2022-10-24 15:18:24,187 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,187 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,187 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,194 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval training set based on eval_every=2***************
2022-10-24 15:18:24,194 - trainer - INFO - {
  "train_loss": 0.12894214689731598
}
2022-10-24 15:18:24,200 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval development set based on eval_every=2***************
2022-10-24 15:18:24,201 - trainer - INFO - {
  "dev_loss": 0.1289285123348236,
  "dev_best_score_for_loss": -0.1289285123348236
}
2022-10-24 15:18:24,201 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:24,202 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:24,202 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:24,203 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_290
2022-10-24 15:18:24,204 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd
2022-10-24 15:18:24,206 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd
2022-10-24 15:18:24,207 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:24,207 - trainer - INFO -   patience: 200
2022-10-24 15:18:24,208 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:24,208 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_296
2022-10-24 15:18:24,212 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_296
2022-10-24 15:18:24,212 - trainer - INFO - 
*****************[epoch: 296, global step: 297] eval training set at end of epoch***************
2022-10-24 15:18:24,213 - trainer - INFO - {
  "train_loss": 0.128941610455513
}
2022-10-24 15:18:24,213 - trainer - INFO - start training epoch 297
2022-10-24 15:18:24,213 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,213 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,214 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,223 - trainer - INFO - 
*****************[epoch: 297, global step: 298] eval training set at end of epoch***************
2022-10-24 15:18:24,224 - trainer - INFO - {
  "train_loss": 0.1289285123348236
}
2022-10-24 15:18:24,224 - trainer - INFO - start training epoch 298
2022-10-24 15:18:24,224 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,225 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,225 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,234 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval training set based on eval_every=2***************
2022-10-24 15:18:24,234 - trainer - INFO - {
  "train_loss": 0.12892655283212662
}
2022-10-24 15:18:24,240 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval development set based on eval_every=2***************
2022-10-24 15:18:24,241 - trainer - INFO - {
  "dev_loss": 0.1289321780204773,
  "dev_best_score_for_loss": -0.1289285123348236
}
2022-10-24 15:18:24,241 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:24,242 - trainer - INFO -   patience: 200
2022-10-24 15:18:24,243 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:24,243 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:24,243 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_292
2022-10-24 15:18:24,244 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_298
2022-10-24 15:18:24,249 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_298
2022-10-24 15:18:24,250 - trainer - INFO - 
*****************[epoch: 298, global step: 299] eval training set at end of epoch***************
2022-10-24 15:18:24,250 - trainer - INFO - {
  "train_loss": 0.12892459332942963
}
2022-10-24 15:18:24,251 - trainer - INFO - start training epoch 299
2022-10-24 15:18:24,251 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,251 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,251 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,258 - trainer - INFO - 
*****************[epoch: 299, global step: 300] eval training set at end of epoch***************
2022-10-24 15:18:24,258 - trainer - INFO - {
  "train_loss": 0.1289321631193161
}
2022-10-24 15:18:24,259 - trainer - INFO - start training epoch 300
2022-10-24 15:18:24,259 - trainer - INFO - training using device=cuda
2022-10-24 15:18:24,259 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:24,260 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_tes1_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:24,270 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval training set based on eval_every=2***************
2022-10-24 15:18:24,271 - trainer - INFO - {
  "train_loss": 0.12892848253250122
}
2022-10-24 15:18:24,278 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval development set based on eval_every=2***************
2022-10-24 15:18:24,278 - trainer - INFO - {
  "dev_loss": 0.12893271446228027,
  "dev_best_score_for_loss": -0.1289285123348236
}
2022-10-24 15:18:24,279 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:24,279 - trainer - INFO -   patience: 200
2022-10-24 15:18:24,281 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:24,281 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:24,281 - trainer - INFO -   Remove checkpoint tmp/mlp_tes1_kdd\ck_294
2022-10-24 15:18:24,282 - trainer - INFO -   Save checkpoint to tmp/mlp_tes1_kdd\ck_300
2022-10-24 15:18:24,287 - trainer - INFO - save model to path: tmp/mlp_tes1_kdd\ck_300
2022-10-24 15:18:24,288 - trainer - INFO - 
*****************[epoch: 300, global step: 301] eval training set at end of epoch***************
2022-10-24 15:18:24,288 - trainer - INFO - {
  "train_loss": 0.12892480194568634
}
