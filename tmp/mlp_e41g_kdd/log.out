2022-10-24 15:19:59,202 - trainer - INFO - MLP(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=32, bias=True)
  )
  (activation_layers): ModuleList(
    (0): ReLU(inplace=True)
    (1): ReLU(inplace=True)
    (2): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (linear_output): Linear(in_features=32, out_features=1, bias=True)
)
2022-10-24 15:19:59,203 - trainer - INFO -   Total params: 10625
2022-10-24 15:19:59,204 - trainer - INFO -   Trainable params: 10625
2022-10-24 15:19:59,204 - trainer - INFO -   Non-trainable params: 0
2022-10-24 15:19:59,204 - trainer - INFO -   There are 14  training examples
2022-10-24 15:19:59,205 - trainer - INFO -   There are 14 examples for development
2022-10-24 15:19:59,325 - trainer - INFO - start training epoch 1
2022-10-24 15:19:59,326 - trainer - INFO - training using device=cuda
2022-10-24 15:19:59,328 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:59,329 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,520 - trainer - INFO - 
*****************[epoch: 1, global step: 2] eval training set at end of epoch***************
2022-10-24 15:20:00,520 - trainer - INFO - {
  "train_loss": 604665.0
}
2022-10-24 15:20:00,521 - trainer - INFO - start training epoch 2
2022-10-24 15:20:00,521 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,522 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,522 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,529 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval training set based on eval_every=2***************
2022-10-24 15:20:00,530 - trainer - INFO - {
  "train_loss": 578667.0
}
2022-10-24 15:20:00,536 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval development set based on eval_every=2***************
2022-10-24 15:20:00,537 - trainer - INFO - {
  "dev_loss": 380678.71875,
  "dev_best_score_for_loss": -380678.71875
}
2022-10-24 15:20:00,537 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:00,538 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:20:00,538 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:00,541 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:00,542 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:00,542 - trainer - INFO -   patience: 200
2022-10-24 15:20:00,543 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:20:00,543 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_2
2022-10-24 15:20:00,547 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_2
2022-10-24 15:20:00,550 - trainer - INFO - 
*****************[epoch: 2, global step: 3] eval training set at end of epoch***************
2022-10-24 15:20:00,552 - trainer - INFO - {
  "train_loss": 552669.0
}
2022-10-24 15:20:00,553 - trainer - INFO - start training epoch 3
2022-10-24 15:20:00,553 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,554 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,554 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,565 - trainer - INFO - 
*****************[epoch: 3, global step: 4] eval training set at end of epoch***************
2022-10-24 15:20:00,566 - trainer - INFO - {
  "train_loss": 380678.71875
}
2022-10-24 15:20:00,566 - trainer - INFO - start training epoch 4
2022-10-24 15:20:00,567 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,567 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,568 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,574 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval training set based on eval_every=2***************
2022-10-24 15:20:00,575 - trainer - INFO - {
  "train_loss": 230560.9375
}
2022-10-24 15:20:00,582 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval development set based on eval_every=2***************
2022-10-24 15:20:00,583 - trainer - INFO - {
  "dev_loss": 268838.875,
  "dev_best_score_for_loss": -268838.875
}
2022-10-24 15:20:00,584 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:00,586 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:20:00,586 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:00,589 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:00,590 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:00,590 - trainer - INFO -   patience: 200
2022-10-24 15:20:00,591 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:20:00,591 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_4
2022-10-24 15:20:00,597 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_4
2022-10-24 15:20:00,600 - trainer - INFO - 
*****************[epoch: 4, global step: 5] eval training set at end of epoch***************
2022-10-24 15:20:00,601 - trainer - INFO - {
  "train_loss": 80443.15625
}
2022-10-24 15:20:00,601 - trainer - INFO - start training epoch 5
2022-10-24 15:20:00,602 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,602 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,603 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,613 - trainer - INFO - 
*****************[epoch: 5, global step: 6] eval training set at end of epoch***************
2022-10-24 15:20:00,613 - trainer - INFO - {
  "train_loss": 268838.875
}
2022-10-24 15:20:00,614 - trainer - INFO - start training epoch 6
2022-10-24 15:20:00,615 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,615 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,616 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,623 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval training set based on eval_every=2***************
2022-10-24 15:20:00,624 - trainer - INFO - {
  "train_loss": 167666.21484375
}
2022-10-24 15:20:00,633 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval development set based on eval_every=2***************
2022-10-24 15:20:00,634 - trainer - INFO - {
  "dev_loss": 15525.916015625,
  "dev_best_score_for_loss": -15525.916015625
}
2022-10-24 15:20:00,635 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:00,636 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:00,636 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:00,639 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:00,640 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:00,640 - trainer - INFO -   patience: 200
2022-10-24 15:20:00,641 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:00,644 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_6
2022-10-24 15:20:00,650 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_6
2022-10-24 15:20:00,651 - trainer - INFO - 
*****************[epoch: 6, global step: 7] eval training set at end of epoch***************
2022-10-24 15:20:00,652 - trainer - INFO - {
  "train_loss": 66493.5546875
}
2022-10-24 15:20:00,653 - trainer - INFO - start training epoch 7
2022-10-24 15:20:00,653 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,654 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,654 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,667 - trainer - INFO - 
*****************[epoch: 7, global step: 8] eval training set at end of epoch***************
2022-10-24 15:20:00,667 - trainer - INFO - {
  "train_loss": 15525.916015625
}
2022-10-24 15:20:00,668 - trainer - INFO - start training epoch 8
2022-10-24 15:20:00,668 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,668 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,669 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,676 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval training set based on eval_every=2***************
2022-10-24 15:20:00,677 - trainer - INFO - {
  "train_loss": 49509.0087890625
}
2022-10-24 15:20:00,685 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval development set based on eval_every=2***************
2022-10-24 15:20:00,685 - trainer - INFO - {
  "dev_loss": 127369.4140625,
  "dev_best_score_for_loss": -15525.916015625
}
2022-10-24 15:20:00,686 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:00,689 - trainer - INFO -   patience: 200
2022-10-24 15:20:00,691 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:00,692 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:00,692 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_2
2022-10-24 15:20:00,695 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_8
2022-10-24 15:20:00,701 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_8
2022-10-24 15:20:00,701 - trainer - INFO - 
*****************[epoch: 8, global step: 9] eval training set at end of epoch***************
2022-10-24 15:20:00,702 - trainer - INFO - {
  "train_loss": 83492.1015625
}
2022-10-24 15:20:00,706 - trainer - INFO - start training epoch 9
2022-10-24 15:20:00,707 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,708 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,709 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,719 - trainer - INFO - 
*****************[epoch: 9, global step: 10] eval training set at end of epoch***************
2022-10-24 15:20:00,719 - trainer - INFO - {
  "train_loss": 127369.4140625
}
2022-10-24 15:20:00,720 - trainer - INFO - start training epoch 10
2022-10-24 15:20:00,720 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,721 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,721 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,730 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval training set based on eval_every=2***************
2022-10-24 15:20:00,731 - trainer - INFO - {
  "train_loss": 125283.6171875
}
2022-10-24 15:20:00,741 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval development set based on eval_every=2***************
2022-10-24 15:20:00,741 - trainer - INFO - {
  "dev_loss": 80802.75,
  "dev_best_score_for_loss": -15525.916015625
}
2022-10-24 15:20:00,742 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:00,743 - trainer - INFO -   patience: 200
2022-10-24 15:20:00,744 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:00,745 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:00,745 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_4
2022-10-24 15:20:00,748 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_10
2022-10-24 15:20:00,753 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_10
2022-10-24 15:20:00,754 - trainer - INFO - 
*****************[epoch: 10, global step: 11] eval training set at end of epoch***************
2022-10-24 15:20:00,755 - trainer - INFO - {
  "train_loss": 123197.8203125
}
2022-10-24 15:20:00,756 - trainer - INFO - start training epoch 11
2022-10-24 15:20:00,756 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,757 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,758 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,771 - trainer - INFO - 
*****************[epoch: 11, global step: 12] eval training set at end of epoch***************
2022-10-24 15:20:00,771 - trainer - INFO - {
  "train_loss": 80802.75
}
2022-10-24 15:20:00,772 - trainer - INFO - start training epoch 12
2022-10-24 15:20:00,772 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,773 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,774 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,783 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval training set based on eval_every=2***************
2022-10-24 15:20:00,783 - trainer - INFO - {
  "train_loss": 53614.5888671875
}
2022-10-24 15:20:00,789 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval development set based on eval_every=2***************
2022-10-24 15:20:00,790 - trainer - INFO - {
  "dev_loss": 10081.88671875,
  "dev_best_score_for_loss": -10081.88671875
}
2022-10-24 15:20:00,790 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:00,792 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:00,792 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:00,792 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_6
2022-10-24 15:20:00,794 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:00,798 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:00,801 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:00,801 - trainer - INFO -   patience: 200
2022-10-24 15:20:00,804 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:00,805 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_12
2022-10-24 15:20:00,810 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_12
2022-10-24 15:20:00,812 - trainer - INFO - 
*****************[epoch: 12, global step: 13] eval training set at end of epoch***************
2022-10-24 15:20:00,813 - trainer - INFO - {
  "train_loss": 26426.427734375
}
2022-10-24 15:20:00,813 - trainer - INFO - start training epoch 13
2022-10-24 15:20:00,818 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,818 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,820 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,830 - trainer - INFO - 
*****************[epoch: 13, global step: 14] eval training set at end of epoch***************
2022-10-24 15:20:00,831 - trainer - INFO - {
  "train_loss": 10081.88671875
}
2022-10-24 15:20:00,831 - trainer - INFO - start training epoch 14
2022-10-24 15:20:00,832 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,832 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,833 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,839 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval training set based on eval_every=2***************
2022-10-24 15:20:00,840 - trainer - INFO - {
  "train_loss": 34243.28125
}
2022-10-24 15:20:00,849 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval development set based on eval_every=2***************
2022-10-24 15:20:00,850 - trainer - INFO - {
  "dev_loss": 76185.71875,
  "dev_best_score_for_loss": -10081.88671875
}
2022-10-24 15:20:00,851 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:00,852 - trainer - INFO -   patience: 200
2022-10-24 15:20:00,853 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:00,853 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:00,854 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_8
2022-10-24 15:20:00,855 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_14
2022-10-24 15:20:00,861 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_14
2022-10-24 15:20:00,863 - trainer - INFO - 
*****************[epoch: 14, global step: 15] eval training set at end of epoch***************
2022-10-24 15:20:00,864 - trainer - INFO - {
  "train_loss": 58404.67578125
}
2022-10-24 15:20:00,865 - trainer - INFO - start training epoch 15
2022-10-24 15:20:00,865 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,866 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,866 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,874 - trainer - INFO - 
*****************[epoch: 15, global step: 16] eval training set at end of epoch***************
2022-10-24 15:20:00,875 - trainer - INFO - {
  "train_loss": 76185.71875
}
2022-10-24 15:20:00,875 - trainer - INFO - start training epoch 16
2022-10-24 15:20:00,876 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,876 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,877 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,885 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval training set based on eval_every=2***************
2022-10-24 15:20:00,885 - trainer - INFO - {
  "train_loss": 53124.5
}
2022-10-24 15:20:00,898 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval development set based on eval_every=2***************
2022-10-24 15:20:00,899 - trainer - INFO - {
  "dev_loss": 7410.79931640625,
  "dev_best_score_for_loss": -7410.79931640625
}
2022-10-24 15:20:00,900 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:00,901 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:00,901 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:00,902 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_10
2022-10-24 15:20:00,903 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:00,907 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:00,907 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:00,908 - trainer - INFO -   patience: 200
2022-10-24 15:20:00,909 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:00,909 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_16
2022-10-24 15:20:00,913 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_16
2022-10-24 15:20:00,914 - trainer - INFO - 
*****************[epoch: 16, global step: 17] eval training set at end of epoch***************
2022-10-24 15:20:00,915 - trainer - INFO - {
  "train_loss": 30063.28125
}
2022-10-24 15:20:00,915 - trainer - INFO - start training epoch 17
2022-10-24 15:20:00,916 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,916 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,917 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,930 - trainer - INFO - 
*****************[epoch: 17, global step: 18] eval training set at end of epoch***************
2022-10-24 15:20:00,931 - trainer - INFO - {
  "train_loss": 7410.79931640625
}
2022-10-24 15:20:00,931 - trainer - INFO - start training epoch 18
2022-10-24 15:20:00,932 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,932 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,933 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,941 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval training set based on eval_every=2***************
2022-10-24 15:20:00,941 - trainer - INFO - {
  "train_loss": 15377.905517578125
}
2022-10-24 15:20:00,952 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval development set based on eval_every=2***************
2022-10-24 15:20:00,953 - trainer - INFO - {
  "dev_loss": 43778.171875,
  "dev_best_score_for_loss": -7410.79931640625
}
2022-10-24 15:20:00,954 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:00,954 - trainer - INFO -   patience: 200
2022-10-24 15:20:00,955 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:00,956 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:00,956 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_12
2022-10-24 15:20:00,958 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_18
2022-10-24 15:20:00,963 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_18
2022-10-24 15:20:00,964 - trainer - INFO - 
*****************[epoch: 18, global step: 19] eval training set at end of epoch***************
2022-10-24 15:20:00,964 - trainer - INFO - {
  "train_loss": 23345.01171875
}
2022-10-24 15:20:00,965 - trainer - INFO - start training epoch 19
2022-10-24 15:20:00,965 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,968 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,969 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,978 - trainer - INFO - 
*****************[epoch: 19, global step: 20] eval training set at end of epoch***************
2022-10-24 15:20:00,978 - trainer - INFO - {
  "train_loss": 43778.171875
}
2022-10-24 15:20:00,980 - trainer - INFO - start training epoch 20
2022-10-24 15:20:00,980 - trainer - INFO - training using device=cuda
2022-10-24 15:20:00,981 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:00,981 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:00,991 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval training set based on eval_every=2***************
2022-10-24 15:20:00,991 - trainer - INFO - {
  "train_loss": 45727.78125
}
2022-10-24 15:20:01,002 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval development set based on eval_every=2***************
2022-10-24 15:20:01,002 - trainer - INFO - {
  "dev_loss": 33637.91796875,
  "dev_best_score_for_loss": -7410.79931640625
}
2022-10-24 15:20:01,003 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:01,004 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,005 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,006 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,006 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_14
2022-10-24 15:20:01,008 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_20
2022-10-24 15:20:01,014 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_20
2022-10-24 15:20:01,015 - trainer - INFO - 
*****************[epoch: 20, global step: 21] eval training set at end of epoch***************
2022-10-24 15:20:01,016 - trainer - INFO - {
  "train_loss": 47677.390625
}
2022-10-24 15:20:01,016 - trainer - INFO - start training epoch 21
2022-10-24 15:20:01,017 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,017 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,018 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,027 - trainer - INFO - 
*****************[epoch: 21, global step: 22] eval training set at end of epoch***************
2022-10-24 15:20:01,027 - trainer - INFO - {
  "train_loss": 33637.91796875
}
2022-10-24 15:20:01,028 - trainer - INFO - start training epoch 22
2022-10-24 15:20:01,033 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,034 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,036 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,047 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval training set based on eval_every=2***************
2022-10-24 15:20:01,048 - trainer - INFO - {
  "train_loss": 23740.41943359375
}
2022-10-24 15:20:01,055 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval development set based on eval_every=2***************
2022-10-24 15:20:01,056 - trainer - INFO - {
  "dev_loss": 7726.44140625,
  "dev_best_score_for_loss": -7410.79931640625
}
2022-10-24 15:20:01,057 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:01,057 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,058 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,058 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,060 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_16
2022-10-24 15:20:01,062 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_22
2022-10-24 15:20:01,066 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_22
2022-10-24 15:20:01,067 - trainer - INFO - 
*****************[epoch: 22, global step: 23] eval training set at end of epoch***************
2022-10-24 15:20:01,068 - trainer - INFO - {
  "train_loss": 13842.9208984375
}
2022-10-24 15:20:01,068 - trainer - INFO - start training epoch 23
2022-10-24 15:20:01,069 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,069 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,070 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,078 - trainer - INFO - 
*****************[epoch: 23, global step: 24] eval training set at end of epoch***************
2022-10-24 15:20:01,079 - trainer - INFO - {
  "train_loss": 7726.44140625
}
2022-10-24 15:20:01,081 - trainer - INFO - start training epoch 24
2022-10-24 15:20:01,082 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,082 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,083 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,091 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval training set based on eval_every=2***************
2022-10-24 15:20:01,092 - trainer - INFO - {
  "train_loss": 14954.96484375
}
2022-10-24 15:20:01,102 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval development set based on eval_every=2***************
2022-10-24 15:20:01,103 - trainer - INFO - {
  "dev_loss": 32097.408203125,
  "dev_best_score_for_loss": -7410.79931640625
}
2022-10-24 15:20:01,104 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:01,104 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,106 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,106 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,107 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_18
2022-10-24 15:20:01,108 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_24
2022-10-24 15:20:01,113 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_24
2022-10-24 15:20:01,114 - trainer - INFO - 
*****************[epoch: 24, global step: 25] eval training set at end of epoch***************
2022-10-24 15:20:01,114 - trainer - INFO - {
  "train_loss": 22183.48828125
}
2022-10-24 15:20:01,115 - trainer - INFO - start training epoch 25
2022-10-24 15:20:01,115 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,116 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,116 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,126 - trainer - INFO - 
*****************[epoch: 25, global step: 26] eval training set at end of epoch***************
2022-10-24 15:20:01,127 - trainer - INFO - {
  "train_loss": 32097.408203125
}
2022-10-24 15:20:01,128 - trainer - INFO - start training epoch 26
2022-10-24 15:20:01,128 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,129 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,130 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,141 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval training set based on eval_every=2***************
2022-10-24 15:20:01,141 - trainer - INFO - {
  "train_loss": 26006.69921875
}
2022-10-24 15:20:01,151 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval development set based on eval_every=2***************
2022-10-24 15:20:01,151 - trainer - INFO - {
  "dev_loss": 7492.0400390625,
  "dev_best_score_for_loss": -7410.79931640625
}
2022-10-24 15:20:01,153 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:20:01,153 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,154 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,154 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,155 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_20
2022-10-24 15:20:01,157 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_26
2022-10-24 15:20:01,161 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_26
2022-10-24 15:20:01,165 - trainer - INFO - 
*****************[epoch: 26, global step: 27] eval training set at end of epoch***************
2022-10-24 15:20:01,165 - trainer - INFO - {
  "train_loss": 19915.990234375
}
2022-10-24 15:20:01,166 - trainer - INFO - start training epoch 27
2022-10-24 15:20:01,166 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,167 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,168 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,180 - trainer - INFO - 
*****************[epoch: 27, global step: 28] eval training set at end of epoch***************
2022-10-24 15:20:01,181 - trainer - INFO - {
  "train_loss": 7492.0400390625
}
2022-10-24 15:20:01,182 - trainer - INFO - start training epoch 28
2022-10-24 15:20:01,182 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,182 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,183 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,192 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval training set based on eval_every=2***************
2022-10-24 15:20:01,193 - trainer - INFO - {
  "train_loss": 8778.39306640625
}
2022-10-24 15:20:01,204 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval development set based on eval_every=2***************
2022-10-24 15:20:01,205 - trainer - INFO - {
  "dev_loss": 18720.859375,
  "dev_best_score_for_loss": -7410.79931640625
}
2022-10-24 15:20:01,205 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:20:01,206 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,207 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,207 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,208 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_22
2022-10-24 15:20:01,210 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_28
2022-10-24 15:20:01,217 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_28
2022-10-24 15:20:01,220 - trainer - INFO - 
*****************[epoch: 28, global step: 29] eval training set at end of epoch***************
2022-10-24 15:20:01,220 - trainer - INFO - {
  "train_loss": 10064.74609375
}
2022-10-24 15:20:01,221 - trainer - INFO - start training epoch 29
2022-10-24 15:20:01,221 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,222 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,223 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,234 - trainer - INFO - 
*****************[epoch: 29, global step: 30] eval training set at end of epoch***************
2022-10-24 15:20:01,235 - trainer - INFO - {
  "train_loss": 18720.859375
}
2022-10-24 15:20:01,236 - trainer - INFO - start training epoch 30
2022-10-24 15:20:01,236 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,237 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,237 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,247 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval training set based on eval_every=2***************
2022-10-24 15:20:01,247 - trainer - INFO - {
  "train_loss": 20135.0517578125
}
2022-10-24 15:20:01,255 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval development set based on eval_every=2***************
2022-10-24 15:20:01,255 - trainer - INFO - {
  "dev_loss": 15894.3359375,
  "dev_best_score_for_loss": -7410.79931640625
}
2022-10-24 15:20:01,256 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:20:01,257 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,258 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,258 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,259 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_24
2022-10-24 15:20:01,260 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_30
2022-10-24 15:20:01,267 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_30
2022-10-24 15:20:01,268 - trainer - INFO - 
*****************[epoch: 30, global step: 31] eval training set at end of epoch***************
2022-10-24 15:20:01,268 - trainer - INFO - {
  "train_loss": 21549.244140625
}
2022-10-24 15:20:01,269 - trainer - INFO - start training epoch 31
2022-10-24 15:20:01,270 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,270 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,270 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,280 - trainer - INFO - 
*****************[epoch: 31, global step: 32] eval training set at end of epoch***************
2022-10-24 15:20:01,281 - trainer - INFO - {
  "train_loss": 15894.3359375
}
2022-10-24 15:20:01,281 - trainer - INFO - start training epoch 32
2022-10-24 15:20:01,282 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,282 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,282 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,289 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval training set based on eval_every=2***************
2022-10-24 15:20:01,290 - trainer - INFO - {
  "train_loss": 12010.02978515625
}
2022-10-24 15:20:01,301 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval development set based on eval_every=2***************
2022-10-24 15:20:01,302 - trainer - INFO - {
  "dev_loss": 7197.2265625,
  "dev_best_score_for_loss": -7197.2265625
}
2022-10-24 15:20:01,302 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:01,304 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,304 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,304 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_26
2022-10-24 15:20:01,306 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:01,311 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:01,314 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:01,314 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,315 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:01,319 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_32
2022-10-24 15:20:01,324 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_32
2022-10-24 15:20:01,326 - trainer - INFO - 
*****************[epoch: 32, global step: 33] eval training set at end of epoch***************
2022-10-24 15:20:01,327 - trainer - INFO - {
  "train_loss": 8125.7236328125
}
2022-10-24 15:20:01,329 - trainer - INFO - start training epoch 33
2022-10-24 15:20:01,330 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,330 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,331 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,342 - trainer - INFO - 
*****************[epoch: 33, global step: 34] eval training set at end of epoch***************
2022-10-24 15:20:01,343 - trainer - INFO - {
  "train_loss": 7197.2255859375
}
2022-10-24 15:20:01,343 - trainer - INFO - start training epoch 34
2022-10-24 15:20:01,346 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,347 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,348 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,357 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval training set based on eval_every=2***************
2022-10-24 15:20:01,358 - trainer - INFO - {
  "train_loss": 10231.01953125
}
2022-10-24 15:20:01,367 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval development set based on eval_every=2***************
2022-10-24 15:20:01,367 - trainer - INFO - {
  "dev_loss": 15346.94921875,
  "dev_best_score_for_loss": -7197.2265625
}
2022-10-24 15:20:01,368 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:01,370 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,371 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,372 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,372 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_28
2022-10-24 15:20:01,374 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_34
2022-10-24 15:20:01,380 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_34
2022-10-24 15:20:01,381 - trainer - INFO - 
*****************[epoch: 34, global step: 35] eval training set at end of epoch***************
2022-10-24 15:20:01,381 - trainer - INFO - {
  "train_loss": 13264.8134765625
}
2022-10-24 15:20:01,382 - trainer - INFO - start training epoch 35
2022-10-24 15:20:01,382 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,383 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,383 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,394 - trainer - INFO - 
*****************[epoch: 35, global step: 36] eval training set at end of epoch***************
2022-10-24 15:20:01,395 - trainer - INFO - {
  "train_loss": 15346.94921875
}
2022-10-24 15:20:01,395 - trainer - INFO - start training epoch 36
2022-10-24 15:20:01,398 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,399 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,400 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,409 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval training set based on eval_every=2***************
2022-10-24 15:20:01,410 - trainer - INFO - {
  "train_loss": 12611.9140625
}
2022-10-24 15:20:01,420 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval development set based on eval_every=2***************
2022-10-24 15:20:01,420 - trainer - INFO - {
  "dev_loss": 6076.00830078125,
  "dev_best_score_for_loss": -6076.00830078125
}
2022-10-24 15:20:01,423 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:01,425 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,426 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,426 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_30
2022-10-24 15:20:01,428 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:01,432 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:01,433 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:01,434 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,437 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:01,437 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_36
2022-10-24 15:20:01,443 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_36
2022-10-24 15:20:01,444 - trainer - INFO - 
*****************[epoch: 36, global step: 37] eval training set at end of epoch***************
2022-10-24 15:20:01,445 - trainer - INFO - {
  "train_loss": 9876.87890625
}
2022-10-24 15:20:01,449 - trainer - INFO - start training epoch 37
2022-10-24 15:20:01,450 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,451 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,452 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,464 - trainer - INFO - 
*****************[epoch: 37, global step: 38] eval training set at end of epoch***************
2022-10-24 15:20:01,465 - trainer - INFO - {
  "train_loss": 6076.00830078125
}
2022-10-24 15:20:01,466 - trainer - INFO - start training epoch 38
2022-10-24 15:20:01,467 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,467 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,468 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,483 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval training set based on eval_every=2***************
2022-10-24 15:20:01,484 - trainer - INFO - {
  "train_loss": 7139.719482421875
}
2022-10-24 15:20:01,495 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval development set based on eval_every=2***************
2022-10-24 15:20:01,499 - trainer - INFO - {
  "dev_loss": 11393.5986328125,
  "dev_best_score_for_loss": -6076.00830078125
}
2022-10-24 15:20:01,500 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:01,502 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,503 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,504 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,505 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_32
2022-10-24 15:20:01,507 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_38
2022-10-24 15:20:01,511 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_38
2022-10-24 15:20:01,513 - trainer - INFO - 
*****************[epoch: 38, global step: 39] eval training set at end of epoch***************
2022-10-24 15:20:01,513 - trainer - INFO - {
  "train_loss": 8203.4306640625
}
2022-10-24 15:20:01,514 - trainer - INFO - start training epoch 39
2022-10-24 15:20:01,515 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,516 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,517 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,530 - trainer - INFO - 
*****************[epoch: 39, global step: 40] eval training set at end of epoch***************
2022-10-24 15:20:01,532 - trainer - INFO - {
  "train_loss": 11393.5986328125
}
2022-10-24 15:20:01,532 - trainer - INFO - start training epoch 40
2022-10-24 15:20:01,536 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,538 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,539 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,556 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval training set based on eval_every=2***************
2022-10-24 15:20:01,557 - trainer - INFO - {
  "train_loss": 11155.00390625
}
2022-10-24 15:20:01,566 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval development set based on eval_every=2***************
2022-10-24 15:20:01,568 - trainer - INFO - {
  "dev_loss": 7511.369140625,
  "dev_best_score_for_loss": -6076.00830078125
}
2022-10-24 15:20:01,569 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:01,570 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,572 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,573 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,574 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_34
2022-10-24 15:20:01,577 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_40
2022-10-24 15:20:01,585 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_40
2022-10-24 15:20:01,587 - trainer - INFO - 
*****************[epoch: 40, global step: 41] eval training set at end of epoch***************
2022-10-24 15:20:01,588 - trainer - INFO - {
  "train_loss": 10916.4091796875
}
2022-10-24 15:20:01,590 - trainer - INFO - start training epoch 41
2022-10-24 15:20:01,591 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,591 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,592 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,608 - trainer - INFO - 
*****************[epoch: 41, global step: 42] eval training set at end of epoch***************
2022-10-24 15:20:01,608 - trainer - INFO - {
  "train_loss": 7511.36962890625
}
2022-10-24 15:20:01,610 - trainer - INFO - start training epoch 42
2022-10-24 15:20:01,614 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,615 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,616 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,632 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval training set based on eval_every=2***************
2022-10-24 15:20:01,634 - trainer - INFO - {
  "train_loss": 6579.9375
}
2022-10-24 15:20:01,644 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval development set based on eval_every=2***************
2022-10-24 15:20:01,644 - trainer - INFO - {
  "dev_loss": 7470.89990234375,
  "dev_best_score_for_loss": -6076.00830078125
}
2022-10-24 15:20:01,648 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:01,650 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,651 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,651 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,652 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_36
2022-10-24 15:20:01,653 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_42
2022-10-24 15:20:01,658 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_42
2022-10-24 15:20:01,661 - trainer - INFO - 
*****************[epoch: 42, global step: 43] eval training set at end of epoch***************
2022-10-24 15:20:01,662 - trainer - INFO - {
  "train_loss": 5648.50537109375
}
2022-10-24 15:20:01,662 - trainer - INFO - start training epoch 43
2022-10-24 15:20:01,663 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,663 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,665 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,675 - trainer - INFO - 
*****************[epoch: 43, global step: 44] eval training set at end of epoch***************
2022-10-24 15:20:01,676 - trainer - INFO - {
  "train_loss": 7470.89990234375
}
2022-10-24 15:20:01,678 - trainer - INFO - start training epoch 44
2022-10-24 15:20:01,679 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,681 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,681 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,689 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval training set based on eval_every=2***************
2022-10-24 15:20:01,690 - trainer - INFO - {
  "train_loss": 8348.041748046875
}
2022-10-24 15:20:01,699 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval development set based on eval_every=2***************
2022-10-24 15:20:01,700 - trainer - INFO - {
  "dev_loss": 7580.9677734375,
  "dev_best_score_for_loss": -6076.00830078125
}
2022-10-24 15:20:01,701 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:01,701 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,702 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,703 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,703 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_38
2022-10-24 15:20:01,704 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_44
2022-10-24 15:20:01,709 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_44
2022-10-24 15:20:01,711 - trainer - INFO - 
*****************[epoch: 44, global step: 45] eval training set at end of epoch***************
2022-10-24 15:20:01,712 - trainer - INFO - {
  "train_loss": 9225.18359375
}
2022-10-24 15:20:01,715 - trainer - INFO - start training epoch 45
2022-10-24 15:20:01,716 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,716 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,717 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,727 - trainer - INFO - 
*****************[epoch: 45, global step: 46] eval training set at end of epoch***************
2022-10-24 15:20:01,728 - trainer - INFO - {
  "train_loss": 7580.9677734375
}
2022-10-24 15:20:01,728 - trainer - INFO - start training epoch 46
2022-10-24 15:20:01,731 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,732 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,733 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,742 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval training set based on eval_every=2***************
2022-10-24 15:20:01,743 - trainer - INFO - {
  "train_loss": 6502.584228515625
}
2022-10-24 15:20:01,751 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval development set based on eval_every=2***************
2022-10-24 15:20:01,752 - trainer - INFO - {
  "dev_loss": 5824.5087890625,
  "dev_best_score_for_loss": -5824.5087890625
}
2022-10-24 15:20:01,753 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:01,754 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,754 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,755 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_40
2022-10-24 15:20:01,757 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:01,761 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:01,761 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:01,761 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,763 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:01,763 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_46
2022-10-24 15:20:01,768 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_46
2022-10-24 15:20:01,769 - trainer - INFO - 
*****************[epoch: 46, global step: 47] eval training set at end of epoch***************
2022-10-24 15:20:01,770 - trainer - INFO - {
  "train_loss": 5424.20068359375
}
2022-10-24 15:20:01,770 - trainer - INFO - start training epoch 47
2022-10-24 15:20:01,771 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,771 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,772 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,783 - trainer - INFO - 
*****************[epoch: 47, global step: 48] eval training set at end of epoch***************
2022-10-24 15:20:01,783 - trainer - INFO - {
  "train_loss": 5824.5087890625
}
2022-10-24 15:20:01,784 - trainer - INFO - start training epoch 48
2022-10-24 15:20:01,784 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,784 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,785 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,796 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval training set based on eval_every=2***************
2022-10-24 15:20:01,797 - trainer - INFO - {
  "train_loss": 6543.225341796875
}
2022-10-24 15:20:01,806 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval development set based on eval_every=2***************
2022-10-24 15:20:01,806 - trainer - INFO - {
  "dev_loss": 7141.78759765625,
  "dev_best_score_for_loss": -5824.5087890625
}
2022-10-24 15:20:01,807 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:01,807 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,808 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,809 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,809 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_42
2022-10-24 15:20:01,811 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_48
2022-10-24 15:20:01,816 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_48
2022-10-24 15:20:01,820 - trainer - INFO - 
*****************[epoch: 48, global step: 49] eval training set at end of epoch***************
2022-10-24 15:20:01,821 - trainer - INFO - {
  "train_loss": 7261.94189453125
}
2022-10-24 15:20:01,823 - trainer - INFO - start training epoch 49
2022-10-24 15:20:01,825 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,826 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,827 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,837 - trainer - INFO - 
*****************[epoch: 49, global step: 50] eval training set at end of epoch***************
2022-10-24 15:20:01,838 - trainer - INFO - {
  "train_loss": 7141.78759765625
}
2022-10-24 15:20:01,838 - trainer - INFO - start training epoch 50
2022-10-24 15:20:01,839 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,839 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,839 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,846 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval training set based on eval_every=2***************
2022-10-24 15:20:01,846 - trainer - INFO - {
  "train_loss": 6372.5
}
2022-10-24 15:20:01,854 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval development set based on eval_every=2***************
2022-10-24 15:20:01,854 - trainer - INFO - {
  "dev_loss": 4871.248046875,
  "dev_best_score_for_loss": -4871.248046875
}
2022-10-24 15:20:01,855 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:01,856 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,856 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,857 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_44
2022-10-24 15:20:01,858 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:01,861 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:01,861 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:01,862 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,863 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:01,864 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_50
2022-10-24 15:20:01,870 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_50
2022-10-24 15:20:01,871 - trainer - INFO - 
*****************[epoch: 50, global step: 51] eval training set at end of epoch***************
2022-10-24 15:20:01,871 - trainer - INFO - {
  "train_loss": 5603.21240234375
}
2022-10-24 15:20:01,872 - trainer - INFO - start training epoch 51
2022-10-24 15:20:01,872 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,873 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,873 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,884 - trainer - INFO - 
*****************[epoch: 51, global step: 52] eval training set at end of epoch***************
2022-10-24 15:20:01,884 - trainer - INFO - {
  "train_loss": 4871.248046875
}
2022-10-24 15:20:01,885 - trainer - INFO - start training epoch 52
2022-10-24 15:20:01,885 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,886 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,887 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,895 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval training set based on eval_every=2***************
2022-10-24 15:20:01,895 - trainer - INFO - {
  "train_loss": 5298.150146484375
}
2022-10-24 15:20:01,907 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval development set based on eval_every=2***************
2022-10-24 15:20:01,908 - trainer - INFO - {
  "dev_loss": 6216.2021484375,
  "dev_best_score_for_loss": -4871.248046875
}
2022-10-24 15:20:01,909 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:01,909 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,911 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,911 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,913 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_46
2022-10-24 15:20:01,915 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_52
2022-10-24 15:20:01,919 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_52
2022-10-24 15:20:01,920 - trainer - INFO - 
*****************[epoch: 52, global step: 53] eval training set at end of epoch***************
2022-10-24 15:20:01,921 - trainer - INFO - {
  "train_loss": 5725.05224609375
}
2022-10-24 15:20:01,921 - trainer - INFO - start training epoch 53
2022-10-24 15:20:01,922 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,922 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,923 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,933 - trainer - INFO - 
*****************[epoch: 53, global step: 54] eval training set at end of epoch***************
2022-10-24 15:20:01,934 - trainer - INFO - {
  "train_loss": 6216.20166015625
}
2022-10-24 15:20:01,936 - trainer - INFO - start training epoch 54
2022-10-24 15:20:01,937 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,937 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,938 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,946 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval training set based on eval_every=2***************
2022-10-24 15:20:01,947 - trainer - INFO - {
  "train_loss": 5741.0830078125
}
2022-10-24 15:20:01,954 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval development set based on eval_every=2***************
2022-10-24 15:20:01,955 - trainer - INFO - {
  "dev_loss": 4487.359375,
  "dev_best_score_for_loss": -4487.359375
}
2022-10-24 15:20:01,955 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:01,957 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:01,957 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:01,957 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_48
2022-10-24 15:20:01,960 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:01,964 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:01,965 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:01,965 - trainer - INFO -   patience: 200
2022-10-24 15:20:01,966 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:01,967 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_54
2022-10-24 15:20:01,971 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_54
2022-10-24 15:20:01,972 - trainer - INFO - 
*****************[epoch: 54, global step: 55] eval training set at end of epoch***************
2022-10-24 15:20:01,973 - trainer - INFO - {
  "train_loss": 5265.96435546875
}
2022-10-24 15:20:01,973 - trainer - INFO - start training epoch 55
2022-10-24 15:20:01,974 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,975 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,976 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:01,986 - trainer - INFO - 
*****************[epoch: 55, global step: 56] eval training set at end of epoch***************
2022-10-24 15:20:01,987 - trainer - INFO - {
  "train_loss": 4487.359375
}
2022-10-24 15:20:01,987 - trainer - INFO - start training epoch 56
2022-10-24 15:20:01,988 - trainer - INFO - training using device=cuda
2022-10-24 15:20:01,988 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:01,989 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,001 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval training set based on eval_every=2***************
2022-10-24 15:20:02,002 - trainer - INFO - {
  "train_loss": 4679.836669921875
}
2022-10-24 15:20:02,008 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval development set based on eval_every=2***************
2022-10-24 15:20:02,009 - trainer - INFO - {
  "dev_loss": 5318.1494140625,
  "dev_best_score_for_loss": -4487.359375
}
2022-10-24 15:20:02,010 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:02,011 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,013 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,013 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,014 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_50
2022-10-24 15:20:02,015 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_56
2022-10-24 15:20:02,019 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_56
2022-10-24 15:20:02,021 - trainer - INFO - 
*****************[epoch: 56, global step: 57] eval training set at end of epoch***************
2022-10-24 15:20:02,022 - trainer - INFO - {
  "train_loss": 4872.31396484375
}
2022-10-24 15:20:02,022 - trainer - INFO - start training epoch 57
2022-10-24 15:20:02,023 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,023 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,024 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,033 - trainer - INFO - 
*****************[epoch: 57, global step: 58] eval training set at end of epoch***************
2022-10-24 15:20:02,033 - trainer - INFO - {
  "train_loss": 5318.1494140625
}
2022-10-24 15:20:02,034 - trainer - INFO - start training epoch 58
2022-10-24 15:20:02,035 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,035 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,037 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,047 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval training set based on eval_every=2***************
2022-10-24 15:20:02,048 - trainer - INFO - {
  "train_loss": 5093.984375
}
2022-10-24 15:20:02,054 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval development set based on eval_every=2***************
2022-10-24 15:20:02,054 - trainer - INFO - {
  "dev_loss": 4174.451171875,
  "dev_best_score_for_loss": -4174.451171875
}
2022-10-24 15:20:02,055 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,057 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,057 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,057 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_52
2022-10-24 15:20:02,059 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,062 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,064 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,065 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,066 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,066 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_58
2022-10-24 15:20:02,070 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_58
2022-10-24 15:20:02,071 - trainer - INFO - 
*****************[epoch: 58, global step: 59] eval training set at end of epoch***************
2022-10-24 15:20:02,071 - trainer - INFO - {
  "train_loss": 4869.8193359375
}
2022-10-24 15:20:02,072 - trainer - INFO - start training epoch 59
2022-10-24 15:20:02,072 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,073 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,073 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,081 - trainer - INFO - 
*****************[epoch: 59, global step: 60] eval training set at end of epoch***************
2022-10-24 15:20:02,081 - trainer - INFO - {
  "train_loss": 4174.451171875
}
2022-10-24 15:20:02,082 - trainer - INFO - start training epoch 60
2022-10-24 15:20:02,083 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,084 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,086 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,099 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval training set based on eval_every=2***************
2022-10-24 15:20:02,100 - trainer - INFO - {
  "train_loss": 4194.154541015625
}
2022-10-24 15:20:02,108 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval development set based on eval_every=2***************
2022-10-24 15:20:02,108 - trainer - INFO - {
  "dev_loss": 4583.12744140625,
  "dev_best_score_for_loss": -4174.451171875
}
2022-10-24 15:20:02,109 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:02,110 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,112 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,112 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,113 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_54
2022-10-24 15:20:02,116 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_60
2022-10-24 15:20:02,121 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_60
2022-10-24 15:20:02,122 - trainer - INFO - 
*****************[epoch: 60, global step: 61] eval training set at end of epoch***************
2022-10-24 15:20:02,123 - trainer - INFO - {
  "train_loss": 4213.85791015625
}
2022-10-24 15:20:02,123 - trainer - INFO - start training epoch 61
2022-10-24 15:20:02,124 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,124 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,125 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,134 - trainer - INFO - 
*****************[epoch: 61, global step: 62] eval training set at end of epoch***************
2022-10-24 15:20:02,135 - trainer - INFO - {
  "train_loss": 4583.12744140625
}
2022-10-24 15:20:02,135 - trainer - INFO - start training epoch 62
2022-10-24 15:20:02,136 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,136 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,137 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,149 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval training set based on eval_every=2***************
2022-10-24 15:20:02,149 - trainer - INFO - {
  "train_loss": 4458.793701171875
}
2022-10-24 15:20:02,155 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval development set based on eval_every=2***************
2022-10-24 15:20:02,156 - trainer - INFO - {
  "dev_loss": 3802.275634765625,
  "dev_best_score_for_loss": -3802.275634765625
}
2022-10-24 15:20:02,157 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,158 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,158 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,158 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_56
2022-10-24 15:20:02,160 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,167 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,167 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,167 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,169 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,169 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_62
2022-10-24 15:20:02,174 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_62
2022-10-24 15:20:02,174 - trainer - INFO - 
*****************[epoch: 62, global step: 63] eval training set at end of epoch***************
2022-10-24 15:20:02,175 - trainer - INFO - {
  "train_loss": 4334.4599609375
}
2022-10-24 15:20:02,180 - trainer - INFO - start training epoch 63
2022-10-24 15:20:02,180 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,181 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,181 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,189 - trainer - INFO - 
*****************[epoch: 63, global step: 64] eval training set at end of epoch***************
2022-10-24 15:20:02,189 - trainer - INFO - {
  "train_loss": 3802.275390625
}
2022-10-24 15:20:02,190 - trainer - INFO - start training epoch 64
2022-10-24 15:20:02,192 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,193 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,193 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,202 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval training set based on eval_every=2***************
2022-10-24 15:20:02,203 - trainer - INFO - {
  "train_loss": 3788.6715087890625
}
2022-10-24 15:20:02,211 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval development set based on eval_every=2***************
2022-10-24 15:20:02,212 - trainer - INFO - {
  "dev_loss": 3993.270263671875,
  "dev_best_score_for_loss": -3802.275634765625
}
2022-10-24 15:20:02,213 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:02,213 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,214 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,215 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,216 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_58
2022-10-24 15:20:02,217 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_64
2022-10-24 15:20:02,222 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_64
2022-10-24 15:20:02,225 - trainer - INFO - 
*****************[epoch: 64, global step: 65] eval training set at end of epoch***************
2022-10-24 15:20:02,227 - trainer - INFO - {
  "train_loss": 3775.067626953125
}
2022-10-24 15:20:02,227 - trainer - INFO - start training epoch 65
2022-10-24 15:20:02,227 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,229 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,229 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,238 - trainer - INFO - 
*****************[epoch: 65, global step: 66] eval training set at end of epoch***************
2022-10-24 15:20:02,239 - trainer - INFO - {
  "train_loss": 3993.27001953125
}
2022-10-24 15:20:02,240 - trainer - INFO - start training epoch 66
2022-10-24 15:20:02,242 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,242 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,243 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,253 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval training set based on eval_every=2***************
2022-10-24 15:20:02,254 - trainer - INFO - {
  "train_loss": 3910.9578857421875
}
2022-10-24 15:20:02,260 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval development set based on eval_every=2***************
2022-10-24 15:20:02,262 - trainer - INFO - {
  "dev_loss": 3436.82373046875,
  "dev_best_score_for_loss": -3436.82373046875
}
2022-10-24 15:20:02,263 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,265 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,265 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,266 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_60
2022-10-24 15:20:02,268 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,271 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,271 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,272 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,273 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,273 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_66
2022-10-24 15:20:02,279 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_66
2022-10-24 15:20:02,280 - trainer - INFO - 
*****************[epoch: 66, global step: 67] eval training set at end of epoch***************
2022-10-24 15:20:02,280 - trainer - INFO - {
  "train_loss": 3828.645751953125
}
2022-10-24 15:20:02,281 - trainer - INFO - start training epoch 67
2022-10-24 15:20:02,282 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,282 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,283 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,297 - trainer - INFO - 
*****************[epoch: 67, global step: 68] eval training set at end of epoch***************
2022-10-24 15:20:02,297 - trainer - INFO - {
  "train_loss": 3436.82373046875
}
2022-10-24 15:20:02,298 - trainer - INFO - start training epoch 68
2022-10-24 15:20:02,300 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,301 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,302 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,314 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval training set based on eval_every=2***************
2022-10-24 15:20:02,315 - trainer - INFO - {
  "train_loss": 3398.8201904296875
}
2022-10-24 15:20:02,324 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval development set based on eval_every=2***************
2022-10-24 15:20:02,324 - trainer - INFO - {
  "dev_loss": 3489.72802734375,
  "dev_best_score_for_loss": -3436.82373046875
}
2022-10-24 15:20:02,325 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:02,326 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,327 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,328 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,328 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_62
2022-10-24 15:20:02,334 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_68
2022-10-24 15:20:02,338 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_68
2022-10-24 15:20:02,339 - trainer - INFO - 
*****************[epoch: 68, global step: 69] eval training set at end of epoch***************
2022-10-24 15:20:02,340 - trainer - INFO - {
  "train_loss": 3360.816650390625
}
2022-10-24 15:20:02,340 - trainer - INFO - start training epoch 69
2022-10-24 15:20:02,341 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,341 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,342 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,354 - trainer - INFO - 
*****************[epoch: 69, global step: 70] eval training set at end of epoch***************
2022-10-24 15:20:02,354 - trainer - INFO - {
  "train_loss": 3489.72802734375
}
2022-10-24 15:20:02,355 - trainer - INFO - start training epoch 70
2022-10-24 15:20:02,356 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,356 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,357 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,366 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval training set based on eval_every=2***************
2022-10-24 15:20:02,366 - trainer - INFO - {
  "train_loss": 3415.1339111328125
}
2022-10-24 15:20:02,372 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval development set based on eval_every=2***************
2022-10-24 15:20:02,373 - trainer - INFO - {
  "dev_loss": 3045.367919921875,
  "dev_best_score_for_loss": -3045.367919921875
}
2022-10-24 15:20:02,374 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,375 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,375 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,376 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_64
2022-10-24 15:20:02,379 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,384 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,384 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,384 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,386 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,386 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_70
2022-10-24 15:20:02,391 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_70
2022-10-24 15:20:02,396 - trainer - INFO - 
*****************[epoch: 70, global step: 71] eval training set at end of epoch***************
2022-10-24 15:20:02,397 - trainer - INFO - {
  "train_loss": 3340.539794921875
}
2022-10-24 15:20:02,398 - trainer - INFO - start training epoch 71
2022-10-24 15:20:02,398 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,398 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,399 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,407 - trainer - INFO - 
*****************[epoch: 71, global step: 72] eval training set at end of epoch***************
2022-10-24 15:20:02,410 - trainer - INFO - {
  "train_loss": 3045.367919921875
}
2022-10-24 15:20:02,410 - trainer - INFO - start training epoch 72
2022-10-24 15:20:02,411 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,411 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,411 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,421 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval training set based on eval_every=2***************
2022-10-24 15:20:02,421 - trainer - INFO - {
  "train_loss": 3020.338134765625
}
2022-10-24 15:20:02,433 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval development set based on eval_every=2***************
2022-10-24 15:20:02,433 - trainer - INFO - {
  "dev_loss": 3045.787353515625,
  "dev_best_score_for_loss": -3045.367919921875
}
2022-10-24 15:20:02,434 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:02,435 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,436 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,436 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,437 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_66
2022-10-24 15:20:02,439 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_72
2022-10-24 15:20:02,445 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_72
2022-10-24 15:20:02,446 - trainer - INFO - 
*****************[epoch: 72, global step: 73] eval training set at end of epoch***************
2022-10-24 15:20:02,447 - trainer - INFO - {
  "train_loss": 2995.308349609375
}
2022-10-24 15:20:02,447 - trainer - INFO - start training epoch 73
2022-10-24 15:20:02,448 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,448 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,449 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,456 - trainer - INFO - 
*****************[epoch: 73, global step: 74] eval training set at end of epoch***************
2022-10-24 15:20:02,457 - trainer - INFO - {
  "train_loss": 3045.787353515625
}
2022-10-24 15:20:02,457 - trainer - INFO - start training epoch 74
2022-10-24 15:20:02,458 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,458 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,459 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,467 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval training set based on eval_every=2***************
2022-10-24 15:20:02,468 - trainer - INFO - {
  "train_loss": 2969.70703125
}
2022-10-24 15:20:02,480 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval development set based on eval_every=2***************
2022-10-24 15:20:02,480 - trainer - INFO - {
  "dev_loss": 2673.51904296875,
  "dev_best_score_for_loss": -2673.51904296875
}
2022-10-24 15:20:02,481 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,482 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,483 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,483 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_68
2022-10-24 15:20:02,485 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,489 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,490 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,490 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,491 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,491 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_74
2022-10-24 15:20:02,496 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_74
2022-10-24 15:20:02,496 - trainer - INFO - 
*****************[epoch: 74, global step: 75] eval training set at end of epoch***************
2022-10-24 15:20:02,497 - trainer - INFO - {
  "train_loss": 2893.626708984375
}
2022-10-24 15:20:02,498 - trainer - INFO - start training epoch 75
2022-10-24 15:20:02,498 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,499 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,499 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,507 - trainer - INFO - 
*****************[epoch: 75, global step: 76] eval training set at end of epoch***************
2022-10-24 15:20:02,507 - trainer - INFO - {
  "train_loss": 2673.51904296875
}
2022-10-24 15:20:02,508 - trainer - INFO - start training epoch 76
2022-10-24 15:20:02,508 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,508 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,508 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,515 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval training set based on eval_every=2***************
2022-10-24 15:20:02,515 - trainer - INFO - {
  "train_loss": 2653.22509765625
}
2022-10-24 15:20:02,525 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval development set based on eval_every=2***************
2022-10-24 15:20:02,525 - trainer - INFO - {
  "dev_loss": 2629.880615234375,
  "dev_best_score_for_loss": -2629.880615234375
}
2022-10-24 15:20:02,526 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,528 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,528 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,529 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_70
2022-10-24 15:20:02,530 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,534 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,534 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,535 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,536 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,536 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_76
2022-10-24 15:20:02,540 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_76
2022-10-24 15:20:02,541 - trainer - INFO - 
*****************[epoch: 76, global step: 77] eval training set at end of epoch***************
2022-10-24 15:20:02,541 - trainer - INFO - {
  "train_loss": 2632.93115234375
}
2022-10-24 15:20:02,541 - trainer - INFO - start training epoch 77
2022-10-24 15:20:02,541 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,542 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,542 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,552 - trainer - INFO - 
*****************[epoch: 77, global step: 78] eval training set at end of epoch***************
2022-10-24 15:20:02,553 - trainer - INFO - {
  "train_loss": 2629.880615234375
}
2022-10-24 15:20:02,553 - trainer - INFO - start training epoch 78
2022-10-24 15:20:02,553 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,553 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,554 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,564 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval training set based on eval_every=2***************
2022-10-24 15:20:02,564 - trainer - INFO - {
  "train_loss": 2550.217529296875
}
2022-10-24 15:20:02,570 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval development set based on eval_every=2***************
2022-10-24 15:20:02,571 - trainer - INFO - {
  "dev_loss": 2313.88720703125,
  "dev_best_score_for_loss": -2313.88720703125
}
2022-10-24 15:20:02,571 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,573 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,573 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,573 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_72
2022-10-24 15:20:02,575 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,580 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,580 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,581 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,583 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,585 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_78
2022-10-24 15:20:02,590 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_78
2022-10-24 15:20:02,591 - trainer - INFO - 
*****************[epoch: 78, global step: 79] eval training set at end of epoch***************
2022-10-24 15:20:02,591 - trainer - INFO - {
  "train_loss": 2470.554443359375
}
2022-10-24 15:20:02,592 - trainer - INFO - start training epoch 79
2022-10-24 15:20:02,592 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,592 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,593 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,602 - trainer - INFO - 
*****************[epoch: 79, global step: 80] eval training set at end of epoch***************
2022-10-24 15:20:02,602 - trainer - INFO - {
  "train_loss": 2313.88720703125
}
2022-10-24 15:20:02,603 - trainer - INFO - start training epoch 80
2022-10-24 15:20:02,603 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,603 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,604 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,611 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval training set based on eval_every=2***************
2022-10-24 15:20:02,611 - trainer - INFO - {
  "train_loss": 2300.839111328125
}
2022-10-24 15:20:02,617 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval development set based on eval_every=2***************
2022-10-24 15:20:02,617 - trainer - INFO - {
  "dev_loss": 2235.96923828125,
  "dev_best_score_for_loss": -2235.96923828125
}
2022-10-24 15:20:02,618 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,619 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,619 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,619 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_74
2022-10-24 15:20:02,620 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,623 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,623 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,623 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,624 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,625 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_80
2022-10-24 15:20:02,632 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_80
2022-10-24 15:20:02,633 - trainer - INFO - 
*****************[epoch: 80, global step: 81] eval training set at end of epoch***************
2022-10-24 15:20:02,633 - trainer - INFO - {
  "train_loss": 2287.791015625
}
2022-10-24 15:20:02,634 - trainer - INFO - start training epoch 81
2022-10-24 15:20:02,634 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,634 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,635 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,642 - trainer - INFO - 
*****************[epoch: 81, global step: 82] eval training set at end of epoch***************
2022-10-24 15:20:02,643 - trainer - INFO - {
  "train_loss": 2235.96923828125
}
2022-10-24 15:20:02,643 - trainer - INFO - start training epoch 82
2022-10-24 15:20:02,643 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,643 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,644 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,650 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval training set based on eval_every=2***************
2022-10-24 15:20:02,650 - trainer - INFO - {
  "train_loss": 2159.94580078125
}
2022-10-24 15:20:02,657 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval development set based on eval_every=2***************
2022-10-24 15:20:02,657 - trainer - INFO - {
  "dev_loss": 1976.210693359375,
  "dev_best_score_for_loss": -1976.210693359375
}
2022-10-24 15:20:02,658 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,659 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,659 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,659 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_76
2022-10-24 15:20:02,661 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,664 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,665 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,665 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,666 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,666 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_82
2022-10-24 15:20:02,670 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_82
2022-10-24 15:20:02,670 - trainer - INFO - 
*****************[epoch: 82, global step: 83] eval training set at end of epoch***************
2022-10-24 15:20:02,671 - trainer - INFO - {
  "train_loss": 2083.92236328125
}
2022-10-24 15:20:02,672 - trainer - INFO - start training epoch 83
2022-10-24 15:20:02,672 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,676 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,676 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,684 - trainer - INFO - 
*****************[epoch: 83, global step: 84] eval training set at end of epoch***************
2022-10-24 15:20:02,685 - trainer - INFO - {
  "train_loss": 1976.210693359375
}
2022-10-24 15:20:02,685 - trainer - INFO - start training epoch 84
2022-10-24 15:20:02,685 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,685 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,686 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,693 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval training set based on eval_every=2***************
2022-10-24 15:20:02,694 - trainer - INFO - {
  "train_loss": 1960.4849243164062
}
2022-10-24 15:20:02,701 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval development set based on eval_every=2***************
2022-10-24 15:20:02,701 - trainer - INFO - {
  "dev_loss": 1857.39599609375,
  "dev_best_score_for_loss": -1857.39599609375
}
2022-10-24 15:20:02,702 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,703 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,705 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,705 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_78
2022-10-24 15:20:02,709 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,714 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,714 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,714 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,716 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,716 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_84
2022-10-24 15:20:02,722 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_84
2022-10-24 15:20:02,723 - trainer - INFO - 
*****************[epoch: 84, global step: 85] eval training set at end of epoch***************
2022-10-24 15:20:02,723 - trainer - INFO - {
  "train_loss": 1944.7591552734375
}
2022-10-24 15:20:02,724 - trainer - INFO - start training epoch 85
2022-10-24 15:20:02,725 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,725 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,725 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,735 - trainer - INFO - 
*****************[epoch: 85, global step: 86] eval training set at end of epoch***************
2022-10-24 15:20:02,736 - trainer - INFO - {
  "train_loss": 1857.39599609375
}
2022-10-24 15:20:02,736 - trainer - INFO - start training epoch 86
2022-10-24 15:20:02,736 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,737 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,737 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,745 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval training set based on eval_every=2***************
2022-10-24 15:20:02,745 - trainer - INFO - {
  "train_loss": 1791.9718017578125
}
2022-10-24 15:20:02,755 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval development set based on eval_every=2***************
2022-10-24 15:20:02,756 - trainer - INFO - {
  "dev_loss": 1659.522216796875,
  "dev_best_score_for_loss": -1659.522216796875
}
2022-10-24 15:20:02,756 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,758 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,758 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,758 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_80
2022-10-24 15:20:02,760 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,764 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,766 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,766 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,767 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,767 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_86
2022-10-24 15:20:02,772 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_86
2022-10-24 15:20:02,773 - trainer - INFO - 
*****************[epoch: 86, global step: 87] eval training set at end of epoch***************
2022-10-24 15:20:02,773 - trainer - INFO - {
  "train_loss": 1726.547607421875
}
2022-10-24 15:20:02,774 - trainer - INFO - start training epoch 87
2022-10-24 15:20:02,774 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,774 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,775 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,783 - trainer - INFO - 
*****************[epoch: 87, global step: 88] eval training set at end of epoch***************
2022-10-24 15:20:02,783 - trainer - INFO - {
  "train_loss": 1659.522216796875
}
2022-10-24 15:20:02,783 - trainer - INFO - start training epoch 88
2022-10-24 15:20:02,784 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,784 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,784 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,791 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval training set based on eval_every=2***************
2022-10-24 15:20:02,792 - trainer - INFO - {
  "train_loss": 1635.0331420898438
}
2022-10-24 15:20:02,802 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval development set based on eval_every=2***************
2022-10-24 15:20:02,802 - trainer - INFO - {
  "dev_loss": 1505.7413330078125,
  "dev_best_score_for_loss": -1505.7413330078125
}
2022-10-24 15:20:02,803 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,804 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,805 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,805 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_82
2022-10-24 15:20:02,807 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,811 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,811 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,812 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,813 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,813 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_88
2022-10-24 15:20:02,819 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_88
2022-10-24 15:20:02,820 - trainer - INFO - 
*****************[epoch: 88, global step: 89] eval training set at end of epoch***************
2022-10-24 15:20:02,820 - trainer - INFO - {
  "train_loss": 1610.5440673828125
}
2022-10-24 15:20:02,820 - trainer - INFO - start training epoch 89
2022-10-24 15:20:02,821 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,821 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,821 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,830 - trainer - INFO - 
*****************[epoch: 89, global step: 90] eval training set at end of epoch***************
2022-10-24 15:20:02,831 - trainer - INFO - {
  "train_loss": 1505.7410888671875
}
2022-10-24 15:20:02,831 - trainer - INFO - start training epoch 90
2022-10-24 15:20:02,831 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,832 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,832 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,838 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval training set based on eval_every=2***************
2022-10-24 15:20:02,838 - trainer - INFO - {
  "train_loss": 1456.6699829101562
}
2022-10-24 15:20:02,844 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval development set based on eval_every=2***************
2022-10-24 15:20:02,846 - trainer - INFO - {
  "dev_loss": 1356.764404296875,
  "dev_best_score_for_loss": -1356.764404296875
}
2022-10-24 15:20:02,847 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,851 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,851 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,851 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_84
2022-10-24 15:20:02,853 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,856 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,857 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,858 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,859 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,859 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_90
2022-10-24 15:20:02,865 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_90
2022-10-24 15:20:02,865 - trainer - INFO - 
*****************[epoch: 90, global step: 91] eval training set at end of epoch***************
2022-10-24 15:20:02,866 - trainer - INFO - {
  "train_loss": 1407.598876953125
}
2022-10-24 15:20:02,866 - trainer - INFO - start training epoch 91
2022-10-24 15:20:02,866 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,867 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,867 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,875 - trainer - INFO - 
*****************[epoch: 91, global step: 92] eval training set at end of epoch***************
2022-10-24 15:20:02,876 - trainer - INFO - {
  "train_loss": 1356.7642822265625
}
2022-10-24 15:20:02,876 - trainer - INFO - start training epoch 92
2022-10-24 15:20:02,877 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,877 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,877 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,885 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval training set based on eval_every=2***************
2022-10-24 15:20:02,886 - trainer - INFO - {
  "train_loss": 1320.8442993164062
}
2022-10-24 15:20:02,897 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval development set based on eval_every=2***************
2022-10-24 15:20:02,897 - trainer - INFO - {
  "dev_loss": 1186.774658203125,
  "dev_best_score_for_loss": -1186.774658203125
}
2022-10-24 15:20:02,898 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,899 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,900 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,900 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_86
2022-10-24 15:20:02,902 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,906 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,906 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,906 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,907 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,907 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_92
2022-10-24 15:20:02,911 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_92
2022-10-24 15:20:02,912 - trainer - INFO - 
*****************[epoch: 92, global step: 93] eval training set at end of epoch***************
2022-10-24 15:20:02,913 - trainer - INFO - {
  "train_loss": 1284.92431640625
}
2022-10-24 15:20:02,913 - trainer - INFO - start training epoch 93
2022-10-24 15:20:02,913 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,913 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,914 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,921 - trainer - INFO - 
*****************[epoch: 93, global step: 94] eval training set at end of epoch***************
2022-10-24 15:20:02,921 - trainer - INFO - {
  "train_loss": 1186.7744140625
}
2022-10-24 15:20:02,922 - trainer - INFO - start training epoch 94
2022-10-24 15:20:02,922 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,922 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,922 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,930 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval training set based on eval_every=2***************
2022-10-24 15:20:02,931 - trainer - INFO - {
  "train_loss": 1155.4085083007812
}
2022-10-24 15:20:02,940 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval development set based on eval_every=2***************
2022-10-24 15:20:02,941 - trainer - INFO - {
  "dev_loss": 1070.780517578125,
  "dev_best_score_for_loss": -1070.780517578125
}
2022-10-24 15:20:02,941 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,943 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,943 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,943 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_88
2022-10-24 15:20:02,945 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,948 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,948 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,948 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,949 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,950 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_94
2022-10-24 15:20:02,955 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_94
2022-10-24 15:20:02,955 - trainer - INFO - 
*****************[epoch: 94, global step: 95] eval training set at end of epoch***************
2022-10-24 15:20:02,955 - trainer - INFO - {
  "train_loss": 1124.0426025390625
}
2022-10-24 15:20:02,956 - trainer - INFO - start training epoch 95
2022-10-24 15:20:02,956 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,956 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,957 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,964 - trainer - INFO - 
*****************[epoch: 95, global step: 96] eval training set at end of epoch***************
2022-10-24 15:20:02,965 - trainer - INFO - {
  "train_loss": 1070.780517578125
}
2022-10-24 15:20:02,965 - trainer - INFO - start training epoch 96
2022-10-24 15:20:02,966 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,966 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,967 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:02,972 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval training set based on eval_every=2***************
2022-10-24 15:20:02,972 - trainer - INFO - {
  "train_loss": 1029.5475769042969
}
2022-10-24 15:20:02,978 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval development set based on eval_every=2***************
2022-10-24 15:20:02,979 - trainer - INFO - {
  "dev_loss": 915.1871337890625,
  "dev_best_score_for_loss": -915.1871337890625
}
2022-10-24 15:20:02,979 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:02,981 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:02,981 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:02,982 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_90
2022-10-24 15:20:02,985 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:02,990 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:02,990 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:02,991 - trainer - INFO -   patience: 200
2022-10-24 15:20:02,992 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:02,992 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_96
2022-10-24 15:20:02,997 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_96
2022-10-24 15:20:02,998 - trainer - INFO - 
*****************[epoch: 96, global step: 97] eval training set at end of epoch***************
2022-10-24 15:20:02,998 - trainer - INFO - {
  "train_loss": 988.3146362304688
}
2022-10-24 15:20:02,999 - trainer - INFO - start training epoch 97
2022-10-24 15:20:02,999 - trainer - INFO - training using device=cuda
2022-10-24 15:20:02,999 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:02,999 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,008 - trainer - INFO - 
*****************[epoch: 97, global step: 98] eval training set at end of epoch***************
2022-10-24 15:20:03,008 - trainer - INFO - {
  "train_loss": 915.1871337890625
}
2022-10-24 15:20:03,008 - trainer - INFO - start training epoch 98
2022-10-24 15:20:03,009 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,009 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,009 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,017 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval training set based on eval_every=2***************
2022-10-24 15:20:03,017 - trainer - INFO - {
  "train_loss": 890.3759460449219
}
2022-10-24 15:20:03,023 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval development set based on eval_every=2***************
2022-10-24 15:20:03,023 - trainer - INFO - {
  "dev_loss": 800.1163940429688,
  "dev_best_score_for_loss": -800.1163940429688
}
2022-10-24 15:20:03,024 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,025 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,025 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,026 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_92
2022-10-24 15:20:03,027 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,033 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,033 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,033 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,034 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,035 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_98
2022-10-24 15:20:03,039 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_98
2022-10-24 15:20:03,040 - trainer - INFO - 
*****************[epoch: 98, global step: 99] eval training set at end of epoch***************
2022-10-24 15:20:03,040 - trainer - INFO - {
  "train_loss": 865.5647583007812
}
2022-10-24 15:20:03,040 - trainer - INFO - start training epoch 99
2022-10-24 15:20:03,041 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,041 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,041 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,049 - trainer - INFO - 
*****************[epoch: 99, global step: 100] eval training set at end of epoch***************
2022-10-24 15:20:03,050 - trainer - INFO - {
  "train_loss": 800.1163940429688
}
2022-10-24 15:20:03,050 - trainer - INFO - start training epoch 100
2022-10-24 15:20:03,050 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,051 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,051 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,058 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval training set based on eval_every=2***************
2022-10-24 15:20:03,059 - trainer - INFO - {
  "train_loss": 764.6326293945312
}
2022-10-24 15:20:03,066 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval development set based on eval_every=2***************
2022-10-24 15:20:03,067 - trainer - INFO - {
  "dev_loss": 681.406982421875,
  "dev_best_score_for_loss": -681.406982421875
}
2022-10-24 15:20:03,067 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,069 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,069 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,069 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_94
2022-10-24 15:20:03,070 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,073 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,074 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,074 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,076 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,079 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_100
2022-10-24 15:20:03,084 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_100
2022-10-24 15:20:03,085 - trainer - INFO - 
*****************[epoch: 100, global step: 101] eval training set at end of epoch***************
2022-10-24 15:20:03,085 - trainer - INFO - {
  "train_loss": 729.1488647460938
}
2022-10-24 15:20:03,086 - trainer - INFO - start training epoch 101
2022-10-24 15:20:03,086 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,087 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,087 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,097 - trainer - INFO - 
*****************[epoch: 101, global step: 102] eval training set at end of epoch***************
2022-10-24 15:20:03,098 - trainer - INFO - {
  "train_loss": 681.406982421875
}
2022-10-24 15:20:03,098 - trainer - INFO - start training epoch 102
2022-10-24 15:20:03,098 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,098 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,099 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,106 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval training set based on eval_every=2***************
2022-10-24 15:20:03,107 - trainer - INFO - {
  "train_loss": 656.0758666992188
}
2022-10-24 15:20:03,113 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval development set based on eval_every=2***************
2022-10-24 15:20:03,113 - trainer - INFO - {
  "dev_loss": 569.479248046875,
  "dev_best_score_for_loss": -569.479248046875
}
2022-10-24 15:20:03,114 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,115 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,115 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,116 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_96
2022-10-24 15:20:03,117 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,120 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,121 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,122 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,123 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,126 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_102
2022-10-24 15:20:03,132 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_102
2022-10-24 15:20:03,133 - trainer - INFO - 
*****************[epoch: 102, global step: 103] eval training set at end of epoch***************
2022-10-24 15:20:03,133 - trainer - INFO - {
  "train_loss": 630.7447509765625
}
2022-10-24 15:20:03,134 - trainer - INFO - start training epoch 103
2022-10-24 15:20:03,134 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,134 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,135 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,146 - trainer - INFO - 
*****************[epoch: 103, global step: 104] eval training set at end of epoch***************
2022-10-24 15:20:03,146 - trainer - INFO - {
  "train_loss": 569.4791870117188
}
2022-10-24 15:20:03,147 - trainer - INFO - start training epoch 104
2022-10-24 15:20:03,147 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,147 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,148 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,155 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval training set based on eval_every=2***************
2022-10-24 15:20:03,156 - trainer - INFO - {
  "train_loss": 545.783447265625
}
2022-10-24 15:20:03,164 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval development set based on eval_every=2***************
2022-10-24 15:20:03,164 - trainer - INFO - {
  "dev_loss": 479.854736328125,
  "dev_best_score_for_loss": -479.854736328125
}
2022-10-24 15:20:03,165 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,166 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,166 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,166 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_98
2022-10-24 15:20:03,168 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,174 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,174 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,174 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,176 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,176 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_104
2022-10-24 15:20:03,181 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_104
2022-10-24 15:20:03,183 - trainer - INFO - 
*****************[epoch: 104, global step: 105] eval training set at end of epoch***************
2022-10-24 15:20:03,184 - trainer - INFO - {
  "train_loss": 522.0877075195312
}
2022-10-24 15:20:03,184 - trainer - INFO - start training epoch 105
2022-10-24 15:20:03,184 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,185 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,185 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,195 - trainer - INFO - 
*****************[epoch: 105, global step: 106] eval training set at end of epoch***************
2022-10-24 15:20:03,195 - trainer - INFO - {
  "train_loss": 479.854736328125
}
2022-10-24 15:20:03,196 - trainer - INFO - start training epoch 106
2022-10-24 15:20:03,196 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,197 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,197 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,204 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval training set based on eval_every=2***************
2022-10-24 15:20:03,204 - trainer - INFO - {
  "train_loss": 454.30181884765625
}
2022-10-24 15:20:03,210 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval development set based on eval_every=2***************
2022-10-24 15:20:03,210 - trainer - INFO - {
  "dev_loss": 386.2159118652344,
  "dev_best_score_for_loss": -386.2159118652344
}
2022-10-24 15:20:03,211 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,212 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,212 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,213 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_100
2022-10-24 15:20:03,214 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,218 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,219 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,219 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,220 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,220 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_106
2022-10-24 15:20:03,224 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_106
2022-10-24 15:20:03,224 - trainer - INFO - 
*****************[epoch: 106, global step: 107] eval training set at end of epoch***************
2022-10-24 15:20:03,225 - trainer - INFO - {
  "train_loss": 428.7489013671875
}
2022-10-24 15:20:03,225 - trainer - INFO - start training epoch 107
2022-10-24 15:20:03,225 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,225 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,226 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,235 - trainer - INFO - 
*****************[epoch: 107, global step: 108] eval training set at end of epoch***************
2022-10-24 15:20:03,235 - trainer - INFO - {
  "train_loss": 386.2159118652344
}
2022-10-24 15:20:03,236 - trainer - INFO - start training epoch 108
2022-10-24 15:20:03,236 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,236 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,236 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,242 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval training set based on eval_every=2***************
2022-10-24 15:20:03,242 - trainer - INFO - {
  "train_loss": 369.1248779296875
}
2022-10-24 15:20:03,248 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval development set based on eval_every=2***************
2022-10-24 15:20:03,249 - trainer - INFO - {
  "dev_loss": 311.28179931640625,
  "dev_best_score_for_loss": -311.28179931640625
}
2022-10-24 15:20:03,249 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,250 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,251 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,251 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_102
2022-10-24 15:20:03,252 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,255 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,255 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,255 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,256 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,256 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_108
2022-10-24 15:20:03,260 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_108
2022-10-24 15:20:03,262 - trainer - INFO - 
*****************[epoch: 108, global step: 109] eval training set at end of epoch***************
2022-10-24 15:20:03,263 - trainer - INFO - {
  "train_loss": 352.0338439941406
}
2022-10-24 15:20:03,264 - trainer - INFO - start training epoch 109
2022-10-24 15:20:03,267 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,268 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,268 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,276 - trainer - INFO - 
*****************[epoch: 109, global step: 110] eval training set at end of epoch***************
2022-10-24 15:20:03,276 - trainer - INFO - {
  "train_loss": 311.28179931640625
}
2022-10-24 15:20:03,277 - trainer - INFO - start training epoch 110
2022-10-24 15:20:03,277 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,277 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,278 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,285 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval training set based on eval_every=2***************
2022-10-24 15:20:03,285 - trainer - INFO - {
  "train_loss": 293.09779357910156
}
2022-10-24 15:20:03,291 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval development set based on eval_every=2***************
2022-10-24 15:20:03,292 - trainer - INFO - {
  "dev_loss": 247.1978759765625,
  "dev_best_score_for_loss": -247.1978759765625
}
2022-10-24 15:20:03,292 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,294 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,294 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,294 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_104
2022-10-24 15:20:03,296 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,299 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,299 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,300 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,300 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,300 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_110
2022-10-24 15:20:03,304 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_110
2022-10-24 15:20:03,305 - trainer - INFO - 
*****************[epoch: 110, global step: 111] eval training set at end of epoch***************
2022-10-24 15:20:03,305 - trainer - INFO - {
  "train_loss": 274.9137878417969
}
2022-10-24 15:20:03,306 - trainer - INFO - start training epoch 111
2022-10-24 15:20:03,307 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,308 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,309 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,320 - trainer - INFO - 
*****************[epoch: 111, global step: 112] eval training set at end of epoch***************
2022-10-24 15:20:03,321 - trainer - INFO - {
  "train_loss": 247.1978759765625
}
2022-10-24 15:20:03,321 - trainer - INFO - start training epoch 112
2022-10-24 15:20:03,321 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,321 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,322 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,332 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval training set based on eval_every=2***************
2022-10-24 15:20:03,332 - trainer - INFO - {
  "train_loss": 231.2218475341797
}
2022-10-24 15:20:03,338 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval development set based on eval_every=2***************
2022-10-24 15:20:03,338 - trainer - INFO - {
  "dev_loss": 186.0571746826172,
  "dev_best_score_for_loss": -186.0571746826172
}
2022-10-24 15:20:03,339 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,340 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,340 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,341 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_106
2022-10-24 15:20:03,342 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,346 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,346 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,346 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,347 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,348 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_112
2022-10-24 15:20:03,352 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_112
2022-10-24 15:20:03,354 - trainer - INFO - 
*****************[epoch: 112, global step: 113] eval training set at end of epoch***************
2022-10-24 15:20:03,355 - trainer - INFO - {
  "train_loss": 215.24581909179688
}
2022-10-24 15:20:03,359 - trainer - INFO - start training epoch 113
2022-10-24 15:20:03,359 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,360 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,360 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,370 - trainer - INFO - 
*****************[epoch: 113, global step: 114] eval training set at end of epoch***************
2022-10-24 15:20:03,371 - trainer - INFO - {
  "train_loss": 186.0571746826172
}
2022-10-24 15:20:03,371 - trainer - INFO - start training epoch 114
2022-10-24 15:20:03,371 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,372 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,372 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,378 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval training set based on eval_every=2***************
2022-10-24 15:20:03,378 - trainer - INFO - {
  "train_loss": 175.2366485595703
}
2022-10-24 15:20:03,386 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval development set based on eval_every=2***************
2022-10-24 15:20:03,386 - trainer - INFO - {
  "dev_loss": 140.1198272705078,
  "dev_best_score_for_loss": -140.1198272705078
}
2022-10-24 15:20:03,387 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,388 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,388 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,389 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_108
2022-10-24 15:20:03,390 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,394 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,394 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,394 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,395 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,395 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_114
2022-10-24 15:20:03,400 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_114
2022-10-24 15:20:03,402 - trainer - INFO - 
*****************[epoch: 114, global step: 115] eval training set at end of epoch***************
2022-10-24 15:20:03,403 - trainer - INFO - {
  "train_loss": 164.41612243652344
}
2022-10-24 15:20:03,406 - trainer - INFO - start training epoch 115
2022-10-24 15:20:03,406 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,406 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,407 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,414 - trainer - INFO - 
*****************[epoch: 115, global step: 116] eval training set at end of epoch***************
2022-10-24 15:20:03,416 - trainer - INFO - {
  "train_loss": 140.1198272705078
}
2022-10-24 15:20:03,417 - trainer - INFO - start training epoch 116
2022-10-24 15:20:03,417 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,418 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,418 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,425 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval training set based on eval_every=2***************
2022-10-24 15:20:03,425 - trainer - INFO - {
  "train_loss": 128.9581184387207
}
2022-10-24 15:20:03,432 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval development set based on eval_every=2***************
2022-10-24 15:20:03,432 - trainer - INFO - {
  "dev_loss": 101.83699035644531,
  "dev_best_score_for_loss": -101.83699035644531
}
2022-10-24 15:20:03,433 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,434 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,435 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,435 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_110
2022-10-24 15:20:03,436 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,439 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,439 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,440 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,440 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,441 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_116
2022-10-24 15:20:03,444 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_116
2022-10-24 15:20:03,445 - trainer - INFO - 
*****************[epoch: 116, global step: 117] eval training set at end of epoch***************
2022-10-24 15:20:03,445 - trainer - INFO - {
  "train_loss": 117.7964096069336
}
2022-10-24 15:20:03,446 - trainer - INFO - start training epoch 117
2022-10-24 15:20:03,447 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,447 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,449 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,460 - trainer - INFO - 
*****************[epoch: 117, global step: 118] eval training set at end of epoch***************
2022-10-24 15:20:03,460 - trainer - INFO - {
  "train_loss": 101.83699798583984
}
2022-10-24 15:20:03,461 - trainer - INFO - start training epoch 118
2022-10-24 15:20:03,461 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,461 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,463 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,472 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval training set based on eval_every=2***************
2022-10-24 15:20:03,473 - trainer - INFO - {
  "train_loss": 93.10323715209961
}
2022-10-24 15:20:03,480 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval development set based on eval_every=2***************
2022-10-24 15:20:03,480 - trainer - INFO - {
  "dev_loss": 69.15048217773438,
  "dev_best_score_for_loss": -69.15048217773438
}
2022-10-24 15:20:03,481 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,483 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,483 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,483 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_112
2022-10-24 15:20:03,485 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,488 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,488 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,488 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,489 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,489 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_118
2022-10-24 15:20:03,495 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_118
2022-10-24 15:20:03,499 - trainer - INFO - 
*****************[epoch: 118, global step: 119] eval training set at end of epoch***************
2022-10-24 15:20:03,500 - trainer - INFO - {
  "train_loss": 84.36947631835938
}
2022-10-24 15:20:03,500 - trainer - INFO - start training epoch 119
2022-10-24 15:20:03,500 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,500 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,501 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,510 - trainer - INFO - 
*****************[epoch: 119, global step: 120] eval training set at end of epoch***************
2022-10-24 15:20:03,510 - trainer - INFO - {
  "train_loss": 69.15048217773438
}
2022-10-24 15:20:03,511 - trainer - INFO - start training epoch 120
2022-10-24 15:20:03,511 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,511 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,512 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,518 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval training set based on eval_every=2***************
2022-10-24 15:20:03,519 - trainer - INFO - {
  "train_loss": 63.96054267883301
}
2022-10-24 15:20:03,525 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval development set based on eval_every=2***************
2022-10-24 15:20:03,525 - trainer - INFO - {
  "dev_loss": 46.86039733886719,
  "dev_best_score_for_loss": -46.86039733886719
}
2022-10-24 15:20:03,525 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,526 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,527 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,527 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_114
2022-10-24 15:20:03,528 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,532 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,532 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,532 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,533 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,533 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_120
2022-10-24 15:20:03,537 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_120
2022-10-24 15:20:03,538 - trainer - INFO - 
*****************[epoch: 120, global step: 121] eval training set at end of epoch***************
2022-10-24 15:20:03,538 - trainer - INFO - {
  "train_loss": 58.77060317993164
}
2022-10-24 15:20:03,540 - trainer - INFO - start training epoch 121
2022-10-24 15:20:03,541 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,542 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,543 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,553 - trainer - INFO - 
*****************[epoch: 121, global step: 122] eval training set at end of epoch***************
2022-10-24 15:20:03,554 - trainer - INFO - {
  "train_loss": 46.86039352416992
}
2022-10-24 15:20:03,554 - trainer - INFO - start training epoch 122
2022-10-24 15:20:03,555 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,555 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,556 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,564 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval training set based on eval_every=2***************
2022-10-24 15:20:03,564 - trainer - INFO - {
  "train_loss": 41.98435020446777
}
2022-10-24 15:20:03,570 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval development set based on eval_every=2***************
2022-10-24 15:20:03,571 - trainer - INFO - {
  "dev_loss": 30.629709243774414,
  "dev_best_score_for_loss": -30.629709243774414
}
2022-10-24 15:20:03,571 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,572 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,573 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,573 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_116
2022-10-24 15:20:03,574 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,577 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,577 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,578 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,578 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,579 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_122
2022-10-24 15:20:03,582 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_122
2022-10-24 15:20:03,583 - trainer - INFO - 
*****************[epoch: 122, global step: 123] eval training set at end of epoch***************
2022-10-24 15:20:03,584 - trainer - INFO - {
  "train_loss": 37.108306884765625
}
2022-10-24 15:20:03,584 - trainer - INFO - start training epoch 123
2022-10-24 15:20:03,584 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,584 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,585 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,594 - trainer - INFO - 
*****************[epoch: 123, global step: 124] eval training set at end of epoch***************
2022-10-24 15:20:03,594 - trainer - INFO - {
  "train_loss": 30.629709243774414
}
2022-10-24 15:20:03,595 - trainer - INFO - start training epoch 124
2022-10-24 15:20:03,595 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,595 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,596 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,603 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval training set based on eval_every=2***************
2022-10-24 15:20:03,603 - trainer - INFO - {
  "train_loss": 26.802440643310547
}
2022-10-24 15:20:03,609 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval development set based on eval_every=2***************
2022-10-24 15:20:03,609 - trainer - INFO - {
  "dev_loss": 17.702402114868164,
  "dev_best_score_for_loss": -17.702402114868164
}
2022-10-24 15:20:03,610 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,611 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,611 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,612 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_118
2022-10-24 15:20:03,613 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,616 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,616 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,616 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,618 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,618 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_124
2022-10-24 15:20:03,622 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_124
2022-10-24 15:20:03,623 - trainer - INFO - 
*****************[epoch: 124, global step: 125] eval training set at end of epoch***************
2022-10-24 15:20:03,623 - trainer - INFO - {
  "train_loss": 22.97517204284668
}
2022-10-24 15:20:03,623 - trainer - INFO - start training epoch 125
2022-10-24 15:20:03,624 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,624 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,624 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,633 - trainer - INFO - 
*****************[epoch: 125, global step: 126] eval training set at end of epoch***************
2022-10-24 15:20:03,636 - trainer - INFO - {
  "train_loss": 17.702402114868164
}
2022-10-24 15:20:03,637 - trainer - INFO - start training epoch 126
2022-10-24 15:20:03,637 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,637 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,637 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,644 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval training set based on eval_every=2***************
2022-10-24 15:20:03,645 - trainer - INFO - {
  "train_loss": 16.0440616607666
}
2022-10-24 15:20:03,652 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval development set based on eval_every=2***************
2022-10-24 15:20:03,652 - trainer - INFO - {
  "dev_loss": 10.080771446228027,
  "dev_best_score_for_loss": -10.080771446228027
}
2022-10-24 15:20:03,653 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,654 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,655 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,655 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_120
2022-10-24 15:20:03,657 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,660 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,660 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,660 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,661 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,661 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_126
2022-10-24 15:20:03,666 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_126
2022-10-24 15:20:03,669 - trainer - INFO - 
*****************[epoch: 126, global step: 127] eval training set at end of epoch***************
2022-10-24 15:20:03,669 - trainer - INFO - {
  "train_loss": 14.385721206665039
}
2022-10-24 15:20:03,670 - trainer - INFO - start training epoch 127
2022-10-24 15:20:03,670 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,670 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,671 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,680 - trainer - INFO - 
*****************[epoch: 127, global step: 128] eval training set at end of epoch***************
2022-10-24 15:20:03,680 - trainer - INFO - {
  "train_loss": 10.080771446228027
}
2022-10-24 15:20:03,681 - trainer - INFO - start training epoch 128
2022-10-24 15:20:03,681 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,681 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,681 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,688 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval training set based on eval_every=2***************
2022-10-24 15:20:03,688 - trainer - INFO - {
  "train_loss": 8.932882308959961
}
2022-10-24 15:20:03,693 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval development set based on eval_every=2***************
2022-10-24 15:20:03,693 - trainer - INFO - {
  "dev_loss": 6.36875581741333,
  "dev_best_score_for_loss": -6.36875581741333
}
2022-10-24 15:20:03,694 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,695 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,696 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,697 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_122
2022-10-24 15:20:03,699 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,704 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,704 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,704 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,705 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,705 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_128
2022-10-24 15:20:03,710 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_128
2022-10-24 15:20:03,712 - trainer - INFO - 
*****************[epoch: 128, global step: 129] eval training set at end of epoch***************
2022-10-24 15:20:03,712 - trainer - INFO - {
  "train_loss": 7.7849931716918945
}
2022-10-24 15:20:03,713 - trainer - INFO - start training epoch 129
2022-10-24 15:20:03,713 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,713 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,713 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,721 - trainer - INFO - 
*****************[epoch: 129, global step: 130] eval training set at end of epoch***************
2022-10-24 15:20:03,722 - trainer - INFO - {
  "train_loss": 6.368756294250488
}
2022-10-24 15:20:03,722 - trainer - INFO - start training epoch 130
2022-10-24 15:20:03,722 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,722 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,723 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,730 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval training set based on eval_every=2***************
2022-10-24 15:20:03,731 - trainer - INFO - {
  "train_loss": 5.345752239227295
}
2022-10-24 15:20:03,736 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval development set based on eval_every=2***************
2022-10-24 15:20:03,737 - trainer - INFO - {
  "dev_loss": 3.9747300148010254,
  "dev_best_score_for_loss": -3.9747300148010254
}
2022-10-24 15:20:03,737 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,739 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,739 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,739 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_124
2022-10-24 15:20:03,740 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,744 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,744 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,745 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,746 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,746 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_130
2022-10-24 15:20:03,750 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_130
2022-10-24 15:20:03,751 - trainer - INFO - 
*****************[epoch: 130, global step: 131] eval training set at end of epoch***************
2022-10-24 15:20:03,752 - trainer - INFO - {
  "train_loss": 4.322748184204102
}
2022-10-24 15:20:03,752 - trainer - INFO - start training epoch 131
2022-10-24 15:20:03,753 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,753 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,753 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,763 - trainer - INFO - 
*****************[epoch: 131, global step: 132] eval training set at end of epoch***************
2022-10-24 15:20:03,763 - trainer - INFO - {
  "train_loss": 3.9747300148010254
}
2022-10-24 15:20:03,764 - trainer - INFO - start training epoch 132
2022-10-24 15:20:03,764 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,764 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,764 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,774 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval training set based on eval_every=2***************
2022-10-24 15:20:03,776 - trainer - INFO - {
  "train_loss": 3.7913055419921875
}
2022-10-24 15:20:03,784 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval development set based on eval_every=2***************
2022-10-24 15:20:03,784 - trainer - INFO - {
  "dev_loss": 2.994868755340576,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:03,785 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:03,786 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,787 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,788 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_126
2022-10-24 15:20:03,789 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:03,793 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:03,794 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:03,794 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,795 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:03,795 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_132
2022-10-24 15:20:03,800 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_132
2022-10-24 15:20:03,801 - trainer - INFO - 
*****************[epoch: 132, global step: 133] eval training set at end of epoch***************
2022-10-24 15:20:03,802 - trainer - INFO - {
  "train_loss": 3.6078810691833496
}
2022-10-24 15:20:03,803 - trainer - INFO - start training epoch 133
2022-10-24 15:20:03,804 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,805 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,809 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,821 - trainer - INFO - 
*****************[epoch: 133, global step: 134] eval training set at end of epoch***************
2022-10-24 15:20:03,821 - trainer - INFO - {
  "train_loss": 2.994868755340576
}
2022-10-24 15:20:03,822 - trainer - INFO - start training epoch 134
2022-10-24 15:20:03,822 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,822 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,823 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,831 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval training set based on eval_every=2***************
2022-10-24 15:20:03,832 - trainer - INFO - {
  "train_loss": 3.260203719139099
}
2022-10-24 15:20:03,840 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval development set based on eval_every=2***************
2022-10-24 15:20:03,841 - trainer - INFO - {
  "dev_loss": 3.5886919498443604,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:03,842 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:03,842 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,843 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,844 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,844 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_128
2022-10-24 15:20:03,845 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_134
2022-10-24 15:20:03,850 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_134
2022-10-24 15:20:03,852 - trainer - INFO - 
*****************[epoch: 134, global step: 135] eval training set at end of epoch***************
2022-10-24 15:20:03,856 - trainer - INFO - {
  "train_loss": 3.525538682937622
}
2022-10-24 15:20:03,856 - trainer - INFO - start training epoch 135
2022-10-24 15:20:03,856 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,857 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,857 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,870 - trainer - INFO - 
*****************[epoch: 135, global step: 136] eval training set at end of epoch***************
2022-10-24 15:20:03,871 - trainer - INFO - {
  "train_loss": 3.5886919498443604
}
2022-10-24 15:20:03,871 - trainer - INFO - start training epoch 136
2022-10-24 15:20:03,872 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,872 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,872 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,882 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval training set based on eval_every=2***************
2022-10-24 15:20:03,882 - trainer - INFO - {
  "train_loss": 3.673641324043274
}
2022-10-24 15:20:03,889 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval development set based on eval_every=2***************
2022-10-24 15:20:03,889 - trainer - INFO - {
  "dev_loss": 4.518836975097656,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:03,890 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:03,890 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,891 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,892 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,892 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_130
2022-10-24 15:20:03,894 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_136
2022-10-24 15:20:03,900 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_136
2022-10-24 15:20:03,903 - trainer - INFO - 
*****************[epoch: 136, global step: 137] eval training set at end of epoch***************
2022-10-24 15:20:03,904 - trainer - INFO - {
  "train_loss": 3.7585906982421875
}
2022-10-24 15:20:03,904 - trainer - INFO - start training epoch 137
2022-10-24 15:20:03,905 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,905 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,905 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,915 - trainer - INFO - 
*****************[epoch: 137, global step: 138] eval training set at end of epoch***************
2022-10-24 15:20:03,916 - trainer - INFO - {
  "train_loss": 4.518836498260498
}
2022-10-24 15:20:03,916 - trainer - INFO - start training epoch 138
2022-10-24 15:20:03,917 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,917 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,917 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,923 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval training set based on eval_every=2***************
2022-10-24 15:20:03,924 - trainer - INFO - {
  "train_loss": 4.601952075958252
}
2022-10-24 15:20:03,931 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval development set based on eval_every=2***************
2022-10-24 15:20:03,931 - trainer - INFO - {
  "dev_loss": 5.170439720153809,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:03,932 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:03,932 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,933 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,933 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,933 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_132
2022-10-24 15:20:03,934 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_138
2022-10-24 15:20:03,938 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_138
2022-10-24 15:20:03,939 - trainer - INFO - 
*****************[epoch: 138, global step: 139] eval training set at end of epoch***************
2022-10-24 15:20:03,939 - trainer - INFO - {
  "train_loss": 4.685067653656006
}
2022-10-24 15:20:03,940 - trainer - INFO - start training epoch 139
2022-10-24 15:20:03,940 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,940 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,940 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,950 - trainer - INFO - 
*****************[epoch: 139, global step: 140] eval training set at end of epoch***************
2022-10-24 15:20:03,950 - trainer - INFO - {
  "train_loss": 5.170439720153809
}
2022-10-24 15:20:03,951 - trainer - INFO - start training epoch 140
2022-10-24 15:20:03,951 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,951 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,952 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,959 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval training set based on eval_every=2***************
2022-10-24 15:20:03,960 - trainer - INFO - {
  "train_loss": 5.460266828536987
}
2022-10-24 15:20:03,968 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval development set based on eval_every=2***************
2022-10-24 15:20:03,968 - trainer - INFO - {
  "dev_loss": 5.87865686416626,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:03,969 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:03,969 - trainer - INFO -   patience: 200
2022-10-24 15:20:03,970 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:03,971 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:03,971 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_134
2022-10-24 15:20:03,972 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_140
2022-10-24 15:20:03,977 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_140
2022-10-24 15:20:03,978 - trainer - INFO - 
*****************[epoch: 140, global step: 141] eval training set at end of epoch***************
2022-10-24 15:20:03,978 - trainer - INFO - {
  "train_loss": 5.750093936920166
}
2022-10-24 15:20:03,979 - trainer - INFO - start training epoch 141
2022-10-24 15:20:03,979 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,979 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,979 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,986 - trainer - INFO - 
*****************[epoch: 141, global step: 142] eval training set at end of epoch***************
2022-10-24 15:20:03,987 - trainer - INFO - {
  "train_loss": 5.878656387329102
}
2022-10-24 15:20:03,987 - trainer - INFO - start training epoch 142
2022-10-24 15:20:03,987 - trainer - INFO - training using device=cuda
2022-10-24 15:20:03,987 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:03,988 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:03,998 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval training set based on eval_every=2***************
2022-10-24 15:20:03,999 - trainer - INFO - {
  "train_loss": 6.11554217338562
}
2022-10-24 15:20:04,008 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval development set based on eval_every=2***************
2022-10-24 15:20:04,008 - trainer - INFO - {
  "dev_loss": 6.588968753814697,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,009 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:20:04,009 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,010 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,011 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,011 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_136
2022-10-24 15:20:04,013 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_142
2022-10-24 15:20:04,017 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_142
2022-10-24 15:20:04,017 - trainer - INFO - 
*****************[epoch: 142, global step: 143] eval training set at end of epoch***************
2022-10-24 15:20:04,018 - trainer - INFO - {
  "train_loss": 6.352427959442139
}
2022-10-24 15:20:04,018 - trainer - INFO - start training epoch 143
2022-10-24 15:20:04,019 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,019 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,019 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,027 - trainer - INFO - 
*****************[epoch: 143, global step: 144] eval training set at end of epoch***************
2022-10-24 15:20:04,027 - trainer - INFO - {
  "train_loss": 6.588968753814697
}
2022-10-24 15:20:04,027 - trainer - INFO - start training epoch 144
2022-10-24 15:20:04,028 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,028 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,028 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,036 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval training set based on eval_every=2***************
2022-10-24 15:20:04,036 - trainer - INFO - {
  "train_loss": 6.61484956741333
}
2022-10-24 15:20:04,046 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval development set based on eval_every=2***************
2022-10-24 15:20:04,047 - trainer - INFO - {
  "dev_loss": 6.904594898223877,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,047 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:20:04,048 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,049 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,049 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,049 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_138
2022-10-24 15:20:04,051 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_144
2022-10-24 15:20:04,056 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_144
2022-10-24 15:20:04,057 - trainer - INFO - 
*****************[epoch: 144, global step: 145] eval training set at end of epoch***************
2022-10-24 15:20:04,057 - trainer - INFO - {
  "train_loss": 6.640730381011963
}
2022-10-24 15:20:04,057 - trainer - INFO - start training epoch 145
2022-10-24 15:20:04,058 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,058 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,058 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,067 - trainer - INFO - 
*****************[epoch: 145, global step: 146] eval training set at end of epoch***************
2022-10-24 15:20:04,068 - trainer - INFO - {
  "train_loss": 6.904594898223877
}
2022-10-24 15:20:04,068 - trainer - INFO - start training epoch 146
2022-10-24 15:20:04,068 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,068 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,069 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,074 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval training set based on eval_every=2***************
2022-10-24 15:20:04,075 - trainer - INFO - {
  "train_loss": 6.86994481086731
}
2022-10-24 15:20:04,080 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval development set based on eval_every=2***************
2022-10-24 15:20:04,081 - trainer - INFO - {
  "dev_loss": 6.810380458831787,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,083 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:20:04,083 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,084 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,087 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,087 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_140
2022-10-24 15:20:04,088 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_146
2022-10-24 15:20:04,093 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_146
2022-10-24 15:20:04,094 - trainer - INFO - 
*****************[epoch: 146, global step: 147] eval training set at end of epoch***************
2022-10-24 15:20:04,094 - trainer - INFO - {
  "train_loss": 6.835294723510742
}
2022-10-24 15:20:04,094 - trainer - INFO - start training epoch 147
2022-10-24 15:20:04,095 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,095 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,095 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,104 - trainer - INFO - 
*****************[epoch: 147, global step: 148] eval training set at end of epoch***************
2022-10-24 15:20:04,104 - trainer - INFO - {
  "train_loss": 6.810380458831787
}
2022-10-24 15:20:04,105 - trainer - INFO - start training epoch 148
2022-10-24 15:20:04,105 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,105 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,105 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,112 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval training set based on eval_every=2***************
2022-10-24 15:20:04,112 - trainer - INFO - {
  "train_loss": 6.808500289916992
}
2022-10-24 15:20:04,121 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval development set based on eval_every=2***************
2022-10-24 15:20:04,121 - trainer - INFO - {
  "dev_loss": 6.565743923187256,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,122 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:20:04,122 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,123 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,123 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,124 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_142
2022-10-24 15:20:04,125 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_148
2022-10-24 15:20:04,130 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_148
2022-10-24 15:20:04,130 - trainer - INFO - 
*****************[epoch: 148, global step: 149] eval training set at end of epoch***************
2022-10-24 15:20:04,131 - trainer - INFO - {
  "train_loss": 6.806620121002197
}
2022-10-24 15:20:04,131 - trainer - INFO - start training epoch 149
2022-10-24 15:20:04,132 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,132 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,132 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,139 - trainer - INFO - 
*****************[epoch: 149, global step: 150] eval training set at end of epoch***************
2022-10-24 15:20:04,139 - trainer - INFO - {
  "train_loss": 6.565743923187256
}
2022-10-24 15:20:04,139 - trainer - INFO - start training epoch 150
2022-10-24 15:20:04,140 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,140 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,140 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,150 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval training set based on eval_every=2***************
2022-10-24 15:20:04,150 - trainer - INFO - {
  "train_loss": 6.514077663421631
}
2022-10-24 15:20:04,158 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval development set based on eval_every=2***************
2022-10-24 15:20:04,158 - trainer - INFO - {
  "dev_loss": 6.246333599090576,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,159 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:20:04,161 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,163 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,163 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,163 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_144
2022-10-24 15:20:04,165 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_150
2022-10-24 15:20:04,170 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_150
2022-10-24 15:20:04,171 - trainer - INFO - 
*****************[epoch: 150, global step: 151] eval training set at end of epoch***************
2022-10-24 15:20:04,171 - trainer - INFO - {
  "train_loss": 6.462411403656006
}
2022-10-24 15:20:04,171 - trainer - INFO - start training epoch 151
2022-10-24 15:20:04,172 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,172 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,172 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,182 - trainer - INFO - 
*****************[epoch: 151, global step: 152] eval training set at end of epoch***************
2022-10-24 15:20:04,182 - trainer - INFO - {
  "train_loss": 6.246333599090576
}
2022-10-24 15:20:04,182 - trainer - INFO - start training epoch 152
2022-10-24 15:20:04,183 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,183 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,183 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,190 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval training set based on eval_every=2***************
2022-10-24 15:20:04,192 - trainer - INFO - {
  "train_loss": 6.104713439941406
}
2022-10-24 15:20:04,202 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval development set based on eval_every=2***************
2022-10-24 15:20:04,203 - trainer - INFO - {
  "dev_loss": 5.7821364402771,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,204 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:20:04,204 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,205 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,206 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,206 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_146
2022-10-24 15:20:04,208 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_152
2022-10-24 15:20:04,214 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_152
2022-10-24 15:20:04,215 - trainer - INFO - 
*****************[epoch: 152, global step: 153] eval training set at end of epoch***************
2022-10-24 15:20:04,216 - trainer - INFO - {
  "train_loss": 5.963093280792236
}
2022-10-24 15:20:04,216 - trainer - INFO - start training epoch 153
2022-10-24 15:20:04,216 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,217 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,217 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,225 - trainer - INFO - 
*****************[epoch: 153, global step: 154] eval training set at end of epoch***************
2022-10-24 15:20:04,226 - trainer - INFO - {
  "train_loss": 5.7821364402771
}
2022-10-24 15:20:04,226 - trainer - INFO - start training epoch 154
2022-10-24 15:20:04,227 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,227 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,227 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,235 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval training set based on eval_every=2***************
2022-10-24 15:20:04,235 - trainer - INFO - {
  "train_loss": 5.624971389770508
}
2022-10-24 15:20:04,244 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval development set based on eval_every=2***************
2022-10-24 15:20:04,244 - trainer - INFO - {
  "dev_loss": 5.214094161987305,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,245 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:20:04,245 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,247 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,247 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,248 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_148
2022-10-24 15:20:04,250 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_154
2022-10-24 15:20:04,255 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_154
2022-10-24 15:20:04,256 - trainer - INFO - 
*****************[epoch: 154, global step: 155] eval training set at end of epoch***************
2022-10-24 15:20:04,256 - trainer - INFO - {
  "train_loss": 5.467806339263916
}
2022-10-24 15:20:04,257 - trainer - INFO - start training epoch 155
2022-10-24 15:20:04,257 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,258 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,258 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,267 - trainer - INFO - 
*****************[epoch: 155, global step: 156] eval training set at end of epoch***************
2022-10-24 15:20:04,267 - trainer - INFO - {
  "train_loss": 5.214094638824463
}
2022-10-24 15:20:04,267 - trainer - INFO - start training epoch 156
2022-10-24 15:20:04,268 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,269 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,269 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,275 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval training set based on eval_every=2***************
2022-10-24 15:20:04,275 - trainer - INFO - {
  "train_loss": 5.095313787460327
}
2022-10-24 15:20:04,282 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval development set based on eval_every=2***************
2022-10-24 15:20:04,282 - trainer - INFO - {
  "dev_loss": 4.674088954925537,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,283 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:20:04,284 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,285 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,286 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,286 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_150
2022-10-24 15:20:04,291 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_156
2022-10-24 15:20:04,296 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_156
2022-10-24 15:20:04,297 - trainer - INFO - 
*****************[epoch: 156, global step: 157] eval training set at end of epoch***************
2022-10-24 15:20:04,298 - trainer - INFO - {
  "train_loss": 4.976532936096191
}
2022-10-24 15:20:04,298 - trainer - INFO - start training epoch 157
2022-10-24 15:20:04,300 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,301 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,301 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,308 - trainer - INFO - 
*****************[epoch: 157, global step: 158] eval training set at end of epoch***************
2022-10-24 15:20:04,308 - trainer - INFO - {
  "train_loss": 4.674088954925537
}
2022-10-24 15:20:04,308 - trainer - INFO - start training epoch 158
2022-10-24 15:20:04,309 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,309 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,309 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,315 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval training set based on eval_every=2***************
2022-10-24 15:20:04,316 - trainer - INFO - {
  "train_loss": 4.5698277950286865
}
2022-10-24 15:20:04,321 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval development set based on eval_every=2***************
2022-10-24 15:20:04,321 - trainer - INFO - {
  "dev_loss": 4.216855525970459,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,322 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:20:04,322 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,323 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,323 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,324 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_152
2022-10-24 15:20:04,325 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_158
2022-10-24 15:20:04,329 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_158
2022-10-24 15:20:04,330 - trainer - INFO - 
*****************[epoch: 158, global step: 159] eval training set at end of epoch***************
2022-10-24 15:20:04,331 - trainer - INFO - {
  "train_loss": 4.465566635131836
}
2022-10-24 15:20:04,335 - trainer - INFO - start training epoch 159
2022-10-24 15:20:04,335 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,336 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,336 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,344 - trainer - INFO - 
*****************[epoch: 159, global step: 160] eval training set at end of epoch***************
2022-10-24 15:20:04,345 - trainer - INFO - {
  "train_loss": 4.216855525970459
}
2022-10-24 15:20:04,347 - trainer - INFO - start training epoch 160
2022-10-24 15:20:04,347 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,347 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,348 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,354 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval training set based on eval_every=2***************
2022-10-24 15:20:04,355 - trainer - INFO - {
  "train_loss": 4.103771209716797
}
2022-10-24 15:20:04,363 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval development set based on eval_every=2***************
2022-10-24 15:20:04,364 - trainer - INFO - {
  "dev_loss": 3.821087598800659,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,365 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:20:04,368 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,368 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,369 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,369 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_154
2022-10-24 15:20:04,370 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_160
2022-10-24 15:20:04,375 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_160
2022-10-24 15:20:04,375 - trainer - INFO - 
*****************[epoch: 160, global step: 161] eval training set at end of epoch***************
2022-10-24 15:20:04,376 - trainer - INFO - {
  "train_loss": 3.9906868934631348
}
2022-10-24 15:20:04,379 - trainer - INFO - start training epoch 161
2022-10-24 15:20:04,379 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,380 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,380 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,387 - trainer - INFO - 
*****************[epoch: 161, global step: 162] eval training set at end of epoch***************
2022-10-24 15:20:04,387 - trainer - INFO - {
  "train_loss": 3.821087598800659
}
2022-10-24 15:20:04,388 - trainer - INFO - start training epoch 162
2022-10-24 15:20:04,388 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,388 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,388 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,397 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval training set based on eval_every=2***************
2022-10-24 15:20:04,398 - trainer - INFO - {
  "train_loss": 3.718390464782715
}
2022-10-24 15:20:04,404 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval development set based on eval_every=2***************
2022-10-24 15:20:04,404 - trainer - INFO - {
  "dev_loss": 3.4753899574279785,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,405 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:20:04,405 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,406 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,406 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,407 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_156
2022-10-24 15:20:04,409 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_162
2022-10-24 15:20:04,416 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_162
2022-10-24 15:20:04,417 - trainer - INFO - 
*****************[epoch: 162, global step: 163] eval training set at end of epoch***************
2022-10-24 15:20:04,417 - trainer - INFO - {
  "train_loss": 3.6156933307647705
}
2022-10-24 15:20:04,417 - trainer - INFO - start training epoch 163
2022-10-24 15:20:04,417 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,418 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,418 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,426 - trainer - INFO - 
*****************[epoch: 163, global step: 164] eval training set at end of epoch***************
2022-10-24 15:20:04,427 - trainer - INFO - {
  "train_loss": 3.4753901958465576
}
2022-10-24 15:20:04,427 - trainer - INFO - start training epoch 164
2022-10-24 15:20:04,427 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,428 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,428 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,436 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval training set based on eval_every=2***************
2022-10-24 15:20:04,436 - trainer - INFO - {
  "train_loss": 3.406743884086609
}
2022-10-24 15:20:04,442 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval development set based on eval_every=2***************
2022-10-24 15:20:04,442 - trainer - INFO - {
  "dev_loss": 3.206164598464966,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,443 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:20:04,444 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,445 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,445 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,445 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_158
2022-10-24 15:20:04,446 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_164
2022-10-24 15:20:04,450 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_164
2022-10-24 15:20:04,451 - trainer - INFO - 
*****************[epoch: 164, global step: 165] eval training set at end of epoch***************
2022-10-24 15:20:04,451 - trainer - INFO - {
  "train_loss": 3.33809757232666
}
2022-10-24 15:20:04,451 - trainer - INFO - start training epoch 165
2022-10-24 15:20:04,452 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,452 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,452 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,464 - trainer - INFO - 
*****************[epoch: 165, global step: 166] eval training set at end of epoch***************
2022-10-24 15:20:04,464 - trainer - INFO - {
  "train_loss": 3.206164836883545
}
2022-10-24 15:20:04,464 - trainer - INFO - start training epoch 166
2022-10-24 15:20:04,465 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,465 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,465 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,473 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval training set based on eval_every=2***************
2022-10-24 15:20:04,473 - trainer - INFO - {
  "train_loss": 3.1654566526412964
}
2022-10-24 15:20:04,480 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval development set based on eval_every=2***************
2022-10-24 15:20:04,480 - trainer - INFO - {
  "dev_loss": 3.026733636856079,
  "dev_best_score_for_loss": -2.994868755340576
}
2022-10-24 15:20:04,481 - trainer - INFO -   no_improve_count: 17
2022-10-24 15:20:04,481 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,482 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,482 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,482 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_160
2022-10-24 15:20:04,484 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_166
2022-10-24 15:20:04,489 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_166
2022-10-24 15:20:04,489 - trainer - INFO - 
*****************[epoch: 166, global step: 167] eval training set at end of epoch***************
2022-10-24 15:20:04,490 - trainer - INFO - {
  "train_loss": 3.124748468399048
}
2022-10-24 15:20:04,490 - trainer - INFO - start training epoch 167
2022-10-24 15:20:04,491 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,491 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,491 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,502 - trainer - INFO - 
*****************[epoch: 167, global step: 168] eval training set at end of epoch***************
2022-10-24 15:20:04,502 - trainer - INFO - {
  "train_loss": 3.026733636856079
}
2022-10-24 15:20:04,502 - trainer - INFO - start training epoch 168
2022-10-24 15:20:04,502 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,503 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,503 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,510 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval training set based on eval_every=2***************
2022-10-24 15:20:04,510 - trainer - INFO - {
  "train_loss": 2.995529532432556
}
2022-10-24 15:20:04,517 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval development set based on eval_every=2***************
2022-10-24 15:20:04,518 - trainer - INFO - {
  "dev_loss": 2.9152615070343018,
  "dev_best_score_for_loss": -2.9152615070343018
}
2022-10-24 15:20:04,519 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:04,523 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,523 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,523 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_162
2022-10-24 15:20:04,525 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:04,529 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:04,529 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:04,529 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,530 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:04,530 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_168
2022-10-24 15:20:04,537 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_168
2022-10-24 15:20:04,538 - trainer - INFO - 
*****************[epoch: 168, global step: 169] eval training set at end of epoch***************
2022-10-24 15:20:04,538 - trainer - INFO - {
  "train_loss": 2.964325428009033
}
2022-10-24 15:20:04,539 - trainer - INFO - start training epoch 169
2022-10-24 15:20:04,539 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,539 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,539 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,547 - trainer - INFO - 
*****************[epoch: 169, global step: 170] eval training set at end of epoch***************
2022-10-24 15:20:04,548 - trainer - INFO - {
  "train_loss": 2.9152615070343018
}
2022-10-24 15:20:04,548 - trainer - INFO - start training epoch 170
2022-10-24 15:20:04,548 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,548 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,549 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,555 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval training set based on eval_every=2***************
2022-10-24 15:20:04,555 - trainer - INFO - {
  "train_loss": 2.8890981674194336
}
2022-10-24 15:20:04,568 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval development set based on eval_every=2***************
2022-10-24 15:20:04,568 - trainer - INFO - {
  "dev_loss": 2.843461751937866,
  "dev_best_score_for_loss": -2.843461751937866
}
2022-10-24 15:20:04,569 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:04,570 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,570 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,571 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_164
2022-10-24 15:20:04,572 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:04,575 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:04,576 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:04,576 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,577 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:04,577 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_170
2022-10-24 15:20:04,583 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_170
2022-10-24 15:20:04,584 - trainer - INFO - 
*****************[epoch: 170, global step: 171] eval training set at end of epoch***************
2022-10-24 15:20:04,584 - trainer - INFO - {
  "train_loss": 2.8629348278045654
}
2022-10-24 15:20:04,585 - trainer - INFO - start training epoch 171
2022-10-24 15:20:04,585 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,585 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,585 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,592 - trainer - INFO - 
*****************[epoch: 171, global step: 172] eval training set at end of epoch***************
2022-10-24 15:20:04,593 - trainer - INFO - {
  "train_loss": 2.843461751937866
}
2022-10-24 15:20:04,595 - trainer - INFO - start training epoch 172
2022-10-24 15:20:04,598 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,598 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,599 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,606 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval training set based on eval_every=2***************
2022-10-24 15:20:04,607 - trainer - INFO - {
  "train_loss": 2.829048275947571
}
2022-10-24 15:20:04,615 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval development set based on eval_every=2***************
2022-10-24 15:20:04,616 - trainer - INFO - {
  "dev_loss": 2.8015763759613037,
  "dev_best_score_for_loss": -2.8015763759613037
}
2022-10-24 15:20:04,616 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:04,618 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,618 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,618 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_166
2022-10-24 15:20:04,620 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:04,624 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:04,625 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:04,626 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,627 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:04,627 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_172
2022-10-24 15:20:04,633 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_172
2022-10-24 15:20:04,634 - trainer - INFO - 
*****************[epoch: 172, global step: 173] eval training set at end of epoch***************
2022-10-24 15:20:04,635 - trainer - INFO - {
  "train_loss": 2.8146347999572754
}
2022-10-24 15:20:04,635 - trainer - INFO - start training epoch 173
2022-10-24 15:20:04,635 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,636 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,636 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,650 - trainer - INFO - 
*****************[epoch: 173, global step: 174] eval training set at end of epoch***************
2022-10-24 15:20:04,651 - trainer - INFO - {
  "train_loss": 2.8015761375427246
}
2022-10-24 15:20:04,651 - trainer - INFO - start training epoch 174
2022-10-24 15:20:04,651 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,652 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,652 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,661 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval training set based on eval_every=2***************
2022-10-24 15:20:04,662 - trainer - INFO - {
  "train_loss": 2.7999073266983032
}
2022-10-24 15:20:04,674 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval development set based on eval_every=2***************
2022-10-24 15:20:04,678 - trainer - INFO - {
  "dev_loss": 2.7881226539611816,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:04,679 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:04,682 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,682 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,682 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_168
2022-10-24 15:20:04,684 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:04,688 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:04,689 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:04,689 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,690 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:04,690 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_174
2022-10-24 15:20:04,695 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_174
2022-10-24 15:20:04,695 - trainer - INFO - 
*****************[epoch: 174, global step: 175] eval training set at end of epoch***************
2022-10-24 15:20:04,696 - trainer - INFO - {
  "train_loss": 2.798238515853882
}
2022-10-24 15:20:04,696 - trainer - INFO - start training epoch 175
2022-10-24 15:20:04,696 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,697 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,697 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,706 - trainer - INFO - 
*****************[epoch: 175, global step: 176] eval training set at end of epoch***************
2022-10-24 15:20:04,706 - trainer - INFO - {
  "train_loss": 2.7881226539611816
}
2022-10-24 15:20:04,706 - trainer - INFO - start training epoch 176
2022-10-24 15:20:04,707 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,707 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,708 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,717 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval training set based on eval_every=2***************
2022-10-24 15:20:04,718 - trainer - INFO - {
  "train_loss": 2.7917213439941406
}
2022-10-24 15:20:04,730 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval development set based on eval_every=2***************
2022-10-24 15:20:04,731 - trainer - INFO - {
  "dev_loss": 2.795334577560425,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:04,731 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:04,732 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,735 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,735 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,736 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_170
2022-10-24 15:20:04,737 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_176
2022-10-24 15:20:04,742 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_176
2022-10-24 15:20:04,742 - trainer - INFO - 
*****************[epoch: 176, global step: 177] eval training set at end of epoch***************
2022-10-24 15:20:04,743 - trainer - INFO - {
  "train_loss": 2.7953200340270996
}
2022-10-24 15:20:04,743 - trainer - INFO - start training epoch 177
2022-10-24 15:20:04,744 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,744 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,744 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,752 - trainer - INFO - 
*****************[epoch: 177, global step: 178] eval training set at end of epoch***************
2022-10-24 15:20:04,753 - trainer - INFO - {
  "train_loss": 2.7953343391418457
}
2022-10-24 15:20:04,753 - trainer - INFO - start training epoch 178
2022-10-24 15:20:04,753 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,753 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,754 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,761 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval training set based on eval_every=2***************
2022-10-24 15:20:04,762 - trainer - INFO - {
  "train_loss": 2.797742486000061
}
2022-10-24 15:20:04,772 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval development set based on eval_every=2***************
2022-10-24 15:20:04,773 - trainer - INFO - {
  "dev_loss": 2.8097777366638184,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:04,773 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:04,774 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,775 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,775 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,776 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_172
2022-10-24 15:20:04,777 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_178
2022-10-24 15:20:04,782 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_178
2022-10-24 15:20:04,783 - trainer - INFO - 
*****************[epoch: 178, global step: 179] eval training set at end of epoch***************
2022-10-24 15:20:04,784 - trainer - INFO - {
  "train_loss": 2.8001506328582764
}
2022-10-24 15:20:04,784 - trainer - INFO - start training epoch 179
2022-10-24 15:20:04,784 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,784 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,785 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,792 - trainer - INFO - 
*****************[epoch: 179, global step: 180] eval training set at end of epoch***************
2022-10-24 15:20:04,792 - trainer - INFO - {
  "train_loss": 2.8097777366638184
}
2022-10-24 15:20:04,792 - trainer - INFO - start training epoch 180
2022-10-24 15:20:04,792 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,793 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,793 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,803 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval training set based on eval_every=2***************
2022-10-24 15:20:04,803 - trainer - INFO - {
  "train_loss": 2.810841679573059
}
2022-10-24 15:20:04,809 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval development set based on eval_every=2***************
2022-10-24 15:20:04,809 - trainer - INFO - {
  "dev_loss": 2.822258710861206,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:04,810 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:04,811 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,812 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,813 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,814 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_174
2022-10-24 15:20:04,815 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_180
2022-10-24 15:20:04,823 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_180
2022-10-24 15:20:04,824 - trainer - INFO - 
*****************[epoch: 180, global step: 181] eval training set at end of epoch***************
2022-10-24 15:20:04,824 - trainer - INFO - {
  "train_loss": 2.8119056224823
}
2022-10-24 15:20:04,825 - trainer - INFO - start training epoch 181
2022-10-24 15:20:04,825 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,825 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,826 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,836 - trainer - INFO - 
*****************[epoch: 181, global step: 182] eval training set at end of epoch***************
2022-10-24 15:20:04,836 - trainer - INFO - {
  "train_loss": 2.822258710861206
}
2022-10-24 15:20:04,837 - trainer - INFO - start training epoch 182
2022-10-24 15:20:04,837 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,837 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,837 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,848 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval training set based on eval_every=2***************
2022-10-24 15:20:04,849 - trainer - INFO - {
  "train_loss": 2.8243887424468994
}
2022-10-24 15:20:04,855 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval development set based on eval_every=2***************
2022-10-24 15:20:04,856 - trainer - INFO - {
  "dev_loss": 2.831486463546753,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:04,858 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:04,858 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,860 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,863 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,864 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_176
2022-10-24 15:20:04,866 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_182
2022-10-24 15:20:04,871 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_182
2022-10-24 15:20:04,873 - trainer - INFO - 
*****************[epoch: 182, global step: 183] eval training set at end of epoch***************
2022-10-24 15:20:04,873 - trainer - INFO - {
  "train_loss": 2.8265187740325928
}
2022-10-24 15:20:04,874 - trainer - INFO - start training epoch 183
2022-10-24 15:20:04,874 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,875 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,875 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,885 - trainer - INFO - 
*****************[epoch: 183, global step: 184] eval training set at end of epoch***************
2022-10-24 15:20:04,885 - trainer - INFO - {
  "train_loss": 2.831486463546753
}
2022-10-24 15:20:04,886 - trainer - INFO - start training epoch 184
2022-10-24 15:20:04,886 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,886 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,887 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,896 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval training set based on eval_every=2***************
2022-10-24 15:20:04,896 - trainer - INFO - {
  "train_loss": 2.834776997566223
}
2022-10-24 15:20:04,908 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval development set based on eval_every=2***************
2022-10-24 15:20:04,909 - trainer - INFO - {
  "dev_loss": 2.838735342025757,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:04,909 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:20:04,910 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,911 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,911 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,912 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_178
2022-10-24 15:20:04,913 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_184
2022-10-24 15:20:04,919 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_184
2022-10-24 15:20:04,921 - trainer - INFO - 
*****************[epoch: 184, global step: 185] eval training set at end of epoch***************
2022-10-24 15:20:04,921 - trainer - INFO - {
  "train_loss": 2.8380675315856934
}
2022-10-24 15:20:04,921 - trainer - INFO - start training epoch 185
2022-10-24 15:20:04,922 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,922 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,922 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,935 - trainer - INFO - 
*****************[epoch: 185, global step: 186] eval training set at end of epoch***************
2022-10-24 15:20:04,935 - trainer - INFO - {
  "train_loss": 2.838735342025757
}
2022-10-24 15:20:04,936 - trainer - INFO - start training epoch 186
2022-10-24 15:20:04,936 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,936 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,936 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,945 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval training set based on eval_every=2***************
2022-10-24 15:20:04,945 - trainer - INFO - {
  "train_loss": 2.8409337997436523
}
2022-10-24 15:20:04,953 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval development set based on eval_every=2***************
2022-10-24 15:20:04,954 - trainer - INFO - {
  "dev_loss": 2.8433053493499756,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:04,954 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:20:04,955 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,956 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,956 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,956 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_180
2022-10-24 15:20:04,958 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_186
2022-10-24 15:20:04,963 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_186
2022-10-24 15:20:04,964 - trainer - INFO - 
*****************[epoch: 186, global step: 187] eval training set at end of epoch***************
2022-10-24 15:20:04,964 - trainer - INFO - {
  "train_loss": 2.843132257461548
}
2022-10-24 15:20:04,966 - trainer - INFO - start training epoch 187
2022-10-24 15:20:04,966 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,967 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,967 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,974 - trainer - INFO - 
*****************[epoch: 187, global step: 188] eval training set at end of epoch***************
2022-10-24 15:20:04,975 - trainer - INFO - {
  "train_loss": 2.8433053493499756
}
2022-10-24 15:20:04,975 - trainer - INFO - start training epoch 188
2022-10-24 15:20:04,976 - trainer - INFO - training using device=cuda
2022-10-24 15:20:04,976 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:04,977 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:04,984 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval training set based on eval_every=2***************
2022-10-24 15:20:04,984 - trainer - INFO - {
  "train_loss": 2.8433339595794678
}
2022-10-24 15:20:04,990 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval development set based on eval_every=2***************
2022-10-24 15:20:04,991 - trainer - INFO - {
  "dev_loss": 2.843738317489624,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:04,991 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:20:04,992 - trainer - INFO -   patience: 200
2022-10-24 15:20:04,993 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:04,993 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:04,994 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_182
2022-10-24 15:20:04,995 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_188
2022-10-24 15:20:05,001 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_188
2022-10-24 15:20:05,002 - trainer - INFO - 
*****************[epoch: 188, global step: 189] eval training set at end of epoch***************
2022-10-24 15:20:05,003 - trainer - INFO - {
  "train_loss": 2.84336256980896
}
2022-10-24 15:20:05,003 - trainer - INFO - start training epoch 189
2022-10-24 15:20:05,003 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,004 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,004 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,020 - trainer - INFO - 
*****************[epoch: 189, global step: 190] eval training set at end of epoch***************
2022-10-24 15:20:05,020 - trainer - INFO - {
  "train_loss": 2.843738079071045
}
2022-10-24 15:20:05,021 - trainer - INFO - start training epoch 190
2022-10-24 15:20:05,021 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,021 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,022 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,033 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval training set based on eval_every=2***************
2022-10-24 15:20:05,033 - trainer - INFO - {
  "train_loss": 2.8421634435653687
}
2022-10-24 15:20:05,041 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval development set based on eval_every=2***************
2022-10-24 15:20:05,041 - trainer - INFO - {
  "dev_loss": 2.8396999835968018,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,042 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:20:05,042 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,045 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,049 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,049 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_184
2022-10-24 15:20:05,051 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_190
2022-10-24 15:20:05,057 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_190
2022-10-24 15:20:05,057 - trainer - INFO - 
*****************[epoch: 190, global step: 191] eval training set at end of epoch***************
2022-10-24 15:20:05,059 - trainer - INFO - {
  "train_loss": 2.8405888080596924
}
2022-10-24 15:20:05,059 - trainer - INFO - start training epoch 191
2022-10-24 15:20:05,060 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,060 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,060 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,071 - trainer - INFO - 
*****************[epoch: 191, global step: 192] eval training set at end of epoch***************
2022-10-24 15:20:05,072 - trainer - INFO - {
  "train_loss": 2.8396999835968018
}
2022-10-24 15:20:05,072 - trainer - INFO - start training epoch 192
2022-10-24 15:20:05,072 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,072 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,073 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,081 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval training set based on eval_every=2***************
2022-10-24 15:20:05,082 - trainer - INFO - {
  "train_loss": 2.837862014770508
}
2022-10-24 15:20:05,089 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval development set based on eval_every=2***************
2022-10-24 15:20:05,090 - trainer - INFO - {
  "dev_loss": 2.832750082015991,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,091 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:20:05,092 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,093 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,096 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,096 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_186
2022-10-24 15:20:05,098 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_192
2022-10-24 15:20:05,104 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_192
2022-10-24 15:20:05,105 - trainer - INFO - 
*****************[epoch: 192, global step: 193] eval training set at end of epoch***************
2022-10-24 15:20:05,106 - trainer - INFO - {
  "train_loss": 2.836024045944214
}
2022-10-24 15:20:05,106 - trainer - INFO - start training epoch 193
2022-10-24 15:20:05,107 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,107 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,107 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,117 - trainer - INFO - 
*****************[epoch: 193, global step: 194] eval training set at end of epoch***************
2022-10-24 15:20:05,117 - trainer - INFO - {
  "train_loss": 2.832750082015991
}
2022-10-24 15:20:05,118 - trainer - INFO - start training epoch 194
2022-10-24 15:20:05,118 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,118 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,119 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,132 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval training set based on eval_every=2***************
2022-10-24 15:20:05,132 - trainer - INFO - {
  "train_loss": 2.831211566925049
}
2022-10-24 15:20:05,140 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval development set based on eval_every=2***************
2022-10-24 15:20:05,140 - trainer - INFO - {
  "dev_loss": 2.8250811100006104,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,141 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:20:05,141 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,143 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,143 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,143 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_188
2022-10-24 15:20:05,145 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_194
2022-10-24 15:20:05,150 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_194
2022-10-24 15:20:05,150 - trainer - INFO - 
*****************[epoch: 194, global step: 195] eval training set at end of epoch***************
2022-10-24 15:20:05,152 - trainer - INFO - {
  "train_loss": 2.8296730518341064
}
2022-10-24 15:20:05,152 - trainer - INFO - start training epoch 195
2022-10-24 15:20:05,152 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,152 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,153 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,161 - trainer - INFO - 
*****************[epoch: 195, global step: 196] eval training set at end of epoch***************
2022-10-24 15:20:05,161 - trainer - INFO - {
  "train_loss": 2.8250813484191895
}
2022-10-24 15:20:05,162 - trainer - INFO - start training epoch 196
2022-10-24 15:20:05,162 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,162 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,163 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,173 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval training set based on eval_every=2***************
2022-10-24 15:20:05,173 - trainer - INFO - {
  "train_loss": 2.82357120513916
}
2022-10-24 15:20:05,180 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval development set based on eval_every=2***************
2022-10-24 15:20:05,181 - trainer - INFO - {
  "dev_loss": 2.8175723552703857,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,181 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:20:05,181 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,184 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,187 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,187 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_190
2022-10-24 15:20:05,189 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_196
2022-10-24 15:20:05,193 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_196
2022-10-24 15:20:05,194 - trainer - INFO - 
*****************[epoch: 196, global step: 197] eval training set at end of epoch***************
2022-10-24 15:20:05,194 - trainer - INFO - {
  "train_loss": 2.822061061859131
}
2022-10-24 15:20:05,195 - trainer - INFO - start training epoch 197
2022-10-24 15:20:05,195 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,195 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,196 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,205 - trainer - INFO - 
*****************[epoch: 197, global step: 198] eval training set at end of epoch***************
2022-10-24 15:20:05,206 - trainer - INFO - {
  "train_loss": 2.8175721168518066
}
2022-10-24 15:20:05,206 - trainer - INFO - start training epoch 198
2022-10-24 15:20:05,207 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,207 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,207 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,215 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval training set based on eval_every=2***************
2022-10-24 15:20:05,215 - trainer - INFO - {
  "train_loss": 2.8157917261123657
}
2022-10-24 15:20:05,221 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval development set based on eval_every=2***************
2022-10-24 15:20:05,221 - trainer - INFO - {
  "dev_loss": 2.810573101043701,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,222 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:20:05,222 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,223 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,223 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,223 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_192
2022-10-24 15:20:05,225 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_198
2022-10-24 15:20:05,229 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_198
2022-10-24 15:20:05,230 - trainer - INFO - 
*****************[epoch: 198, global step: 199] eval training set at end of epoch***************
2022-10-24 15:20:05,231 - trainer - INFO - {
  "train_loss": 2.814011335372925
}
2022-10-24 15:20:05,232 - trainer - INFO - start training epoch 199
2022-10-24 15:20:05,235 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,236 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,236 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,245 - trainer - INFO - 
*****************[epoch: 199, global step: 200] eval training set at end of epoch***************
2022-10-24 15:20:05,246 - trainer - INFO - {
  "train_loss": 2.810573101043701
}
2022-10-24 15:20:05,246 - trainer - INFO - start training epoch 200
2022-10-24 15:20:05,246 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,247 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,247 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,255 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval training set based on eval_every=2***************
2022-10-24 15:20:05,255 - trainer - INFO - {
  "train_loss": 2.8086862564086914
}
2022-10-24 15:20:05,269 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval development set based on eval_every=2***************
2022-10-24 15:20:05,269 - trainer - INFO - {
  "dev_loss": 2.8041415214538574,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,270 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:20:05,271 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,272 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,272 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,272 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_194
2022-10-24 15:20:05,274 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_200
2022-10-24 15:20:05,279 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_200
2022-10-24 15:20:05,280 - trainer - INFO - 
*****************[epoch: 200, global step: 201] eval training set at end of epoch***************
2022-10-24 15:20:05,280 - trainer - INFO - {
  "train_loss": 2.8067994117736816
}
2022-10-24 15:20:05,281 - trainer - INFO - start training epoch 201
2022-10-24 15:20:05,281 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,281 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,281 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,289 - trainer - INFO - 
*****************[epoch: 201, global step: 202] eval training set at end of epoch***************
2022-10-24 15:20:05,289 - trainer - INFO - {
  "train_loss": 2.8041415214538574
}
2022-10-24 15:20:05,289 - trainer - INFO - start training epoch 202
2022-10-24 15:20:05,290 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,290 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,290 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,301 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval training set based on eval_every=2***************
2022-10-24 15:20:05,301 - trainer - INFO - {
  "train_loss": 2.802546501159668
}
2022-10-24 15:20:05,310 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval development set based on eval_every=2***************
2022-10-24 15:20:05,310 - trainer - INFO - {
  "dev_loss": 2.7986350059509277,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,311 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:20:05,311 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,312 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,313 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,313 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_196
2022-10-24 15:20:05,314 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_202
2022-10-24 15:20:05,319 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_202
2022-10-24 15:20:05,319 - trainer - INFO - 
*****************[epoch: 202, global step: 203] eval training set at end of epoch***************
2022-10-24 15:20:05,320 - trainer - INFO - {
  "train_loss": 2.8009514808654785
}
2022-10-24 15:20:05,320 - trainer - INFO - start training epoch 203
2022-10-24 15:20:05,321 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,321 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,321 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,333 - trainer - INFO - 
*****************[epoch: 203, global step: 204] eval training set at end of epoch***************
2022-10-24 15:20:05,334 - trainer - INFO - {
  "train_loss": 2.7986350059509277
}
2022-10-24 15:20:05,334 - trainer - INFO - start training epoch 204
2022-10-24 15:20:05,334 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,334 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,335 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,342 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval training set based on eval_every=2***************
2022-10-24 15:20:05,342 - trainer - INFO - {
  "train_loss": 2.797525405883789
}
2022-10-24 15:20:05,350 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval development set based on eval_every=2***************
2022-10-24 15:20:05,350 - trainer - INFO - {
  "dev_loss": 2.794377088546753,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,351 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:20:05,351 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,352 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,353 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,353 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_198
2022-10-24 15:20:05,355 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_204
2022-10-24 15:20:05,358 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_204
2022-10-24 15:20:05,359 - trainer - INFO - 
*****************[epoch: 204, global step: 205] eval training set at end of epoch***************
2022-10-24 15:20:05,359 - trainer - INFO - {
  "train_loss": 2.7964158058166504
}
2022-10-24 15:20:05,360 - trainer - INFO - start training epoch 205
2022-10-24 15:20:05,360 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,360 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,361 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,370 - trainer - INFO - 
*****************[epoch: 205, global step: 206] eval training set at end of epoch***************
2022-10-24 15:20:05,373 - trainer - INFO - {
  "train_loss": 2.794377088546753
}
2022-10-24 15:20:05,373 - trainer - INFO - start training epoch 206
2022-10-24 15:20:05,373 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,373 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,374 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,382 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval training set based on eval_every=2***************
2022-10-24 15:20:05,383 - trainer - INFO - {
  "train_loss": 2.793745279312134
}
2022-10-24 15:20:05,390 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval development set based on eval_every=2***************
2022-10-24 15:20:05,390 - trainer - INFO - {
  "dev_loss": 2.791501522064209,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,391 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:20:05,391 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,392 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,392 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,393 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_200
2022-10-24 15:20:05,394 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_206
2022-10-24 15:20:05,398 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_206
2022-10-24 15:20:05,401 - trainer - INFO - 
*****************[epoch: 206, global step: 207] eval training set at end of epoch***************
2022-10-24 15:20:05,402 - trainer - INFO - {
  "train_loss": 2.7931134700775146
}
2022-10-24 15:20:05,403 - trainer - INFO - start training epoch 207
2022-10-24 15:20:05,405 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,406 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,406 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,415 - trainer - INFO - 
*****************[epoch: 207, global step: 208] eval training set at end of epoch***************
2022-10-24 15:20:05,416 - trainer - INFO - {
  "train_loss": 2.791501760482788
}
2022-10-24 15:20:05,417 - trainer - INFO - start training epoch 208
2022-10-24 15:20:05,417 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,417 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,417 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,423 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval training set based on eval_every=2***************
2022-10-24 15:20:05,424 - trainer - INFO - {
  "train_loss": 2.791070342063904
}
2022-10-24 15:20:05,437 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval development set based on eval_every=2***************
2022-10-24 15:20:05,437 - trainer - INFO - {
  "dev_loss": 2.78965425491333,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,438 - trainer - INFO -   no_improve_count: 17
2022-10-24 15:20:05,438 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,439 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,439 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,440 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_202
2022-10-24 15:20:05,441 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_208
2022-10-24 15:20:05,445 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_208
2022-10-24 15:20:05,447 - trainer - INFO - 
*****************[epoch: 208, global step: 209] eval training set at end of epoch***************
2022-10-24 15:20:05,447 - trainer - INFO - {
  "train_loss": 2.7906389236450195
}
2022-10-24 15:20:05,450 - trainer - INFO - start training epoch 209
2022-10-24 15:20:05,451 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,451 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,451 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,462 - trainer - INFO - 
*****************[epoch: 209, global step: 210] eval training set at end of epoch***************
2022-10-24 15:20:05,462 - trainer - INFO - {
  "train_loss": 2.78965425491333
}
2022-10-24 15:20:05,463 - trainer - INFO - start training epoch 210
2022-10-24 15:20:05,463 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,463 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,464 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,471 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval training set based on eval_every=2***************
2022-10-24 15:20:05,471 - trainer - INFO - {
  "train_loss": 2.7893874645233154
}
2022-10-24 15:20:05,479 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval development set based on eval_every=2***************
2022-10-24 15:20:05,482 - trainer - INFO - {
  "dev_loss": 2.7887284755706787,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,483 - trainer - INFO -   no_improve_count: 18
2022-10-24 15:20:05,484 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,485 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,485 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,485 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_204
2022-10-24 15:20:05,487 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_210
2022-10-24 15:20:05,498 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_210
2022-10-24 15:20:05,500 - trainer - INFO - 
*****************[epoch: 210, global step: 211] eval training set at end of epoch***************
2022-10-24 15:20:05,501 - trainer - INFO - {
  "train_loss": 2.789120674133301
}
2022-10-24 15:20:05,502 - trainer - INFO - start training epoch 211
2022-10-24 15:20:05,502 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,503 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,503 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,514 - trainer - INFO - 
*****************[epoch: 211, global step: 212] eval training set at end of epoch***************
2022-10-24 15:20:05,514 - trainer - INFO - {
  "train_loss": 2.7887284755706787
}
2022-10-24 15:20:05,514 - trainer - INFO - start training epoch 212
2022-10-24 15:20:05,515 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,515 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,515 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,522 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval training set based on eval_every=2***************
2022-10-24 15:20:05,524 - trainer - INFO - {
  "train_loss": 2.7885326147079468
}
2022-10-24 15:20:05,538 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval development set based on eval_every=2***************
2022-10-24 15:20:05,539 - trainer - INFO - {
  "dev_loss": 2.7883477210998535,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,540 - trainer - INFO -   no_improve_count: 19
2022-10-24 15:20:05,541 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,542 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,542 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,543 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_206
2022-10-24 15:20:05,544 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_212
2022-10-24 15:20:05,549 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_212
2022-10-24 15:20:05,550 - trainer - INFO - 
*****************[epoch: 212, global step: 213] eval training set at end of epoch***************
2022-10-24 15:20:05,550 - trainer - INFO - {
  "train_loss": 2.788336753845215
}
2022-10-24 15:20:05,550 - trainer - INFO - start training epoch 213
2022-10-24 15:20:05,550 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,551 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,551 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,558 - trainer - INFO - 
*****************[epoch: 213, global step: 214] eval training set at end of epoch***************
2022-10-24 15:20:05,558 - trainer - INFO - {
  "train_loss": 2.7883479595184326
}
2022-10-24 15:20:05,558 - trainer - INFO - start training epoch 214
2022-10-24 15:20:05,559 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,559 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,559 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,567 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval training set based on eval_every=2***************
2022-10-24 15:20:05,567 - trainer - INFO - {
  "train_loss": 2.7882189750671387
}
2022-10-24 15:20:05,574 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval development set based on eval_every=2***************
2022-10-24 15:20:05,575 - trainer - INFO - {
  "dev_loss": 2.788130760192871,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,575 - trainer - INFO -   no_improve_count: 20
2022-10-24 15:20:05,576 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,577 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,577 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,578 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_208
2022-10-24 15:20:05,579 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_214
2022-10-24 15:20:05,584 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_214
2022-10-24 15:20:05,585 - trainer - INFO - 
*****************[epoch: 214, global step: 215] eval training set at end of epoch***************
2022-10-24 15:20:05,586 - trainer - INFO - {
  "train_loss": 2.7880899906158447
}
2022-10-24 15:20:05,586 - trainer - INFO - start training epoch 215
2022-10-24 15:20:05,586 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,587 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,587 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,594 - trainer - INFO - 
*****************[epoch: 215, global step: 216] eval training set at end of epoch***************
2022-10-24 15:20:05,595 - trainer - INFO - {
  "train_loss": 2.788130760192871
}
2022-10-24 15:20:05,595 - trainer - INFO - start training epoch 216
2022-10-24 15:20:05,595 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,596 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,596 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,604 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval training set based on eval_every=2***************
2022-10-24 15:20:05,605 - trainer - INFO - {
  "train_loss": 2.788181185722351
}
2022-10-24 15:20:05,612 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval development set based on eval_every=2***************
2022-10-24 15:20:05,613 - trainer - INFO - {
  "dev_loss": 2.788316011428833,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,613 - trainer - INFO -   no_improve_count: 21
2022-10-24 15:20:05,614 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,615 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,615 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,616 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_210
2022-10-24 15:20:05,618 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_216
2022-10-24 15:20:05,623 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_216
2022-10-24 15:20:05,623 - trainer - INFO - 
*****************[epoch: 216, global step: 217] eval training set at end of epoch***************
2022-10-24 15:20:05,624 - trainer - INFO - {
  "train_loss": 2.788231611251831
}
2022-10-24 15:20:05,624 - trainer - INFO - start training epoch 217
2022-10-24 15:20:05,624 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,624 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,625 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,638 - trainer - INFO - 
*****************[epoch: 217, global step: 218] eval training set at end of epoch***************
2022-10-24 15:20:05,638 - trainer - INFO - {
  "train_loss": 2.788316011428833
}
2022-10-24 15:20:05,639 - trainer - INFO - start training epoch 218
2022-10-24 15:20:05,639 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,639 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,639 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,647 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval training set based on eval_every=2***************
2022-10-24 15:20:05,648 - trainer - INFO - {
  "train_loss": 2.78837513923645
}
2022-10-24 15:20:05,653 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval development set based on eval_every=2***************
2022-10-24 15:20:05,654 - trainer - INFO - {
  "dev_loss": 2.7885923385620117,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,654 - trainer - INFO -   no_improve_count: 22
2022-10-24 15:20:05,655 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,656 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,656 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,656 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_212
2022-10-24 15:20:05,657 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_218
2022-10-24 15:20:05,661 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_218
2022-10-24 15:20:05,663 - trainer - INFO - 
*****************[epoch: 218, global step: 219] eval training set at end of epoch***************
2022-10-24 15:20:05,664 - trainer - INFO - {
  "train_loss": 2.7884342670440674
}
2022-10-24 15:20:05,666 - trainer - INFO - start training epoch 219
2022-10-24 15:20:05,669 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,669 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,669 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,680 - trainer - INFO - 
*****************[epoch: 219, global step: 220] eval training set at end of epoch***************
2022-10-24 15:20:05,681 - trainer - INFO - {
  "train_loss": 2.7885923385620117
}
2022-10-24 15:20:05,681 - trainer - INFO - start training epoch 220
2022-10-24 15:20:05,681 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,681 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,682 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,688 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval training set based on eval_every=2***************
2022-10-24 15:20:05,689 - trainer - INFO - {
  "train_loss": 2.788670301437378
}
2022-10-24 15:20:05,696 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval development set based on eval_every=2***************
2022-10-24 15:20:05,697 - trainer - INFO - {
  "dev_loss": 2.7888023853302,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,697 - trainer - INFO -   no_improve_count: 23
2022-10-24 15:20:05,698 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,699 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,699 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,699 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_214
2022-10-24 15:20:05,701 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_220
2022-10-24 15:20:05,705 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_220
2022-10-24 15:20:05,705 - trainer - INFO - 
*****************[epoch: 220, global step: 221] eval training set at end of epoch***************
2022-10-24 15:20:05,705 - trainer - INFO - {
  "train_loss": 2.788748264312744
}
2022-10-24 15:20:05,706 - trainer - INFO - start training epoch 221
2022-10-24 15:20:05,706 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,706 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,707 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,717 - trainer - INFO - 
*****************[epoch: 221, global step: 222] eval training set at end of epoch***************
2022-10-24 15:20:05,717 - trainer - INFO - {
  "train_loss": 2.7888023853302
}
2022-10-24 15:20:05,718 - trainer - INFO - start training epoch 222
2022-10-24 15:20:05,718 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,718 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,718 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,727 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval training set based on eval_every=2***************
2022-10-24 15:20:05,728 - trainer - INFO - {
  "train_loss": 2.788838744163513
}
2022-10-24 15:20:05,737 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval development set based on eval_every=2***************
2022-10-24 15:20:05,737 - trainer - INFO - {
  "dev_loss": 2.7888827323913574,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,738 - trainer - INFO -   no_improve_count: 24
2022-10-24 15:20:05,738 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,739 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,739 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,740 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_216
2022-10-24 15:20:05,742 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_222
2022-10-24 15:20:05,746 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_222
2022-10-24 15:20:05,747 - trainer - INFO - 
*****************[epoch: 222, global step: 223] eval training set at end of epoch***************
2022-10-24 15:20:05,747 - trainer - INFO - {
  "train_loss": 2.788875102996826
}
2022-10-24 15:20:05,748 - trainer - INFO - start training epoch 223
2022-10-24 15:20:05,748 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,748 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,749 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,757 - trainer - INFO - 
*****************[epoch: 223, global step: 224] eval training set at end of epoch***************
2022-10-24 15:20:05,758 - trainer - INFO - {
  "train_loss": 2.7888827323913574
}
2022-10-24 15:20:05,758 - trainer - INFO - start training epoch 224
2022-10-24 15:20:05,758 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,758 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,759 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,767 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval training set based on eval_every=2***************
2022-10-24 15:20:05,768 - trainer - INFO - {
  "train_loss": 2.788960576057434
}
2022-10-24 15:20:05,776 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval development set based on eval_every=2***************
2022-10-24 15:20:05,777 - trainer - INFO - {
  "dev_loss": 2.789032459259033,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,777 - trainer - INFO -   no_improve_count: 25
2022-10-24 15:20:05,778 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,779 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,779 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,780 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_218
2022-10-24 15:20:05,781 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_224
2022-10-24 15:20:05,787 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_224
2022-10-24 15:20:05,789 - trainer - INFO - 
*****************[epoch: 224, global step: 225] eval training set at end of epoch***************
2022-10-24 15:20:05,789 - trainer - INFO - {
  "train_loss": 2.7890384197235107
}
2022-10-24 15:20:05,790 - trainer - INFO - start training epoch 225
2022-10-24 15:20:05,790 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,790 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,791 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,804 - trainer - INFO - 
*****************[epoch: 225, global step: 226] eval training set at end of epoch***************
2022-10-24 15:20:05,805 - trainer - INFO - {
  "train_loss": 2.789032220840454
}
2022-10-24 15:20:05,808 - trainer - INFO - start training epoch 226
2022-10-24 15:20:05,808 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,808 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,809 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,817 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval training set based on eval_every=2***************
2022-10-24 15:20:05,818 - trainer - INFO - {
  "train_loss": 2.7890446186065674
}
2022-10-24 15:20:05,826 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval development set based on eval_every=2***************
2022-10-24 15:20:05,826 - trainer - INFO - {
  "dev_loss": 2.7890493869781494,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,827 - trainer - INFO -   no_improve_count: 26
2022-10-24 15:20:05,827 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,829 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,829 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,830 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_220
2022-10-24 15:20:05,831 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_226
2022-10-24 15:20:05,837 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_226
2022-10-24 15:20:05,838 - trainer - INFO - 
*****************[epoch: 226, global step: 227] eval training set at end of epoch***************
2022-10-24 15:20:05,838 - trainer - INFO - {
  "train_loss": 2.7890570163726807
}
2022-10-24 15:20:05,839 - trainer - INFO - start training epoch 227
2022-10-24 15:20:05,839 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,839 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,840 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,848 - trainer - INFO - 
*****************[epoch: 227, global step: 228] eval training set at end of epoch***************
2022-10-24 15:20:05,850 - trainer - INFO - {
  "train_loss": 2.7890493869781494
}
2022-10-24 15:20:05,851 - trainer - INFO - start training epoch 228
2022-10-24 15:20:05,854 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,855 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,855 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,866 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval training set based on eval_every=2***************
2022-10-24 15:20:05,867 - trainer - INFO - {
  "train_loss": 2.7890180349349976
}
2022-10-24 15:20:05,874 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval development set based on eval_every=2***************
2022-10-24 15:20:05,874 - trainer - INFO - {
  "dev_loss": 2.7890939712524414,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,875 - trainer - INFO -   no_improve_count: 27
2022-10-24 15:20:05,875 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,876 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,876 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,877 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_222
2022-10-24 15:20:05,878 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_228
2022-10-24 15:20:05,882 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_228
2022-10-24 15:20:05,885 - trainer - INFO - 
*****************[epoch: 228, global step: 229] eval training set at end of epoch***************
2022-10-24 15:20:05,886 - trainer - INFO - {
  "train_loss": 2.7889866828918457
}
2022-10-24 15:20:05,886 - trainer - INFO - start training epoch 229
2022-10-24 15:20:05,886 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,886 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,887 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,898 - trainer - INFO - 
*****************[epoch: 229, global step: 230] eval training set at end of epoch***************
2022-10-24 15:20:05,898 - trainer - INFO - {
  "train_loss": 2.7890939712524414
}
2022-10-24 15:20:05,899 - trainer - INFO - start training epoch 230
2022-10-24 15:20:05,899 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,900 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,900 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,907 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval training set based on eval_every=2***************
2022-10-24 15:20:05,907 - trainer - INFO - {
  "train_loss": 2.789050340652466
}
2022-10-24 15:20:05,914 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval development set based on eval_every=2***************
2022-10-24 15:20:05,914 - trainer - INFO - {
  "dev_loss": 2.7889955043792725,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,915 - trainer - INFO -   no_improve_count: 28
2022-10-24 15:20:05,916 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,917 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,917 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,918 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_224
2022-10-24 15:20:05,919 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_230
2022-10-24 15:20:05,923 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_230
2022-10-24 15:20:05,924 - trainer - INFO - 
*****************[epoch: 230, global step: 231] eval training set at end of epoch***************
2022-10-24 15:20:05,924 - trainer - INFO - {
  "train_loss": 2.7890067100524902
}
2022-10-24 15:20:05,924 - trainer - INFO - start training epoch 231
2022-10-24 15:20:05,925 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,925 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,925 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,935 - trainer - INFO - 
*****************[epoch: 231, global step: 232] eval training set at end of epoch***************
2022-10-24 15:20:05,935 - trainer - INFO - {
  "train_loss": 2.7889955043792725
}
2022-10-24 15:20:05,936 - trainer - INFO - start training epoch 232
2022-10-24 15:20:05,936 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,936 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,936 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,944 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval training set based on eval_every=2***************
2022-10-24 15:20:05,947 - trainer - INFO - {
  "train_loss": 2.788956642150879
}
2022-10-24 15:20:05,954 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval development set based on eval_every=2***************
2022-10-24 15:20:05,955 - trainer - INFO - {
  "dev_loss": 2.7888097763061523,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:05,955 - trainer - INFO -   no_improve_count: 29
2022-10-24 15:20:05,956 - trainer - INFO -   patience: 200
2022-10-24 15:20:05,957 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:05,958 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:05,958 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_226
2022-10-24 15:20:05,959 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_232
2022-10-24 15:20:05,965 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_232
2022-10-24 15:20:05,966 - trainer - INFO - 
*****************[epoch: 232, global step: 233] eval training set at end of epoch***************
2022-10-24 15:20:05,966 - trainer - INFO - {
  "train_loss": 2.7889177799224854
}
2022-10-24 15:20:05,967 - trainer - INFO - start training epoch 233
2022-10-24 15:20:05,967 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,968 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,968 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,979 - trainer - INFO - 
*****************[epoch: 233, global step: 234] eval training set at end of epoch***************
2022-10-24 15:20:05,979 - trainer - INFO - {
  "train_loss": 2.7888097763061523
}
2022-10-24 15:20:05,980 - trainer - INFO - start training epoch 234
2022-10-24 15:20:05,980 - trainer - INFO - training using device=cuda
2022-10-24 15:20:05,980 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:05,981 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:05,990 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval training set based on eval_every=2***************
2022-10-24 15:20:05,991 - trainer - INFO - {
  "train_loss": 2.7887866497039795
}
2022-10-24 15:20:06,001 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval development set based on eval_every=2***************
2022-10-24 15:20:06,002 - trainer - INFO - {
  "dev_loss": 2.7885797023773193,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:06,003 - trainer - INFO -   no_improve_count: 30
2022-10-24 15:20:06,003 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,005 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,006 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,007 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_228
2022-10-24 15:20:06,011 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_234
2022-10-24 15:20:06,016 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_234
2022-10-24 15:20:06,017 - trainer - INFO - 
*****************[epoch: 234, global step: 235] eval training set at end of epoch***************
2022-10-24 15:20:06,018 - trainer - INFO - {
  "train_loss": 2.7887635231018066
}
2022-10-24 15:20:06,018 - trainer - INFO - start training epoch 235
2022-10-24 15:20:06,019 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,020 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,020 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,031 - trainer - INFO - 
*****************[epoch: 235, global step: 236] eval training set at end of epoch***************
2022-10-24 15:20:06,031 - trainer - INFO - {
  "train_loss": 2.7885797023773193
}
2022-10-24 15:20:06,032 - trainer - INFO - start training epoch 236
2022-10-24 15:20:06,032 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,032 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,033 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,041 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval training set based on eval_every=2***************
2022-10-24 15:20:06,041 - trainer - INFO - {
  "train_loss": 2.788582921028137
}
2022-10-24 15:20:06,049 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval development set based on eval_every=2***************
2022-10-24 15:20:06,050 - trainer - INFO - {
  "dev_loss": 2.788396120071411,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:06,052 - trainer - INFO -   no_improve_count: 31
2022-10-24 15:20:06,053 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,055 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,058 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,059 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_230
2022-10-24 15:20:06,060 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_236
2022-10-24 15:20:06,067 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_236
2022-10-24 15:20:06,068 - trainer - INFO - 
*****************[epoch: 236, global step: 237] eval training set at end of epoch***************
2022-10-24 15:20:06,068 - trainer - INFO - {
  "train_loss": 2.788586139678955
}
2022-10-24 15:20:06,069 - trainer - INFO - start training epoch 237
2022-10-24 15:20:06,069 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,069 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,070 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,079 - trainer - INFO - 
*****************[epoch: 237, global step: 238] eval training set at end of epoch***************
2022-10-24 15:20:06,080 - trainer - INFO - {
  "train_loss": 2.788396120071411
}
2022-10-24 15:20:06,080 - trainer - INFO - start training epoch 238
2022-10-24 15:20:06,080 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,080 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,081 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,088 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval training set based on eval_every=2***************
2022-10-24 15:20:06,088 - trainer - INFO - {
  "train_loss": 2.7883975505828857
}
2022-10-24 15:20:06,095 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval development set based on eval_every=2***************
2022-10-24 15:20:06,095 - trainer - INFO - {
  "dev_loss": 2.7883570194244385,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:06,096 - trainer - INFO -   no_improve_count: 32
2022-10-24 15:20:06,096 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,098 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,099 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,100 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_232
2022-10-24 15:20:06,104 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_238
2022-10-24 15:20:06,110 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_238
2022-10-24 15:20:06,113 - trainer - INFO - 
*****************[epoch: 238, global step: 239] eval training set at end of epoch***************
2022-10-24 15:20:06,113 - trainer - INFO - {
  "train_loss": 2.7883989810943604
}
2022-10-24 15:20:06,114 - trainer - INFO - start training epoch 239
2022-10-24 15:20:06,114 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,114 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,115 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,124 - trainer - INFO - 
*****************[epoch: 239, global step: 240] eval training set at end of epoch***************
2022-10-24 15:20:06,125 - trainer - INFO - {
  "train_loss": 2.7883570194244385
}
2022-10-24 15:20:06,125 - trainer - INFO - start training epoch 240
2022-10-24 15:20:06,126 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,126 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,126 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,136 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval training set based on eval_every=2***************
2022-10-24 15:20:06,137 - trainer - INFO - {
  "train_loss": 2.7883400917053223
}
2022-10-24 15:20:06,143 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval development set based on eval_every=2***************
2022-10-24 15:20:06,144 - trainer - INFO - {
  "dev_loss": 2.7882063388824463,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:06,145 - trainer - INFO -   no_improve_count: 33
2022-10-24 15:20:06,149 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,150 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,150 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,151 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_234
2022-10-24 15:20:06,152 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_240
2022-10-24 15:20:06,158 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_240
2022-10-24 15:20:06,161 - trainer - INFO - 
*****************[epoch: 240, global step: 241] eval training set at end of epoch***************
2022-10-24 15:20:06,162 - trainer - INFO - {
  "train_loss": 2.788323163986206
}
2022-10-24 15:20:06,162 - trainer - INFO - start training epoch 241
2022-10-24 15:20:06,162 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,163 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,163 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,173 - trainer - INFO - 
*****************[epoch: 241, global step: 242] eval training set at end of epoch***************
2022-10-24 15:20:06,173 - trainer - INFO - {
  "train_loss": 2.7882065773010254
}
2022-10-24 15:20:06,174 - trainer - INFO - start training epoch 242
2022-10-24 15:20:06,175 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,175 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,175 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,184 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval training set based on eval_every=2***************
2022-10-24 15:20:06,184 - trainer - INFO - {
  "train_loss": 2.7881972789764404
}
2022-10-24 15:20:06,192 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval development set based on eval_every=2***************
2022-10-24 15:20:06,196 - trainer - INFO - {
  "dev_loss": 2.788165807723999,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:06,197 - trainer - INFO -   no_improve_count: 34
2022-10-24 15:20:06,198 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,199 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,199 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,200 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_236
2022-10-24 15:20:06,201 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_242
2022-10-24 15:20:06,208 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_242
2022-10-24 15:20:06,208 - trainer - INFO - 
*****************[epoch: 242, global step: 243] eval training set at end of epoch***************
2022-10-24 15:20:06,209 - trainer - INFO - {
  "train_loss": 2.7881879806518555
}
2022-10-24 15:20:06,209 - trainer - INFO - start training epoch 243
2022-10-24 15:20:06,209 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,210 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,210 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,217 - trainer - INFO - 
*****************[epoch: 243, global step: 244] eval training set at end of epoch***************
2022-10-24 15:20:06,218 - trainer - INFO - {
  "train_loss": 2.78816556930542
}
2022-10-24 15:20:06,218 - trainer - INFO - start training epoch 244
2022-10-24 15:20:06,218 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,219 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,219 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,226 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval training set based on eval_every=2***************
2022-10-24 15:20:06,226 - trainer - INFO - {
  "train_loss": 2.7881520986557007
}
2022-10-24 15:20:06,235 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval development set based on eval_every=2***************
2022-10-24 15:20:06,237 - trainer - INFO - {
  "dev_loss": 2.788132905960083,
  "dev_best_score_for_loss": -2.7881226539611816
}
2022-10-24 15:20:06,238 - trainer - INFO -   no_improve_count: 35
2022-10-24 15:20:06,239 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,240 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,243 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,243 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_238
2022-10-24 15:20:06,245 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_244
2022-10-24 15:20:06,249 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_244
2022-10-24 15:20:06,250 - trainer - INFO - 
*****************[epoch: 244, global step: 245] eval training set at end of epoch***************
2022-10-24 15:20:06,250 - trainer - INFO - {
  "train_loss": 2.7881386280059814
}
2022-10-24 15:20:06,251 - trainer - INFO - start training epoch 245
2022-10-24 15:20:06,254 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,254 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,254 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,265 - trainer - INFO - 
*****************[epoch: 245, global step: 246] eval training set at end of epoch***************
2022-10-24 15:20:06,265 - trainer - INFO - {
  "train_loss": 2.788132905960083
}
2022-10-24 15:20:06,266 - trainer - INFO - start training epoch 246
2022-10-24 15:20:06,266 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,266 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,268 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,277 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval training set based on eval_every=2***************
2022-10-24 15:20:06,277 - trainer - INFO - {
  "train_loss": 2.788115620613098
}
2022-10-24 15:20:06,288 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval development set based on eval_every=2***************
2022-10-24 15:20:06,288 - trainer - INFO - {
  "dev_loss": 2.7881219387054443,
  "dev_best_score_for_loss": -2.7881219387054443
}
2022-10-24 15:20:06,289 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:06,291 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,291 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,291 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_240
2022-10-24 15:20:06,293 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:06,298 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:06,299 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:06,299 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,301 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:06,301 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_246
2022-10-24 15:20:06,306 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_246
2022-10-24 15:20:06,306 - trainer - INFO - 
*****************[epoch: 246, global step: 247] eval training set at end of epoch***************
2022-10-24 15:20:06,307 - trainer - INFO - {
  "train_loss": 2.7880983352661133
}
2022-10-24 15:20:06,307 - trainer - INFO - start training epoch 247
2022-10-24 15:20:06,307 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,308 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,308 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,319 - trainer - INFO - 
*****************[epoch: 247, global step: 248] eval training set at end of epoch***************
2022-10-24 15:20:06,319 - trainer - INFO - {
  "train_loss": 2.7881219387054443
}
2022-10-24 15:20:06,319 - trainer - INFO - start training epoch 248
2022-10-24 15:20:06,320 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,320 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,320 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,328 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval training set based on eval_every=2***************
2022-10-24 15:20:06,330 - trainer - INFO - {
  "train_loss": 2.7881247997283936
}
2022-10-24 15:20:06,339 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval development set based on eval_every=2***************
2022-10-24 15:20:06,339 - trainer - INFO - {
  "dev_loss": 2.7880990505218506,
  "dev_best_score_for_loss": -2.7880990505218506
}
2022-10-24 15:20:06,340 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:06,341 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,341 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,341 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_242
2022-10-24 15:20:06,343 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:06,347 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:06,350 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:06,350 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,351 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:06,351 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_248
2022-10-24 15:20:06,356 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_248
2022-10-24 15:20:06,357 - trainer - INFO - 
*****************[epoch: 248, global step: 249] eval training set at end of epoch***************
2022-10-24 15:20:06,357 - trainer - INFO - {
  "train_loss": 2.7881276607513428
}
2022-10-24 15:20:06,358 - trainer - INFO - start training epoch 249
2022-10-24 15:20:06,358 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,358 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,358 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,369 - trainer - INFO - 
*****************[epoch: 249, global step: 250] eval training set at end of epoch***************
2022-10-24 15:20:06,370 - trainer - INFO - {
  "train_loss": 2.7880990505218506
}
2022-10-24 15:20:06,370 - trainer - INFO - start training epoch 250
2022-10-24 15:20:06,370 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,371 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,371 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,378 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval training set based on eval_every=2***************
2022-10-24 15:20:06,378 - trainer - INFO - {
  "train_loss": 2.788106083869934
}
2022-10-24 15:20:06,385 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval development set based on eval_every=2***************
2022-10-24 15:20:06,386 - trainer - INFO - {
  "dev_loss": 2.7880964279174805,
  "dev_best_score_for_loss": -2.7880964279174805
}
2022-10-24 15:20:06,386 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:06,387 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,387 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,388 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_244
2022-10-24 15:20:06,389 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:06,393 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:06,394 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:06,395 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,399 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:06,399 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_250
2022-10-24 15:20:06,404 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_250
2022-10-24 15:20:06,405 - trainer - INFO - 
*****************[epoch: 250, global step: 251] eval training set at end of epoch***************
2022-10-24 15:20:06,406 - trainer - INFO - {
  "train_loss": 2.7881131172180176
}
2022-10-24 15:20:06,406 - trainer - INFO - start training epoch 251
2022-10-24 15:20:06,408 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,409 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,409 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,418 - trainer - INFO - 
*****************[epoch: 251, global step: 252] eval training set at end of epoch***************
2022-10-24 15:20:06,418 - trainer - INFO - {
  "train_loss": 2.7880964279174805
}
2022-10-24 15:20:06,418 - trainer - INFO - start training epoch 252
2022-10-24 15:20:06,418 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,419 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,419 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,425 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval training set based on eval_every=2***************
2022-10-24 15:20:06,425 - trainer - INFO - {
  "train_loss": 2.7880979776382446
}
2022-10-24 15:20:06,433 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval development set based on eval_every=2***************
2022-10-24 15:20:06,434 - trainer - INFO - {
  "dev_loss": 2.788055896759033,
  "dev_best_score_for_loss": -2.788055896759033
}
2022-10-24 15:20:06,434 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:06,436 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,436 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,436 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_246
2022-10-24 15:20:06,438 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:06,444 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:06,444 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:06,444 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,445 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:06,446 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_252
2022-10-24 15:20:06,450 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_252
2022-10-24 15:20:06,451 - trainer - INFO - 
*****************[epoch: 252, global step: 253] eval training set at end of epoch***************
2022-10-24 15:20:06,451 - trainer - INFO - {
  "train_loss": 2.788099527359009
}
2022-10-24 15:20:06,451 - trainer - INFO - start training epoch 253
2022-10-24 15:20:06,452 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,452 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,452 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,462 - trainer - INFO - 
*****************[epoch: 253, global step: 254] eval training set at end of epoch***************
2022-10-24 15:20:06,462 - trainer - INFO - {
  "train_loss": 2.7880561351776123
}
2022-10-24 15:20:06,463 - trainer - INFO - start training epoch 254
2022-10-24 15:20:06,463 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,463 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,464 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,474 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval training set based on eval_every=2***************
2022-10-24 15:20:06,474 - trainer - INFO - {
  "train_loss": 2.788090467453003
}
2022-10-24 15:20:06,481 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval development set based on eval_every=2***************
2022-10-24 15:20:06,482 - trainer - INFO - {
  "dev_loss": 2.7880959510803223,
  "dev_best_score_for_loss": -2.788055896759033
}
2022-10-24 15:20:06,483 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:06,483 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,487 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,487 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,487 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_248
2022-10-24 15:20:06,489 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_254
2022-10-24 15:20:06,493 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_254
2022-10-24 15:20:06,494 - trainer - INFO - 
*****************[epoch: 254, global step: 255] eval training set at end of epoch***************
2022-10-24 15:20:06,495 - trainer - INFO - {
  "train_loss": 2.7881247997283936
}
2022-10-24 15:20:06,496 - trainer - INFO - start training epoch 255
2022-10-24 15:20:06,496 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,496 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,497 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,507 - trainer - INFO - 
*****************[epoch: 255, global step: 256] eval training set at end of epoch***************
2022-10-24 15:20:06,508 - trainer - INFO - {
  "train_loss": 2.788095712661743
}
2022-10-24 15:20:06,508 - trainer - INFO - start training epoch 256
2022-10-24 15:20:06,509 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,509 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,509 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,518 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval training set based on eval_every=2***************
2022-10-24 15:20:06,518 - trainer - INFO - {
  "train_loss": 2.7881076335906982
}
2022-10-24 15:20:06,524 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval development set based on eval_every=2***************
2022-10-24 15:20:06,525 - trainer - INFO - {
  "dev_loss": 2.7880873680114746,
  "dev_best_score_for_loss": -2.788055896759033
}
2022-10-24 15:20:06,525 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:06,526 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,527 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,528 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,528 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_250
2022-10-24 15:20:06,530 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_256
2022-10-24 15:20:06,535 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_256
2022-10-24 15:20:06,536 - trainer - INFO - 
*****************[epoch: 256, global step: 257] eval training set at end of epoch***************
2022-10-24 15:20:06,536 - trainer - INFO - {
  "train_loss": 2.7881195545196533
}
2022-10-24 15:20:06,537 - trainer - INFO - start training epoch 257
2022-10-24 15:20:06,537 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,537 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,537 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,545 - trainer - INFO - 
*****************[epoch: 257, global step: 258] eval training set at end of epoch***************
2022-10-24 15:20:06,546 - trainer - INFO - {
  "train_loss": 2.7880876064300537
}
2022-10-24 15:20:06,547 - trainer - INFO - start training epoch 258
2022-10-24 15:20:06,547 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,547 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,548 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,554 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval training set based on eval_every=2***************
2022-10-24 15:20:06,554 - trainer - INFO - {
  "train_loss": 2.788072109222412
}
2022-10-24 15:20:06,560 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval development set based on eval_every=2***************
2022-10-24 15:20:06,561 - trainer - INFO - {
  "dev_loss": 2.788034200668335,
  "dev_best_score_for_loss": -2.788034200668335
}
2022-10-24 15:20:06,561 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:06,563 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,563 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,563 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_252
2022-10-24 15:20:06,568 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:06,572 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:06,572 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:06,572 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,573 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:06,573 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_258
2022-10-24 15:20:06,579 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_258
2022-10-24 15:20:06,579 - trainer - INFO - 
*****************[epoch: 258, global step: 259] eval training set at end of epoch***************
2022-10-24 15:20:06,580 - trainer - INFO - {
  "train_loss": 2.7880566120147705
}
2022-10-24 15:20:06,580 - trainer - INFO - start training epoch 259
2022-10-24 15:20:06,581 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,581 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,581 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,588 - trainer - INFO - 
*****************[epoch: 259, global step: 260] eval training set at end of epoch***************
2022-10-24 15:20:06,589 - trainer - INFO - {
  "train_loss": 2.788034200668335
}
2022-10-24 15:20:06,589 - trainer - INFO - start training epoch 260
2022-10-24 15:20:06,589 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,590 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,590 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,600 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval training set based on eval_every=2***************
2022-10-24 15:20:06,600 - trainer - INFO - {
  "train_loss": 2.7880693674087524
}
2022-10-24 15:20:06,607 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval development set based on eval_every=2***************
2022-10-24 15:20:06,608 - trainer - INFO - {
  "dev_loss": 2.788104295730591,
  "dev_best_score_for_loss": -2.788034200668335
}
2022-10-24 15:20:06,609 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:06,610 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,611 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,611 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,611 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_254
2022-10-24 15:20:06,613 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_260
2022-10-24 15:20:06,616 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_260
2022-10-24 15:20:06,617 - trainer - INFO - 
*****************[epoch: 260, global step: 261] eval training set at end of epoch***************
2022-10-24 15:20:06,617 - trainer - INFO - {
  "train_loss": 2.78810453414917
}
2022-10-24 15:20:06,618 - trainer - INFO - start training epoch 261
2022-10-24 15:20:06,618 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,619 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,619 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,630 - trainer - INFO - 
*****************[epoch: 261, global step: 262] eval training set at end of epoch***************
2022-10-24 15:20:06,630 - trainer - INFO - {
  "train_loss": 2.78810453414917
}
2022-10-24 15:20:06,630 - trainer - INFO - start training epoch 262
2022-10-24 15:20:06,631 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,631 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,631 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,640 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval training set based on eval_every=2***************
2022-10-24 15:20:06,641 - trainer - INFO - {
  "train_loss": 2.7881271839141846
}
2022-10-24 15:20:06,649 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval development set based on eval_every=2***************
2022-10-24 15:20:06,649 - trainer - INFO - {
  "dev_loss": 2.7881128787994385,
  "dev_best_score_for_loss": -2.788034200668335
}
2022-10-24 15:20:06,650 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:06,650 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,651 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,651 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,652 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_256
2022-10-24 15:20:06,653 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_262
2022-10-24 15:20:06,657 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_262
2022-10-24 15:20:06,658 - trainer - INFO - 
*****************[epoch: 262, global step: 263] eval training set at end of epoch***************
2022-10-24 15:20:06,658 - trainer - INFO - {
  "train_loss": 2.788149833679199
}
2022-10-24 15:20:06,659 - trainer - INFO - start training epoch 263
2022-10-24 15:20:06,659 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,659 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,660 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,669 - trainer - INFO - 
*****************[epoch: 263, global step: 264] eval training set at end of epoch***************
2022-10-24 15:20:06,669 - trainer - INFO - {
  "train_loss": 2.7881128787994385
}
2022-10-24 15:20:06,670 - trainer - INFO - start training epoch 264
2022-10-24 15:20:06,671 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,671 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,673 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,685 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval training set based on eval_every=2***************
2022-10-24 15:20:06,686 - trainer - INFO - {
  "train_loss": 2.7881182432174683
}
2022-10-24 15:20:06,693 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval development set based on eval_every=2***************
2022-10-24 15:20:06,693 - trainer - INFO - {
  "dev_loss": 2.7881228923797607,
  "dev_best_score_for_loss": -2.788034200668335
}
2022-10-24 15:20:06,694 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:06,694 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,695 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,696 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,696 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_258
2022-10-24 15:20:06,697 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_264
2022-10-24 15:20:06,701 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_264
2022-10-24 15:20:06,702 - trainer - INFO - 
*****************[epoch: 264, global step: 265] eval training set at end of epoch***************
2022-10-24 15:20:06,703 - trainer - INFO - {
  "train_loss": 2.788123607635498
}
2022-10-24 15:20:06,703 - trainer - INFO - start training epoch 265
2022-10-24 15:20:06,703 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,703 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,704 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,714 - trainer - INFO - 
*****************[epoch: 265, global step: 266] eval training set at end of epoch***************
2022-10-24 15:20:06,714 - trainer - INFO - {
  "train_loss": 2.78812313079834
}
2022-10-24 15:20:06,714 - trainer - INFO - start training epoch 266
2022-10-24 15:20:06,715 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,715 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,715 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,724 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval training set based on eval_every=2***************
2022-10-24 15:20:06,724 - trainer - INFO - {
  "train_loss": 2.7880889177322388
}
2022-10-24 15:20:06,733 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval development set based on eval_every=2***************
2022-10-24 15:20:06,733 - trainer - INFO - {
  "dev_loss": 2.7879984378814697,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:06,734 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:06,736 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,736 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,736 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_260
2022-10-24 15:20:06,738 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd
2022-10-24 15:20:06,741 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd
2022-10-24 15:20:06,742 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:06,742 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,743 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:06,743 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_266
2022-10-24 15:20:06,747 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_266
2022-10-24 15:20:06,749 - trainer - INFO - 
*****************[epoch: 266, global step: 267] eval training set at end of epoch***************
2022-10-24 15:20:06,749 - trainer - INFO - {
  "train_loss": 2.7880547046661377
}
2022-10-24 15:20:06,750 - trainer - INFO - start training epoch 267
2022-10-24 15:20:06,750 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,750 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,750 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,758 - trainer - INFO - 
*****************[epoch: 267, global step: 268] eval training set at end of epoch***************
2022-10-24 15:20:06,758 - trainer - INFO - {
  "train_loss": 2.7879984378814697
}
2022-10-24 15:20:06,758 - trainer - INFO - start training epoch 268
2022-10-24 15:20:06,758 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,759 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,759 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,771 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval training set based on eval_every=2***************
2022-10-24 15:20:06,771 - trainer - INFO - {
  "train_loss": 2.788079261779785
}
2022-10-24 15:20:06,778 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval development set based on eval_every=2***************
2022-10-24 15:20:06,780 - trainer - INFO - {
  "dev_loss": 2.7880587577819824,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:06,781 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:06,781 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,782 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,783 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,783 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_262
2022-10-24 15:20:06,784 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_268
2022-10-24 15:20:06,788 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_268
2022-10-24 15:20:06,789 - trainer - INFO - 
*****************[epoch: 268, global step: 269] eval training set at end of epoch***************
2022-10-24 15:20:06,789 - trainer - INFO - {
  "train_loss": 2.7881600856781006
}
2022-10-24 15:20:06,789 - trainer - INFO - start training epoch 269
2022-10-24 15:20:06,790 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,790 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,790 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,800 - trainer - INFO - 
*****************[epoch: 269, global step: 270] eval training set at end of epoch***************
2022-10-24 15:20:06,800 - trainer - INFO - {
  "train_loss": 2.7880587577819824
}
2022-10-24 15:20:06,800 - trainer - INFO - start training epoch 270
2022-10-24 15:20:06,801 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,801 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,801 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,807 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval training set based on eval_every=2***************
2022-10-24 15:20:06,808 - trainer - INFO - {
  "train_loss": 2.788079261779785
}
2022-10-24 15:20:06,817 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval development set based on eval_every=2***************
2022-10-24 15:20:06,817 - trainer - INFO - {
  "dev_loss": 2.788153886795044,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:06,818 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:06,818 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,819 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,820 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,820 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_264
2022-10-24 15:20:06,822 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_270
2022-10-24 15:20:06,830 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_270
2022-10-24 15:20:06,831 - trainer - INFO - 
*****************[epoch: 270, global step: 271] eval training set at end of epoch***************
2022-10-24 15:20:06,831 - trainer - INFO - {
  "train_loss": 2.788099765777588
}
2022-10-24 15:20:06,831 - trainer - INFO - start training epoch 271
2022-10-24 15:20:06,832 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,832 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,832 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,843 - trainer - INFO - 
*****************[epoch: 271, global step: 272] eval training set at end of epoch***************
2022-10-24 15:20:06,843 - trainer - INFO - {
  "train_loss": 2.788153886795044
}
2022-10-24 15:20:06,844 - trainer - INFO - start training epoch 272
2022-10-24 15:20:06,844 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,844 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,845 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,852 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval training set based on eval_every=2***************
2022-10-24 15:20:06,852 - trainer - INFO - {
  "train_loss": 2.7881215810775757
}
2022-10-24 15:20:06,859 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval development set based on eval_every=2***************
2022-10-24 15:20:06,859 - trainer - INFO - {
  "dev_loss": 2.788102865219116,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:06,864 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:06,865 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,866 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,867 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,867 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_266
2022-10-24 15:20:06,869 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_272
2022-10-24 15:20:06,874 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_272
2022-10-24 15:20:06,875 - trainer - INFO - 
*****************[epoch: 272, global step: 273] eval training set at end of epoch***************
2022-10-24 15:20:06,875 - trainer - INFO - {
  "train_loss": 2.7880892753601074
}
2022-10-24 15:20:06,876 - trainer - INFO - start training epoch 273
2022-10-24 15:20:06,876 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,876 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,877 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,884 - trainer - INFO - 
*****************[epoch: 273, global step: 274] eval training set at end of epoch***************
2022-10-24 15:20:06,885 - trainer - INFO - {
  "train_loss": 2.788102865219116
}
2022-10-24 15:20:06,885 - trainer - INFO - start training epoch 274
2022-10-24 15:20:06,885 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,885 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,886 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,892 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval training set based on eval_every=2***************
2022-10-24 15:20:06,893 - trainer - INFO - {
  "train_loss": 2.7880779504776
}
2022-10-24 15:20:06,902 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval development set based on eval_every=2***************
2022-10-24 15:20:06,902 - trainer - INFO - {
  "dev_loss": 2.788102388381958,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:06,903 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:06,904 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,906 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,908 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,909 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_268
2022-10-24 15:20:06,910 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_274
2022-10-24 15:20:06,915 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_274
2022-10-24 15:20:06,916 - trainer - INFO - 
*****************[epoch: 274, global step: 275] eval training set at end of epoch***************
2022-10-24 15:20:06,917 - trainer - INFO - {
  "train_loss": 2.788053035736084
}
2022-10-24 15:20:06,917 - trainer - INFO - start training epoch 275
2022-10-24 15:20:06,917 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,917 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,918 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,928 - trainer - INFO - 
*****************[epoch: 275, global step: 276] eval training set at end of epoch***************
2022-10-24 15:20:06,928 - trainer - INFO - {
  "train_loss": 2.788102388381958
}
2022-10-24 15:20:06,928 - trainer - INFO - start training epoch 276
2022-10-24 15:20:06,929 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,929 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,929 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,936 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval training set based on eval_every=2***************
2022-10-24 15:20:06,937 - trainer - INFO - {
  "train_loss": 2.7880852222442627
}
2022-10-24 15:20:06,942 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval development set based on eval_every=2***************
2022-10-24 15:20:06,942 - trainer - INFO - {
  "dev_loss": 2.7880332469940186,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:06,943 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:20:06,943 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,944 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,945 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,945 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_270
2022-10-24 15:20:06,946 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_276
2022-10-24 15:20:06,951 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_276
2022-10-24 15:20:06,954 - trainer - INFO - 
*****************[epoch: 276, global step: 277] eval training set at end of epoch***************
2022-10-24 15:20:06,954 - trainer - INFO - {
  "train_loss": 2.7880680561065674
}
2022-10-24 15:20:06,955 - trainer - INFO - start training epoch 277
2022-10-24 15:20:06,955 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,955 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,955 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,966 - trainer - INFO - 
*****************[epoch: 277, global step: 278] eval training set at end of epoch***************
2022-10-24 15:20:06,967 - trainer - INFO - {
  "train_loss": 2.7880332469940186
}
2022-10-24 15:20:06,967 - trainer - INFO - start training epoch 278
2022-10-24 15:20:06,967 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,968 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,968 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:06,974 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval training set based on eval_every=2***************
2022-10-24 15:20:06,975 - trainer - INFO - {
  "train_loss": 2.788050413131714
}
2022-10-24 15:20:06,982 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval development set based on eval_every=2***************
2022-10-24 15:20:06,982 - trainer - INFO - {
  "dev_loss": 2.7880337238311768,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:06,983 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:20:06,983 - trainer - INFO -   patience: 200
2022-10-24 15:20:06,984 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:06,984 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:06,985 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_272
2022-10-24 15:20:06,986 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_278
2022-10-24 15:20:06,990 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_278
2022-10-24 15:20:06,990 - trainer - INFO - 
*****************[epoch: 278, global step: 279] eval training set at end of epoch***************
2022-10-24 15:20:06,991 - trainer - INFO - {
  "train_loss": 2.788067579269409
}
2022-10-24 15:20:06,991 - trainer - INFO - start training epoch 279
2022-10-24 15:20:06,991 - trainer - INFO - training using device=cuda
2022-10-24 15:20:06,992 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:06,992 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,002 - trainer - INFO - 
*****************[epoch: 279, global step: 280] eval training set at end of epoch***************
2022-10-24 15:20:07,002 - trainer - INFO - {
  "train_loss": 2.7880337238311768
}
2022-10-24 15:20:07,003 - trainer - INFO - start training epoch 280
2022-10-24 15:20:07,003 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,003 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,003 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,010 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval training set based on eval_every=2***************
2022-10-24 15:20:07,012 - trainer - INFO - {
  "train_loss": 2.788085103034973
}
2022-10-24 15:20:07,019 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval development set based on eval_every=2***************
2022-10-24 15:20:07,020 - trainer - INFO - {
  "dev_loss": 2.7880566120147705,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,020 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:20:07,020 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,022 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,022 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,022 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_274
2022-10-24 15:20:07,023 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_280
2022-10-24 15:20:07,028 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_280
2022-10-24 15:20:07,029 - trainer - INFO - 
*****************[epoch: 280, global step: 281] eval training set at end of epoch***************
2022-10-24 15:20:07,029 - trainer - INFO - {
  "train_loss": 2.7881364822387695
}
2022-10-24 15:20:07,030 - trainer - INFO - start training epoch 281
2022-10-24 15:20:07,030 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,030 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,030 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,040 - trainer - INFO - 
*****************[epoch: 281, global step: 282] eval training set at end of epoch***************
2022-10-24 15:20:07,041 - trainer - INFO - {
  "train_loss": 2.7880568504333496
}
2022-10-24 15:20:07,041 - trainer - INFO - start training epoch 282
2022-10-24 15:20:07,041 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,043 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,044 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,054 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval training set based on eval_every=2***************
2022-10-24 15:20:07,055 - trainer - INFO - {
  "train_loss": 2.7880730628967285
}
2022-10-24 15:20:07,067 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval development set based on eval_every=2***************
2022-10-24 15:20:07,067 - trainer - INFO - {
  "dev_loss": 2.788053274154663,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,069 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:20:07,069 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,070 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,071 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,071 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_276
2022-10-24 15:20:07,073 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_282
2022-10-24 15:20:07,079 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_282
2022-10-24 15:20:07,079 - trainer - INFO - 
*****************[epoch: 282, global step: 283] eval training set at end of epoch***************
2022-10-24 15:20:07,080 - trainer - INFO - {
  "train_loss": 2.7880892753601074
}
2022-10-24 15:20:07,080 - trainer - INFO - start training epoch 283
2022-10-24 15:20:07,080 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,081 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,081 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,088 - trainer - INFO - 
*****************[epoch: 283, global step: 284] eval training set at end of epoch***************
2022-10-24 15:20:07,089 - trainer - INFO - {
  "train_loss": 2.788053274154663
}
2022-10-24 15:20:07,090 - trainer - INFO - start training epoch 284
2022-10-24 15:20:07,093 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,093 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,094 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,106 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval training set based on eval_every=2***************
2022-10-24 15:20:07,106 - trainer - INFO - {
  "train_loss": 2.7880808115005493
}
2022-10-24 15:20:07,115 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval development set based on eval_every=2***************
2022-10-24 15:20:07,115 - trainer - INFO - {
  "dev_loss": 2.7880899906158447,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,116 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:20:07,116 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,118 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,118 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,118 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_278
2022-10-24 15:20:07,120 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_284
2022-10-24 15:20:07,125 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_284
2022-10-24 15:20:07,126 - trainer - INFO - 
*****************[epoch: 284, global step: 285] eval training set at end of epoch***************
2022-10-24 15:20:07,126 - trainer - INFO - {
  "train_loss": 2.7881083488464355
}
2022-10-24 15:20:07,127 - trainer - INFO - start training epoch 285
2022-10-24 15:20:07,127 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,127 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,128 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,141 - trainer - INFO - 
*****************[epoch: 285, global step: 286] eval training set at end of epoch***************
2022-10-24 15:20:07,142 - trainer - INFO - {
  "train_loss": 2.7880899906158447
}
2022-10-24 15:20:07,142 - trainer - INFO - start training epoch 286
2022-10-24 15:20:07,142 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,143 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,143 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,154 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval training set based on eval_every=2***************
2022-10-24 15:20:07,155 - trainer - INFO - {
  "train_loss": 2.7881038188934326
}
2022-10-24 15:20:07,168 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval development set based on eval_every=2***************
2022-10-24 15:20:07,169 - trainer - INFO - {
  "dev_loss": 2.7880852222442627,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,170 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:20:07,170 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,171 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,172 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,172 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_280
2022-10-24 15:20:07,173 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_286
2022-10-24 15:20:07,178 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_286
2022-10-24 15:20:07,179 - trainer - INFO - 
*****************[epoch: 286, global step: 287] eval training set at end of epoch***************
2022-10-24 15:20:07,180 - trainer - INFO - {
  "train_loss": 2.7881176471710205
}
2022-10-24 15:20:07,180 - trainer - INFO - start training epoch 287
2022-10-24 15:20:07,180 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,181 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,181 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,194 - trainer - INFO - 
*****************[epoch: 287, global step: 288] eval training set at end of epoch***************
2022-10-24 15:20:07,195 - trainer - INFO - {
  "train_loss": 2.788085460662842
}
2022-10-24 15:20:07,195 - trainer - INFO - start training epoch 288
2022-10-24 15:20:07,196 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,196 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,196 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,206 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval training set based on eval_every=2***************
2022-10-24 15:20:07,207 - trainer - INFO - {
  "train_loss": 2.7880779504776
}
2022-10-24 15:20:07,218 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval development set based on eval_every=2***************
2022-10-24 15:20:07,218 - trainer - INFO - {
  "dev_loss": 2.7880659103393555,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,219 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:20:07,219 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,220 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,220 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,221 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_282
2022-10-24 15:20:07,222 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_288
2022-10-24 15:20:07,227 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_288
2022-10-24 15:20:07,228 - trainer - INFO - 
*****************[epoch: 288, global step: 289] eval training set at end of epoch***************
2022-10-24 15:20:07,229 - trainer - INFO - {
  "train_loss": 2.7880704402923584
}
2022-10-24 15:20:07,230 - trainer - INFO - start training epoch 289
2022-10-24 15:20:07,231 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,232 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,233 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,246 - trainer - INFO - 
*****************[epoch: 289, global step: 290] eval training set at end of epoch***************
2022-10-24 15:20:07,246 - trainer - INFO - {
  "train_loss": 2.7880659103393555
}
2022-10-24 15:20:07,247 - trainer - INFO - start training epoch 290
2022-10-24 15:20:07,247 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,247 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,248 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,254 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval training set based on eval_every=2***************
2022-10-24 15:20:07,254 - trainer - INFO - {
  "train_loss": 2.7880661487579346
}
2022-10-24 15:20:07,262 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval development set based on eval_every=2***************
2022-10-24 15:20:07,262 - trainer - INFO - {
  "dev_loss": 2.7880594730377197,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,263 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:20:07,263 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,264 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,265 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,265 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_284
2022-10-24 15:20:07,266 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_290
2022-10-24 15:20:07,270 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_290
2022-10-24 15:20:07,271 - trainer - INFO - 
*****************[epoch: 290, global step: 291] eval training set at end of epoch***************
2022-10-24 15:20:07,271 - trainer - INFO - {
  "train_loss": 2.7880663871765137
}
2022-10-24 15:20:07,272 - trainer - INFO - start training epoch 291
2022-10-24 15:20:07,272 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,272 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,272 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,283 - trainer - INFO - 
*****************[epoch: 291, global step: 292] eval training set at end of epoch***************
2022-10-24 15:20:07,283 - trainer - INFO - {
  "train_loss": 2.7880594730377197
}
2022-10-24 15:20:07,284 - trainer - INFO - start training epoch 292
2022-10-24 15:20:07,284 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,284 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,285 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,292 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval training set based on eval_every=2***************
2022-10-24 15:20:07,293 - trainer - INFO - {
  "train_loss": 2.7880871295928955
}
2022-10-24 15:20:07,302 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval development set based on eval_every=2***************
2022-10-24 15:20:07,302 - trainer - INFO - {
  "dev_loss": 2.78806734085083,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,303 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:20:07,303 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,304 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,304 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,305 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_286
2022-10-24 15:20:07,306 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_292
2022-10-24 15:20:07,310 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_292
2022-10-24 15:20:07,311 - trainer - INFO - 
*****************[epoch: 292, global step: 293] eval training set at end of epoch***************
2022-10-24 15:20:07,312 - trainer - INFO - {
  "train_loss": 2.7881147861480713
}
2022-10-24 15:20:07,312 - trainer - INFO - start training epoch 293
2022-10-24 15:20:07,313 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,313 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,314 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,328 - trainer - INFO - 
*****************[epoch: 293, global step: 294] eval training set at end of epoch***************
2022-10-24 15:20:07,328 - trainer - INFO - {
  "train_loss": 2.788067102432251
}
2022-10-24 15:20:07,328 - trainer - INFO - start training epoch 294
2022-10-24 15:20:07,329 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,329 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,329 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,338 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval training set based on eval_every=2***************
2022-10-24 15:20:07,338 - trainer - INFO - {
  "train_loss": 2.788040041923523
}
2022-10-24 15:20:07,346 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval development set based on eval_every=2***************
2022-10-24 15:20:07,346 - trainer - INFO - {
  "dev_loss": 2.788036823272705,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,347 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:20:07,347 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,348 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,349 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,349 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_288
2022-10-24 15:20:07,350 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_294
2022-10-24 15:20:07,355 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_294
2022-10-24 15:20:07,355 - trainer - INFO - 
*****************[epoch: 294, global step: 295] eval training set at end of epoch***************
2022-10-24 15:20:07,356 - trainer - INFO - {
  "train_loss": 2.788012981414795
}
2022-10-24 15:20:07,356 - trainer - INFO - start training epoch 295
2022-10-24 15:20:07,357 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,357 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,357 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,369 - trainer - INFO - 
*****************[epoch: 295, global step: 296] eval training set at end of epoch***************
2022-10-24 15:20:07,370 - trainer - INFO - {
  "train_loss": 2.788036584854126
}
2022-10-24 15:20:07,371 - trainer - INFO - start training epoch 296
2022-10-24 15:20:07,371 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,374 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,374 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,381 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval training set based on eval_every=2***************
2022-10-24 15:20:07,382 - trainer - INFO - {
  "train_loss": 2.7880728244781494
}
2022-10-24 15:20:07,392 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval development set based on eval_every=2***************
2022-10-24 15:20:07,392 - trainer - INFO - {
  "dev_loss": 2.788097620010376,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,393 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:20:07,393 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,395 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,395 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,395 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_290
2022-10-24 15:20:07,398 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_296
2022-10-24 15:20:07,403 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_296
2022-10-24 15:20:07,404 - trainer - INFO - 
*****************[epoch: 296, global step: 297] eval training set at end of epoch***************
2022-10-24 15:20:07,404 - trainer - INFO - {
  "train_loss": 2.788109064102173
}
2022-10-24 15:20:07,404 - trainer - INFO - start training epoch 297
2022-10-24 15:20:07,405 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,405 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,406 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,420 - trainer - INFO - 
*****************[epoch: 297, global step: 298] eval training set at end of epoch***************
2022-10-24 15:20:07,420 - trainer - INFO - {
  "train_loss": 2.788097620010376
}
2022-10-24 15:20:07,421 - trainer - INFO - start training epoch 298
2022-10-24 15:20:07,421 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,421 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,421 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,433 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval training set based on eval_every=2***************
2022-10-24 15:20:07,434 - trainer - INFO - {
  "train_loss": 2.7880923748016357
}
2022-10-24 15:20:07,441 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval development set based on eval_every=2***************
2022-10-24 15:20:07,441 - trainer - INFO - {
  "dev_loss": 2.788172721862793,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,442 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:20:07,442 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,443 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,444 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,444 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_292
2022-10-24 15:20:07,445 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_298
2022-10-24 15:20:07,449 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_298
2022-10-24 15:20:07,450 - trainer - INFO - 
*****************[epoch: 298, global step: 299] eval training set at end of epoch***************
2022-10-24 15:20:07,450 - trainer - INFO - {
  "train_loss": 2.7880871295928955
}
2022-10-24 15:20:07,450 - trainer - INFO - start training epoch 299
2022-10-24 15:20:07,451 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,451 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,451 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,459 - trainer - INFO - 
*****************[epoch: 299, global step: 300] eval training set at end of epoch***************
2022-10-24 15:20:07,459 - trainer - INFO - {
  "train_loss": 2.788172721862793
}
2022-10-24 15:20:07,460 - trainer - INFO - start training epoch 300
2022-10-24 15:20:07,462 - trainer - INFO - training using device=cuda
2022-10-24 15:20:07,463 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:07,465 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_e41g_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:07,475 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval training set based on eval_every=2***************
2022-10-24 15:20:07,475 - trainer - INFO - {
  "train_loss": 2.7881187200546265
}
2022-10-24 15:20:07,485 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval development set based on eval_every=2***************
2022-10-24 15:20:07,485 - trainer - INFO - {
  "dev_loss": 2.7880170345306396,
  "dev_best_score_for_loss": -2.7879984378814697
}
2022-10-24 15:20:07,486 - trainer - INFO -   no_improve_count: 17
2022-10-24 15:20:07,486 - trainer - INFO -   patience: 200
2022-10-24 15:20:07,488 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:07,488 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:07,488 - trainer - INFO -   Remove checkpoint tmp/mlp_e41g_kdd\ck_294
2022-10-24 15:20:07,490 - trainer - INFO -   Save checkpoint to tmp/mlp_e41g_kdd\ck_300
2022-10-24 15:20:07,496 - trainer - INFO - save model to path: tmp/mlp_e41g_kdd\ck_300
2022-10-24 15:20:07,497 - trainer - INFO - 
*****************[epoch: 300, global step: 301] eval training set at end of epoch***************
2022-10-24 15:20:07,497 - trainer - INFO - {
  "train_loss": 2.78806471824646
}
