2022-10-24 15:20:20,208 - trainer - INFO - MLP(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=32, bias=True)
  )
  (activation_layers): ModuleList(
    (0): ReLU(inplace=True)
    (1): ReLU(inplace=True)
    (2): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (linear_output): Linear(in_features=32, out_features=1, bias=True)
)
2022-10-24 15:20:20,209 - trainer - INFO -   Total params: 10625
2022-10-24 15:20:20,210 - trainer - INFO -   Trainable params: 10625
2022-10-24 15:20:20,210 - trainer - INFO -   Non-trainable params: 0
2022-10-24 15:20:20,211 - trainer - INFO -   There are 12  training examples
2022-10-24 15:20:20,212 - trainer - INFO -   There are 12 examples for development
2022-10-24 15:20:20,313 - trainer - INFO - start training epoch 1
2022-10-24 15:20:20,314 - trainer - INFO - training using device=cuda
2022-10-24 15:20:20,314 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:20,315 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,521 - trainer - INFO - 
*****************[epoch: 1, global step: 2] eval training set at end of epoch***************
2022-10-24 15:20:21,522 - trainer - INFO - {
  "train_loss": 518027.8125
}
2022-10-24 15:20:21,522 - trainer - INFO - start training epoch 2
2022-10-24 15:20:21,523 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,523 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,524 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,531 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval training set based on eval_every=2***************
2022-10-24 15:20:21,531 - trainer - INFO - {
  "train_loss": 500576.203125
}
2022-10-24 15:20:21,540 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval development set based on eval_every=2***************
2022-10-24 15:20:21,540 - trainer - INFO - {
  "dev_loss": 364424.625,
  "dev_best_score_for_loss": -364424.625
}
2022-10-24 15:20:21,541 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:21,542 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:20:21,542 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:21,546 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:21,546 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:21,549 - trainer - INFO -   patience: 200
2022-10-24 15:20:21,550 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:20:21,551 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_2
2022-10-24 15:20:21,556 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_2
2022-10-24 15:20:21,556 - trainer - INFO - 
*****************[epoch: 2, global step: 3] eval training set at end of epoch***************
2022-10-24 15:20:21,557 - trainer - INFO - {
  "train_loss": 483124.59375
}
2022-10-24 15:20:21,559 - trainer - INFO - start training epoch 3
2022-10-24 15:20:21,559 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,561 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,561 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,570 - trainer - INFO - 
*****************[epoch: 3, global step: 4] eval training set at end of epoch***************
2022-10-24 15:20:21,570 - trainer - INFO - {
  "train_loss": 364424.6875
}
2022-10-24 15:20:21,571 - trainer - INFO - start training epoch 4
2022-10-24 15:20:21,571 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,572 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,572 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,579 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval training set based on eval_every=2***************
2022-10-24 15:20:21,579 - trainer - INFO - {
  "train_loss": 246715.3125
}
2022-10-24 15:20:21,588 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval development set based on eval_every=2***************
2022-10-24 15:20:21,589 - trainer - INFO - {
  "dev_loss": 62300.7734375,
  "dev_best_score_for_loss": -62300.7734375
}
2022-10-24 15:20:21,590 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:21,591 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:20:21,591 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:21,596 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:21,597 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:21,598 - trainer - INFO -   patience: 200
2022-10-24 15:20:21,599 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:20:21,599 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_4
2022-10-24 15:20:21,604 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_4
2022-10-24 15:20:21,605 - trainer - INFO - 
*****************[epoch: 4, global step: 5] eval training set at end of epoch***************
2022-10-24 15:20:21,605 - trainer - INFO - {
  "train_loss": 129005.9375
}
2022-10-24 15:20:21,606 - trainer - INFO - start training epoch 5
2022-10-24 15:20:21,606 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,607 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,607 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,617 - trainer - INFO - 
*****************[epoch: 5, global step: 6] eval training set at end of epoch***************
2022-10-24 15:20:21,617 - trainer - INFO - {
  "train_loss": 62300.7734375
}
2022-10-24 15:20:21,618 - trainer - INFO - start training epoch 6
2022-10-24 15:20:21,618 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,618 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,619 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,627 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval training set based on eval_every=2***************
2022-10-24 15:20:21,629 - trainer - INFO - {
  "train_loss": 88784.56640625
}
2022-10-24 15:20:21,640 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval development set based on eval_every=2***************
2022-10-24 15:20:21,641 - trainer - INFO - {
  "dev_loss": 11573.529296875,
  "dev_best_score_for_loss": -11573.529296875
}
2022-10-24 15:20:21,642 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:21,643 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:21,644 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:21,650 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:21,650 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:21,651 - trainer - INFO -   patience: 200
2022-10-24 15:20:21,652 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:21,652 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_6
2022-10-24 15:20:21,657 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_6
2022-10-24 15:20:21,658 - trainer - INFO - 
*****************[epoch: 6, global step: 7] eval training set at end of epoch***************
2022-10-24 15:20:21,659 - trainer - INFO - {
  "train_loss": 115268.359375
}
2022-10-24 15:20:21,661 - trainer - INFO - start training epoch 7
2022-10-24 15:20:21,662 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,664 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,664 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,673 - trainer - INFO - 
*****************[epoch: 7, global step: 8] eval training set at end of epoch***************
2022-10-24 15:20:21,673 - trainer - INFO - {
  "train_loss": 11573.5302734375
}
2022-10-24 15:20:21,677 - trainer - INFO - start training epoch 8
2022-10-24 15:20:21,678 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,678 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,679 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,688 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval training set based on eval_every=2***************
2022-10-24 15:20:21,688 - trainer - INFO - {
  "train_loss": 25433.11474609375
}
2022-10-24 15:20:21,703 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval development set based on eval_every=2***************
2022-10-24 15:20:21,703 - trainer - INFO - {
  "dev_loss": 76067.75,
  "dev_best_score_for_loss": -11573.529296875
}
2022-10-24 15:20:21,704 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:21,706 - trainer - INFO -   patience: 200
2022-10-24 15:20:21,707 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:21,708 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:21,708 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_2
2022-10-24 15:20:21,710 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_8
2022-10-24 15:20:21,716 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_8
2022-10-24 15:20:21,717 - trainer - INFO - 
*****************[epoch: 8, global step: 9] eval training set at end of epoch***************
2022-10-24 15:20:21,717 - trainer - INFO - {
  "train_loss": 39292.69921875
}
2022-10-24 15:20:21,718 - trainer - INFO - start training epoch 9
2022-10-24 15:20:21,718 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,719 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,719 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,730 - trainer - INFO - 
*****************[epoch: 9, global step: 10] eval training set at end of epoch***************
2022-10-24 15:20:21,731 - trainer - INFO - {
  "train_loss": 76067.75
}
2022-10-24 15:20:21,731 - trainer - INFO - start training epoch 10
2022-10-24 15:20:21,732 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,732 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,733 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,740 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval training set based on eval_every=2***************
2022-10-24 15:20:21,741 - trainer - INFO - {
  "train_loss": 74517.2265625
}
2022-10-24 15:20:21,748 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval development set based on eval_every=2***************
2022-10-24 15:20:21,749 - trainer - INFO - {
  "dev_loss": 39656.984375,
  "dev_best_score_for_loss": -11573.529296875
}
2022-10-24 15:20:21,750 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:21,752 - trainer - INFO -   patience: 200
2022-10-24 15:20:21,754 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:21,754 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:21,755 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_4
2022-10-24 15:20:21,756 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_10
2022-10-24 15:20:21,761 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_10
2022-10-24 15:20:21,762 - trainer - INFO - 
*****************[epoch: 10, global step: 11] eval training set at end of epoch***************
2022-10-24 15:20:21,764 - trainer - INFO - {
  "train_loss": 72966.703125
}
2022-10-24 15:20:21,765 - trainer - INFO - start training epoch 11
2022-10-24 15:20:21,765 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,765 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,766 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,778 - trainer - INFO - 
*****************[epoch: 11, global step: 12] eval training set at end of epoch***************
2022-10-24 15:20:21,778 - trainer - INFO - {
  "train_loss": 39656.984375
}
2022-10-24 15:20:21,781 - trainer - INFO - start training epoch 12
2022-10-24 15:20:21,783 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,784 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,785 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,795 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval training set based on eval_every=2***************
2022-10-24 15:20:21,796 - trainer - INFO - {
  "train_loss": 25042.8876953125
}
2022-10-24 15:20:21,806 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval development set based on eval_every=2***************
2022-10-24 15:20:21,807 - trainer - INFO - {
  "dev_loss": 27252.208984375,
  "dev_best_score_for_loss": -11573.529296875
}
2022-10-24 15:20:21,807 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:21,808 - trainer - INFO -   patience: 200
2022-10-24 15:20:21,809 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:21,809 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:21,810 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_6
2022-10-24 15:20:21,812 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_12
2022-10-24 15:20:21,818 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_12
2022-10-24 15:20:21,820 - trainer - INFO - 
*****************[epoch: 12, global step: 13] eval training set at end of epoch***************
2022-10-24 15:20:21,820 - trainer - INFO - {
  "train_loss": 10428.791015625
}
2022-10-24 15:20:21,821 - trainer - INFO - start training epoch 13
2022-10-24 15:20:21,821 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,822 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,822 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,833 - trainer - INFO - 
*****************[epoch: 13, global step: 14] eval training set at end of epoch***************
2022-10-24 15:20:21,834 - trainer - INFO - {
  "train_loss": 27252.208984375
}
2022-10-24 15:20:21,835 - trainer - INFO - start training epoch 14
2022-10-24 15:20:21,835 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,836 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,837 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,846 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval training set based on eval_every=2***************
2022-10-24 15:20:21,847 - trainer - INFO - {
  "train_loss": 40346.9794921875
}
2022-10-24 15:20:21,854 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval development set based on eval_every=2***************
2022-10-24 15:20:21,855 - trainer - INFO - {
  "dev_loss": 28299.60546875,
  "dev_best_score_for_loss": -11573.529296875
}
2022-10-24 15:20:21,856 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:21,856 - trainer - INFO -   patience: 200
2022-10-24 15:20:21,857 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:21,857 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:21,858 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_8
2022-10-24 15:20:21,860 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_14
2022-10-24 15:20:21,867 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_14
2022-10-24 15:20:21,868 - trainer - INFO - 
*****************[epoch: 14, global step: 15] eval training set at end of epoch***************
2022-10-24 15:20:21,869 - trainer - INFO - {
  "train_loss": 53441.75
}
2022-10-24 15:20:21,870 - trainer - INFO - start training epoch 15
2022-10-24 15:20:21,870 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,871 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,872 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,881 - trainer - INFO - 
*****************[epoch: 15, global step: 16] eval training set at end of epoch***************
2022-10-24 15:20:21,881 - trainer - INFO - {
  "train_loss": 28299.60546875
}
2022-10-24 15:20:21,882 - trainer - INFO - start training epoch 16
2022-10-24 15:20:21,883 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,883 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,884 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,891 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval training set based on eval_every=2***************
2022-10-24 15:20:21,893 - trainer - INFO - {
  "train_loss": 18687.0419921875
}
2022-10-24 15:20:21,905 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval development set based on eval_every=2***************
2022-10-24 15:20:21,905 - trainer - INFO - {
  "dev_loss": 19470.734375,
  "dev_best_score_for_loss": -11573.529296875
}
2022-10-24 15:20:21,906 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:20:21,908 - trainer - INFO -   patience: 200
2022-10-24 15:20:21,911 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:21,911 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:21,913 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_10
2022-10-24 15:20:21,915 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_16
2022-10-24 15:20:21,920 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_16
2022-10-24 15:20:21,921 - trainer - INFO - 
*****************[epoch: 16, global step: 17] eval training set at end of epoch***************
2022-10-24 15:20:21,921 - trainer - INFO - {
  "train_loss": 9074.478515625
}
2022-10-24 15:20:21,922 - trainer - INFO - start training epoch 17
2022-10-24 15:20:21,923 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,925 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,925 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,938 - trainer - INFO - 
*****************[epoch: 17, global step: 18] eval training set at end of epoch***************
2022-10-24 15:20:21,940 - trainer - INFO - {
  "train_loss": 19470.734375
}
2022-10-24 15:20:21,941 - trainer - INFO - start training epoch 18
2022-10-24 15:20:21,943 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,943 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,943 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,954 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval training set based on eval_every=2***************
2022-10-24 15:20:21,955 - trainer - INFO - {
  "train_loss": 26289.0546875
}
2022-10-24 15:20:21,965 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval development set based on eval_every=2***************
2022-10-24 15:20:21,966 - trainer - INFO - {
  "dev_loss": 32523.0078125,
  "dev_best_score_for_loss": -11573.529296875
}
2022-10-24 15:20:21,967 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:20:21,967 - trainer - INFO -   patience: 200
2022-10-24 15:20:21,969 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:21,970 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:21,970 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_12
2022-10-24 15:20:21,973 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_18
2022-10-24 15:20:21,979 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_18
2022-10-24 15:20:21,980 - trainer - INFO - 
*****************[epoch: 18, global step: 19] eval training set at end of epoch***************
2022-10-24 15:20:21,981 - trainer - INFO - {
  "train_loss": 33107.375
}
2022-10-24 15:20:21,982 - trainer - INFO - start training epoch 19
2022-10-24 15:20:21,983 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,983 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:21,984 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:21,997 - trainer - INFO - 
*****************[epoch: 19, global step: 20] eval training set at end of epoch***************
2022-10-24 15:20:21,997 - trainer - INFO - {
  "train_loss": 32523.005859375
}
2022-10-24 15:20:21,998 - trainer - INFO - start training epoch 20
2022-10-24 15:20:21,999 - trainer - INFO - training using device=cuda
2022-10-24 15:20:21,999 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,001 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,012 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval training set based on eval_every=2***************
2022-10-24 15:20:22,012 - trainer - INFO - {
  "train_loss": 26042.10546875
}
2022-10-24 15:20:22,024 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval development set based on eval_every=2***************
2022-10-24 15:20:22,024 - trainer - INFO - {
  "dev_loss": 8873.7626953125,
  "dev_best_score_for_loss": -8873.7626953125
}
2022-10-24 15:20:22,025 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:22,026 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,026 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,027 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_14
2022-10-24 15:20:22,028 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:22,032 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:22,033 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:22,034 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,036 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:22,036 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_20
2022-10-24 15:20:22,041 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_20
2022-10-24 15:20:22,042 - trainer - INFO - 
*****************[epoch: 20, global step: 21] eval training set at end of epoch***************
2022-10-24 15:20:22,042 - trainer - INFO - {
  "train_loss": 19561.205078125
}
2022-10-24 15:20:22,043 - trainer - INFO - start training epoch 21
2022-10-24 15:20:22,043 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,044 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,046 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,057 - trainer - INFO - 
*****************[epoch: 21, global step: 22] eval training set at end of epoch***************
2022-10-24 15:20:22,058 - trainer - INFO - {
  "train_loss": 8873.7626953125
}
2022-10-24 15:20:22,059 - trainer - INFO - start training epoch 22
2022-10-24 15:20:22,059 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,060 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,060 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,074 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval training set based on eval_every=2***************
2022-10-24 15:20:22,074 - trainer - INFO - {
  "train_loss": 11615.82080078125
}
2022-10-24 15:20:22,084 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval development set based on eval_every=2***************
2022-10-24 15:20:22,084 - trainer - INFO - {
  "dev_loss": 24764.896484375,
  "dev_best_score_for_loss": -8873.7626953125
}
2022-10-24 15:20:22,085 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:22,086 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,087 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,087 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,087 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_16
2022-10-24 15:20:22,089 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_22
2022-10-24 15:20:22,094 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_22
2022-10-24 15:20:22,098 - trainer - INFO - 
*****************[epoch: 22, global step: 23] eval training set at end of epoch***************
2022-10-24 15:20:22,100 - trainer - INFO - {
  "train_loss": 14357.87890625
}
2022-10-24 15:20:22,101 - trainer - INFO - start training epoch 23
2022-10-24 15:20:22,102 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,102 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,103 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,113 - trainer - INFO - 
*****************[epoch: 23, global step: 24] eval training set at end of epoch***************
2022-10-24 15:20:22,114 - trainer - INFO - {
  "train_loss": 24764.896484375
}
2022-10-24 15:20:22,115 - trainer - INFO - start training epoch 24
2022-10-24 15:20:22,115 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,116 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,116 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,125 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval training set based on eval_every=2***************
2022-10-24 15:20:22,126 - trainer - INFO - {
  "train_loss": 21815.4287109375
}
2022-10-24 15:20:22,135 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval development set based on eval_every=2***************
2022-10-24 15:20:22,135 - trainer - INFO - {
  "dev_loss": 8902.19921875,
  "dev_best_score_for_loss": -8873.7626953125
}
2022-10-24 15:20:22,136 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:22,137 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,138 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,138 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,138 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_18
2022-10-24 15:20:22,141 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_24
2022-10-24 15:20:22,148 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_24
2022-10-24 15:20:22,149 - trainer - INFO - 
*****************[epoch: 24, global step: 25] eval training set at end of epoch***************
2022-10-24 15:20:22,150 - trainer - INFO - {
  "train_loss": 18865.9609375
}
2022-10-24 15:20:22,150 - trainer - INFO - start training epoch 25
2022-10-24 15:20:22,151 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,152 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,152 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,165 - trainer - INFO - 
*****************[epoch: 25, global step: 26] eval training set at end of epoch***************
2022-10-24 15:20:22,165 - trainer - INFO - {
  "train_loss": 8902.19921875
}
2022-10-24 15:20:22,166 - trainer - INFO - start training epoch 26
2022-10-24 15:20:22,166 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,167 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,167 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,176 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval training set based on eval_every=2***************
2022-10-24 15:20:22,177 - trainer - INFO - {
  "train_loss": 9595.50390625
}
2022-10-24 15:20:22,185 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval development set based on eval_every=2***************
2022-10-24 15:20:22,186 - trainer - INFO - {
  "dev_loss": 16611.3671875,
  "dev_best_score_for_loss": -8873.7626953125
}
2022-10-24 15:20:22,187 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:22,188 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,189 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,189 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,190 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_20
2022-10-24 15:20:22,192 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_26
2022-10-24 15:20:22,197 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_26
2022-10-24 15:20:22,198 - trainer - INFO - 
*****************[epoch: 26, global step: 27] eval training set at end of epoch***************
2022-10-24 15:20:22,199 - trainer - INFO - {
  "train_loss": 10288.80859375
}
2022-10-24 15:20:22,199 - trainer - INFO - start training epoch 27
2022-10-24 15:20:22,200 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,200 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,201 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,212 - trainer - INFO - 
*****************[epoch: 27, global step: 28] eval training set at end of epoch***************
2022-10-24 15:20:22,213 - trainer - INFO - {
  "train_loss": 16611.3671875
}
2022-10-24 15:20:22,214 - trainer - INFO - start training epoch 28
2022-10-24 15:20:22,214 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,215 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,215 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,230 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval training set based on eval_every=2***************
2022-10-24 15:20:22,231 - trainer - INFO - {
  "train_loss": 17128.548828125
}
2022-10-24 15:20:22,240 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval development set based on eval_every=2***************
2022-10-24 15:20:22,241 - trainer - INFO - {
  "dev_loss": 12294.3701171875,
  "dev_best_score_for_loss": -8873.7626953125
}
2022-10-24 15:20:22,242 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:22,242 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,244 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,244 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,245 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_22
2022-10-24 15:20:22,246 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_28
2022-10-24 15:20:22,251 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_28
2022-10-24 15:20:22,252 - trainer - INFO - 
*****************[epoch: 28, global step: 29] eval training set at end of epoch***************
2022-10-24 15:20:22,252 - trainer - INFO - {
  "train_loss": 17645.73046875
}
2022-10-24 15:20:22,253 - trainer - INFO - start training epoch 29
2022-10-24 15:20:22,253 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,254 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,255 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,267 - trainer - INFO - 
*****************[epoch: 29, global step: 30] eval training set at end of epoch***************
2022-10-24 15:20:22,267 - trainer - INFO - {
  "train_loss": 12294.369140625
}
2022-10-24 15:20:22,268 - trainer - INFO - start training epoch 30
2022-10-24 15:20:22,269 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,269 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,270 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,278 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval training set based on eval_every=2***************
2022-10-24 15:20:22,279 - trainer - INFO - {
  "train_loss": 9965.539794921875
}
2022-10-24 15:20:22,288 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval development set based on eval_every=2***************
2022-10-24 15:20:22,289 - trainer - INFO - {
  "dev_loss": 9924.8037109375,
  "dev_best_score_for_loss": -8873.7626953125
}
2022-10-24 15:20:22,289 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:20:22,290 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,291 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,291 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,291 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_24
2022-10-24 15:20:22,293 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_30
2022-10-24 15:20:22,298 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_30
2022-10-24 15:20:22,300 - trainer - INFO - 
*****************[epoch: 30, global step: 31] eval training set at end of epoch***************
2022-10-24 15:20:22,301 - trainer - INFO - {
  "train_loss": 7636.71044921875
}
2022-10-24 15:20:22,301 - trainer - INFO - start training epoch 31
2022-10-24 15:20:22,302 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,302 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,303 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,315 - trainer - INFO - 
*****************[epoch: 31, global step: 32] eval training set at end of epoch***************
2022-10-24 15:20:22,315 - trainer - INFO - {
  "train_loss": 9924.8046875
}
2022-10-24 15:20:22,316 - trainer - INFO - start training epoch 32
2022-10-24 15:20:22,317 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,317 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,321 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,336 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval training set based on eval_every=2***************
2022-10-24 15:20:22,336 - trainer - INFO - {
  "train_loss": 11941.21875
}
2022-10-24 15:20:22,344 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval development set based on eval_every=2***************
2022-10-24 15:20:22,344 - trainer - INFO - {
  "dev_loss": 11448.49609375,
  "dev_best_score_for_loss": -8873.7626953125
}
2022-10-24 15:20:22,345 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:20:22,348 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,349 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,350 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,350 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_26
2022-10-24 15:20:22,352 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_32
2022-10-24 15:20:22,358 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_32
2022-10-24 15:20:22,359 - trainer - INFO - 
*****************[epoch: 32, global step: 33] eval training set at end of epoch***************
2022-10-24 15:20:22,362 - trainer - INFO - {
  "train_loss": 13957.6328125
}
2022-10-24 15:20:22,363 - trainer - INFO - start training epoch 33
2022-10-24 15:20:22,364 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,364 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,365 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,375 - trainer - INFO - 
*****************[epoch: 33, global step: 34] eval training set at end of epoch***************
2022-10-24 15:20:22,375 - trainer - INFO - {
  "train_loss": 11448.498046875
}
2022-10-24 15:20:22,376 - trainer - INFO - start training epoch 34
2022-10-24 15:20:22,376 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,377 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,378 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,390 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval training set based on eval_every=2***************
2022-10-24 15:20:22,390 - trainer - INFO - {
  "train_loss": 9378.0498046875
}
2022-10-24 15:20:22,399 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval development set based on eval_every=2***************
2022-10-24 15:20:22,401 - trainer - INFO - {
  "dev_loss": 8042.5,
  "dev_best_score_for_loss": -8042.5
}
2022-10-24 15:20:22,402 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:22,403 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,404 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,405 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_28
2022-10-24 15:20:22,407 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:22,413 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:22,413 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:22,418 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,420 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:22,423 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_34
2022-10-24 15:20:22,428 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_34
2022-10-24 15:20:22,429 - trainer - INFO - 
*****************[epoch: 34, global step: 35] eval training set at end of epoch***************
2022-10-24 15:20:22,430 - trainer - INFO - {
  "train_loss": 7307.6015625
}
2022-10-24 15:20:22,431 - trainer - INFO - start training epoch 35
2022-10-24 15:20:22,431 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,432 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,432 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,442 - trainer - INFO - 
*****************[epoch: 35, global step: 36] eval training set at end of epoch***************
2022-10-24 15:20:22,443 - trainer - INFO - {
  "train_loss": 8042.5
}
2022-10-24 15:20:22,448 - trainer - INFO - start training epoch 36
2022-10-24 15:20:22,449 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,451 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,451 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,463 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval training set based on eval_every=2***************
2022-10-24 15:20:22,464 - trainer - INFO - {
  "train_loss": 9371.31640625
}
2022-10-24 15:20:22,474 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval development set based on eval_every=2***************
2022-10-24 15:20:22,475 - trainer - INFO - {
  "dev_loss": 10455.5390625,
  "dev_best_score_for_loss": -8042.5
}
2022-10-24 15:20:22,476 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:22,477 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,482 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,482 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,483 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_30
2022-10-24 15:20:22,484 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_36
2022-10-24 15:20:22,490 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_36
2022-10-24 15:20:22,491 - trainer - INFO - 
*****************[epoch: 36, global step: 37] eval training set at end of epoch***************
2022-10-24 15:20:22,492 - trainer - INFO - {
  "train_loss": 10700.1328125
}
2022-10-24 15:20:22,492 - trainer - INFO - start training epoch 37
2022-10-24 15:20:22,493 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,494 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,494 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,513 - trainer - INFO - 
*****************[epoch: 37, global step: 38] eval training set at end of epoch***************
2022-10-24 15:20:22,514 - trainer - INFO - {
  "train_loss": 10455.5390625
}
2022-10-24 15:20:22,514 - trainer - INFO - start training epoch 38
2022-10-24 15:20:22,515 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,515 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,516 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,525 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval training set based on eval_every=2***************
2022-10-24 15:20:22,526 - trainer - INFO - {
  "train_loss": 9057.93310546875
}
2022-10-24 15:20:22,537 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval development set based on eval_every=2***************
2022-10-24 15:20:22,538 - trainer - INFO - {
  "dev_loss": 6477.6435546875,
  "dev_best_score_for_loss": -6477.6435546875
}
2022-10-24 15:20:22,539 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:22,540 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,541 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,541 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_32
2022-10-24 15:20:22,543 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:22,549 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:22,549 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:22,550 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,552 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:22,552 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_38
2022-10-24 15:20:22,559 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_38
2022-10-24 15:20:22,560 - trainer - INFO - 
*****************[epoch: 38, global step: 39] eval training set at end of epoch***************
2022-10-24 15:20:22,561 - trainer - INFO - {
  "train_loss": 7660.3271484375
}
2022-10-24 15:20:22,564 - trainer - INFO - start training epoch 39
2022-10-24 15:20:22,565 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,566 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,566 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,579 - trainer - INFO - 
*****************[epoch: 39, global step: 40] eval training set at end of epoch***************
2022-10-24 15:20:22,579 - trainer - INFO - {
  "train_loss": 6477.6435546875
}
2022-10-24 15:20:22,580 - trainer - INFO - start training epoch 40
2022-10-24 15:20:22,581 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,581 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,582 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,590 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval training set based on eval_every=2***************
2022-10-24 15:20:22,593 - trainer - INFO - {
  "train_loss": 7367.69091796875
}
2022-10-24 15:20:22,606 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval development set based on eval_every=2***************
2022-10-24 15:20:22,606 - trainer - INFO - {
  "dev_loss": 8917.029296875,
  "dev_best_score_for_loss": -6477.6435546875
}
2022-10-24 15:20:22,607 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:22,608 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,609 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,610 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,610 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_34
2022-10-24 15:20:22,612 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_40
2022-10-24 15:20:22,616 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_40
2022-10-24 15:20:22,617 - trainer - INFO - 
*****************[epoch: 40, global step: 41] eval training set at end of epoch***************
2022-10-24 15:20:22,618 - trainer - INFO - {
  "train_loss": 8257.73828125
}
2022-10-24 15:20:22,619 - trainer - INFO - start training epoch 41
2022-10-24 15:20:22,619 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,620 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,621 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,631 - trainer - INFO - 
*****************[epoch: 41, global step: 42] eval training set at end of epoch***************
2022-10-24 15:20:22,632 - trainer - INFO - {
  "train_loss": 8917.029296875
}
2022-10-24 15:20:22,633 - trainer - INFO - start training epoch 42
2022-10-24 15:20:22,633 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,634 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,634 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,646 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval training set based on eval_every=2***************
2022-10-24 15:20:22,647 - trainer - INFO - {
  "train_loss": 7916.71826171875
}
2022-10-24 15:20:22,654 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval development set based on eval_every=2***************
2022-10-24 15:20:22,655 - trainer - INFO - {
  "dev_loss": 5929.177734375,
  "dev_best_score_for_loss": -5929.177734375
}
2022-10-24 15:20:22,656 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:22,657 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,658 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,658 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_36
2022-10-24 15:20:22,660 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:22,664 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:22,665 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:22,665 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,667 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:22,667 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_42
2022-10-24 15:20:22,673 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_42
2022-10-24 15:20:22,674 - trainer - INFO - 
*****************[epoch: 42, global step: 43] eval training set at end of epoch***************
2022-10-24 15:20:22,674 - trainer - INFO - {
  "train_loss": 6916.4072265625
}
2022-10-24 15:20:22,675 - trainer - INFO - start training epoch 43
2022-10-24 15:20:22,675 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,676 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,676 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,691 - trainer - INFO - 
*****************[epoch: 43, global step: 44] eval training set at end of epoch***************
2022-10-24 15:20:22,691 - trainer - INFO - {
  "train_loss": 5929.1787109375
}
2022-10-24 15:20:22,692 - trainer - INFO - start training epoch 44
2022-10-24 15:20:22,693 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,693 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,694 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,707 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval training set based on eval_every=2***************
2022-10-24 15:20:22,707 - trainer - INFO - {
  "train_loss": 6461.617431640625
}
2022-10-24 15:20:22,714 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval development set based on eval_every=2***************
2022-10-24 15:20:22,714 - trainer - INFO - {
  "dev_loss": 7514.97265625,
  "dev_best_score_for_loss": -5929.177734375
}
2022-10-24 15:20:22,715 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:22,716 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,717 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,717 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,717 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_38
2022-10-24 15:20:22,719 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_44
2022-10-24 15:20:22,723 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_44
2022-10-24 15:20:22,724 - trainer - INFO - 
*****************[epoch: 44, global step: 45] eval training set at end of epoch***************
2022-10-24 15:20:22,725 - trainer - INFO - {
  "train_loss": 6994.05615234375
}
2022-10-24 15:20:22,725 - trainer - INFO - start training epoch 45
2022-10-24 15:20:22,726 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,727 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,727 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,737 - trainer - INFO - 
*****************[epoch: 45, global step: 46] eval training set at end of epoch***************
2022-10-24 15:20:22,737 - trainer - INFO - {
  "train_loss": 7514.97265625
}
2022-10-24 15:20:22,738 - trainer - INFO - start training epoch 46
2022-10-24 15:20:22,738 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,739 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,739 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,749 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval training set based on eval_every=2***************
2022-10-24 15:20:22,750 - trainer - INFO - {
  "train_loss": 6941.4521484375
}
2022-10-24 15:20:22,756 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval development set based on eval_every=2***************
2022-10-24 15:20:22,757 - trainer - INFO - {
  "dev_loss": 5377.0234375,
  "dev_best_score_for_loss": -5377.0234375
}
2022-10-24 15:20:22,758 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:22,759 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,760 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,760 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_40
2022-10-24 15:20:22,762 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:22,766 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:22,767 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:22,767 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,768 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:22,768 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_46
2022-10-24 15:20:22,772 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_46
2022-10-24 15:20:22,773 - trainer - INFO - 
*****************[epoch: 46, global step: 47] eval training set at end of epoch***************
2022-10-24 15:20:22,774 - trainer - INFO - {
  "train_loss": 6367.931640625
}
2022-10-24 15:20:22,774 - trainer - INFO - start training epoch 47
2022-10-24 15:20:22,777 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,778 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,780 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,793 - trainer - INFO - 
*****************[epoch: 47, global step: 48] eval training set at end of epoch***************
2022-10-24 15:20:22,793 - trainer - INFO - {
  "train_loss": 5377.0234375
}
2022-10-24 15:20:22,795 - trainer - INFO - start training epoch 48
2022-10-24 15:20:22,795 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,796 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,798 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,806 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval training set based on eval_every=2***************
2022-10-24 15:20:22,806 - trainer - INFO - {
  "train_loss": 5654.0498046875
}
2022-10-24 15:20:22,816 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval development set based on eval_every=2***************
2022-10-24 15:20:22,818 - trainer - INFO - {
  "dev_loss": 6403.3349609375,
  "dev_best_score_for_loss": -5377.0234375
}
2022-10-24 15:20:22,820 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:22,821 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,823 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,824 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,824 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_42
2022-10-24 15:20:22,828 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_48
2022-10-24 15:20:22,833 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_48
2022-10-24 15:20:22,834 - trainer - INFO - 
*****************[epoch: 48, global step: 49] eval training set at end of epoch***************
2022-10-24 15:20:22,835 - trainer - INFO - {
  "train_loss": 5931.076171875
}
2022-10-24 15:20:22,835 - trainer - INFO - start training epoch 49
2022-10-24 15:20:22,836 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,836 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,838 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,848 - trainer - INFO - 
*****************[epoch: 49, global step: 50] eval training set at end of epoch***************
2022-10-24 15:20:22,848 - trainer - INFO - {
  "train_loss": 6403.3349609375
}
2022-10-24 15:20:22,850 - trainer - INFO - start training epoch 50
2022-10-24 15:20:22,850 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,851 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,851 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,859 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval training set based on eval_every=2***************
2022-10-24 15:20:22,859 - trainer - INFO - {
  "train_loss": 5961.65185546875
}
2022-10-24 15:20:22,866 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval development set based on eval_every=2***************
2022-10-24 15:20:22,867 - trainer - INFO - {
  "dev_loss": 4868.212890625,
  "dev_best_score_for_loss": -4868.212890625
}
2022-10-24 15:20:22,868 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:22,869 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,872 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,872 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_44
2022-10-24 15:20:22,874 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:22,878 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:22,878 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:22,879 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,882 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:22,883 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_50
2022-10-24 15:20:22,889 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_50
2022-10-24 15:20:22,890 - trainer - INFO - 
*****************[epoch: 50, global step: 51] eval training set at end of epoch***************
2022-10-24 15:20:22,891 - trainer - INFO - {
  "train_loss": 5519.96875
}
2022-10-24 15:20:22,891 - trainer - INFO - start training epoch 51
2022-10-24 15:20:22,892 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,892 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,893 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,903 - trainer - INFO - 
*****************[epoch: 51, global step: 52] eval training set at end of epoch***************
2022-10-24 15:20:22,904 - trainer - INFO - {
  "train_loss": 4868.212890625
}
2022-10-24 15:20:22,905 - trainer - INFO - start training epoch 52
2022-10-24 15:20:22,905 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,906 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,906 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,917 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval training set based on eval_every=2***************
2022-10-24 15:20:22,917 - trainer - INFO - {
  "train_loss": 5070.68212890625
}
2022-10-24 15:20:22,923 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval development set based on eval_every=2***************
2022-10-24 15:20:22,924 - trainer - INFO - {
  "dev_loss": 5463.3583984375,
  "dev_best_score_for_loss": -4868.212890625
}
2022-10-24 15:20:22,925 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:22,927 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,928 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,929 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,929 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_46
2022-10-24 15:20:22,932 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_52
2022-10-24 15:20:22,937 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_52
2022-10-24 15:20:22,938 - trainer - INFO - 
*****************[epoch: 52, global step: 53] eval training set at end of epoch***************
2022-10-24 15:20:22,938 - trainer - INFO - {
  "train_loss": 5273.1513671875
}
2022-10-24 15:20:22,939 - trainer - INFO - start training epoch 53
2022-10-24 15:20:22,939 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,940 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,940 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,949 - trainer - INFO - 
*****************[epoch: 53, global step: 54] eval training set at end of epoch***************
2022-10-24 15:20:22,950 - trainer - INFO - {
  "train_loss": 5463.3583984375
}
2022-10-24 15:20:22,950 - trainer - INFO - start training epoch 54
2022-10-24 15:20:22,951 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,951 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,952 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:22,962 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval training set based on eval_every=2***************
2022-10-24 15:20:22,963 - trainer - INFO - {
  "train_loss": 5142.25244140625
}
2022-10-24 15:20:22,973 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval development set based on eval_every=2***************
2022-10-24 15:20:22,974 - trainer - INFO - {
  "dev_loss": 4359.81005859375,
  "dev_best_score_for_loss": -4359.81005859375
}
2022-10-24 15:20:22,975 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:22,977 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:22,978 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:22,980 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_48
2022-10-24 15:20:22,981 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:22,985 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:22,985 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:22,986 - trainer - INFO -   patience: 200
2022-10-24 15:20:22,987 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:22,987 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_54
2022-10-24 15:20:22,992 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_54
2022-10-24 15:20:22,995 - trainer - INFO - 
*****************[epoch: 54, global step: 55] eval training set at end of epoch***************
2022-10-24 15:20:22,996 - trainer - INFO - {
  "train_loss": 4821.146484375
}
2022-10-24 15:20:22,997 - trainer - INFO - start training epoch 55
2022-10-24 15:20:22,997 - trainer - INFO - training using device=cuda
2022-10-24 15:20:22,998 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:22,999 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,009 - trainer - INFO - 
*****************[epoch: 55, global step: 56] eval training set at end of epoch***************
2022-10-24 15:20:23,013 - trainer - INFO - {
  "train_loss": 4359.81005859375
}
2022-10-24 15:20:23,014 - trainer - INFO - start training epoch 56
2022-10-24 15:20:23,015 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,015 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,016 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,024 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval training set based on eval_every=2***************
2022-10-24 15:20:23,024 - trainer - INFO - {
  "train_loss": 4495.86865234375
}
2022-10-24 15:20:23,032 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval development set based on eval_every=2***************
2022-10-24 15:20:23,032 - trainer - INFO - {
  "dev_loss": 4661.15576171875,
  "dev_best_score_for_loss": -4359.81005859375
}
2022-10-24 15:20:23,033 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:23,034 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,035 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,035 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,035 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_50
2022-10-24 15:20:23,037 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_56
2022-10-24 15:20:23,041 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_56
2022-10-24 15:20:23,042 - trainer - INFO - 
*****************[epoch: 56, global step: 57] eval training set at end of epoch***************
2022-10-24 15:20:23,043 - trainer - INFO - {
  "train_loss": 4631.92724609375
}
2022-10-24 15:20:23,044 - trainer - INFO - start training epoch 57
2022-10-24 15:20:23,044 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,046 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,046 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,063 - trainer - INFO - 
*****************[epoch: 57, global step: 58] eval training set at end of epoch***************
2022-10-24 15:20:23,064 - trainer - INFO - {
  "train_loss": 4661.1552734375
}
2022-10-24 15:20:23,065 - trainer - INFO - start training epoch 58
2022-10-24 15:20:23,066 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,066 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,068 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,079 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval training set based on eval_every=2***************
2022-10-24 15:20:23,080 - trainer - INFO - {
  "train_loss": 4385.455078125
}
2022-10-24 15:20:23,089 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval development set based on eval_every=2***************
2022-10-24 15:20:23,090 - trainer - INFO - {
  "dev_loss": 3890.85205078125,
  "dev_best_score_for_loss": -3890.85205078125
}
2022-10-24 15:20:23,091 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,093 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,094 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,094 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_52
2022-10-24 15:20:23,099 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,103 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,104 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,105 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,106 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,107 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_58
2022-10-24 15:20:23,112 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_58
2022-10-24 15:20:23,117 - trainer - INFO - 
*****************[epoch: 58, global step: 59] eval training set at end of epoch***************
2022-10-24 15:20:23,118 - trainer - INFO - {
  "train_loss": 4109.7548828125
}
2022-10-24 15:20:23,119 - trainer - INFO - start training epoch 59
2022-10-24 15:20:23,120 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,121 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,122 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,135 - trainer - INFO - 
*****************[epoch: 59, global step: 60] eval training set at end of epoch***************
2022-10-24 15:20:23,135 - trainer - INFO - {
  "train_loss": 3890.852294921875
}
2022-10-24 15:20:23,138 - trainer - INFO - start training epoch 60
2022-10-24 15:20:23,138 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,139 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,139 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,149 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval training set based on eval_every=2***************
2022-10-24 15:20:23,150 - trainer - INFO - {
  "train_loss": 3983.073974609375
}
2022-10-24 15:20:23,158 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval development set based on eval_every=2***************
2022-10-24 15:20:23,159 - trainer - INFO - {
  "dev_loss": 3933.04638671875,
  "dev_best_score_for_loss": -3890.85205078125
}
2022-10-24 15:20:23,160 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:23,164 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,167 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,168 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,169 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_54
2022-10-24 15:20:23,171 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_60
2022-10-24 15:20:23,178 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_60
2022-10-24 15:20:23,183 - trainer - INFO - 
*****************[epoch: 60, global step: 61] eval training set at end of epoch***************
2022-10-24 15:20:23,184 - trainer - INFO - {
  "train_loss": 4075.295654296875
}
2022-10-24 15:20:23,184 - trainer - INFO - start training epoch 61
2022-10-24 15:20:23,185 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,185 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,186 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,195 - trainer - INFO - 
*****************[epoch: 61, global step: 62] eval training set at end of epoch***************
2022-10-24 15:20:23,195 - trainer - INFO - {
  "train_loss": 3933.04638671875
}
2022-10-24 15:20:23,196 - trainer - INFO - start training epoch 62
2022-10-24 15:20:23,196 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,197 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,197 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,204 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval training set based on eval_every=2***************
2022-10-24 15:20:23,205 - trainer - INFO - {
  "train_loss": 3723.5972900390625
}
2022-10-24 15:20:23,214 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval development set based on eval_every=2***************
2022-10-24 15:20:23,215 - trainer - INFO - {
  "dev_loss": 3424.037109375,
  "dev_best_score_for_loss": -3424.037109375
}
2022-10-24 15:20:23,216 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,217 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,217 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,218 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_56
2022-10-24 15:20:23,220 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,223 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,223 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,223 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,226 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,227 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_62
2022-10-24 15:20:23,232 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_62
2022-10-24 15:20:23,232 - trainer - INFO - 
*****************[epoch: 62, global step: 63] eval training set at end of epoch***************
2022-10-24 15:20:23,233 - trainer - INFO - {
  "train_loss": 3514.148193359375
}
2022-10-24 15:20:23,233 - trainer - INFO - start training epoch 63
2022-10-24 15:20:23,234 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,234 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,235 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,245 - trainer - INFO - 
*****************[epoch: 63, global step: 64] eval training set at end of epoch***************
2022-10-24 15:20:23,245 - trainer - INFO - {
  "train_loss": 3424.037109375
}
2022-10-24 15:20:23,247 - trainer - INFO - start training epoch 64
2022-10-24 15:20:23,248 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,248 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,248 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,262 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval training set based on eval_every=2***************
2022-10-24 15:20:23,262 - trainer - INFO - {
  "train_loss": 3464.363037109375
}
2022-10-24 15:20:23,269 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval development set based on eval_every=2***************
2022-10-24 15:20:23,269 - trainer - INFO - {
  "dev_loss": 3256.930419921875,
  "dev_best_score_for_loss": -3256.930419921875
}
2022-10-24 15:20:23,270 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,273 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,273 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,274 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_58
2022-10-24 15:20:23,275 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,280 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,280 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,281 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,282 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,282 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_64
2022-10-24 15:20:23,286 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_64
2022-10-24 15:20:23,289 - trainer - INFO - 
*****************[epoch: 64, global step: 65] eval training set at end of epoch***************
2022-10-24 15:20:23,289 - trainer - INFO - {
  "train_loss": 3504.68896484375
}
2022-10-24 15:20:23,290 - trainer - INFO - start training epoch 65
2022-10-24 15:20:23,290 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,291 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,291 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,303 - trainer - INFO - 
*****************[epoch: 65, global step: 66] eval training set at end of epoch***************
2022-10-24 15:20:23,303 - trainer - INFO - {
  "train_loss": 3256.930419921875
}
2022-10-24 15:20:23,304 - trainer - INFO - start training epoch 66
2022-10-24 15:20:23,304 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,305 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,305 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,313 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval training set based on eval_every=2***************
2022-10-24 15:20:23,313 - trainer - INFO - {
  "train_loss": 3117.683349609375
}
2022-10-24 15:20:23,322 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval development set based on eval_every=2***************
2022-10-24 15:20:23,325 - trainer - INFO - {
  "dev_loss": 2983.253662109375,
  "dev_best_score_for_loss": -2983.253662109375
}
2022-10-24 15:20:23,326 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,327 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,328 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,328 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_60
2022-10-24 15:20:23,330 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,336 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,336 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,337 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,338 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,338 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_66
2022-10-24 15:20:23,343 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_66
2022-10-24 15:20:23,344 - trainer - INFO - 
*****************[epoch: 66, global step: 67] eval training set at end of epoch***************
2022-10-24 15:20:23,344 - trainer - INFO - {
  "train_loss": 2978.436279296875
}
2022-10-24 15:20:23,345 - trainer - INFO - start training epoch 67
2022-10-24 15:20:23,346 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,346 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,346 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,356 - trainer - INFO - 
*****************[epoch: 67, global step: 68] eval training set at end of epoch***************
2022-10-24 15:20:23,357 - trainer - INFO - {
  "train_loss": 2983.253662109375
}
2022-10-24 15:20:23,357 - trainer - INFO - start training epoch 68
2022-10-24 15:20:23,358 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,358 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,359 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,370 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval training set based on eval_every=2***************
2022-10-24 15:20:23,371 - trainer - INFO - {
  "train_loss": 2954.587158203125
}
2022-10-24 15:20:23,381 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval development set based on eval_every=2***************
2022-10-24 15:20:23,382 - trainer - INFO - {
  "dev_loss": 2660.306396484375,
  "dev_best_score_for_loss": -2660.306396484375
}
2022-10-24 15:20:23,383 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,384 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,385 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,385 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_62
2022-10-24 15:20:23,387 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,391 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,391 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,392 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,393 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,393 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_68
2022-10-24 15:20:23,399 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_68
2022-10-24 15:20:23,400 - trainer - INFO - 
*****************[epoch: 68, global step: 69] eval training set at end of epoch***************
2022-10-24 15:20:23,401 - trainer - INFO - {
  "train_loss": 2925.920654296875
}
2022-10-24 15:20:23,401 - trainer - INFO - start training epoch 69
2022-10-24 15:20:23,402 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,402 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,403 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,414 - trainer - INFO - 
*****************[epoch: 69, global step: 70] eval training set at end of epoch***************
2022-10-24 15:20:23,414 - trainer - INFO - {
  "train_loss": 2660.306396484375
}
2022-10-24 15:20:23,419 - trainer - INFO - start training epoch 70
2022-10-24 15:20:23,419 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,420 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,421 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,434 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval training set based on eval_every=2***************
2022-10-24 15:20:23,435 - trainer - INFO - {
  "train_loss": 2588.7332763671875
}
2022-10-24 15:20:23,440 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval development set based on eval_every=2***************
2022-10-24 15:20:23,442 - trainer - INFO - {
  "dev_loss": 2511.388671875,
  "dev_best_score_for_loss": -2511.388671875
}
2022-10-24 15:20:23,443 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,446 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,447 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,447 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_64
2022-10-24 15:20:23,449 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,452 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,454 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,455 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,456 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,456 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_70
2022-10-24 15:20:23,461 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_70
2022-10-24 15:20:23,463 - trainer - INFO - 
*****************[epoch: 70, global step: 71] eval training set at end of epoch***************
2022-10-24 15:20:23,464 - trainer - INFO - {
  "train_loss": 2517.16015625
}
2022-10-24 15:20:23,465 - trainer - INFO - start training epoch 71
2022-10-24 15:20:23,465 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,466 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,466 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,475 - trainer - INFO - 
*****************[epoch: 71, global step: 72] eval training set at end of epoch***************
2022-10-24 15:20:23,476 - trainer - INFO - {
  "train_loss": 2511.388671875
}
2022-10-24 15:20:23,477 - trainer - INFO - start training epoch 72
2022-10-24 15:20:23,478 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,478 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,479 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,485 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval training set based on eval_every=2***************
2022-10-24 15:20:23,485 - trainer - INFO - {
  "train_loss": 2429.8812255859375
}
2022-10-24 15:20:23,494 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval development set based on eval_every=2***************
2022-10-24 15:20:23,495 - trainer - INFO - {
  "dev_loss": 2145.9697265625,
  "dev_best_score_for_loss": -2145.9697265625
}
2022-10-24 15:20:23,496 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,498 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,498 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,499 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_66
2022-10-24 15:20:23,500 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,505 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,505 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,506 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,507 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,507 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_72
2022-10-24 15:20:23,512 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_72
2022-10-24 15:20:23,513 - trainer - INFO - 
*****************[epoch: 72, global step: 73] eval training set at end of epoch***************
2022-10-24 15:20:23,514 - trainer - INFO - {
  "train_loss": 2348.373779296875
}
2022-10-24 15:20:23,514 - trainer - INFO - start training epoch 73
2022-10-24 15:20:23,515 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,516 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,516 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,524 - trainer - INFO - 
*****************[epoch: 73, global step: 74] eval training set at end of epoch***************
2022-10-24 15:20:23,525 - trainer - INFO - {
  "train_loss": 2145.969482421875
}
2022-10-24 15:20:23,525 - trainer - INFO - start training epoch 74
2022-10-24 15:20:23,526 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,526 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,527 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,537 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval training set based on eval_every=2***************
2022-10-24 15:20:23,537 - trainer - INFO - {
  "train_loss": 2121.3150634765625
}
2022-10-24 15:20:23,544 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval development set based on eval_every=2***************
2022-10-24 15:20:23,545 - trainer - INFO - {
  "dev_loss": 2014.179443359375,
  "dev_best_score_for_loss": -2014.179443359375
}
2022-10-24 15:20:23,546 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,547 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,547 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,548 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_68
2022-10-24 15:20:23,550 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,555 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,555 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,556 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,556 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,557 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_74
2022-10-24 15:20:23,562 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_74
2022-10-24 15:20:23,562 - trainer - INFO - 
*****************[epoch: 74, global step: 75] eval training set at end of epoch***************
2022-10-24 15:20:23,563 - trainer - INFO - {
  "train_loss": 2096.66064453125
}
2022-10-24 15:20:23,564 - trainer - INFO - start training epoch 75
2022-10-24 15:20:23,564 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,564 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,565 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,574 - trainer - INFO - 
*****************[epoch: 75, global step: 76] eval training set at end of epoch***************
2022-10-24 15:20:23,574 - trainer - INFO - {
  "train_loss": 2014.179443359375
}
2022-10-24 15:20:23,575 - trainer - INFO - start training epoch 76
2022-10-24 15:20:23,575 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,575 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,576 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,583 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval training set based on eval_every=2***************
2022-10-24 15:20:23,583 - trainer - INFO - {
  "train_loss": 1919.426513671875
}
2022-10-24 15:20:23,589 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval development set based on eval_every=2***************
2022-10-24 15:20:23,589 - trainer - INFO - {
  "dev_loss": 1718.059326171875,
  "dev_best_score_for_loss": -1718.059326171875
}
2022-10-24 15:20:23,589 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,590 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,591 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,591 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_70
2022-10-24 15:20:23,592 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,596 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,597 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,597 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,598 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,598 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_76
2022-10-24 15:20:23,603 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_76
2022-10-24 15:20:23,603 - trainer - INFO - 
*****************[epoch: 76, global step: 77] eval training set at end of epoch***************
2022-10-24 15:20:23,604 - trainer - INFO - {
  "train_loss": 1824.673583984375
}
2022-10-24 15:20:23,604 - trainer - INFO - start training epoch 77
2022-10-24 15:20:23,604 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,604 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,605 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,611 - trainer - INFO - 
*****************[epoch: 77, global step: 78] eval training set at end of epoch***************
2022-10-24 15:20:23,613 - trainer - INFO - {
  "train_loss": 1718.059326171875
}
2022-10-24 15:20:23,613 - trainer - INFO - start training epoch 78
2022-10-24 15:20:23,615 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,615 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,618 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,626 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval training set based on eval_every=2***************
2022-10-24 15:20:23,626 - trainer - INFO - {
  "train_loss": 1691.0010986328125
}
2022-10-24 15:20:23,635 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval development set based on eval_every=2***************
2022-10-24 15:20:23,636 - trainer - INFO - {
  "dev_loss": 1516.2572021484375,
  "dev_best_score_for_loss": -1516.2572021484375
}
2022-10-24 15:20:23,636 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,638 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,638 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,638 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_72
2022-10-24 15:20:23,639 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,642 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,643 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,643 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,644 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,645 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_78
2022-10-24 15:20:23,649 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_78
2022-10-24 15:20:23,650 - trainer - INFO - 
*****************[epoch: 78, global step: 79] eval training set at end of epoch***************
2022-10-24 15:20:23,650 - trainer - INFO - {
  "train_loss": 1663.94287109375
}
2022-10-24 15:20:23,650 - trainer - INFO - start training epoch 79
2022-10-24 15:20:23,650 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,651 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,651 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,659 - trainer - INFO - 
*****************[epoch: 79, global step: 80] eval training set at end of epoch***************
2022-10-24 15:20:23,660 - trainer - INFO - {
  "train_loss": 1516.2572021484375
}
2022-10-24 15:20:23,662 - trainer - INFO - start training epoch 80
2022-10-24 15:20:23,663 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,663 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,666 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,673 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval training set based on eval_every=2***************
2022-10-24 15:20:23,673 - trainer - INFO - {
  "train_loss": 1451.8384399414062
}
2022-10-24 15:20:23,682 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval development set based on eval_every=2***************
2022-10-24 15:20:23,683 - trainer - INFO - {
  "dev_loss": 1332.669921875,
  "dev_best_score_for_loss": -1332.669921875
}
2022-10-24 15:20:23,683 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,685 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,685 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,685 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_74
2022-10-24 15:20:23,687 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,690 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,691 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,691 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,691 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,692 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_80
2022-10-24 15:20:23,696 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_80
2022-10-24 15:20:23,697 - trainer - INFO - 
*****************[epoch: 80, global step: 81] eval training set at end of epoch***************
2022-10-24 15:20:23,697 - trainer - INFO - {
  "train_loss": 1387.419677734375
}
2022-10-24 15:20:23,698 - trainer - INFO - start training epoch 81
2022-10-24 15:20:23,698 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,698 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,699 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,709 - trainer - INFO - 
*****************[epoch: 81, global step: 82] eval training set at end of epoch***************
2022-10-24 15:20:23,709 - trainer - INFO - {
  "train_loss": 1332.669921875
}
2022-10-24 15:20:23,710 - trainer - INFO - start training epoch 82
2022-10-24 15:20:23,710 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,711 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,711 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,719 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval training set based on eval_every=2***************
2022-10-24 15:20:23,719 - trainer - INFO - {
  "train_loss": 1277.963134765625
}
2022-10-24 15:20:23,730 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval development set based on eval_every=2***************
2022-10-24 15:20:23,731 - trainer - INFO - {
  "dev_loss": 1092.8480224609375,
  "dev_best_score_for_loss": -1092.8480224609375
}
2022-10-24 15:20:23,732 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,733 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,733 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,734 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_76
2022-10-24 15:20:23,736 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,740 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,740 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,741 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,742 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,742 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_82
2022-10-24 15:20:23,747 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_82
2022-10-24 15:20:23,748 - trainer - INFO - 
*****************[epoch: 82, global step: 83] eval training set at end of epoch***************
2022-10-24 15:20:23,748 - trainer - INFO - {
  "train_loss": 1223.25634765625
}
2022-10-24 15:20:23,749 - trainer - INFO - start training epoch 83
2022-10-24 15:20:23,749 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,749 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,750 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,759 - trainer - INFO - 
*****************[epoch: 83, global step: 84] eval training set at end of epoch***************
2022-10-24 15:20:23,760 - trainer - INFO - {
  "train_loss": 1092.8480224609375
}
2022-10-24 15:20:23,760 - trainer - INFO - start training epoch 84
2022-10-24 15:20:23,760 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,761 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,761 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,774 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval training set based on eval_every=2***************
2022-10-24 15:20:23,774 - trainer - INFO - {
  "train_loss": 1061.065673828125
}
2022-10-24 15:20:23,783 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval development set based on eval_every=2***************
2022-10-24 15:20:23,783 - trainer - INFO - {
  "dev_loss": 944.5490112304688,
  "dev_best_score_for_loss": -944.5490112304688
}
2022-10-24 15:20:23,784 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,786 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,786 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,786 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_78
2022-10-24 15:20:23,788 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,792 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,792 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,792 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,793 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,794 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_84
2022-10-24 15:20:23,799 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_84
2022-10-24 15:20:23,800 - trainer - INFO - 
*****************[epoch: 84, global step: 85] eval training set at end of epoch***************
2022-10-24 15:20:23,801 - trainer - INFO - {
  "train_loss": 1029.2833251953125
}
2022-10-24 15:20:23,801 - trainer - INFO - start training epoch 85
2022-10-24 15:20:23,801 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,801 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,802 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,810 - trainer - INFO - 
*****************[epoch: 85, global step: 86] eval training set at end of epoch***************
2022-10-24 15:20:23,810 - trainer - INFO - {
  "train_loss": 944.5490112304688
}
2022-10-24 15:20:23,811 - trainer - INFO - start training epoch 86
2022-10-24 15:20:23,811 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,811 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,812 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,823 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval training set based on eval_every=2***************
2022-10-24 15:20:23,824 - trainer - INFO - {
  "train_loss": 887.4806518554688
}
2022-10-24 15:20:23,837 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval development set based on eval_every=2***************
2022-10-24 15:20:23,838 - trainer - INFO - {
  "dev_loss": 766.6476440429688,
  "dev_best_score_for_loss": -766.6476440429688
}
2022-10-24 15:20:23,839 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,840 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,841 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,841 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_80
2022-10-24 15:20:23,843 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,847 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,847 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,847 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,848 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,849 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_86
2022-10-24 15:20:23,853 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_86
2022-10-24 15:20:23,854 - trainer - INFO - 
*****************[epoch: 86, global step: 87] eval training set at end of epoch***************
2022-10-24 15:20:23,854 - trainer - INFO - {
  "train_loss": 830.4122924804688
}
2022-10-24 15:20:23,855 - trainer - INFO - start training epoch 87
2022-10-24 15:20:23,855 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,855 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,855 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,868 - trainer - INFO - 
*****************[epoch: 87, global step: 88] eval training set at end of epoch***************
2022-10-24 15:20:23,868 - trainer - INFO - {
  "train_loss": 766.6476440429688
}
2022-10-24 15:20:23,869 - trainer - INFO - start training epoch 88
2022-10-24 15:20:23,869 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,870 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,870 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,879 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval training set based on eval_every=2***************
2022-10-24 15:20:23,880 - trainer - INFO - {
  "train_loss": 732.4617004394531
}
2022-10-24 15:20:23,886 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval development set based on eval_every=2***************
2022-10-24 15:20:23,887 - trainer - INFO - {
  "dev_loss": 601.05078125,
  "dev_best_score_for_loss": -601.05078125
}
2022-10-24 15:20:23,887 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,889 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,889 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,890 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_82
2022-10-24 15:20:23,892 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,896 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,896 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,897 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,898 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,898 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_88
2022-10-24 15:20:23,902 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_88
2022-10-24 15:20:23,903 - trainer - INFO - 
*****************[epoch: 88, global step: 89] eval training set at end of epoch***************
2022-10-24 15:20:23,903 - trainer - INFO - {
  "train_loss": 698.2757568359375
}
2022-10-24 15:20:23,904 - trainer - INFO - start training epoch 89
2022-10-24 15:20:23,904 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,904 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,904 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,915 - trainer - INFO - 
*****************[epoch: 89, global step: 90] eval training set at end of epoch***************
2022-10-24 15:20:23,915 - trainer - INFO - {
  "train_loss": 601.05078125
}
2022-10-24 15:20:23,916 - trainer - INFO - start training epoch 90
2022-10-24 15:20:23,916 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,917 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,917 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,925 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval training set based on eval_every=2***************
2022-10-24 15:20:23,926 - trainer - INFO - {
  "train_loss": 571.3108520507812
}
2022-10-24 15:20:23,934 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval development set based on eval_every=2***************
2022-10-24 15:20:23,935 - trainer - INFO - {
  "dev_loss": 485.68048095703125,
  "dev_best_score_for_loss": -485.68048095703125
}
2022-10-24 15:20:23,935 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,937 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,938 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,938 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_84
2022-10-24 15:20:23,940 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,944 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,944 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,944 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,946 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,946 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_90
2022-10-24 15:20:23,951 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_90
2022-10-24 15:20:23,952 - trainer - INFO - 
*****************[epoch: 90, global step: 91] eval training set at end of epoch***************
2022-10-24 15:20:23,952 - trainer - INFO - {
  "train_loss": 541.5709228515625
}
2022-10-24 15:20:23,952 - trainer - INFO - start training epoch 91
2022-10-24 15:20:23,954 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,955 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,956 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,970 - trainer - INFO - 
*****************[epoch: 91, global step: 92] eval training set at end of epoch***************
2022-10-24 15:20:23,970 - trainer - INFO - {
  "train_loss": 485.680419921875
}
2022-10-24 15:20:23,971 - trainer - INFO - start training epoch 92
2022-10-24 15:20:23,971 - trainer - INFO - training using device=cuda
2022-10-24 15:20:23,971 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:23,972 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:23,980 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval training set based on eval_every=2***************
2022-10-24 15:20:23,980 - trainer - INFO - {
  "train_loss": 446.9299774169922
}
2022-10-24 15:20:23,986 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval development set based on eval_every=2***************
2022-10-24 15:20:23,986 - trainer - INFO - {
  "dev_loss": 360.5757751464844,
  "dev_best_score_for_loss": -360.5757751464844
}
2022-10-24 15:20:23,987 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:23,988 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:23,988 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:23,989 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_86
2022-10-24 15:20:23,991 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:23,994 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:23,994 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:23,995 - trainer - INFO -   patience: 200
2022-10-24 15:20:23,996 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:23,996 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_92
2022-10-24 15:20:24,005 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_92
2022-10-24 15:20:24,006 - trainer - INFO - 
*****************[epoch: 92, global step: 93] eval training set at end of epoch***************
2022-10-24 15:20:24,006 - trainer - INFO - {
  "train_loss": 408.1795349121094
}
2022-10-24 15:20:24,007 - trainer - INFO - start training epoch 93
2022-10-24 15:20:24,007 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,007 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,008 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,017 - trainer - INFO - 
*****************[epoch: 93, global step: 94] eval training set at end of epoch***************
2022-10-24 15:20:24,017 - trainer - INFO - {
  "train_loss": 360.5757751464844
}
2022-10-24 15:20:24,018 - trainer - INFO - start training epoch 94
2022-10-24 15:20:24,018 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,018 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,018 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,025 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval training set based on eval_every=2***************
2022-10-24 15:20:24,025 - trainer - INFO - {
  "train_loss": 338.26739501953125
}
2022-10-24 15:20:24,032 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval development set based on eval_every=2***************
2022-10-24 15:20:24,033 - trainer - INFO - {
  "dev_loss": 255.7795867919922,
  "dev_best_score_for_loss": -255.7795867919922
}
2022-10-24 15:20:24,034 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:24,037 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,037 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,037 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_88
2022-10-24 15:20:24,039 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:24,042 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:24,042 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:24,043 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,044 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:24,044 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_94
2022-10-24 15:20:24,050 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_94
2022-10-24 15:20:24,051 - trainer - INFO - 
*****************[epoch: 94, global step: 95] eval training set at end of epoch***************
2022-10-24 15:20:24,051 - trainer - INFO - {
  "train_loss": 315.9590148925781
}
2022-10-24 15:20:24,052 - trainer - INFO - start training epoch 95
2022-10-24 15:20:24,052 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,052 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,052 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,059 - trainer - INFO - 
*****************[epoch: 95, global step: 96] eval training set at end of epoch***************
2022-10-24 15:20:24,059 - trainer - INFO - {
  "train_loss": 255.7795867919922
}
2022-10-24 15:20:24,060 - trainer - INFO - start training epoch 96
2022-10-24 15:20:24,060 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,060 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,061 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,068 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval training set based on eval_every=2***************
2022-10-24 15:20:24,068 - trainer - INFO - {
  "train_loss": 238.37203216552734
}
2022-10-24 15:20:24,073 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval development set based on eval_every=2***************
2022-10-24 15:20:24,074 - trainer - INFO - {
  "dev_loss": 187.52598571777344,
  "dev_best_score_for_loss": -187.52598571777344
}
2022-10-24 15:20:24,074 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:24,075 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,076 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,076 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_90
2022-10-24 15:20:24,078 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:24,084 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:24,085 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:24,085 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,086 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:24,086 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_96
2022-10-24 15:20:24,090 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_96
2022-10-24 15:20:24,091 - trainer - INFO - 
*****************[epoch: 96, global step: 97] eval training set at end of epoch***************
2022-10-24 15:20:24,091 - trainer - INFO - {
  "train_loss": 220.9644775390625
}
2022-10-24 15:20:24,092 - trainer - INFO - start training epoch 97
2022-10-24 15:20:24,093 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,094 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,094 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,103 - trainer - INFO - 
*****************[epoch: 97, global step: 98] eval training set at end of epoch***************
2022-10-24 15:20:24,103 - trainer - INFO - {
  "train_loss": 187.52598571777344
}
2022-10-24 15:20:24,103 - trainer - INFO - start training epoch 98
2022-10-24 15:20:24,104 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,104 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,104 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,111 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval training set based on eval_every=2***************
2022-10-24 15:20:24,111 - trainer - INFO - {
  "train_loss": 165.79612731933594
}
2022-10-24 15:20:24,117 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval development set based on eval_every=2***************
2022-10-24 15:20:24,118 - trainer - INFO - {
  "dev_loss": 122.02941131591797,
  "dev_best_score_for_loss": -122.02941131591797
}
2022-10-24 15:20:24,118 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:24,119 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,120 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,120 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_92
2022-10-24 15:20:24,121 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:24,125 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:24,128 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:24,128 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,130 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:24,130 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_98
2022-10-24 15:20:24,134 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_98
2022-10-24 15:20:24,135 - trainer - INFO - 
*****************[epoch: 98, global step: 99] eval training set at end of epoch***************
2022-10-24 15:20:24,135 - trainer - INFO - {
  "train_loss": 144.06626892089844
}
2022-10-24 15:20:24,136 - trainer - INFO - start training epoch 99
2022-10-24 15:20:24,136 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,137 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,137 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,145 - trainer - INFO - 
*****************[epoch: 99, global step: 100] eval training set at end of epoch***************
2022-10-24 15:20:24,146 - trainer - INFO - {
  "train_loss": 122.02940368652344
}
2022-10-24 15:20:24,146 - trainer - INFO - start training epoch 100
2022-10-24 15:20:24,146 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,146 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,147 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,153 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval training set based on eval_every=2***************
2022-10-24 15:20:24,153 - trainer - INFO - {
  "train_loss": 109.61479187011719
}
2022-10-24 15:20:24,159 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval development set based on eval_every=2***************
2022-10-24 15:20:24,159 - trainer - INFO - {
  "dev_loss": 68.83242797851562,
  "dev_best_score_for_loss": -68.83242797851562
}
2022-10-24 15:20:24,160 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:24,161 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,161 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,162 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_94
2022-10-24 15:20:24,163 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:24,166 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:24,167 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:24,167 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,168 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:24,168 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_100
2022-10-24 15:20:24,173 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_100
2022-10-24 15:20:24,177 - trainer - INFO - 
*****************[epoch: 100, global step: 101] eval training set at end of epoch***************
2022-10-24 15:20:24,177 - trainer - INFO - {
  "train_loss": 97.20018005371094
}
2022-10-24 15:20:24,177 - trainer - INFO - start training epoch 101
2022-10-24 15:20:24,178 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,178 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,178 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,186 - trainer - INFO - 
*****************[epoch: 101, global step: 102] eval training set at end of epoch***************
2022-10-24 15:20:24,186 - trainer - INFO - {
  "train_loss": 68.83243560791016
}
2022-10-24 15:20:24,187 - trainer - INFO - start training epoch 102
2022-10-24 15:20:24,187 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,187 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,188 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,195 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval training set based on eval_every=2***************
2022-10-24 15:20:24,196 - trainer - INFO - {
  "train_loss": 63.2009220123291
}
2022-10-24 15:20:24,202 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval development set based on eval_every=2***************
2022-10-24 15:20:24,203 - trainer - INFO - {
  "dev_loss": 40.053016662597656,
  "dev_best_score_for_loss": -40.053016662597656
}
2022-10-24 15:20:24,203 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:24,205 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,205 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,205 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_96
2022-10-24 15:20:24,206 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:24,210 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:24,210 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:24,211 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,212 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:24,212 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_102
2022-10-24 15:20:24,218 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_102
2022-10-24 15:20:24,221 - trainer - INFO - 
*****************[epoch: 102, global step: 103] eval training set at end of epoch***************
2022-10-24 15:20:24,224 - trainer - INFO - {
  "train_loss": 57.56940841674805
}
2022-10-24 15:20:24,224 - trainer - INFO - start training epoch 103
2022-10-24 15:20:24,225 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,225 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,225 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,236 - trainer - INFO - 
*****************[epoch: 103, global step: 104] eval training set at end of epoch***************
2022-10-24 15:20:24,237 - trainer - INFO - {
  "train_loss": 40.053016662597656
}
2022-10-24 15:20:24,237 - trainer - INFO - start training epoch 104
2022-10-24 15:20:24,238 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,238 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,239 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,249 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval training set based on eval_every=2***************
2022-10-24 15:20:24,249 - trainer - INFO - {
  "train_loss": 32.78519344329834
}
2022-10-24 15:20:24,256 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval development set based on eval_every=2***************
2022-10-24 15:20:24,256 - trainer - INFO - {
  "dev_loss": 21.627681732177734,
  "dev_best_score_for_loss": -21.627681732177734
}
2022-10-24 15:20:24,257 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:24,259 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,259 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,259 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_98
2022-10-24 15:20:24,261 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:24,266 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:24,270 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:24,270 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,271 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:24,271 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_104
2022-10-24 15:20:24,276 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_104
2022-10-24 15:20:24,277 - trainer - INFO - 
*****************[epoch: 104, global step: 105] eval training set at end of epoch***************
2022-10-24 15:20:24,277 - trainer - INFO - {
  "train_loss": 25.517370223999023
}
2022-10-24 15:20:24,278 - trainer - INFO - start training epoch 105
2022-10-24 15:20:24,279 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,279 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,280 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,288 - trainer - INFO - 
*****************[epoch: 105, global step: 106] eval training set at end of epoch***************
2022-10-24 15:20:24,288 - trainer - INFO - {
  "train_loss": 21.627681732177734
}
2022-10-24 15:20:24,289 - trainer - INFO - start training epoch 106
2022-10-24 15:20:24,289 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,289 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,289 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,297 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval training set based on eval_every=2***************
2022-10-24 15:20:24,297 - trainer - INFO - {
  "train_loss": 16.10316562652588
}
2022-10-24 15:20:24,303 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval development set based on eval_every=2***************
2022-10-24 15:20:24,304 - trainer - INFO - {
  "dev_loss": 6.335954666137695,
  "dev_best_score_for_loss": -6.335954666137695
}
2022-10-24 15:20:24,304 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:24,306 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,306 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,306 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_100
2022-10-24 15:20:24,307 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:24,311 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:24,312 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:24,313 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,314 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:24,317 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_106
2022-10-24 15:20:24,321 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_106
2022-10-24 15:20:24,322 - trainer - INFO - 
*****************[epoch: 106, global step: 107] eval training set at end of epoch***************
2022-10-24 15:20:24,323 - trainer - INFO - {
  "train_loss": 10.578649520874023
}
2022-10-24 15:20:24,323 - trainer - INFO - start training epoch 107
2022-10-24 15:20:24,323 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,323 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,324 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,334 - trainer - INFO - 
*****************[epoch: 107, global step: 108] eval training set at end of epoch***************
2022-10-24 15:20:24,335 - trainer - INFO - {
  "train_loss": 6.335954189300537
}
2022-10-24 15:20:24,335 - trainer - INFO - start training epoch 108
2022-10-24 15:20:24,335 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,335 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,336 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,342 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval training set based on eval_every=2***************
2022-10-24 15:20:24,342 - trainer - INFO - {
  "train_loss": 6.193196773529053
}
2022-10-24 15:20:24,348 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval development set based on eval_every=2***************
2022-10-24 15:20:24,349 - trainer - INFO - {
  "dev_loss": 0.7208293676376343,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,349 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:24,350 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,351 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,351 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_102
2022-10-24 15:20:24,352 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:24,355 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:24,356 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:24,357 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,359 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:24,359 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_108
2022-10-24 15:20:24,367 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_108
2022-10-24 15:20:24,368 - trainer - INFO - 
*****************[epoch: 108, global step: 109] eval training set at end of epoch***************
2022-10-24 15:20:24,368 - trainer - INFO - {
  "train_loss": 6.050439357757568
}
2022-10-24 15:20:24,369 - trainer - INFO - start training epoch 109
2022-10-24 15:20:24,369 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,370 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,370 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,379 - trainer - INFO - 
*****************[epoch: 109, global step: 110] eval training set at end of epoch***************
2022-10-24 15:20:24,380 - trainer - INFO - {
  "train_loss": 0.7208293676376343
}
2022-10-24 15:20:24,380 - trainer - INFO - start training epoch 110
2022-10-24 15:20:24,380 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,380 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,380 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,386 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval training set based on eval_every=2***************
2022-10-24 15:20:24,387 - trainer - INFO - {
  "train_loss": 1.9385973811149597
}
2022-10-24 15:20:24,392 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval development set based on eval_every=2***************
2022-10-24 15:20:24,393 - trainer - INFO - {
  "dev_loss": 3.1804370880126953,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,393 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:24,394 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,395 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,396 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,396 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_104
2022-10-24 15:20:24,397 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_110
2022-10-24 15:20:24,401 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_110
2022-10-24 15:20:24,403 - trainer - INFO - 
*****************[epoch: 110, global step: 111] eval training set at end of epoch***************
2022-10-24 15:20:24,406 - trainer - INFO - {
  "train_loss": 3.156365394592285
}
2022-10-24 15:20:24,407 - trainer - INFO - start training epoch 111
2022-10-24 15:20:24,407 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,407 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,408 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,415 - trainer - INFO - 
*****************[epoch: 111, global step: 112] eval training set at end of epoch***************
2022-10-24 15:20:24,416 - trainer - INFO - {
  "train_loss": 3.1804370880126953
}
2022-10-24 15:20:24,416 - trainer - INFO - start training epoch 112
2022-10-24 15:20:24,416 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,417 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,417 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,425 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval training set based on eval_every=2***************
2022-10-24 15:20:24,426 - trainer - INFO - {
  "train_loss": 2.8792132139205933
}
2022-10-24 15:20:24,432 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval development set based on eval_every=2***************
2022-10-24 15:20:24,432 - trainer - INFO - {
  "dev_loss": 6.649686336517334,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,433 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:24,434 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,435 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,435 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,435 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_106
2022-10-24 15:20:24,436 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_112
2022-10-24 15:20:24,440 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_112
2022-10-24 15:20:24,441 - trainer - INFO - 
*****************[epoch: 112, global step: 113] eval training set at end of epoch***************
2022-10-24 15:20:24,441 - trainer - INFO - {
  "train_loss": 2.577989339828491
}
2022-10-24 15:20:24,442 - trainer - INFO - start training epoch 113
2022-10-24 15:20:24,442 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,442 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,442 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,455 - trainer - INFO - 
*****************[epoch: 113, global step: 114] eval training set at end of epoch***************
2022-10-24 15:20:24,455 - trainer - INFO - {
  "train_loss": 6.649686336517334
}
2022-10-24 15:20:24,456 - trainer - INFO - start training epoch 114
2022-10-24 15:20:24,456 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,456 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,456 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,465 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval training set based on eval_every=2***************
2022-10-24 15:20:24,466 - trainer - INFO - {
  "train_loss": 6.434978008270264
}
2022-10-24 15:20:24,472 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval development set based on eval_every=2***************
2022-10-24 15:20:24,472 - trainer - INFO - {
  "dev_loss": 8.571074485778809,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,473 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:24,473 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,474 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,474 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,475 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_108
2022-10-24 15:20:24,476 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_114
2022-10-24 15:20:24,481 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_114
2022-10-24 15:20:24,481 - trainer - INFO - 
*****************[epoch: 114, global step: 115] eval training set at end of epoch***************
2022-10-24 15:20:24,482 - trainer - INFO - {
  "train_loss": 6.220269680023193
}
2022-10-24 15:20:24,482 - trainer - INFO - start training epoch 115
2022-10-24 15:20:24,482 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,483 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,483 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,490 - trainer - INFO - 
*****************[epoch: 115, global step: 116] eval training set at end of epoch***************
2022-10-24 15:20:24,490 - trainer - INFO - {
  "train_loss": 8.571074485778809
}
2022-10-24 15:20:24,490 - trainer - INFO - start training epoch 116
2022-10-24 15:20:24,490 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,491 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,491 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,502 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval training set based on eval_every=2***************
2022-10-24 15:20:24,503 - trainer - INFO - {
  "train_loss": 9.764575481414795
}
2022-10-24 15:20:24,508 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval development set based on eval_every=2***************
2022-10-24 15:20:24,509 - trainer - INFO - {
  "dev_loss": 10.75345230102539,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,509 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:24,510 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,512 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,512 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,512 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_110
2022-10-24 15:20:24,514 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_116
2022-10-24 15:20:24,518 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_116
2022-10-24 15:20:24,519 - trainer - INFO - 
*****************[epoch: 116, global step: 117] eval training set at end of epoch***************
2022-10-24 15:20:24,519 - trainer - INFO - {
  "train_loss": 10.958076477050781
}
2022-10-24 15:20:24,520 - trainer - INFO - start training epoch 117
2022-10-24 15:20:24,520 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,520 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,521 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,528 - trainer - INFO - 
*****************[epoch: 117, global step: 118] eval training set at end of epoch***************
2022-10-24 15:20:24,528 - trainer - INFO - {
  "train_loss": 10.75345230102539
}
2022-10-24 15:20:24,529 - trainer - INFO - start training epoch 118
2022-10-24 15:20:24,529 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,529 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,530 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,536 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval training set based on eval_every=2***************
2022-10-24 15:20:24,537 - trainer - INFO - {
  "train_loss": 12.128549098968506
}
2022-10-24 15:20:24,543 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval development set based on eval_every=2***************
2022-10-24 15:20:24,543 - trainer - INFO - {
  "dev_loss": 13.522566795349121,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,544 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:20:24,548 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,549 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,549 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,549 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_112
2022-10-24 15:20:24,551 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_118
2022-10-24 15:20:24,555 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_118
2022-10-24 15:20:24,555 - trainer - INFO - 
*****************[epoch: 118, global step: 119] eval training set at end of epoch***************
2022-10-24 15:20:24,556 - trainer - INFO - {
  "train_loss": 13.503645896911621
}
2022-10-24 15:20:24,556 - trainer - INFO - start training epoch 119
2022-10-24 15:20:24,556 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,556 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,557 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,567 - trainer - INFO - 
*****************[epoch: 119, global step: 120] eval training set at end of epoch***************
2022-10-24 15:20:24,567 - trainer - INFO - {
  "train_loss": 13.522567749023438
}
2022-10-24 15:20:24,568 - trainer - INFO - start training epoch 120
2022-10-24 15:20:24,568 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,568 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,568 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,576 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval training set based on eval_every=2***************
2022-10-24 15:20:24,576 - trainer - INFO - {
  "train_loss": 13.777109146118164
}
2022-10-24 15:20:24,583 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval development set based on eval_every=2***************
2022-10-24 15:20:24,583 - trainer - INFO - {
  "dev_loss": 15.11227798461914,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,584 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:20:24,584 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,585 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,586 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,586 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_114
2022-10-24 15:20:24,587 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_120
2022-10-24 15:20:24,592 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_120
2022-10-24 15:20:24,593 - trainer - INFO - 
*****************[epoch: 120, global step: 121] eval training set at end of epoch***************
2022-10-24 15:20:24,593 - trainer - INFO - {
  "train_loss": 14.03165054321289
}
2022-10-24 15:20:24,594 - trainer - INFO - start training epoch 121
2022-10-24 15:20:24,594 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,595 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,595 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,605 - trainer - INFO - 
*****************[epoch: 121, global step: 122] eval training set at end of epoch***************
2022-10-24 15:20:24,605 - trainer - INFO - {
  "train_loss": 15.11227798461914
}
2022-10-24 15:20:24,606 - trainer - INFO - start training epoch 122
2022-10-24 15:20:24,606 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,606 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,607 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,613 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval training set based on eval_every=2***************
2022-10-24 15:20:24,614 - trainer - INFO - {
  "train_loss": 14.567054271697998
}
2022-10-24 15:20:24,621 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval development set based on eval_every=2***************
2022-10-24 15:20:24,622 - trainer - INFO - {
  "dev_loss": 14.606369018554688,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,623 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:20:24,626 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,627 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,627 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,628 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_116
2022-10-24 15:20:24,630 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_122
2022-10-24 15:20:24,636 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_122
2022-10-24 15:20:24,637 - trainer - INFO - 
*****************[epoch: 122, global step: 123] eval training set at end of epoch***************
2022-10-24 15:20:24,638 - trainer - INFO - {
  "train_loss": 14.021830558776855
}
2022-10-24 15:20:24,638 - trainer - INFO - start training epoch 123
2022-10-24 15:20:24,638 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,639 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,639 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,649 - trainer - INFO - 
*****************[epoch: 123, global step: 124] eval training set at end of epoch***************
2022-10-24 15:20:24,649 - trainer - INFO - {
  "train_loss": 14.606369018554688
}
2022-10-24 15:20:24,649 - trainer - INFO - start training epoch 124
2022-10-24 15:20:24,650 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,651 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,652 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,659 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval training set based on eval_every=2***************
2022-10-24 15:20:24,660 - trainer - INFO - {
  "train_loss": 14.170922756195068
}
2022-10-24 15:20:24,670 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval development set based on eval_every=2***************
2022-10-24 15:20:24,670 - trainer - INFO - {
  "dev_loss": 12.819765090942383,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,671 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:20:24,671 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,673 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,673 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,673 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_118
2022-10-24 15:20:24,675 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_124
2022-10-24 15:20:24,680 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_124
2022-10-24 15:20:24,682 - trainer - INFO - 
*****************[epoch: 124, global step: 125] eval training set at end of epoch***************
2022-10-24 15:20:24,682 - trainer - INFO - {
  "train_loss": 13.73547649383545
}
2022-10-24 15:20:24,683 - trainer - INFO - start training epoch 125
2022-10-24 15:20:24,683 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,683 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,683 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,690 - trainer - INFO - 
*****************[epoch: 125, global step: 126] eval training set at end of epoch***************
2022-10-24 15:20:24,691 - trainer - INFO - {
  "train_loss": 12.819765090942383
}
2022-10-24 15:20:24,691 - trainer - INFO - start training epoch 126
2022-10-24 15:20:24,691 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,691 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,692 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,698 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval training set based on eval_every=2***************
2022-10-24 15:20:24,699 - trainer - INFO - {
  "train_loss": 12.64577341079712
}
2022-10-24 15:20:24,704 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval development set based on eval_every=2***************
2022-10-24 15:20:24,704 - trainer - INFO - {
  "dev_loss": 10.810107231140137,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,705 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:20:24,705 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,706 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,706 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,707 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_120
2022-10-24 15:20:24,708 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_126
2022-10-24 15:20:24,712 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_126
2022-10-24 15:20:24,714 - trainer - INFO - 
*****************[epoch: 126, global step: 127] eval training set at end of epoch***************
2022-10-24 15:20:24,715 - trainer - INFO - {
  "train_loss": 12.471781730651855
}
2022-10-24 15:20:24,719 - trainer - INFO - start training epoch 127
2022-10-24 15:20:24,719 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,719 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,719 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,726 - trainer - INFO - 
*****************[epoch: 127, global step: 128] eval training set at end of epoch***************
2022-10-24 15:20:24,727 - trainer - INFO - {
  "train_loss": 10.810107231140137
}
2022-10-24 15:20:24,727 - trainer - INFO - start training epoch 128
2022-10-24 15:20:24,729 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,729 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,729 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,736 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval training set based on eval_every=2***************
2022-10-24 15:20:24,736 - trainer - INFO - {
  "train_loss": 10.50525426864624
}
2022-10-24 15:20:24,742 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval development set based on eval_every=2***************
2022-10-24 15:20:24,742 - trainer - INFO - {
  "dev_loss": 8.972684860229492,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,743 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:20:24,743 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,744 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,745 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,745 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_122
2022-10-24 15:20:24,746 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_128
2022-10-24 15:20:24,750 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_128
2022-10-24 15:20:24,750 - trainer - INFO - 
*****************[epoch: 128, global step: 129] eval training set at end of epoch***************
2022-10-24 15:20:24,751 - trainer - INFO - {
  "train_loss": 10.200401306152344
}
2022-10-24 15:20:24,751 - trainer - INFO - start training epoch 129
2022-10-24 15:20:24,751 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,751 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,752 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,760 - trainer - INFO - 
*****************[epoch: 129, global step: 130] eval training set at end of epoch***************
2022-10-24 15:20:24,762 - trainer - INFO - {
  "train_loss": 8.972683906555176
}
2022-10-24 15:20:24,766 - trainer - INFO - start training epoch 130
2022-10-24 15:20:24,766 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,766 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,767 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,774 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval training set based on eval_every=2***************
2022-10-24 15:20:24,775 - trainer - INFO - {
  "train_loss": 8.360177278518677
}
2022-10-24 15:20:24,781 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval development set based on eval_every=2***************
2022-10-24 15:20:24,781 - trainer - INFO - {
  "dev_loss": 7.104000091552734,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,782 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:20:24,782 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,783 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,783 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,784 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_124
2022-10-24 15:20:24,785 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_130
2022-10-24 15:20:24,789 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_130
2022-10-24 15:20:24,790 - trainer - INFO - 
*****************[epoch: 130, global step: 131] eval training set at end of epoch***************
2022-10-24 15:20:24,790 - trainer - INFO - {
  "train_loss": 7.747670650482178
}
2022-10-24 15:20:24,791 - trainer - INFO - start training epoch 131
2022-10-24 15:20:24,791 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,791 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,791 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,800 - trainer - INFO - 
*****************[epoch: 131, global step: 132] eval training set at end of epoch***************
2022-10-24 15:20:24,800 - trainer - INFO - {
  "train_loss": 7.104001045227051
}
2022-10-24 15:20:24,801 - trainer - INFO - start training epoch 132
2022-10-24 15:20:24,801 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,801 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,801 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,812 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval training set based on eval_every=2***************
2022-10-24 15:20:24,812 - trainer - INFO - {
  "train_loss": 6.44718861579895
}
2022-10-24 15:20:24,818 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval development set based on eval_every=2***************
2022-10-24 15:20:24,848 - trainer - INFO - {
  "dev_loss": 5.222314834594727,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,848 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:20:24,849 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,850 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,850 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,851 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_126
2022-10-24 15:20:24,852 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_132
2022-10-24 15:20:24,857 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_132
2022-10-24 15:20:24,858 - trainer - INFO - 
*****************[epoch: 132, global step: 133] eval training set at end of epoch***************
2022-10-24 15:20:24,858 - trainer - INFO - {
  "train_loss": 5.79037618637085
}
2022-10-24 15:20:24,859 - trainer - INFO - start training epoch 133
2022-10-24 15:20:24,859 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,859 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,860 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,869 - trainer - INFO - 
*****************[epoch: 133, global step: 134] eval training set at end of epoch***************
2022-10-24 15:20:24,870 - trainer - INFO - {
  "train_loss": 5.222314834594727
}
2022-10-24 15:20:24,870 - trainer - INFO - start training epoch 134
2022-10-24 15:20:24,870 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,871 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,871 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,876 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval training set based on eval_every=2***************
2022-10-24 15:20:24,877 - trainer - INFO - {
  "train_loss": 4.814001798629761
}
2022-10-24 15:20:24,883 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval development set based on eval_every=2***************
2022-10-24 15:20:24,884 - trainer - INFO - {
  "dev_loss": 3.6729068756103516,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,886 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:20:24,888 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,890 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,890 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,890 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_128
2022-10-24 15:20:24,892 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_134
2022-10-24 15:20:24,897 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_134
2022-10-24 15:20:24,899 - trainer - INFO - 
*****************[epoch: 134, global step: 135] eval training set at end of epoch***************
2022-10-24 15:20:24,900 - trainer - INFO - {
  "train_loss": 4.405688762664795
}
2022-10-24 15:20:24,900 - trainer - INFO - start training epoch 135
2022-10-24 15:20:24,900 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,901 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,901 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,909 - trainer - INFO - 
*****************[epoch: 135, global step: 136] eval training set at end of epoch***************
2022-10-24 15:20:24,909 - trainer - INFO - {
  "train_loss": 3.6729068756103516
}
2022-10-24 15:20:24,910 - trainer - INFO - start training epoch 136
2022-10-24 15:20:24,910 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,910 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,911 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,919 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval training set based on eval_every=2***************
2022-10-24 15:20:24,919 - trainer - INFO - {
  "train_loss": 3.5130046606063843
}
2022-10-24 15:20:24,926 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval development set based on eval_every=2***************
2022-10-24 15:20:24,926 - trainer - INFO - {
  "dev_loss": 2.6812543869018555,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:24,927 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:20:24,928 - trainer - INFO -   patience: 200
2022-10-24 15:20:24,929 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:24,930 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:24,931 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_130
2022-10-24 15:20:24,932 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_136
2022-10-24 15:20:24,936 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_136
2022-10-24 15:20:24,937 - trainer - INFO - 
*****************[epoch: 136, global step: 137] eval training set at end of epoch***************
2022-10-24 15:20:24,937 - trainer - INFO - {
  "train_loss": 3.353102445602417
}
2022-10-24 15:20:24,938 - trainer - INFO - start training epoch 137
2022-10-24 15:20:24,938 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,938 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,939 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,952 - trainer - INFO - 
*****************[epoch: 137, global step: 138] eval training set at end of epoch***************
2022-10-24 15:20:24,952 - trainer - INFO - {
  "train_loss": 2.6812548637390137
}
2022-10-24 15:20:24,953 - trainer - INFO - start training epoch 138
2022-10-24 15:20:24,953 - trainer - INFO - training using device=cuda
2022-10-24 15:20:24,953 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:24,954 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:24,989 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval training set based on eval_every=2***************
2022-10-24 15:20:24,990 - trainer - INFO - {
  "train_loss": 2.5854852199554443
}
2022-10-24 15:20:24,999 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval development set based on eval_every=2***************
2022-10-24 15:20:24,999 - trainer - INFO - {
  "dev_loss": 2.1490817070007324,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,000 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:20:25,001 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,002 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,003 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,003 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_132
2022-10-24 15:20:25,005 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_138
2022-10-24 15:20:25,013 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_138
2022-10-24 15:20:25,014 - trainer - INFO - 
*****************[epoch: 138, global step: 139] eval training set at end of epoch***************
2022-10-24 15:20:25,014 - trainer - INFO - {
  "train_loss": 2.489715576171875
}
2022-10-24 15:20:25,015 - trainer - INFO - start training epoch 139
2022-10-24 15:20:25,015 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,016 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,016 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,026 - trainer - INFO - 
*****************[epoch: 139, global step: 140] eval training set at end of epoch***************
2022-10-24 15:20:25,026 - trainer - INFO - {
  "train_loss": 2.1490817070007324
}
2022-10-24 15:20:25,026 - trainer - INFO - start training epoch 140
2022-10-24 15:20:25,027 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,027 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,028 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,036 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval training set based on eval_every=2***************
2022-10-24 15:20:25,036 - trainer - INFO - {
  "train_loss": 2.0251200199127197
}
2022-10-24 15:20:25,047 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval development set based on eval_every=2***************
2022-10-24 15:20:25,047 - trainer - INFO - {
  "dev_loss": 1.8726543188095093,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,048 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:20:25,048 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,050 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,050 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,050 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_134
2022-10-24 15:20:25,052 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_140
2022-10-24 15:20:25,056 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_140
2022-10-24 15:20:25,057 - trainer - INFO - 
*****************[epoch: 140, global step: 141] eval training set at end of epoch***************
2022-10-24 15:20:25,057 - trainer - INFO - {
  "train_loss": 1.901158332824707
}
2022-10-24 15:20:25,057 - trainer - INFO - start training epoch 141
2022-10-24 15:20:25,058 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,058 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,058 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,066 - trainer - INFO - 
*****************[epoch: 141, global step: 142] eval training set at end of epoch***************
2022-10-24 15:20:25,066 - trainer - INFO - {
  "train_loss": 1.8726543188095093
}
2022-10-24 15:20:25,067 - trainer - INFO - start training epoch 142
2022-10-24 15:20:25,067 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,067 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,067 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,074 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval training set based on eval_every=2***************
2022-10-24 15:20:25,075 - trainer - INFO - {
  "train_loss": 1.7535237073898315
}
2022-10-24 15:20:25,081 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval development set based on eval_every=2***************
2022-10-24 15:20:25,082 - trainer - INFO - {
  "dev_loss": 1.6760138273239136,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,082 - trainer - INFO -   no_improve_count: 17
2022-10-24 15:20:25,083 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,083 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,085 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,085 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_136
2022-10-24 15:20:25,087 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_142
2022-10-24 15:20:25,091 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_142
2022-10-24 15:20:25,092 - trainer - INFO - 
*****************[epoch: 142, global step: 143] eval training set at end of epoch***************
2022-10-24 15:20:25,093 - trainer - INFO - {
  "train_loss": 1.6343930959701538
}
2022-10-24 15:20:25,093 - trainer - INFO - start training epoch 143
2022-10-24 15:20:25,094 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,094 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,094 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,104 - trainer - INFO - 
*****************[epoch: 143, global step: 144] eval training set at end of epoch***************
2022-10-24 15:20:25,104 - trainer - INFO - {
  "train_loss": 1.6760138273239136
}
2022-10-24 15:20:25,105 - trainer - INFO - start training epoch 144
2022-10-24 15:20:25,105 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,105 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,105 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,113 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval training set based on eval_every=2***************
2022-10-24 15:20:25,114 - trainer - INFO - {
  "train_loss": 1.6268309950828552
}
2022-10-24 15:20:25,120 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval development set based on eval_every=2***************
2022-10-24 15:20:25,121 - trainer - INFO - {
  "dev_loss": 1.5322730541229248,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,121 - trainer - INFO -   no_improve_count: 18
2022-10-24 15:20:25,122 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,123 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,123 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,123 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_138
2022-10-24 15:20:25,125 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_144
2022-10-24 15:20:25,129 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_144
2022-10-24 15:20:25,130 - trainer - INFO - 
*****************[epoch: 144, global step: 145] eval training set at end of epoch***************
2022-10-24 15:20:25,130 - trainer - INFO - {
  "train_loss": 1.5776481628417969
}
2022-10-24 15:20:25,132 - trainer - INFO - start training epoch 145
2022-10-24 15:20:25,137 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,137 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,138 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,148 - trainer - INFO - 
*****************[epoch: 145, global step: 146] eval training set at end of epoch***************
2022-10-24 15:20:25,148 - trainer - INFO - {
  "train_loss": 1.5322730541229248
}
2022-10-24 15:20:25,148 - trainer - INFO - start training epoch 146
2022-10-24 15:20:25,149 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,149 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,149 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,156 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval training set based on eval_every=2***************
2022-10-24 15:20:25,156 - trainer - INFO - {
  "train_loss": 1.5461103916168213
}
2022-10-24 15:20:25,164 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval development set based on eval_every=2***************
2022-10-24 15:20:25,165 - trainer - INFO - {
  "dev_loss": 1.4411494731903076,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,165 - trainer - INFO -   no_improve_count: 19
2022-10-24 15:20:25,165 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,166 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,167 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,167 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_140
2022-10-24 15:20:25,168 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_146
2022-10-24 15:20:25,172 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_146
2022-10-24 15:20:25,172 - trainer - INFO - 
*****************[epoch: 146, global step: 147] eval training set at end of epoch***************
2022-10-24 15:20:25,173 - trainer - INFO - {
  "train_loss": 1.5599477291107178
}
2022-10-24 15:20:25,173 - trainer - INFO - start training epoch 147
2022-10-24 15:20:25,173 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,173 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,174 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,186 - trainer - INFO - 
*****************[epoch: 147, global step: 148] eval training set at end of epoch***************
2022-10-24 15:20:25,186 - trainer - INFO - {
  "train_loss": 1.4411494731903076
}
2022-10-24 15:20:25,186 - trainer - INFO - start training epoch 148
2022-10-24 15:20:25,187 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,187 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,187 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,197 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval training set based on eval_every=2***************
2022-10-24 15:20:25,197 - trainer - INFO - {
  "train_loss": 1.456652045249939
}
2022-10-24 15:20:25,204 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval development set based on eval_every=2***************
2022-10-24 15:20:25,204 - trainer - INFO - {
  "dev_loss": 1.3698314428329468,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,205 - trainer - INFO -   no_improve_count: 20
2022-10-24 15:20:25,205 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,206 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,206 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,207 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_142
2022-10-24 15:20:25,208 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_148
2022-10-24 15:20:25,212 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_148
2022-10-24 15:20:25,213 - trainer - INFO - 
*****************[epoch: 148, global step: 149] eval training set at end of epoch***************
2022-10-24 15:20:25,213 - trainer - INFO - {
  "train_loss": 1.4721546173095703
}
2022-10-24 15:20:25,214 - trainer - INFO - start training epoch 149
2022-10-24 15:20:25,214 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,214 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,214 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,221 - trainer - INFO - 
*****************[epoch: 149, global step: 150] eval training set at end of epoch***************
2022-10-24 15:20:25,222 - trainer - INFO - {
  "train_loss": 1.3698314428329468
}
2022-10-24 15:20:25,222 - trainer - INFO - start training epoch 150
2022-10-24 15:20:25,222 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,222 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,223 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,233 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval training set based on eval_every=2***************
2022-10-24 15:20:25,234 - trainer - INFO - {
  "train_loss": 1.3464162945747375
}
2022-10-24 15:20:25,240 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval development set based on eval_every=2***************
2022-10-24 15:20:25,241 - trainer - INFO - {
  "dev_loss": 1.2702338695526123,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,241 - trainer - INFO -   no_improve_count: 21
2022-10-24 15:20:25,242 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,243 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,243 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,243 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_144
2022-10-24 15:20:25,245 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_150
2022-10-24 15:20:25,249 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_150
2022-10-24 15:20:25,250 - trainer - INFO - 
*****************[epoch: 150, global step: 151] eval training set at end of epoch***************
2022-10-24 15:20:25,250 - trainer - INFO - {
  "train_loss": 1.3230011463165283
}
2022-10-24 15:20:25,251 - trainer - INFO - start training epoch 151
2022-10-24 15:20:25,251 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,251 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,252 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,260 - trainer - INFO - 
*****************[epoch: 151, global step: 152] eval training set at end of epoch***************
2022-10-24 15:20:25,260 - trainer - INFO - {
  "train_loss": 1.2702338695526123
}
2022-10-24 15:20:25,260 - trainer - INFO - start training epoch 152
2022-10-24 15:20:25,261 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,261 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,261 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,268 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval training set based on eval_every=2***************
2022-10-24 15:20:25,268 - trainer - INFO - {
  "train_loss": 1.2117380499839783
}
2022-10-24 15:20:25,276 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval development set based on eval_every=2***************
2022-10-24 15:20:25,276 - trainer - INFO - {
  "dev_loss": 1.1151182651519775,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,277 - trainer - INFO -   no_improve_count: 22
2022-10-24 15:20:25,278 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,279 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,279 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,280 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_146
2022-10-24 15:20:25,281 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_152
2022-10-24 15:20:25,287 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_152
2022-10-24 15:20:25,288 - trainer - INFO - 
*****************[epoch: 152, global step: 153] eval training set at end of epoch***************
2022-10-24 15:20:25,288 - trainer - INFO - {
  "train_loss": 1.1532422304153442
}
2022-10-24 15:20:25,289 - trainer - INFO - start training epoch 153
2022-10-24 15:20:25,289 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,289 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,290 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,297 - trainer - INFO - 
*****************[epoch: 153, global step: 154] eval training set at end of epoch***************
2022-10-24 15:20:25,298 - trainer - INFO - {
  "train_loss": 1.1151182651519775
}
2022-10-24 15:20:25,298 - trainer - INFO - start training epoch 154
2022-10-24 15:20:25,299 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,299 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,299 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,308 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval training set based on eval_every=2***************
2022-10-24 15:20:25,308 - trainer - INFO - {
  "train_loss": 1.051971971988678
}
2022-10-24 15:20:25,315 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval development set based on eval_every=2***************
2022-10-24 15:20:25,316 - trainer - INFO - {
  "dev_loss": 0.9232836961746216,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,318 - trainer - INFO -   no_improve_count: 23
2022-10-24 15:20:25,318 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,320 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,320 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,320 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_148
2022-10-24 15:20:25,322 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_154
2022-10-24 15:20:25,326 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_154
2022-10-24 15:20:25,327 - trainer - INFO - 
*****************[epoch: 154, global step: 155] eval training set at end of epoch***************
2022-10-24 15:20:25,328 - trainer - INFO - {
  "train_loss": 0.9888256788253784
}
2022-10-24 15:20:25,328 - trainer - INFO - start training epoch 155
2022-10-24 15:20:25,328 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,329 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,329 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,338 - trainer - INFO - 
*****************[epoch: 155, global step: 156] eval training set at end of epoch***************
2022-10-24 15:20:25,338 - trainer - INFO - {
  "train_loss": 0.9232836961746216
}
2022-10-24 15:20:25,339 - trainer - INFO - start training epoch 156
2022-10-24 15:20:25,339 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,339 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,340 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,347 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval training set based on eval_every=2***************
2022-10-24 15:20:25,347 - trainer - INFO - {
  "train_loss": 0.877005398273468
}
2022-10-24 15:20:25,355 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval development set based on eval_every=2***************
2022-10-24 15:20:25,355 - trainer - INFO - {
  "dev_loss": 0.7680343389511108,
  "dev_best_score_for_loss": -0.7208293676376343
}
2022-10-24 15:20:25,356 - trainer - INFO -   no_improve_count: 24
2022-10-24 15:20:25,357 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,358 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,358 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,358 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_150
2022-10-24 15:20:25,360 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_156
2022-10-24 15:20:25,366 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_156
2022-10-24 15:20:25,367 - trainer - INFO - 
*****************[epoch: 156, global step: 157] eval training set at end of epoch***************
2022-10-24 15:20:25,367 - trainer - INFO - {
  "train_loss": 0.8307271003723145
}
2022-10-24 15:20:25,368 - trainer - INFO - start training epoch 157
2022-10-24 15:20:25,369 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,369 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,369 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,377 - trainer - INFO - 
*****************[epoch: 157, global step: 158] eval training set at end of epoch***************
2022-10-24 15:20:25,377 - trainer - INFO - {
  "train_loss": 0.7680343389511108
}
2022-10-24 15:20:25,377 - trainer - INFO - start training epoch 158
2022-10-24 15:20:25,378 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,378 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,380 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,391 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval training set based on eval_every=2***************
2022-10-24 15:20:25,392 - trainer - INFO - {
  "train_loss": 0.7528141140937805
}
2022-10-24 15:20:25,401 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval development set based on eval_every=2***************
2022-10-24 15:20:25,401 - trainer - INFO - {
  "dev_loss": 0.6629428863525391,
  "dev_best_score_for_loss": -0.6629428863525391
}
2022-10-24 15:20:25,402 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,403 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,404 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,404 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_152
2022-10-24 15:20:25,406 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,410 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,411 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,412 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,413 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,414 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_158
2022-10-24 15:20:25,419 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_158
2022-10-24 15:20:25,419 - trainer - INFO - 
*****************[epoch: 158, global step: 159] eval training set at end of epoch***************
2022-10-24 15:20:25,420 - trainer - INFO - {
  "train_loss": 0.7375938892364502
}
2022-10-24 15:20:25,420 - trainer - INFO - start training epoch 159
2022-10-24 15:20:25,420 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,421 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,421 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,431 - trainer - INFO - 
*****************[epoch: 159, global step: 160] eval training set at end of epoch***************
2022-10-24 15:20:25,431 - trainer - INFO - {
  "train_loss": 0.6629428863525391
}
2022-10-24 15:20:25,432 - trainer - INFO - start training epoch 160
2022-10-24 15:20:25,432 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,433 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,433 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,442 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval training set based on eval_every=2***************
2022-10-24 15:20:25,442 - trainer - INFO - {
  "train_loss": 0.6534022986888885
}
2022-10-24 15:20:25,449 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval development set based on eval_every=2***************
2022-10-24 15:20:25,449 - trainer - INFO - {
  "dev_loss": 0.5847064256668091,
  "dev_best_score_for_loss": -0.5847064256668091
}
2022-10-24 15:20:25,450 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,451 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,451 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,451 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_154
2022-10-24 15:20:25,453 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,456 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,457 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,457 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,459 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,463 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_160
2022-10-24 15:20:25,468 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_160
2022-10-24 15:20:25,469 - trainer - INFO - 
*****************[epoch: 160, global step: 161] eval training set at end of epoch***************
2022-10-24 15:20:25,470 - trainer - INFO - {
  "train_loss": 0.643861711025238
}
2022-10-24 15:20:25,470 - trainer - INFO - start training epoch 161
2022-10-24 15:20:25,470 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,471 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,471 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,481 - trainer - INFO - 
*****************[epoch: 161, global step: 162] eval training set at end of epoch***************
2022-10-24 15:20:25,482 - trainer - INFO - {
  "train_loss": 0.5847064256668091
}
2022-10-24 15:20:25,482 - trainer - INFO - start training epoch 162
2022-10-24 15:20:25,483 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,483 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,483 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,490 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval training set based on eval_every=2***************
2022-10-24 15:20:25,490 - trainer - INFO - {
  "train_loss": 0.5707730054855347
}
2022-10-24 15:20:25,497 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval development set based on eval_every=2***************
2022-10-24 15:20:25,498 - trainer - INFO - {
  "dev_loss": 0.5247887969017029,
  "dev_best_score_for_loss": -0.5247887969017029
}
2022-10-24 15:20:25,499 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,500 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,501 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,501 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_156
2022-10-24 15:20:25,502 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,506 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,506 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,506 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,507 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,507 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_162
2022-10-24 15:20:25,513 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_162
2022-10-24 15:20:25,513 - trainer - INFO - 
*****************[epoch: 162, global step: 163] eval training set at end of epoch***************
2022-10-24 15:20:25,514 - trainer - INFO - {
  "train_loss": 0.5568395853042603
}
2022-10-24 15:20:25,514 - trainer - INFO - start training epoch 163
2022-10-24 15:20:25,514 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,515 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,515 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,523 - trainer - INFO - 
*****************[epoch: 163, global step: 164] eval training set at end of epoch***************
2022-10-24 15:20:25,524 - trainer - INFO - {
  "train_loss": 0.5247887372970581
}
2022-10-24 15:20:25,524 - trainer - INFO - start training epoch 164
2022-10-24 15:20:25,524 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,524 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,525 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,532 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval training set based on eval_every=2***************
2022-10-24 15:20:25,532 - trainer - INFO - {
  "train_loss": 0.5061642229557037
}
2022-10-24 15:20:25,540 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval development set based on eval_every=2***************
2022-10-24 15:20:25,540 - trainer - INFO - {
  "dev_loss": 0.47534894943237305,
  "dev_best_score_for_loss": -0.47534894943237305
}
2022-10-24 15:20:25,541 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,542 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,543 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,543 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_158
2022-10-24 15:20:25,545 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,549 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,550 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,550 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,551 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,551 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_164
2022-10-24 15:20:25,555 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_164
2022-10-24 15:20:25,555 - trainer - INFO - 
*****************[epoch: 164, global step: 165] eval training set at end of epoch***************
2022-10-24 15:20:25,556 - trainer - INFO - {
  "train_loss": 0.48753970861434937
}
2022-10-24 15:20:25,556 - trainer - INFO - start training epoch 165
2022-10-24 15:20:25,556 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,556 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,557 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,565 - trainer - INFO - 
*****************[epoch: 165, global step: 166] eval training set at end of epoch***************
2022-10-24 15:20:25,566 - trainer - INFO - {
  "train_loss": 0.47534894943237305
}
2022-10-24 15:20:25,567 - trainer - INFO - start training epoch 166
2022-10-24 15:20:25,568 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,570 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,571 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,578 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval training set based on eval_every=2***************
2022-10-24 15:20:25,578 - trainer - INFO - {
  "train_loss": 0.46472400426864624
}
2022-10-24 15:20:25,585 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval development set based on eval_every=2***************
2022-10-24 15:20:25,585 - trainer - INFO - {
  "dev_loss": 0.4615260064601898,
  "dev_best_score_for_loss": -0.4615260064601898
}
2022-10-24 15:20:25,586 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,587 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,587 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,587 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_160
2022-10-24 15:20:25,589 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,591 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,592 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,592 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,593 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,593 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_166
2022-10-24 15:20:25,598 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_166
2022-10-24 15:20:25,599 - trainer - INFO - 
*****************[epoch: 166, global step: 167] eval training set at end of epoch***************
2022-10-24 15:20:25,602 - trainer - INFO - {
  "train_loss": 0.45409905910491943
}
2022-10-24 15:20:25,602 - trainer - INFO - start training epoch 167
2022-10-24 15:20:25,603 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,603 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,603 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,613 - trainer - INFO - 
*****************[epoch: 167, global step: 168] eval training set at end of epoch***************
2022-10-24 15:20:25,613 - trainer - INFO - {
  "train_loss": 0.4615260362625122
}
2022-10-24 15:20:25,614 - trainer - INFO - start training epoch 168
2022-10-24 15:20:25,614 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,614 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,614 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,621 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval training set based on eval_every=2***************
2022-10-24 15:20:25,622 - trainer - INFO - {
  "train_loss": 0.45396819710731506
}
2022-10-24 15:20:25,629 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval development set based on eval_every=2***************
2022-10-24 15:20:25,629 - trainer - INFO - {
  "dev_loss": 0.44700273871421814,
  "dev_best_score_for_loss": -0.44700273871421814
}
2022-10-24 15:20:25,630 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,631 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,631 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,631 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_162
2022-10-24 15:20:25,633 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,636 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,636 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,636 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,637 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,637 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_168
2022-10-24 15:20:25,641 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_168
2022-10-24 15:20:25,642 - trainer - INFO - 
*****************[epoch: 168, global step: 169] eval training set at end of epoch***************
2022-10-24 15:20:25,643 - trainer - INFO - {
  "train_loss": 0.4464103579521179
}
2022-10-24 15:20:25,645 - trainer - INFO - start training epoch 169
2022-10-24 15:20:25,645 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,648 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,649 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,658 - trainer - INFO - 
*****************[epoch: 169, global step: 170] eval training set at end of epoch***************
2022-10-24 15:20:25,659 - trainer - INFO - {
  "train_loss": 0.44700273871421814
}
2022-10-24 15:20:25,659 - trainer - INFO - start training epoch 170
2022-10-24 15:20:25,659 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,660 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,660 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,668 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval training set based on eval_every=2***************
2022-10-24 15:20:25,669 - trainer - INFO - {
  "train_loss": 0.44392232596874237
}
2022-10-24 15:20:25,674 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval development set based on eval_every=2***************
2022-10-24 15:20:25,675 - trainer - INFO - {
  "dev_loss": 0.4339744448661804,
  "dev_best_score_for_loss": -0.4339744448661804
}
2022-10-24 15:20:25,675 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,677 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,677 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,677 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_164
2022-10-24 15:20:25,678 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,682 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,682 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,682 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,683 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,683 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_170
2022-10-24 15:20:25,688 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_170
2022-10-24 15:20:25,689 - trainer - INFO - 
*****************[epoch: 170, global step: 171] eval training set at end of epoch***************
2022-10-24 15:20:25,690 - trainer - INFO - {
  "train_loss": 0.4408419132232666
}
2022-10-24 15:20:25,691 - trainer - INFO - start training epoch 171
2022-10-24 15:20:25,694 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,694 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,695 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,703 - trainer - INFO - 
*****************[epoch: 171, global step: 172] eval training set at end of epoch***************
2022-10-24 15:20:25,703 - trainer - INFO - {
  "train_loss": 0.43397438526153564
}
2022-10-24 15:20:25,705 - trainer - INFO - start training epoch 172
2022-10-24 15:20:25,706 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,706 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,706 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,714 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval training set based on eval_every=2***************
2022-10-24 15:20:25,714 - trainer - INFO - {
  "train_loss": 0.4344455599784851
}
2022-10-24 15:20:25,721 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval development set based on eval_every=2***************
2022-10-24 15:20:25,721 - trainer - INFO - {
  "dev_loss": 0.42499756813049316,
  "dev_best_score_for_loss": -0.42499756813049316
}
2022-10-24 15:20:25,722 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,723 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,723 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,723 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_166
2022-10-24 15:20:25,725 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,728 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,728 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,728 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,729 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,730 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_172
2022-10-24 15:20:25,734 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_172
2022-10-24 15:20:25,735 - trainer - INFO - 
*****************[epoch: 172, global step: 173] eval training set at end of epoch***************
2022-10-24 15:20:25,736 - trainer - INFO - {
  "train_loss": 0.43491673469543457
}
2022-10-24 15:20:25,737 - trainer - INFO - start training epoch 173
2022-10-24 15:20:25,738 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,739 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,742 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,751 - trainer - INFO - 
*****************[epoch: 173, global step: 174] eval training set at end of epoch***************
2022-10-24 15:20:25,752 - trainer - INFO - {
  "train_loss": 0.42499756813049316
}
2022-10-24 15:20:25,752 - trainer - INFO - start training epoch 174
2022-10-24 15:20:25,752 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,752 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,753 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,760 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval training set based on eval_every=2***************
2022-10-24 15:20:25,761 - trainer - INFO - {
  "train_loss": 0.42657649517059326
}
2022-10-24 15:20:25,767 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval development set based on eval_every=2***************
2022-10-24 15:20:25,768 - trainer - INFO - {
  "dev_loss": 0.42009660601615906,
  "dev_best_score_for_loss": -0.42009660601615906
}
2022-10-24 15:20:25,768 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,769 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,770 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,770 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_168
2022-10-24 15:20:25,771 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,774 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,774 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,775 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,775 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,776 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_174
2022-10-24 15:20:25,779 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_174
2022-10-24 15:20:25,780 - trainer - INFO - 
*****************[epoch: 174, global step: 175] eval training set at end of epoch***************
2022-10-24 15:20:25,780 - trainer - INFO - {
  "train_loss": 0.42815542221069336
}
2022-10-24 15:20:25,781 - trainer - INFO - start training epoch 175
2022-10-24 15:20:25,781 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,781 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,782 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,797 - trainer - INFO - 
*****************[epoch: 175, global step: 176] eval training set at end of epoch***************
2022-10-24 15:20:25,798 - trainer - INFO - {
  "train_loss": 0.42009657621383667
}
2022-10-24 15:20:25,798 - trainer - INFO - start training epoch 176
2022-10-24 15:20:25,799 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,799 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,800 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,807 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval training set based on eval_every=2***************
2022-10-24 15:20:25,808 - trainer - INFO - {
  "train_loss": 0.4208162724971771
}
2022-10-24 15:20:25,816 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval development set based on eval_every=2***************
2022-10-24 15:20:25,816 - trainer - INFO - {
  "dev_loss": 0.4177413880825043,
  "dev_best_score_for_loss": -0.4177413880825043
}
2022-10-24 15:20:25,817 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,818 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,818 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,819 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_170
2022-10-24 15:20:25,820 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,823 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,824 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,824 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,825 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,825 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_176
2022-10-24 15:20:25,834 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_176
2022-10-24 15:20:25,835 - trainer - INFO - 
*****************[epoch: 176, global step: 177] eval training set at end of epoch***************
2022-10-24 15:20:25,836 - trainer - INFO - {
  "train_loss": 0.4215359687805176
}
2022-10-24 15:20:25,837 - trainer - INFO - start training epoch 177
2022-10-24 15:20:25,837 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,837 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,838 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,847 - trainer - INFO - 
*****************[epoch: 177, global step: 178] eval training set at end of epoch***************
2022-10-24 15:20:25,848 - trainer - INFO - {
  "train_loss": 0.4177413880825043
}
2022-10-24 15:20:25,848 - trainer - INFO - start training epoch 178
2022-10-24 15:20:25,848 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,849 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,849 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,856 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval training set based on eval_every=2***************
2022-10-24 15:20:25,857 - trainer - INFO - {
  "train_loss": 0.41705338656902313
}
2022-10-24 15:20:25,866 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval development set based on eval_every=2***************
2022-10-24 15:20:25,866 - trainer - INFO - {
  "dev_loss": 0.4164385199546814,
  "dev_best_score_for_loss": -0.4164385199546814
}
2022-10-24 15:20:25,867 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,869 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,869 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,870 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_172
2022-10-24 15:20:25,871 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,875 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,875 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,877 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,878 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,881 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_178
2022-10-24 15:20:25,886 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_178
2022-10-24 15:20:25,887 - trainer - INFO - 
*****************[epoch: 178, global step: 179] eval training set at end of epoch***************
2022-10-24 15:20:25,888 - trainer - INFO - {
  "train_loss": 0.416365385055542
}
2022-10-24 15:20:25,888 - trainer - INFO - start training epoch 179
2022-10-24 15:20:25,888 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,888 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,889 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,899 - trainer - INFO - 
*****************[epoch: 179, global step: 180] eval training set at end of epoch***************
2022-10-24 15:20:25,899 - trainer - INFO - {
  "train_loss": 0.4164385199546814
}
2022-10-24 15:20:25,900 - trainer - INFO - start training epoch 180
2022-10-24 15:20:25,900 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,900 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,900 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,907 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval training set based on eval_every=2***************
2022-10-24 15:20:25,907 - trainer - INFO - {
  "train_loss": 0.4148339182138443
}
2022-10-24 15:20:25,913 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval development set based on eval_every=2***************
2022-10-24 15:20:25,913 - trainer - INFO - {
  "dev_loss": 0.41539594531059265,
  "dev_best_score_for_loss": -0.41539594531059265
}
2022-10-24 15:20:25,914 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,915 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,916 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,916 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_174
2022-10-24 15:20:25,917 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,920 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,920 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,921 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,922 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,923 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_180
2022-10-24 15:20:25,928 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_180
2022-10-24 15:20:25,929 - trainer - INFO - 
*****************[epoch: 180, global step: 181] eval training set at end of epoch***************
2022-10-24 15:20:25,930 - trainer - INFO - {
  "train_loss": 0.4132293164730072
}
2022-10-24 15:20:25,930 - trainer - INFO - start training epoch 181
2022-10-24 15:20:25,930 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,930 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,931 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,939 - trainer - INFO - 
*****************[epoch: 181, global step: 182] eval training set at end of epoch***************
2022-10-24 15:20:25,939 - trainer - INFO - {
  "train_loss": 0.41539597511291504
}
2022-10-24 15:20:25,940 - trainer - INFO - start training epoch 182
2022-10-24 15:20:25,940 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,940 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,941 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,947 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval training set based on eval_every=2***************
2022-10-24 15:20:25,948 - trainer - INFO - {
  "train_loss": 0.41375380754470825
}
2022-10-24 15:20:25,953 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval development set based on eval_every=2***************
2022-10-24 15:20:25,954 - trainer - INFO - {
  "dev_loss": 0.4142518937587738,
  "dev_best_score_for_loss": -0.4142518937587738
}
2022-10-24 15:20:25,954 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:25,955 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:25,955 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:25,956 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_176
2022-10-24 15:20:25,957 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:25,960 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:25,960 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:25,960 - trainer - INFO -   patience: 200
2022-10-24 15:20:25,961 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:25,962 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_182
2022-10-24 15:20:25,966 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_182
2022-10-24 15:20:25,968 - trainer - INFO - 
*****************[epoch: 182, global step: 183] eval training set at end of epoch***************
2022-10-24 15:20:25,968 - trainer - INFO - {
  "train_loss": 0.41211163997650146
}
2022-10-24 15:20:25,972 - trainer - INFO - start training epoch 183
2022-10-24 15:20:25,972 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,972 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,972 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,980 - trainer - INFO - 
*****************[epoch: 183, global step: 184] eval training set at end of epoch***************
2022-10-24 15:20:25,981 - trainer - INFO - {
  "train_loss": 0.4142518639564514
}
2022-10-24 15:20:25,981 - trainer - INFO - start training epoch 184
2022-10-24 15:20:25,981 - trainer - INFO - training using device=cuda
2022-10-24 15:20:25,982 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:25,982 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:25,989 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval training set based on eval_every=2***************
2022-10-24 15:20:25,990 - trainer - INFO - {
  "train_loss": 0.4131993055343628
}
2022-10-24 15:20:25,997 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval development set based on eval_every=2***************
2022-10-24 15:20:25,997 - trainer - INFO - {
  "dev_loss": 0.41314244270324707,
  "dev_best_score_for_loss": -0.41314244270324707
}
2022-10-24 15:20:25,998 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,000 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,000 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,000 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_178
2022-10-24 15:20:26,002 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,005 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,005 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,005 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,006 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,006 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_184
2022-10-24 15:20:26,010 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_184
2022-10-24 15:20:26,011 - trainer - INFO - 
*****************[epoch: 184, global step: 185] eval training set at end of epoch***************
2022-10-24 15:20:26,011 - trainer - INFO - {
  "train_loss": 0.41214674711227417
}
2022-10-24 15:20:26,012 - trainer - INFO - start training epoch 185
2022-10-24 15:20:26,012 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,012 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,013 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,023 - trainer - INFO - 
*****************[epoch: 185, global step: 186] eval training set at end of epoch***************
2022-10-24 15:20:26,023 - trainer - INFO - {
  "train_loss": 0.41314244270324707
}
2022-10-24 15:20:26,024 - trainer - INFO - start training epoch 186
2022-10-24 15:20:26,024 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,024 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,024 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,034 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval training set based on eval_every=2***************
2022-10-24 15:20:26,034 - trainer - INFO - {
  "train_loss": 0.41284680366516113
}
2022-10-24 15:20:26,041 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval development set based on eval_every=2***************
2022-10-24 15:20:26,041 - trainer - INFO - {
  "dev_loss": 0.41227954626083374,
  "dev_best_score_for_loss": -0.41227954626083374
}
2022-10-24 15:20:26,042 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,043 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,044 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,044 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_180
2022-10-24 15:20:26,047 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,051 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,052 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,052 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,053 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,053 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_186
2022-10-24 15:20:26,058 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_186
2022-10-24 15:20:26,060 - trainer - INFO - 
*****************[epoch: 186, global step: 187] eval training set at end of epoch***************
2022-10-24 15:20:26,061 - trainer - INFO - {
  "train_loss": 0.4125511646270752
}
2022-10-24 15:20:26,064 - trainer - INFO - start training epoch 187
2022-10-24 15:20:26,064 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,064 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,069 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,078 - trainer - INFO - 
*****************[epoch: 187, global step: 188] eval training set at end of epoch***************
2022-10-24 15:20:26,079 - trainer - INFO - {
  "train_loss": 0.41227954626083374
}
2022-10-24 15:20:26,079 - trainer - INFO - start training epoch 188
2022-10-24 15:20:26,080 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,080 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,080 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,088 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval training set based on eval_every=2***************
2022-10-24 15:20:26,088 - trainer - INFO - {
  "train_loss": 0.4125128537416458
}
2022-10-24 15:20:26,097 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval development set based on eval_every=2***************
2022-10-24 15:20:26,097 - trainer - INFO - {
  "dev_loss": 0.4116472601890564,
  "dev_best_score_for_loss": -0.4116472601890564
}
2022-10-24 15:20:26,098 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,100 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,100 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,101 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_182
2022-10-24 15:20:26,102 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,106 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,107 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,108 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,109 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,114 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_188
2022-10-24 15:20:26,119 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_188
2022-10-24 15:20:26,120 - trainer - INFO - 
*****************[epoch: 188, global step: 189] eval training set at end of epoch***************
2022-10-24 15:20:26,120 - trainer - INFO - {
  "train_loss": 0.4127461612224579
}
2022-10-24 15:20:26,121 - trainer - INFO - start training epoch 189
2022-10-24 15:20:26,121 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,122 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,123 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,135 - trainer - INFO - 
*****************[epoch: 189, global step: 190] eval training set at end of epoch***************
2022-10-24 15:20:26,135 - trainer - INFO - {
  "train_loss": 0.4116472899913788
}
2022-10-24 15:20:26,136 - trainer - INFO - start training epoch 190
2022-10-24 15:20:26,136 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,136 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,137 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,145 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval training set based on eval_every=2***************
2022-10-24 15:20:26,146 - trainer - INFO - {
  "train_loss": 0.4120897203683853
}
2022-10-24 15:20:26,152 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval development set based on eval_every=2***************
2022-10-24 15:20:26,153 - trainer - INFO - {
  "dev_loss": 0.4112691581249237,
  "dev_best_score_for_loss": -0.4112691581249237
}
2022-10-24 15:20:26,154 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,155 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,156 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,157 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_184
2022-10-24 15:20:26,158 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,165 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,166 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,166 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,167 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,167 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_190
2022-10-24 15:20:26,172 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_190
2022-10-24 15:20:26,173 - trainer - INFO - 
*****************[epoch: 190, global step: 191] eval training set at end of epoch***************
2022-10-24 15:20:26,173 - trainer - INFO - {
  "train_loss": 0.41253215074539185
}
2022-10-24 15:20:26,174 - trainer - INFO - start training epoch 191
2022-10-24 15:20:26,174 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,174 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,175 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,183 - trainer - INFO - 
*****************[epoch: 191, global step: 192] eval training set at end of epoch***************
2022-10-24 15:20:26,183 - trainer - INFO - {
  "train_loss": 0.4112691581249237
}
2022-10-24 15:20:26,186 - trainer - INFO - start training epoch 192
2022-10-24 15:20:26,187 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,187 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,188 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,197 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval training set based on eval_every=2***************
2022-10-24 15:20:26,197 - trainer - INFO - {
  "train_loss": 0.41170749068260193
}
2022-10-24 15:20:26,208 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval development set based on eval_every=2***************
2022-10-24 15:20:26,208 - trainer - INFO - {
  "dev_loss": 0.411191463470459,
  "dev_best_score_for_loss": -0.411191463470459
}
2022-10-24 15:20:26,209 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,211 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,211 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,212 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_186
2022-10-24 15:20:26,214 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,218 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,218 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,218 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,220 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,220 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_192
2022-10-24 15:20:26,226 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_192
2022-10-24 15:20:26,227 - trainer - INFO - 
*****************[epoch: 192, global step: 193] eval training set at end of epoch***************
2022-10-24 15:20:26,228 - trainer - INFO - {
  "train_loss": 0.41214582324028015
}
2022-10-24 15:20:26,228 - trainer - INFO - start training epoch 193
2022-10-24 15:20:26,228 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,229 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,229 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,239 - trainer - INFO - 
*****************[epoch: 193, global step: 194] eval training set at end of epoch***************
2022-10-24 15:20:26,240 - trainer - INFO - {
  "train_loss": 0.411191463470459
}
2022-10-24 15:20:26,240 - trainer - INFO - start training epoch 194
2022-10-24 15:20:26,241 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,241 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,241 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,253 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval training set based on eval_every=2***************
2022-10-24 15:20:26,254 - trainer - INFO - {
  "train_loss": 0.41140083968639374
}
2022-10-24 15:20:26,264 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval development set based on eval_every=2***************
2022-10-24 15:20:26,265 - trainer - INFO - {
  "dev_loss": 0.41103243827819824,
  "dev_best_score_for_loss": -0.41103243827819824
}
2022-10-24 15:20:26,266 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,267 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,267 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,268 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_188
2022-10-24 15:20:26,269 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,272 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,273 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,273 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,274 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,274 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_194
2022-10-24 15:20:26,280 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_194
2022-10-24 15:20:26,281 - trainer - INFO - 
*****************[epoch: 194, global step: 195] eval training set at end of epoch***************
2022-10-24 15:20:26,281 - trainer - INFO - {
  "train_loss": 0.4116102159023285
}
2022-10-24 15:20:26,281 - trainer - INFO - start training epoch 195
2022-10-24 15:20:26,282 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,282 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,282 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,289 - trainer - INFO - 
*****************[epoch: 195, global step: 196] eval training set at end of epoch***************
2022-10-24 15:20:26,290 - trainer - INFO - {
  "train_loss": 0.41103243827819824
}
2022-10-24 15:20:26,290 - trainer - INFO - start training epoch 196
2022-10-24 15:20:26,290 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,290 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,291 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,301 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval training set based on eval_every=2***************
2022-10-24 15:20:26,302 - trainer - INFO - {
  "train_loss": 0.4109717607498169
}
2022-10-24 15:20:26,309 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval development set based on eval_every=2***************
2022-10-24 15:20:26,310 - trainer - INFO - {
  "dev_loss": 0.4107377529144287,
  "dev_best_score_for_loss": -0.4107377529144287
}
2022-10-24 15:20:26,311 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,313 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,313 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,313 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_190
2022-10-24 15:20:26,315 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,319 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,319 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,319 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,320 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,320 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_196
2022-10-24 15:20:26,325 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_196
2022-10-24 15:20:26,326 - trainer - INFO - 
*****************[epoch: 196, global step: 197] eval training set at end of epoch***************
2022-10-24 15:20:26,327 - trainer - INFO - {
  "train_loss": 0.41091108322143555
}
2022-10-24 15:20:26,328 - trainer - INFO - start training epoch 197
2022-10-24 15:20:26,328 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,328 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,329 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,338 - trainer - INFO - 
*****************[epoch: 197, global step: 198] eval training set at end of epoch***************
2022-10-24 15:20:26,338 - trainer - INFO - {
  "train_loss": 0.4107377231121063
}
2022-10-24 15:20:26,340 - trainer - INFO - start training epoch 198
2022-10-24 15:20:26,340 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,341 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,342 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,356 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval training set based on eval_every=2***************
2022-10-24 15:20:26,357 - trainer - INFO - {
  "train_loss": 0.4105027765035629
}
2022-10-24 15:20:26,365 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval development set based on eval_every=2***************
2022-10-24 15:20:26,365 - trainer - INFO - {
  "dev_loss": 0.4103240668773651,
  "dev_best_score_for_loss": -0.4103240668773651
}
2022-10-24 15:20:26,366 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,367 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,367 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,368 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_192
2022-10-24 15:20:26,369 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,373 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,373 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,373 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,374 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,374 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_198
2022-10-24 15:20:26,378 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_198
2022-10-24 15:20:26,379 - trainer - INFO - 
*****************[epoch: 198, global step: 199] eval training set at end of epoch***************
2022-10-24 15:20:26,380 - trainer - INFO - {
  "train_loss": 0.41026782989501953
}
2022-10-24 15:20:26,380 - trainer - INFO - start training epoch 199
2022-10-24 15:20:26,380 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,380 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,381 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,392 - trainer - INFO - 
*****************[epoch: 199, global step: 200] eval training set at end of epoch***************
2022-10-24 15:20:26,392 - trainer - INFO - {
  "train_loss": 0.4103240966796875
}
2022-10-24 15:20:26,392 - trainer - INFO - start training epoch 200
2022-10-24 15:20:26,392 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,393 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,393 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,403 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval training set based on eval_every=2***************
2022-10-24 15:20:26,403 - trainer - INFO - {
  "train_loss": 0.4100196212530136
}
2022-10-24 15:20:26,409 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval development set based on eval_every=2***************
2022-10-24 15:20:26,409 - trainer - INFO - {
  "dev_loss": 0.40984320640563965,
  "dev_best_score_for_loss": -0.40984320640563965
}
2022-10-24 15:20:26,410 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,411 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,412 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,412 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_194
2022-10-24 15:20:26,413 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,417 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,417 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,417 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,418 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,418 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_200
2022-10-24 15:20:26,423 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_200
2022-10-24 15:20:26,423 - trainer - INFO - 
*****************[epoch: 200, global step: 201] eval training set at end of epoch***************
2022-10-24 15:20:26,424 - trainer - INFO - {
  "train_loss": 0.4097151458263397
}
2022-10-24 15:20:26,424 - trainer - INFO - start training epoch 201
2022-10-24 15:20:26,424 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,424 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,425 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,435 - trainer - INFO - 
*****************[epoch: 201, global step: 202] eval training set at end of epoch***************
2022-10-24 15:20:26,439 - trainer - INFO - {
  "train_loss": 0.40984320640563965
}
2022-10-24 15:20:26,440 - trainer - INFO - start training epoch 202
2022-10-24 15:20:26,440 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,441 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,441 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,450 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval training set based on eval_every=2***************
2022-10-24 15:20:26,451 - trainer - INFO - {
  "train_loss": 0.40954825282096863
}
2022-10-24 15:20:26,458 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval development set based on eval_every=2***************
2022-10-24 15:20:26,458 - trainer - INFO - {
  "dev_loss": 0.40934285521507263,
  "dev_best_score_for_loss": -0.40934285521507263
}
2022-10-24 15:20:26,459 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,460 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,460 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,460 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_196
2022-10-24 15:20:26,462 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,466 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,466 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,466 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,467 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,467 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_202
2022-10-24 15:20:26,471 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_202
2022-10-24 15:20:26,472 - trainer - INFO - 
*****************[epoch: 202, global step: 203] eval training set at end of epoch***************
2022-10-24 15:20:26,472 - trainer - INFO - {
  "train_loss": 0.4092532992362976
}
2022-10-24 15:20:26,473 - trainer - INFO - start training epoch 203
2022-10-24 15:20:26,473 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,473 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,473 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,486 - trainer - INFO - 
*****************[epoch: 203, global step: 204] eval training set at end of epoch***************
2022-10-24 15:20:26,487 - trainer - INFO - {
  "train_loss": 0.40934282541275024
}
2022-10-24 15:20:26,487 - trainer - INFO - start training epoch 204
2022-10-24 15:20:26,487 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,488 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,488 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,497 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval training set based on eval_every=2***************
2022-10-24 15:20:26,498 - trainer - INFO - {
  "train_loss": 0.4091174155473709
}
2022-10-24 15:20:26,504 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval development set based on eval_every=2***************
2022-10-24 15:20:26,505 - trainer - INFO - {
  "dev_loss": 0.40891966223716736,
  "dev_best_score_for_loss": -0.40891966223716736
}
2022-10-24 15:20:26,505 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,507 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,507 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,507 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_198
2022-10-24 15:20:26,509 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,512 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,512 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,513 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,513 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,514 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_204
2022-10-24 15:20:26,518 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_204
2022-10-24 15:20:26,518 - trainer - INFO - 
*****************[epoch: 204, global step: 205] eval training set at end of epoch***************
2022-10-24 15:20:26,519 - trainer - INFO - {
  "train_loss": 0.4088920056819916
}
2022-10-24 15:20:26,519 - trainer - INFO - start training epoch 205
2022-10-24 15:20:26,519 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,520 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,520 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,531 - trainer - INFO - 
*****************[epoch: 205, global step: 206] eval training set at end of epoch***************
2022-10-24 15:20:26,532 - trainer - INFO - {
  "train_loss": 0.40891966223716736
}
2022-10-24 15:20:26,532 - trainer - INFO - start training epoch 206
2022-10-24 15:20:26,532 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,533 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,533 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,542 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval training set based on eval_every=2***************
2022-10-24 15:20:26,542 - trainer - INFO - {
  "train_loss": 0.4087773412466049
}
2022-10-24 15:20:26,549 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval development set based on eval_every=2***************
2022-10-24 15:20:26,550 - trainer - INFO - {
  "dev_loss": 0.40857505798339844,
  "dev_best_score_for_loss": -0.40857505798339844
}
2022-10-24 15:20:26,550 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,552 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,552 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,552 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_200
2022-10-24 15:20:26,553 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,557 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,557 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,558 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,558 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,559 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_206
2022-10-24 15:20:26,564 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_206
2022-10-24 15:20:26,564 - trainer - INFO - 
*****************[epoch: 206, global step: 207] eval training set at end of epoch***************
2022-10-24 15:20:26,565 - trainer - INFO - {
  "train_loss": 0.4086350202560425
}
2022-10-24 15:20:26,565 - trainer - INFO - start training epoch 207
2022-10-24 15:20:26,566 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,566 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,566 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,580 - trainer - INFO - 
*****************[epoch: 207, global step: 208] eval training set at end of epoch***************
2022-10-24 15:20:26,580 - trainer - INFO - {
  "train_loss": 0.40857505798339844
}
2022-10-24 15:20:26,580 - trainer - INFO - start training epoch 208
2022-10-24 15:20:26,581 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,581 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,581 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,589 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval training set based on eval_every=2***************
2022-10-24 15:20:26,589 - trainer - INFO - {
  "train_loss": 0.4085344970226288
}
2022-10-24 15:20:26,598 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval development set based on eval_every=2***************
2022-10-24 15:20:26,598 - trainer - INFO - {
  "dev_loss": 0.40842723846435547,
  "dev_best_score_for_loss": -0.40842723846435547
}
2022-10-24 15:20:26,599 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,600 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,600 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,600 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_202
2022-10-24 15:20:26,602 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,605 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,605 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,605 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,606 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,606 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_208
2022-10-24 15:20:26,610 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_208
2022-10-24 15:20:26,610 - trainer - INFO - 
*****************[epoch: 208, global step: 209] eval training set at end of epoch***************
2022-10-24 15:20:26,611 - trainer - INFO - {
  "train_loss": 0.40849393606185913
}
2022-10-24 15:20:26,611 - trainer - INFO - start training epoch 209
2022-10-24 15:20:26,611 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,612 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,612 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,625 - trainer - INFO - 
*****************[epoch: 209, global step: 210] eval training set at end of epoch***************
2022-10-24 15:20:26,626 - trainer - INFO - {
  "train_loss": 0.40842723846435547
}
2022-10-24 15:20:26,626 - trainer - INFO - start training epoch 210
2022-10-24 15:20:26,626 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,627 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,627 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,637 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval training set based on eval_every=2***************
2022-10-24 15:20:26,638 - trainer - INFO - {
  "train_loss": 0.40843217074871063
}
2022-10-24 15:20:26,645 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval development set based on eval_every=2***************
2022-10-24 15:20:26,645 - trainer - INFO - {
  "dev_loss": 0.4083293080329895,
  "dev_best_score_for_loss": -0.4083293080329895
}
2022-10-24 15:20:26,646 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,647 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,648 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,648 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_204
2022-10-24 15:20:26,650 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,653 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,653 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,654 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,655 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,655 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_210
2022-10-24 15:20:26,660 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_210
2022-10-24 15:20:26,661 - trainer - INFO - 
*****************[epoch: 210, global step: 211] eval training set at end of epoch***************
2022-10-24 15:20:26,662 - trainer - INFO - {
  "train_loss": 0.4084371030330658
}
2022-10-24 15:20:26,662 - trainer - INFO - start training epoch 211
2022-10-24 15:20:26,662 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,663 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,663 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,673 - trainer - INFO - 
*****************[epoch: 211, global step: 212] eval training set at end of epoch***************
2022-10-24 15:20:26,674 - trainer - INFO - {
  "train_loss": 0.4083293378353119
}
2022-10-24 15:20:26,674 - trainer - INFO - start training epoch 212
2022-10-24 15:20:26,675 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,675 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,675 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,685 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval training set based on eval_every=2***************
2022-10-24 15:20:26,686 - trainer - INFO - {
  "train_loss": 0.408372238278389
}
2022-10-24 15:20:26,693 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval development set based on eval_every=2***************
2022-10-24 15:20:26,694 - trainer - INFO - {
  "dev_loss": 0.40833109617233276,
  "dev_best_score_for_loss": -0.4083293080329895
}
2022-10-24 15:20:26,695 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:26,697 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,698 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,699 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,699 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_206
2022-10-24 15:20:26,701 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_212
2022-10-24 15:20:26,705 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_212
2022-10-24 15:20:26,706 - trainer - INFO - 
*****************[epoch: 212, global step: 213] eval training set at end of epoch***************
2022-10-24 15:20:26,706 - trainer - INFO - {
  "train_loss": 0.40841513872146606
}
2022-10-24 15:20:26,707 - trainer - INFO - start training epoch 213
2022-10-24 15:20:26,707 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,707 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,708 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,716 - trainer - INFO - 
*****************[epoch: 213, global step: 214] eval training set at end of epoch***************
2022-10-24 15:20:26,717 - trainer - INFO - {
  "train_loss": 0.40833112597465515
}
2022-10-24 15:20:26,717 - trainer - INFO - start training epoch 214
2022-10-24 15:20:26,718 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,718 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,718 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,729 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval training set based on eval_every=2***************
2022-10-24 15:20:26,729 - trainer - INFO - {
  "train_loss": 0.4083665907382965
}
2022-10-24 15:20:26,736 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval development set based on eval_every=2***************
2022-10-24 15:20:26,736 - trainer - INFO - {
  "dev_loss": 0.40832075476646423,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:26,737 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:26,738 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,738 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,738 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_208
2022-10-24 15:20:26,740 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:26,743 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:26,744 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:26,745 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,746 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:26,750 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_214
2022-10-24 15:20:26,755 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_214
2022-10-24 15:20:26,755 - trainer - INFO - 
*****************[epoch: 214, global step: 215] eval training set at end of epoch***************
2022-10-24 15:20:26,756 - trainer - INFO - {
  "train_loss": 0.40840205550193787
}
2022-10-24 15:20:26,756 - trainer - INFO - start training epoch 215
2022-10-24 15:20:26,756 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,757 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,757 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,766 - trainer - INFO - 
*****************[epoch: 215, global step: 216] eval training set at end of epoch***************
2022-10-24 15:20:26,766 - trainer - INFO - {
  "train_loss": 0.40832075476646423
}
2022-10-24 15:20:26,767 - trainer - INFO - start training epoch 216
2022-10-24 15:20:26,767 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,767 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,768 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,774 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval training set based on eval_every=2***************
2022-10-24 15:20:26,775 - trainer - INFO - {
  "train_loss": 0.40836039185523987
}
2022-10-24 15:20:26,781 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval development set based on eval_every=2***************
2022-10-24 15:20:26,781 - trainer - INFO - {
  "dev_loss": 0.40832528471946716,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:26,782 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:26,783 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,784 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,784 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,784 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_210
2022-10-24 15:20:26,786 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_216
2022-10-24 15:20:26,791 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_216
2022-10-24 15:20:26,792 - trainer - INFO - 
*****************[epoch: 216, global step: 217] eval training set at end of epoch***************
2022-10-24 15:20:26,796 - trainer - INFO - {
  "train_loss": 0.4084000289440155
}
2022-10-24 15:20:26,797 - trainer - INFO - start training epoch 217
2022-10-24 15:20:26,797 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,797 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,798 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,806 - trainer - INFO - 
*****************[epoch: 217, global step: 218] eval training set at end of epoch***************
2022-10-24 15:20:26,806 - trainer - INFO - {
  "train_loss": 0.40832528471946716
}
2022-10-24 15:20:26,806 - trainer - INFO - start training epoch 218
2022-10-24 15:20:26,806 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,807 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,807 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,814 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval training set based on eval_every=2***************
2022-10-24 15:20:26,815 - trainer - INFO - {
  "train_loss": 0.4083714634180069
}
2022-10-24 15:20:26,822 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval development set based on eval_every=2***************
2022-10-24 15:20:26,822 - trainer - INFO - {
  "dev_loss": 0.4083629548549652,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:26,823 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:26,823 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,824 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,825 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,825 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_212
2022-10-24 15:20:26,826 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_218
2022-10-24 15:20:26,832 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_218
2022-10-24 15:20:26,832 - trainer - INFO - 
*****************[epoch: 218, global step: 219] eval training set at end of epoch***************
2022-10-24 15:20:26,833 - trainer - INFO - {
  "train_loss": 0.40841764211654663
}
2022-10-24 15:20:26,833 - trainer - INFO - start training epoch 219
2022-10-24 15:20:26,833 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,834 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,834 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,843 - trainer - INFO - 
*****************[epoch: 219, global step: 220] eval training set at end of epoch***************
2022-10-24 15:20:26,844 - trainer - INFO - {
  "train_loss": 0.4083629548549652
}
2022-10-24 15:20:26,844 - trainer - INFO - start training epoch 220
2022-10-24 15:20:26,844 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,845 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,845 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,853 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval training set based on eval_every=2***************
2022-10-24 15:20:26,853 - trainer - INFO - {
  "train_loss": 0.4083913415670395
}
2022-10-24 15:20:26,859 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval development set based on eval_every=2***************
2022-10-24 15:20:26,860 - trainer - INFO - {
  "dev_loss": 0.40840643644332886,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:26,861 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:26,861 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,862 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,863 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,863 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_214
2022-10-24 15:20:26,864 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_220
2022-10-24 15:20:26,869 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_220
2022-10-24 15:20:26,873 - trainer - INFO - 
*****************[epoch: 220, global step: 221] eval training set at end of epoch***************
2022-10-24 15:20:26,873 - trainer - INFO - {
  "train_loss": 0.40841972827911377
}
2022-10-24 15:20:26,874 - trainer - INFO - start training epoch 221
2022-10-24 15:20:26,874 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,874 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,874 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,883 - trainer - INFO - 
*****************[epoch: 221, global step: 222] eval training set at end of epoch***************
2022-10-24 15:20:26,883 - trainer - INFO - {
  "train_loss": 0.40840643644332886
}
2022-10-24 15:20:26,884 - trainer - INFO - start training epoch 222
2022-10-24 15:20:26,884 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,884 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,884 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,891 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval training set based on eval_every=2***************
2022-10-24 15:20:26,891 - trainer - INFO - {
  "train_loss": 0.4083985984325409
}
2022-10-24 15:20:26,899 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval development set based on eval_every=2***************
2022-10-24 15:20:26,899 - trainer - INFO - {
  "dev_loss": 0.40837419033050537,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:26,900 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:26,901 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,902 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,902 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,902 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_216
2022-10-24 15:20:26,904 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_222
2022-10-24 15:20:26,908 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_222
2022-10-24 15:20:26,908 - trainer - INFO - 
*****************[epoch: 222, global step: 223] eval training set at end of epoch***************
2022-10-24 15:20:26,909 - trainer - INFO - {
  "train_loss": 0.40839076042175293
}
2022-10-24 15:20:26,909 - trainer - INFO - start training epoch 223
2022-10-24 15:20:26,909 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,910 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,910 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,918 - trainer - INFO - 
*****************[epoch: 223, global step: 224] eval training set at end of epoch***************
2022-10-24 15:20:26,918 - trainer - INFO - {
  "train_loss": 0.40837424993515015
}
2022-10-24 15:20:26,919 - trainer - INFO - start training epoch 224
2022-10-24 15:20:26,919 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,919 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,920 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,929 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval training set based on eval_every=2***************
2022-10-24 15:20:26,930 - trainer - INFO - {
  "train_loss": 0.4083873927593231
}
2022-10-24 15:20:26,936 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval development set based on eval_every=2***************
2022-10-24 15:20:26,936 - trainer - INFO - {
  "dev_loss": 0.4084088206291199,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:26,937 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:20:26,937 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,938 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,939 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,939 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_218
2022-10-24 15:20:26,940 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_224
2022-10-24 15:20:26,945 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_224
2022-10-24 15:20:26,947 - trainer - INFO - 
*****************[epoch: 224, global step: 225] eval training set at end of epoch***************
2022-10-24 15:20:26,950 - trainer - INFO - {
  "train_loss": 0.4084005355834961
}
2022-10-24 15:20:26,950 - trainer - INFO - start training epoch 225
2022-10-24 15:20:26,950 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,950 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,951 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,958 - trainer - INFO - 
*****************[epoch: 225, global step: 226] eval training set at end of epoch***************
2022-10-24 15:20:26,958 - trainer - INFO - {
  "train_loss": 0.4084087610244751
}
2022-10-24 15:20:26,959 - trainer - INFO - start training epoch 226
2022-10-24 15:20:26,960 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,960 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,961 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,969 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval training set based on eval_every=2***************
2022-10-24 15:20:26,969 - trainer - INFO - {
  "train_loss": 0.408395916223526
}
2022-10-24 15:20:26,975 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval development set based on eval_every=2***************
2022-10-24 15:20:26,975 - trainer - INFO - {
  "dev_loss": 0.40836724638938904,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:26,976 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:20:26,976 - trainer - INFO -   patience: 200
2022-10-24 15:20:26,977 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:26,977 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:26,978 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_220
2022-10-24 15:20:26,979 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_226
2022-10-24 15:20:26,983 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_226
2022-10-24 15:20:26,983 - trainer - INFO - 
*****************[epoch: 226, global step: 227] eval training set at end of epoch***************
2022-10-24 15:20:26,984 - trainer - INFO - {
  "train_loss": 0.4083830714225769
}
2022-10-24 15:20:26,984 - trainer - INFO - start training epoch 227
2022-10-24 15:20:26,984 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,985 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,985 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:26,997 - trainer - INFO - 
*****************[epoch: 227, global step: 228] eval training set at end of epoch***************
2022-10-24 15:20:26,998 - trainer - INFO - {
  "train_loss": 0.4083672761917114
}
2022-10-24 15:20:26,998 - trainer - INFO - start training epoch 228
2022-10-24 15:20:26,998 - trainer - INFO - training using device=cuda
2022-10-24 15:20:26,999 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:26,999 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,007 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval training set based on eval_every=2***************
2022-10-24 15:20:27,007 - trainer - INFO - {
  "train_loss": 0.4083595871925354
}
2022-10-24 15:20:27,014 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval development set based on eval_every=2***************
2022-10-24 15:20:27,014 - trainer - INFO - {
  "dev_loss": 0.4083452820777893,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:27,015 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:20:27,015 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,016 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,017 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,017 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_222
2022-10-24 15:20:27,018 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_228
2022-10-24 15:20:27,022 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_228
2022-10-24 15:20:27,023 - trainer - INFO - 
*****************[epoch: 228, global step: 229] eval training set at end of epoch***************
2022-10-24 15:20:27,023 - trainer - INFO - {
  "train_loss": 0.4083518981933594
}
2022-10-24 15:20:27,024 - trainer - INFO - start training epoch 229
2022-10-24 15:20:27,024 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,024 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,025 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,033 - trainer - INFO - 
*****************[epoch: 229, global step: 230] eval training set at end of epoch***************
2022-10-24 15:20:27,033 - trainer - INFO - {
  "train_loss": 0.4083452820777893
}
2022-10-24 15:20:27,033 - trainer - INFO - start training epoch 230
2022-10-24 15:20:27,034 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,034 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,034 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,041 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval training set based on eval_every=2***************
2022-10-24 15:20:27,041 - trainer - INFO - {
  "train_loss": 0.408345103263855
}
2022-10-24 15:20:27,048 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval development set based on eval_every=2***************
2022-10-24 15:20:27,049 - trainer - INFO - {
  "dev_loss": 0.4083281457424164,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:27,050 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:20:27,050 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,052 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,053 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,053 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_224
2022-10-24 15:20:27,055 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_230
2022-10-24 15:20:27,062 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_230
2022-10-24 15:20:27,064 - trainer - INFO - 
*****************[epoch: 230, global step: 231] eval training set at end of epoch***************
2022-10-24 15:20:27,066 - trainer - INFO - {
  "train_loss": 0.40834492444992065
}
2022-10-24 15:20:27,067 - trainer - INFO - start training epoch 231
2022-10-24 15:20:27,070 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,070 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,071 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,085 - trainer - INFO - 
*****************[epoch: 231, global step: 232] eval training set at end of epoch***************
2022-10-24 15:20:27,085 - trainer - INFO - {
  "train_loss": 0.4083281457424164
}
2022-10-24 15:20:27,086 - trainer - INFO - start training epoch 232
2022-10-24 15:20:27,086 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,086 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,087 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,095 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval training set based on eval_every=2***************
2022-10-24 15:20:27,096 - trainer - INFO - {
  "train_loss": 0.4083406776189804
}
2022-10-24 15:20:27,104 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval development set based on eval_every=2***************
2022-10-24 15:20:27,105 - trainer - INFO - {
  "dev_loss": 0.4083333909511566,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:27,105 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:20:27,106 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,107 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,107 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,107 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_226
2022-10-24 15:20:27,109 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_232
2022-10-24 15:20:27,115 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_232
2022-10-24 15:20:27,116 - trainer - INFO - 
*****************[epoch: 232, global step: 233] eval training set at end of epoch***************
2022-10-24 15:20:27,117 - trainer - INFO - {
  "train_loss": 0.40835320949554443
}
2022-10-24 15:20:27,118 - trainer - INFO - start training epoch 233
2022-10-24 15:20:27,118 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,122 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,122 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,134 - trainer - INFO - 
*****************[epoch: 233, global step: 234] eval training set at end of epoch***************
2022-10-24 15:20:27,134 - trainer - INFO - {
  "train_loss": 0.4083333909511566
}
2022-10-24 15:20:27,135 - trainer - INFO - start training epoch 234
2022-10-24 15:20:27,135 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,136 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,136 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,142 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval training set based on eval_every=2***************
2022-10-24 15:20:27,142 - trainer - INFO - {
  "train_loss": 0.40831805765628815
}
2022-10-24 15:20:27,148 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval development set based on eval_every=2***************
2022-10-24 15:20:27,148 - trainer - INFO - {
  "dev_loss": 0.4083244502544403,
  "dev_best_score_for_loss": -0.40832075476646423
}
2022-10-24 15:20:27,149 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:20:27,149 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,150 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,150 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,151 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_228
2022-10-24 15:20:27,152 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_234
2022-10-24 15:20:27,156 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_234
2022-10-24 15:20:27,157 - trainer - INFO - 
*****************[epoch: 234, global step: 235] eval training set at end of epoch***************
2022-10-24 15:20:27,157 - trainer - INFO - {
  "train_loss": 0.4083027243614197
}
2022-10-24 15:20:27,157 - trainer - INFO - start training epoch 235
2022-10-24 15:20:27,157 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,158 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,158 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,169 - trainer - INFO - 
*****************[epoch: 235, global step: 236] eval training set at end of epoch***************
2022-10-24 15:20:27,169 - trainer - INFO - {
  "train_loss": 0.4083244800567627
}
2022-10-24 15:20:27,169 - trainer - INFO - start training epoch 236
2022-10-24 15:20:27,170 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,170 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,170 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,177 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval training set based on eval_every=2***************
2022-10-24 15:20:27,178 - trainer - INFO - {
  "train_loss": 0.4083150625228882
}
2022-10-24 15:20:27,184 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval development set based on eval_every=2***************
2022-10-24 15:20:27,184 - trainer - INFO - {
  "dev_loss": 0.4083119332790375,
  "dev_best_score_for_loss": -0.4083119332790375
}
2022-10-24 15:20:27,185 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:27,186 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,186 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,187 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_230
2022-10-24 15:20:27,188 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:27,192 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:27,193 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:27,193 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,194 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:27,194 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_236
2022-10-24 15:20:27,199 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_236
2022-10-24 15:20:27,200 - trainer - INFO - 
*****************[epoch: 236, global step: 237] eval training set at end of epoch***************
2022-10-24 15:20:27,201 - trainer - INFO - {
  "train_loss": 0.40830564498901367
}
2022-10-24 15:20:27,201 - trainer - INFO - start training epoch 237
2022-10-24 15:20:27,201 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,201 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,202 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,214 - trainer - INFO - 
*****************[epoch: 237, global step: 238] eval training set at end of epoch***************
2022-10-24 15:20:27,215 - trainer - INFO - {
  "train_loss": 0.4083119332790375
}
2022-10-24 15:20:27,215 - trainer - INFO - start training epoch 238
2022-10-24 15:20:27,216 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,216 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,216 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,226 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval training set based on eval_every=2***************
2022-10-24 15:20:27,226 - trainer - INFO - {
  "train_loss": 0.4083174020051956
}
2022-10-24 15:20:27,234 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval development set based on eval_every=2***************
2022-10-24 15:20:27,235 - trainer - INFO - {
  "dev_loss": 0.4083179831504822,
  "dev_best_score_for_loss": -0.4083119332790375
}
2022-10-24 15:20:27,236 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:27,236 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,238 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,239 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,239 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_232
2022-10-24 15:20:27,241 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_238
2022-10-24 15:20:27,246 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_238
2022-10-24 15:20:27,247 - trainer - INFO - 
*****************[epoch: 238, global step: 239] eval training set at end of epoch***************
2022-10-24 15:20:27,247 - trainer - INFO - {
  "train_loss": 0.40832287073135376
}
2022-10-24 15:20:27,248 - trainer - INFO - start training epoch 239
2022-10-24 15:20:27,249 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,249 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,249 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,259 - trainer - INFO - 
*****************[epoch: 239, global step: 240] eval training set at end of epoch***************
2022-10-24 15:20:27,260 - trainer - INFO - {
  "train_loss": 0.4083179831504822
}
2022-10-24 15:20:27,260 - trainer - INFO - start training epoch 240
2022-10-24 15:20:27,260 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,261 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,261 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,271 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval training set based on eval_every=2***************
2022-10-24 15:20:27,272 - trainer - INFO - {
  "train_loss": 0.4083065986633301
}
2022-10-24 15:20:27,278 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval development set based on eval_every=2***************
2022-10-24 15:20:27,279 - trainer - INFO - {
  "dev_loss": 0.40830573439598083,
  "dev_best_score_for_loss": -0.40830573439598083
}
2022-10-24 15:20:27,280 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:27,281 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,281 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,282 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_234
2022-10-24 15:20:27,283 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:27,287 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:27,290 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:27,290 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,291 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:27,291 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_240
2022-10-24 15:20:27,297 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_240
2022-10-24 15:20:27,298 - trainer - INFO - 
*****************[epoch: 240, global step: 241] eval training set at end of epoch***************
2022-10-24 15:20:27,298 - trainer - INFO - {
  "train_loss": 0.408295214176178
}
2022-10-24 15:20:27,298 - trainer - INFO - start training epoch 241
2022-10-24 15:20:27,299 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,299 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,299 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,307 - trainer - INFO - 
*****************[epoch: 241, global step: 242] eval training set at end of epoch***************
2022-10-24 15:20:27,308 - trainer - INFO - {
  "train_loss": 0.40830573439598083
}
2022-10-24 15:20:27,308 - trainer - INFO - start training epoch 242
2022-10-24 15:20:27,308 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,308 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,309 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,317 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval training set based on eval_every=2***************
2022-10-24 15:20:27,321 - trainer - INFO - {
  "train_loss": 0.40830059349536896
}
2022-10-24 15:20:27,327 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval development set based on eval_every=2***************
2022-10-24 15:20:27,328 - trainer - INFO - {
  "dev_loss": 0.4083070755004883,
  "dev_best_score_for_loss": -0.40830573439598083
}
2022-10-24 15:20:27,329 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:27,329 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,330 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,331 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,332 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_236
2022-10-24 15:20:27,333 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_242
2022-10-24 15:20:27,338 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_242
2022-10-24 15:20:27,339 - trainer - INFO - 
*****************[epoch: 242, global step: 243] eval training set at end of epoch***************
2022-10-24 15:20:27,339 - trainer - INFO - {
  "train_loss": 0.4082954525947571
}
2022-10-24 15:20:27,340 - trainer - INFO - start training epoch 243
2022-10-24 15:20:27,340 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,340 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,340 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,353 - trainer - INFO - 
*****************[epoch: 243, global step: 244] eval training set at end of epoch***************
2022-10-24 15:20:27,353 - trainer - INFO - {
  "train_loss": 0.4083070755004883
}
2022-10-24 15:20:27,354 - trainer - INFO - start training epoch 244
2022-10-24 15:20:27,354 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,354 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,355 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,363 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval training set based on eval_every=2***************
2022-10-24 15:20:27,364 - trainer - INFO - {
  "train_loss": 0.40830597281455994
}
2022-10-24 15:20:27,370 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval development set based on eval_every=2***************
2022-10-24 15:20:27,370 - trainer - INFO - {
  "dev_loss": 0.4083237051963806,
  "dev_best_score_for_loss": -0.40830573439598083
}
2022-10-24 15:20:27,371 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:27,371 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,372 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,372 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,373 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_238
2022-10-24 15:20:27,374 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_244
2022-10-24 15:20:27,380 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_244
2022-10-24 15:20:27,381 - trainer - INFO - 
*****************[epoch: 244, global step: 245] eval training set at end of epoch***************
2022-10-24 15:20:27,381 - trainer - INFO - {
  "train_loss": 0.4083048701286316
}
2022-10-24 15:20:27,381 - trainer - INFO - start training epoch 245
2022-10-24 15:20:27,381 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,382 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,382 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,389 - trainer - INFO - 
*****************[epoch: 245, global step: 246] eval training set at end of epoch***************
2022-10-24 15:20:27,390 - trainer - INFO - {
  "train_loss": 0.4083237051963806
}
2022-10-24 15:20:27,390 - trainer - INFO - start training epoch 246
2022-10-24 15:20:27,390 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,390 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,391 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,399 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval training set based on eval_every=2***************
2022-10-24 15:20:27,400 - trainer - INFO - {
  "train_loss": 0.40831005573272705
}
2022-10-24 15:20:27,406 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval development set based on eval_every=2***************
2022-10-24 15:20:27,407 - trainer - INFO - {
  "dev_loss": 0.4083022475242615,
  "dev_best_score_for_loss": -0.4083022475242615
}
2022-10-24 15:20:27,407 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:27,408 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,410 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,410 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_240
2022-10-24 15:20:27,412 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:27,418 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:27,418 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:27,419 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,420 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:27,420 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_246
2022-10-24 15:20:27,424 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_246
2022-10-24 15:20:27,426 - trainer - INFO - 
*****************[epoch: 246, global step: 247] eval training set at end of epoch***************
2022-10-24 15:20:27,427 - trainer - INFO - {
  "train_loss": 0.4082964062690735
}
2022-10-24 15:20:27,428 - trainer - INFO - start training epoch 247
2022-10-24 15:20:27,428 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,432 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,432 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,442 - trainer - INFO - 
*****************[epoch: 247, global step: 248] eval training set at end of epoch***************
2022-10-24 15:20:27,442 - trainer - INFO - {
  "train_loss": 0.40830227732658386
}
2022-10-24 15:20:27,443 - trainer - INFO - start training epoch 248
2022-10-24 15:20:27,443 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,443 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,443 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,450 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval training set based on eval_every=2***************
2022-10-24 15:20:27,451 - trainer - INFO - {
  "train_loss": 0.40830783545970917
}
2022-10-24 15:20:27,458 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval development set based on eval_every=2***************
2022-10-24 15:20:27,459 - trainer - INFO - {
  "dev_loss": 0.40830275416374207,
  "dev_best_score_for_loss": -0.4083022475242615
}
2022-10-24 15:20:27,459 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:27,460 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,461 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,461 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,461 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_242
2022-10-24 15:20:27,463 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_248
2022-10-24 15:20:27,468 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_248
2022-10-24 15:20:27,468 - trainer - INFO - 
*****************[epoch: 248, global step: 249] eval training set at end of epoch***************
2022-10-24 15:20:27,469 - trainer - INFO - {
  "train_loss": 0.4083133935928345
}
2022-10-24 15:20:27,469 - trainer - INFO - start training epoch 249
2022-10-24 15:20:27,470 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,470 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,470 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,483 - trainer - INFO - 
*****************[epoch: 249, global step: 250] eval training set at end of epoch***************
2022-10-24 15:20:27,483 - trainer - INFO - {
  "train_loss": 0.40830275416374207
}
2022-10-24 15:20:27,484 - trainer - INFO - start training epoch 250
2022-10-24 15:20:27,484 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,484 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,485 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,496 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval training set based on eval_every=2***************
2022-10-24 15:20:27,497 - trainer - INFO - {
  "train_loss": 0.4083046615123749
}
2022-10-24 15:20:27,504 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval development set based on eval_every=2***************
2022-10-24 15:20:27,507 - trainer - INFO - {
  "dev_loss": 0.40830039978027344,
  "dev_best_score_for_loss": -0.40830039978027344
}
2022-10-24 15:20:27,508 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:27,510 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,510 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,510 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_244
2022-10-24 15:20:27,513 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:27,517 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:27,518 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:27,518 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,519 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:27,519 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_250
2022-10-24 15:20:27,523 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_250
2022-10-24 15:20:27,524 - trainer - INFO - 
*****************[epoch: 250, global step: 251] eval training set at end of epoch***************
2022-10-24 15:20:27,524 - trainer - INFO - {
  "train_loss": 0.4083065688610077
}
2022-10-24 15:20:27,525 - trainer - INFO - start training epoch 251
2022-10-24 15:20:27,525 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,525 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,525 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,538 - trainer - INFO - 
*****************[epoch: 251, global step: 252] eval training set at end of epoch***************
2022-10-24 15:20:27,538 - trainer - INFO - {
  "train_loss": 0.4083004593849182
}
2022-10-24 15:20:27,539 - trainer - INFO - start training epoch 252
2022-10-24 15:20:27,539 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,539 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,540 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,548 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval training set based on eval_every=2***************
2022-10-24 15:20:27,549 - trainer - INFO - {
  "train_loss": 0.40830397605895996
}
2022-10-24 15:20:27,555 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval development set based on eval_every=2***************
2022-10-24 15:20:27,555 - trainer - INFO - {
  "dev_loss": 0.4083089828491211,
  "dev_best_score_for_loss": -0.40830039978027344
}
2022-10-24 15:20:27,556 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:27,556 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,557 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,557 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,557 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_246
2022-10-24 15:20:27,559 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_252
2022-10-24 15:20:27,563 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_252
2022-10-24 15:20:27,565 - trainer - INFO - 
*****************[epoch: 252, global step: 253] eval training set at end of epoch***************
2022-10-24 15:20:27,566 - trainer - INFO - {
  "train_loss": 0.4083074927330017
}
2022-10-24 15:20:27,567 - trainer - INFO - start training epoch 253
2022-10-24 15:20:27,569 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,569 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,570 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,579 - trainer - INFO - 
*****************[epoch: 253, global step: 254] eval training set at end of epoch***************
2022-10-24 15:20:27,580 - trainer - INFO - {
  "train_loss": 0.4083089530467987
}
2022-10-24 15:20:27,581 - trainer - INFO - start training epoch 254
2022-10-24 15:20:27,581 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,581 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,582 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,587 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval training set based on eval_every=2***************
2022-10-24 15:20:27,588 - trainer - INFO - {
  "train_loss": 0.4083064943552017
}
2022-10-24 15:20:27,594 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval development set based on eval_every=2***************
2022-10-24 15:20:27,594 - trainer - INFO - {
  "dev_loss": 0.4082943797111511,
  "dev_best_score_for_loss": -0.4082943797111511
}
2022-10-24 15:20:27,596 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:27,598 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,598 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,599 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_248
2022-10-24 15:20:27,601 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:27,606 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:27,607 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:27,607 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,608 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:27,608 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_254
2022-10-24 15:20:27,613 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_254
2022-10-24 15:20:27,614 - trainer - INFO - 
*****************[epoch: 254, global step: 255] eval training set at end of epoch***************
2022-10-24 15:20:27,614 - trainer - INFO - {
  "train_loss": 0.40830403566360474
}
2022-10-24 15:20:27,614 - trainer - INFO - start training epoch 255
2022-10-24 15:20:27,614 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,615 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,615 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,621 - trainer - INFO - 
*****************[epoch: 255, global step: 256] eval training set at end of epoch***************
2022-10-24 15:20:27,622 - trainer - INFO - {
  "train_loss": 0.40829432010650635
}
2022-10-24 15:20:27,622 - trainer - INFO - start training epoch 256
2022-10-24 15:20:27,622 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,622 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,623 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,630 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval training set based on eval_every=2***************
2022-10-24 15:20:27,631 - trainer - INFO - {
  "train_loss": 0.40830354392528534
}
2022-10-24 15:20:27,636 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval development set based on eval_every=2***************
2022-10-24 15:20:27,637 - trainer - INFO - {
  "dev_loss": 0.40830010175704956,
  "dev_best_score_for_loss": -0.4082943797111511
}
2022-10-24 15:20:27,637 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:27,638 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,639 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,639 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,639 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_250
2022-10-24 15:20:27,640 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_256
2022-10-24 15:20:27,647 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_256
2022-10-24 15:20:27,648 - trainer - INFO - 
*****************[epoch: 256, global step: 257] eval training set at end of epoch***************
2022-10-24 15:20:27,649 - trainer - INFO - {
  "train_loss": 0.40831276774406433
}
2022-10-24 15:20:27,649 - trainer - INFO - start training epoch 257
2022-10-24 15:20:27,649 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,649 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,650 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,658 - trainer - INFO - 
*****************[epoch: 257, global step: 258] eval training set at end of epoch***************
2022-10-24 15:20:27,659 - trainer - INFO - {
  "train_loss": 0.4083000421524048
}
2022-10-24 15:20:27,659 - trainer - INFO - start training epoch 258
2022-10-24 15:20:27,659 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,659 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,660 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,667 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval training set based on eval_every=2***************
2022-10-24 15:20:27,667 - trainer - INFO - {
  "train_loss": 0.40829406678676605
}
2022-10-24 15:20:27,673 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval development set based on eval_every=2***************
2022-10-24 15:20:27,674 - trainer - INFO - {
  "dev_loss": 0.40829965472221375,
  "dev_best_score_for_loss": -0.4082943797111511
}
2022-10-24 15:20:27,674 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:27,675 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,676 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,676 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,676 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_252
2022-10-24 15:20:27,677 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_258
2022-10-24 15:20:27,682 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_258
2022-10-24 15:20:27,682 - trainer - INFO - 
*****************[epoch: 258, global step: 259] eval training set at end of epoch***************
2022-10-24 15:20:27,683 - trainer - INFO - {
  "train_loss": 0.4082880914211273
}
2022-10-24 15:20:27,683 - trainer - INFO - start training epoch 259
2022-10-24 15:20:27,683 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,684 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,684 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,694 - trainer - INFO - 
*****************[epoch: 259, global step: 260] eval training set at end of epoch***************
2022-10-24 15:20:27,695 - trainer - INFO - {
  "train_loss": 0.40829965472221375
}
2022-10-24 15:20:27,695 - trainer - INFO - start training epoch 260
2022-10-24 15:20:27,695 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,696 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,696 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,702 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval training set based on eval_every=2***************
2022-10-24 15:20:27,704 - trainer - INFO - {
  "train_loss": 0.4083110839128494
}
2022-10-24 15:20:27,711 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval development set based on eval_every=2***************
2022-10-24 15:20:27,711 - trainer - INFO - {
  "dev_loss": 0.40831059217453003,
  "dev_best_score_for_loss": -0.4082943797111511
}
2022-10-24 15:20:27,712 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:27,712 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,713 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,714 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,714 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_254
2022-10-24 15:20:27,715 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_260
2022-10-24 15:20:27,720 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_260
2022-10-24 15:20:27,721 - trainer - INFO - 
*****************[epoch: 260, global step: 261] eval training set at end of epoch***************
2022-10-24 15:20:27,721 - trainer - INFO - {
  "train_loss": 0.4083225131034851
}
2022-10-24 15:20:27,721 - trainer - INFO - start training epoch 261
2022-10-24 15:20:27,721 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,722 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,722 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,731 - trainer - INFO - 
*****************[epoch: 261, global step: 262] eval training set at end of epoch***************
2022-10-24 15:20:27,731 - trainer - INFO - {
  "train_loss": 0.40831059217453003
}
2022-10-24 15:20:27,731 - trainer - INFO - start training epoch 262
2022-10-24 15:20:27,732 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,732 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,732 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,740 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval training set based on eval_every=2***************
2022-10-24 15:20:27,741 - trainer - INFO - {
  "train_loss": 0.40832260251045227
}
2022-10-24 15:20:27,747 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval development set based on eval_every=2***************
2022-10-24 15:20:27,747 - trainer - INFO - {
  "dev_loss": 0.40829023718833923,
  "dev_best_score_for_loss": -0.40829023718833923
}
2022-10-24 15:20:27,748 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:27,750 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,751 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,751 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_256
2022-10-24 15:20:27,753 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:27,757 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:27,757 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:27,757 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,758 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:27,758 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_262
2022-10-24 15:20:27,763 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_262
2022-10-24 15:20:27,764 - trainer - INFO - 
*****************[epoch: 262, global step: 263] eval training set at end of epoch***************
2022-10-24 15:20:27,765 - trainer - INFO - {
  "train_loss": 0.4083346128463745
}
2022-10-24 15:20:27,766 - trainer - INFO - start training epoch 263
2022-10-24 15:20:27,767 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,770 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,770 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,780 - trainer - INFO - 
*****************[epoch: 263, global step: 264] eval training set at end of epoch***************
2022-10-24 15:20:27,780 - trainer - INFO - {
  "train_loss": 0.40829023718833923
}
2022-10-24 15:20:27,781 - trainer - INFO - start training epoch 264
2022-10-24 15:20:27,781 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,782 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,782 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,789 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval training set based on eval_every=2***************
2022-10-24 15:20:27,789 - trainer - INFO - {
  "train_loss": 0.4082920253276825
}
2022-10-24 15:20:27,798 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval development set based on eval_every=2***************
2022-10-24 15:20:27,799 - trainer - INFO - {
  "dev_loss": 0.40829020738601685,
  "dev_best_score_for_loss": -0.40829020738601685
}
2022-10-24 15:20:27,800 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:27,803 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,803 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,804 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_258
2022-10-24 15:20:27,805 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:27,809 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:27,809 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:27,809 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,810 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:27,810 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_264
2022-10-24 15:20:27,816 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_264
2022-10-24 15:20:27,816 - trainer - INFO - 
*****************[epoch: 264, global step: 265] eval training set at end of epoch***************
2022-10-24 15:20:27,817 - trainer - INFO - {
  "train_loss": 0.40829381346702576
}
2022-10-24 15:20:27,818 - trainer - INFO - start training epoch 265
2022-10-24 15:20:27,818 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,818 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,819 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,827 - trainer - INFO - 
*****************[epoch: 265, global step: 266] eval training set at end of epoch***************
2022-10-24 15:20:27,828 - trainer - INFO - {
  "train_loss": 0.40829023718833923
}
2022-10-24 15:20:27,828 - trainer - INFO - start training epoch 266
2022-10-24 15:20:27,829 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,829 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,829 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,837 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval training set based on eval_every=2***************
2022-10-24 15:20:27,837 - trainer - INFO - {
  "train_loss": 0.40828852355480194
}
2022-10-24 15:20:27,845 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval development set based on eval_every=2***************
2022-10-24 15:20:27,846 - trainer - INFO - {
  "dev_loss": 0.408273309469223,
  "dev_best_score_for_loss": -0.408273309469223
}
2022-10-24 15:20:27,847 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:27,851 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,851 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,851 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_260
2022-10-24 15:20:27,853 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:27,856 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:27,857 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:27,857 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,858 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:27,859 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_266
2022-10-24 15:20:27,866 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_266
2022-10-24 15:20:27,867 - trainer - INFO - 
*****************[epoch: 266, global step: 267] eval training set at end of epoch***************
2022-10-24 15:20:27,867 - trainer - INFO - {
  "train_loss": 0.40828680992126465
}
2022-10-24 15:20:27,868 - trainer - INFO - start training epoch 267
2022-10-24 15:20:27,868 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,868 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,868 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,876 - trainer - INFO - 
*****************[epoch: 267, global step: 268] eval training set at end of epoch***************
2022-10-24 15:20:27,876 - trainer - INFO - {
  "train_loss": 0.408273309469223
}
2022-10-24 15:20:27,876 - trainer - INFO - start training epoch 268
2022-10-24 15:20:27,876 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,877 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,877 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,883 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval training set based on eval_every=2***************
2022-10-24 15:20:27,883 - trainer - INFO - {
  "train_loss": 0.4082784205675125
}
2022-10-24 15:20:27,890 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval development set based on eval_every=2***************
2022-10-24 15:20:27,890 - trainer - INFO - {
  "dev_loss": 0.40828168392181396,
  "dev_best_score_for_loss": -0.408273309469223
}
2022-10-24 15:20:27,894 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:27,895 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,896 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,896 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,896 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_262
2022-10-24 15:20:27,898 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_268
2022-10-24 15:20:27,902 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_268
2022-10-24 15:20:27,903 - trainer - INFO - 
*****************[epoch: 268, global step: 269] eval training set at end of epoch***************
2022-10-24 15:20:27,903 - trainer - INFO - {
  "train_loss": 0.408283531665802
}
2022-10-24 15:20:27,904 - trainer - INFO - start training epoch 269
2022-10-24 15:20:27,904 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,905 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,906 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,913 - trainer - INFO - 
*****************[epoch: 269, global step: 270] eval training set at end of epoch***************
2022-10-24 15:20:27,913 - trainer - INFO - {
  "train_loss": 0.40828168392181396
}
2022-10-24 15:20:27,913 - trainer - INFO - start training epoch 270
2022-10-24 15:20:27,913 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,914 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,914 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,922 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval training set based on eval_every=2***************
2022-10-24 15:20:27,923 - trainer - INFO - {
  "train_loss": 0.4082881063222885
}
2022-10-24 15:20:27,933 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval development set based on eval_every=2***************
2022-10-24 15:20:27,934 - trainer - INFO - {
  "dev_loss": 0.40829452872276306,
  "dev_best_score_for_loss": -0.408273309469223
}
2022-10-24 15:20:27,934 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:27,935 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,937 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,937 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,937 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_264
2022-10-24 15:20:27,939 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_270
2022-10-24 15:20:27,943 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_270
2022-10-24 15:20:27,943 - trainer - INFO - 
*****************[epoch: 270, global step: 271] eval training set at end of epoch***************
2022-10-24 15:20:27,944 - trainer - INFO - {
  "train_loss": 0.40829452872276306
}
2022-10-24 15:20:27,944 - trainer - INFO - start training epoch 271
2022-10-24 15:20:27,945 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,945 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,945 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,952 - trainer - INFO - 
*****************[epoch: 271, global step: 272] eval training set at end of epoch***************
2022-10-24 15:20:27,952 - trainer - INFO - {
  "train_loss": 0.40829452872276306
}
2022-10-24 15:20:27,953 - trainer - INFO - start training epoch 272
2022-10-24 15:20:27,953 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,953 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,953 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,959 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval training set based on eval_every=2***************
2022-10-24 15:20:27,960 - trainer - INFO - {
  "train_loss": 0.40830811858177185
}
2022-10-24 15:20:27,966 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval development set based on eval_every=2***************
2022-10-24 15:20:27,967 - trainer - INFO - {
  "dev_loss": 0.40826818346977234,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:27,968 - trainer - INFO -    save the model with best score so far
2022-10-24 15:20:27,970 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:27,972 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:27,973 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_266
2022-10-24 15:20:27,974 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd
2022-10-24 15:20:27,978 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd
2022-10-24 15:20:27,978 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:20:27,978 - trainer - INFO -   patience: 200
2022-10-24 15:20:27,979 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:20:27,980 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_272
2022-10-24 15:20:27,984 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_272
2022-10-24 15:20:27,985 - trainer - INFO - 
*****************[epoch: 272, global step: 273] eval training set at end of epoch***************
2022-10-24 15:20:27,985 - trainer - INFO - {
  "train_loss": 0.40832170844078064
}
2022-10-24 15:20:27,986 - trainer - INFO - start training epoch 273
2022-10-24 15:20:27,986 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,986 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,987 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:27,995 - trainer - INFO - 
*****************[epoch: 273, global step: 274] eval training set at end of epoch***************
2022-10-24 15:20:27,995 - trainer - INFO - {
  "train_loss": 0.40826815366744995
}
2022-10-24 15:20:27,996 - trainer - INFO - start training epoch 274
2022-10-24 15:20:27,996 - trainer - INFO - training using device=cuda
2022-10-24 15:20:27,996 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:27,997 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,003 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval training set based on eval_every=2***************
2022-10-24 15:20:28,004 - trainer - INFO - {
  "train_loss": 0.4082892835140228
}
2022-10-24 15:20:28,009 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval development set based on eval_every=2***************
2022-10-24 15:20:28,009 - trainer - INFO - {
  "dev_loss": 0.40828952193260193,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,010 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:20:28,011 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,012 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,012 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,012 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_268
2022-10-24 15:20:28,014 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_274
2022-10-24 15:20:28,019 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_274
2022-10-24 15:20:28,020 - trainer - INFO - 
*****************[epoch: 274, global step: 275] eval training set at end of epoch***************
2022-10-24 15:20:28,021 - trainer - INFO - {
  "train_loss": 0.4083104133605957
}
2022-10-24 15:20:28,021 - trainer - INFO - start training epoch 275
2022-10-24 15:20:28,021 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,021 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,022 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,031 - trainer - INFO - 
*****************[epoch: 275, global step: 276] eval training set at end of epoch***************
2022-10-24 15:20:28,031 - trainer - INFO - {
  "train_loss": 0.40828952193260193
}
2022-10-24 15:20:28,032 - trainer - INFO - start training epoch 276
2022-10-24 15:20:28,032 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,032 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,033 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,039 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval training set based on eval_every=2***************
2022-10-24 15:20:28,039 - trainer - INFO - {
  "train_loss": 0.408295601606369
}
2022-10-24 15:20:28,046 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval development set based on eval_every=2***************
2022-10-24 15:20:28,046 - trainer - INFO - {
  "dev_loss": 0.40829288959503174,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,047 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:20:28,047 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,048 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,048 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,049 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_270
2022-10-24 15:20:28,050 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_276
2022-10-24 15:20:28,054 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_276
2022-10-24 15:20:28,055 - trainer - INFO - 
*****************[epoch: 276, global step: 277] eval training set at end of epoch***************
2022-10-24 15:20:28,055 - trainer - INFO - {
  "train_loss": 0.4083016812801361
}
2022-10-24 15:20:28,055 - trainer - INFO - start training epoch 277
2022-10-24 15:20:28,056 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,056 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,056 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,065 - trainer - INFO - 
*****************[epoch: 277, global step: 278] eval training set at end of epoch***************
2022-10-24 15:20:28,065 - trainer - INFO - {
  "train_loss": 0.4082929491996765
}
2022-10-24 15:20:28,066 - trainer - INFO - start training epoch 278
2022-10-24 15:20:28,066 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,066 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,067 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,073 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval training set based on eval_every=2***************
2022-10-24 15:20:28,074 - trainer - INFO - {
  "train_loss": 0.40830129384994507
}
2022-10-24 15:20:28,081 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval development set based on eval_every=2***************
2022-10-24 15:20:28,081 - trainer - INFO - {
  "dev_loss": 0.4082869291305542,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,082 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:20:28,083 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,084 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,084 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,085 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_272
2022-10-24 15:20:28,086 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_278
2022-10-24 15:20:28,091 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_278
2022-10-24 15:20:28,092 - trainer - INFO - 
*****************[epoch: 278, global step: 279] eval training set at end of epoch***************
2022-10-24 15:20:28,092 - trainer - INFO - {
  "train_loss": 0.4083096385002136
}
2022-10-24 15:20:28,093 - trainer - INFO - start training epoch 279
2022-10-24 15:20:28,093 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,093 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,094 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,103 - trainer - INFO - 
*****************[epoch: 279, global step: 280] eval training set at end of epoch***************
2022-10-24 15:20:28,103 - trainer - INFO - {
  "train_loss": 0.4082868993282318
}
2022-10-24 15:20:28,104 - trainer - INFO - start training epoch 280
2022-10-24 15:20:28,104 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,104 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,105 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,115 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval training set based on eval_every=2***************
2022-10-24 15:20:28,115 - trainer - INFO - {
  "train_loss": 0.4082923084497452
}
2022-10-24 15:20:28,123 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval development set based on eval_every=2***************
2022-10-24 15:20:28,123 - trainer - INFO - {
  "dev_loss": 0.408292293548584,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,124 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:20:28,124 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,126 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,126 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,126 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_274
2022-10-24 15:20:28,128 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_280
2022-10-24 15:20:28,133 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_280
2022-10-24 15:20:28,133 - trainer - INFO - 
*****************[epoch: 280, global step: 281] eval training set at end of epoch***************
2022-10-24 15:20:28,134 - trainer - INFO - {
  "train_loss": 0.40829771757125854
}
2022-10-24 15:20:28,134 - trainer - INFO - start training epoch 281
2022-10-24 15:20:28,134 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,134 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,135 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,141 - trainer - INFO - 
*****************[epoch: 281, global step: 282] eval training set at end of epoch***************
2022-10-24 15:20:28,142 - trainer - INFO - {
  "train_loss": 0.408292293548584
}
2022-10-24 15:20:28,142 - trainer - INFO - start training epoch 282
2022-10-24 15:20:28,142 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,143 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,143 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,149 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval training set based on eval_every=2***************
2022-10-24 15:20:28,150 - trainer - INFO - {
  "train_loss": 0.4083082675933838
}
2022-10-24 15:20:28,159 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval development set based on eval_every=2***************
2022-10-24 15:20:28,159 - trainer - INFO - {
  "dev_loss": 0.408294141292572,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,160 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:20:28,160 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,162 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,162 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,163 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_276
2022-10-24 15:20:28,165 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_282
2022-10-24 15:20:28,171 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_282
2022-10-24 15:20:28,172 - trainer - INFO - 
*****************[epoch: 282, global step: 283] eval training set at end of epoch***************
2022-10-24 15:20:28,173 - trainer - INFO - {
  "train_loss": 0.4083242416381836
}
2022-10-24 15:20:28,173 - trainer - INFO - start training epoch 283
2022-10-24 15:20:28,174 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,174 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,174 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,183 - trainer - INFO - 
*****************[epoch: 283, global step: 284] eval training set at end of epoch***************
2022-10-24 15:20:28,184 - trainer - INFO - {
  "train_loss": 0.408294141292572
}
2022-10-24 15:20:28,184 - trainer - INFO - start training epoch 284
2022-10-24 15:20:28,185 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,185 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,185 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,192 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval training set based on eval_every=2***************
2022-10-24 15:20:28,192 - trainer - INFO - {
  "train_loss": 0.4082847833633423
}
2022-10-24 15:20:28,200 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval development set based on eval_every=2***************
2022-10-24 15:20:28,201 - trainer - INFO - {
  "dev_loss": 0.40831050276756287,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,203 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:20:28,203 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,207 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,207 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,207 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_278
2022-10-24 15:20:28,209 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_284
2022-10-24 15:20:28,215 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_284
2022-10-24 15:20:28,216 - trainer - INFO - 
*****************[epoch: 284, global step: 285] eval training set at end of epoch***************
2022-10-24 15:20:28,216 - trainer - INFO - {
  "train_loss": 0.40827542543411255
}
2022-10-24 15:20:28,216 - trainer - INFO - start training epoch 285
2022-10-24 15:20:28,217 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,217 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,217 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,225 - trainer - INFO - 
*****************[epoch: 285, global step: 286] eval training set at end of epoch***************
2022-10-24 15:20:28,225 - trainer - INFO - {
  "train_loss": 0.40831050276756287
}
2022-10-24 15:20:28,225 - trainer - INFO - start training epoch 286
2022-10-24 15:20:28,226 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,226 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,226 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,234 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval training set based on eval_every=2***************
2022-10-24 15:20:28,234 - trainer - INFO - {
  "train_loss": 0.4083131402730942
}
2022-10-24 15:20:28,240 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval development set based on eval_every=2***************
2022-10-24 15:20:28,240 - trainer - INFO - {
  "dev_loss": 0.4082811772823334,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,241 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:20:28,241 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,242 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,242 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,243 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_280
2022-10-24 15:20:28,244 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_286
2022-10-24 15:20:28,249 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_286
2022-10-24 15:20:28,252 - trainer - INFO - 
*****************[epoch: 286, global step: 287] eval training set at end of epoch***************
2022-10-24 15:20:28,253 - trainer - INFO - {
  "train_loss": 0.4083157777786255
}
2022-10-24 15:20:28,253 - trainer - INFO - start training epoch 287
2022-10-24 15:20:28,253 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,253 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,254 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,263 - trainer - INFO - 
*****************[epoch: 287, global step: 288] eval training set at end of epoch***************
2022-10-24 15:20:28,264 - trainer - INFO - {
  "train_loss": 0.4082811772823334
}
2022-10-24 15:20:28,264 - trainer - INFO - start training epoch 288
2022-10-24 15:20:28,265 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,265 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,265 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,275 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval training set based on eval_every=2***************
2022-10-24 15:20:28,275 - trainer - INFO - {
  "train_loss": 0.408307209610939
}
2022-10-24 15:20:28,281 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval development set based on eval_every=2***************
2022-10-24 15:20:28,282 - trainer - INFO - {
  "dev_loss": 0.40830692648887634,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,283 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:20:28,283 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,284 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,285 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,285 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_282
2022-10-24 15:20:28,286 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_288
2022-10-24 15:20:28,291 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_288
2022-10-24 15:20:28,293 - trainer - INFO - 
*****************[epoch: 288, global step: 289] eval training set at end of epoch***************
2022-10-24 15:20:28,294 - trainer - INFO - {
  "train_loss": 0.4083332419395447
}
2022-10-24 15:20:28,299 - trainer - INFO - start training epoch 289
2022-10-24 15:20:28,299 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,300 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,300 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,310 - trainer - INFO - 
*****************[epoch: 289, global step: 290] eval training set at end of epoch***************
2022-10-24 15:20:28,310 - trainer - INFO - {
  "train_loss": 0.40830692648887634
}
2022-10-24 15:20:28,311 - trainer - INFO - start training epoch 290
2022-10-24 15:20:28,311 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,311 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,312 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,321 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval training set based on eval_every=2***************
2022-10-24 15:20:28,321 - trainer - INFO - {
  "train_loss": 0.4083136022090912
}
2022-10-24 15:20:28,328 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval development set based on eval_every=2***************
2022-10-24 15:20:28,328 - trainer - INFO - {
  "dev_loss": 0.40832433104515076,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,329 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:20:28,329 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,331 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,331 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,331 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_284
2022-10-24 15:20:28,333 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_290
2022-10-24 15:20:28,339 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_290
2022-10-24 15:20:28,340 - trainer - INFO - 
*****************[epoch: 290, global step: 291] eval training set at end of epoch***************
2022-10-24 15:20:28,342 - trainer - INFO - {
  "train_loss": 0.40832027792930603
}
2022-10-24 15:20:28,342 - trainer - INFO - start training epoch 291
2022-10-24 15:20:28,346 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,346 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,347 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,356 - trainer - INFO - 
*****************[epoch: 291, global step: 292] eval training set at end of epoch***************
2022-10-24 15:20:28,357 - trainer - INFO - {
  "train_loss": 0.40832433104515076
}
2022-10-24 15:20:28,357 - trainer - INFO - start training epoch 292
2022-10-24 15:20:28,357 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,358 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,358 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,367 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval training set based on eval_every=2***************
2022-10-24 15:20:28,367 - trainer - INFO - {
  "train_loss": 0.40832431614398956
}
2022-10-24 15:20:28,375 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval development set based on eval_every=2***************
2022-10-24 15:20:28,376 - trainer - INFO - {
  "dev_loss": 0.40829846262931824,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,377 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:20:28,377 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,378 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,378 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,379 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_286
2022-10-24 15:20:28,380 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_292
2022-10-24 15:20:28,384 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_292
2022-10-24 15:20:28,385 - trainer - INFO - 
*****************[epoch: 292, global step: 293] eval training set at end of epoch***************
2022-10-24 15:20:28,386 - trainer - INFO - {
  "train_loss": 0.40832430124282837
}
2022-10-24 15:20:28,387 - trainer - INFO - start training epoch 293
2022-10-24 15:20:28,388 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,391 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,391 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,400 - trainer - INFO - 
*****************[epoch: 293, global step: 294] eval training set at end of epoch***************
2022-10-24 15:20:28,400 - trainer - INFO - {
  "train_loss": 0.4082984924316406
}
2022-10-24 15:20:28,402 - trainer - INFO - start training epoch 294
2022-10-24 15:20:28,402 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,402 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,403 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,408 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval training set based on eval_every=2***************
2022-10-24 15:20:28,409 - trainer - INFO - {
  "train_loss": 0.4082958996295929
}
2022-10-24 15:20:28,415 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval development set based on eval_every=2***************
2022-10-24 15:20:28,415 - trainer - INFO - {
  "dev_loss": 0.40826892852783203,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,415 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:20:28,417 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,418 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,419 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,419 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_288
2022-10-24 15:20:28,420 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_294
2022-10-24 15:20:28,425 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_294
2022-10-24 15:20:28,426 - trainer - INFO - 
*****************[epoch: 294, global step: 295] eval training set at end of epoch***************
2022-10-24 15:20:28,426 - trainer - INFO - {
  "train_loss": 0.40829330682754517
}
2022-10-24 15:20:28,426 - trainer - INFO - start training epoch 295
2022-10-24 15:20:28,427 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,427 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,427 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,438 - trainer - INFO - 
*****************[epoch: 295, global step: 296] eval training set at end of epoch***************
2022-10-24 15:20:28,438 - trainer - INFO - {
  "train_loss": 0.40826892852783203
}
2022-10-24 15:20:28,439 - trainer - INFO - start training epoch 296
2022-10-24 15:20:28,439 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,439 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,440 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,448 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval training set based on eval_every=2***************
2022-10-24 15:20:28,449 - trainer - INFO - {
  "train_loss": 0.40829163789749146
}
2022-10-24 15:20:28,455 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval development set based on eval_every=2***************
2022-10-24 15:20:28,456 - trainer - INFO - {
  "dev_loss": 0.40829238295555115,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,457 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:20:28,458 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,459 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,459 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,459 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_290
2022-10-24 15:20:28,461 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_296
2022-10-24 15:20:28,466 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_296
2022-10-24 15:20:28,467 - trainer - INFO - 
*****************[epoch: 296, global step: 297] eval training set at end of epoch***************
2022-10-24 15:20:28,468 - trainer - INFO - {
  "train_loss": 0.4083143472671509
}
2022-10-24 15:20:28,468 - trainer - INFO - start training epoch 297
2022-10-24 15:20:28,468 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,468 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,469 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,477 - trainer - INFO - 
*****************[epoch: 297, global step: 298] eval training set at end of epoch***************
2022-10-24 15:20:28,477 - trainer - INFO - {
  "train_loss": 0.40829238295555115
}
2022-10-24 15:20:28,477 - trainer - INFO - start training epoch 298
2022-10-24 15:20:28,478 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,480 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,480 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,493 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval training set based on eval_every=2***************
2022-10-24 15:20:28,493 - trainer - INFO - {
  "train_loss": 0.4082900732755661
}
2022-10-24 15:20:28,503 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval development set based on eval_every=2***************
2022-10-24 15:20:28,504 - trainer - INFO - {
  "dev_loss": 0.40828797221183777,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,505 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:20:28,505 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,506 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,506 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,507 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_292
2022-10-24 15:20:28,508 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_298
2022-10-24 15:20:28,513 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_298
2022-10-24 15:20:28,513 - trainer - INFO - 
*****************[epoch: 298, global step: 299] eval training set at end of epoch***************
2022-10-24 15:20:28,514 - trainer - INFO - {
  "train_loss": 0.40828776359558105
}
2022-10-24 15:20:28,514 - trainer - INFO - start training epoch 299
2022-10-24 15:20:28,514 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,515 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,515 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,522 - trainer - INFO - 
*****************[epoch: 299, global step: 300] eval training set at end of epoch***************
2022-10-24 15:20:28,522 - trainer - INFO - {
  "train_loss": 0.40828797221183777
}
2022-10-24 15:20:28,522 - trainer - INFO - start training epoch 300
2022-10-24 15:20:28,523 - trainer - INFO - training using device=cuda
2022-10-24 15:20:28,523 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:20:28,523 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_pivot_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:20:28,532 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval training set based on eval_every=2***************
2022-10-24 15:20:28,533 - trainer - INFO - {
  "train_loss": 0.408301442861557
}
2022-10-24 15:20:28,539 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval development set based on eval_every=2***************
2022-10-24 15:20:28,539 - trainer - INFO - {
  "dev_loss": 0.40829846262931824,
  "dev_best_score_for_loss": -0.40826818346977234
}
2022-10-24 15:20:28,541 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:20:28,542 - trainer - INFO -   patience: 200
2022-10-24 15:20:28,543 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:20:28,544 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:20:28,544 - trainer - INFO -   Remove checkpoint tmp/mlp_pivot_kdd\ck_294
2022-10-24 15:20:28,545 - trainer - INFO -   Save checkpoint to tmp/mlp_pivot_kdd\ck_300
2022-10-24 15:20:28,550 - trainer - INFO - save model to path: tmp/mlp_pivot_kdd\ck_300
2022-10-24 15:20:28,551 - trainer - INFO - 
*****************[epoch: 300, global step: 301] eval training set at end of epoch***************
2022-10-24 15:20:28,551 - trainer - INFO - {
  "train_loss": 0.40831491351127625
}
