2022-10-24 15:18:41,651 - trainer - INFO - MLP(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=32, bias=True)
  )
  (activation_layers): ModuleList(
    (0): ReLU(inplace=True)
    (1): ReLU(inplace=True)
    (2): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (linear_output): Linear(in_features=32, out_features=1, bias=True)
)
2022-10-24 15:18:41,652 - trainer - INFO -   Total params: 10625
2022-10-24 15:18:41,653 - trainer - INFO -   Trainable params: 10625
2022-10-24 15:18:41,655 - trainer - INFO -   Non-trainable params: 0
2022-10-24 15:18:41,656 - trainer - INFO -   There are 12  training examples
2022-10-24 15:18:41,658 - trainer - INFO -   There are 12 examples for development
2022-10-24 15:18:41,766 - trainer - INFO - start training epoch 1
2022-10-24 15:18:41,767 - trainer - INFO - training using device=cuda
2022-10-24 15:18:41,767 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:41,768 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,021 - trainer - INFO - 
*****************[epoch: 1, global step: 2] eval training set at end of epoch***************
2022-10-24 15:18:43,021 - trainer - INFO - {
  "train_loss": 561654.4375
}
2022-10-24 15:18:43,022 - trainer - INFO - start training epoch 2
2022-10-24 15:18:43,022 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,023 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,023 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,031 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval training set based on eval_every=2***************
2022-10-24 15:18:43,033 - trainer - INFO - {
  "train_loss": 513691.078125
}
2022-10-24 15:18:43,041 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval development set based on eval_every=2***************
2022-10-24 15:18:43,042 - trainer - INFO - {
  "dev_loss": 175636.46875,
  "dev_best_score_for_loss": -175636.46875
}
2022-10-24 15:18:43,043 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:43,045 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:18:43,045 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:43,050 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:43,050 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:43,050 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,051 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:18:43,052 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_2
2022-10-24 15:18:43,055 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_2
2022-10-24 15:18:43,056 - trainer - INFO - 
*****************[epoch: 2, global step: 3] eval training set at end of epoch***************
2022-10-24 15:18:43,057 - trainer - INFO - {
  "train_loss": 465727.71875
}
2022-10-24 15:18:43,057 - trainer - INFO - start training epoch 3
2022-10-24 15:18:43,058 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,058 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,059 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,074 - trainer - INFO - 
*****************[epoch: 3, global step: 4] eval training set at end of epoch***************
2022-10-24 15:18:43,074 - trainer - INFO - {
  "train_loss": 175636.46875
}
2022-10-24 15:18:43,076 - trainer - INFO - start training epoch 4
2022-10-24 15:18:43,078 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,079 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,080 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,089 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval training set based on eval_every=2***************
2022-10-24 15:18:43,089 - trainer - INFO - {
  "train_loss": 154556.515625
}
2022-10-24 15:18:43,098 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval development set based on eval_every=2***************
2022-10-24 15:18:43,099 - trainer - INFO - {
  "dev_loss": 22224.982421875,
  "dev_best_score_for_loss": -22224.982421875
}
2022-10-24 15:18:43,100 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:43,101 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:18:43,102 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:43,106 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:43,106 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:43,107 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,108 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:18:43,109 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_4
2022-10-24 15:18:43,115 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_4
2022-10-24 15:18:43,117 - trainer - INFO - 
*****************[epoch: 4, global step: 5] eval training set at end of epoch***************
2022-10-24 15:18:43,118 - trainer - INFO - {
  "train_loss": 133476.5625
}
2022-10-24 15:18:43,119 - trainer - INFO - start training epoch 5
2022-10-24 15:18:43,120 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,120 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,121 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,132 - trainer - INFO - 
*****************[epoch: 5, global step: 6] eval training set at end of epoch***************
2022-10-24 15:18:43,133 - trainer - INFO - {
  "train_loss": 22224.982421875
}
2022-10-24 15:18:43,134 - trainer - INFO - start training epoch 6
2022-10-24 15:18:43,134 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,135 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,136 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,145 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval training set based on eval_every=2***************
2022-10-24 15:18:43,145 - trainer - INFO - {
  "train_loss": 25965.6474609375
}
2022-10-24 15:18:43,151 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval development set based on eval_every=2***************
2022-10-24 15:18:43,152 - trainer - INFO - {
  "dev_loss": 68844.2734375,
  "dev_best_score_for_loss": -22224.982421875
}
2022-10-24 15:18:43,153 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:43,153 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,154 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:43,155 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_6
2022-10-24 15:18:43,162 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_6
2022-10-24 15:18:43,163 - trainer - INFO - 
*****************[epoch: 6, global step: 7] eval training set at end of epoch***************
2022-10-24 15:18:43,164 - trainer - INFO - {
  "train_loss": 29706.3125
}
2022-10-24 15:18:43,165 - trainer - INFO - start training epoch 7
2022-10-24 15:18:43,165 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,166 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,166 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,175 - trainer - INFO - 
*****************[epoch: 7, global step: 8] eval training set at end of epoch***************
2022-10-24 15:18:43,176 - trainer - INFO - {
  "train_loss": 68844.265625
}
2022-10-24 15:18:43,177 - trainer - INFO - start training epoch 8
2022-10-24 15:18:43,178 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,179 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,179 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,188 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval training set based on eval_every=2***************
2022-10-24 15:18:43,190 - trainer - INFO - {
  "train_loss": 59836.943359375
}
2022-10-24 15:18:43,200 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval development set based on eval_every=2***************
2022-10-24 15:18:43,200 - trainer - INFO - {
  "dev_loss": 10204.1376953125,
  "dev_best_score_for_loss": -10204.1376953125
}
2022-10-24 15:18:43,201 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:43,204 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,204 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,205 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_2
2022-10-24 15:18:43,206 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:43,210 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:43,210 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:43,211 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,212 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:43,212 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_8
2022-10-24 15:18:43,216 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_8
2022-10-24 15:18:43,219 - trainer - INFO - 
*****************[epoch: 8, global step: 9] eval training set at end of epoch***************
2022-10-24 15:18:43,220 - trainer - INFO - {
  "train_loss": 50829.62109375
}
2022-10-24 15:18:43,221 - trainer - INFO - start training epoch 9
2022-10-24 15:18:43,221 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,223 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,223 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,235 - trainer - INFO - 
*****************[epoch: 9, global step: 10] eval training set at end of epoch***************
2022-10-24 15:18:43,236 - trainer - INFO - {
  "train_loss": 10204.1376953125
}
2022-10-24 15:18:43,237 - trainer - INFO - start training epoch 10
2022-10-24 15:18:43,238 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,238 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,239 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,248 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval training set based on eval_every=2***************
2022-10-24 15:18:43,250 - trainer - INFO - {
  "train_loss": 16887.31298828125
}
2022-10-24 15:18:43,258 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval development set based on eval_every=2***************
2022-10-24 15:18:43,259 - trainer - INFO - {
  "dev_loss": 47841.125,
  "dev_best_score_for_loss": -10204.1376953125
}
2022-10-24 15:18:43,261 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:43,262 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,264 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,265 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,266 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_4
2022-10-24 15:18:43,268 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_10
2022-10-24 15:18:43,272 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_10
2022-10-24 15:18:43,273 - trainer - INFO - 
*****************[epoch: 10, global step: 11] eval training set at end of epoch***************
2022-10-24 15:18:43,273 - trainer - INFO - {
  "train_loss": 23570.48828125
}
2022-10-24 15:18:43,274 - trainer - INFO - start training epoch 11
2022-10-24 15:18:43,275 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,275 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,277 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,290 - trainer - INFO - 
*****************[epoch: 11, global step: 12] eval training set at end of epoch***************
2022-10-24 15:18:43,290 - trainer - INFO - {
  "train_loss": 47841.125
}
2022-10-24 15:18:43,291 - trainer - INFO - start training epoch 12
2022-10-24 15:18:43,292 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,294 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,295 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,306 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval training set based on eval_every=2***************
2022-10-24 15:18:43,306 - trainer - INFO - {
  "train_loss": 29875.8291015625
}
2022-10-24 15:18:43,314 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval development set based on eval_every=2***************
2022-10-24 15:18:43,315 - trainer - INFO - {
  "dev_loss": 10966.921875,
  "dev_best_score_for_loss": -10204.1376953125
}
2022-10-24 15:18:43,316 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:43,316 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,318 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,321 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,322 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_6
2022-10-24 15:18:43,326 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_12
2022-10-24 15:18:43,334 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_12
2022-10-24 15:18:43,335 - trainer - INFO - 
*****************[epoch: 12, global step: 13] eval training set at end of epoch***************
2022-10-24 15:18:43,336 - trainer - INFO - {
  "train_loss": 11910.533203125
}
2022-10-24 15:18:43,336 - trainer - INFO - start training epoch 13
2022-10-24 15:18:43,337 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,337 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,338 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,352 - trainer - INFO - 
*****************[epoch: 13, global step: 14] eval training set at end of epoch***************
2022-10-24 15:18:43,352 - trainer - INFO - {
  "train_loss": 10966.9208984375
}
2022-10-24 15:18:43,353 - trainer - INFO - start training epoch 14
2022-10-24 15:18:43,354 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,354 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,355 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,367 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval training set based on eval_every=2***************
2022-10-24 15:18:43,368 - trainer - INFO - {
  "train_loss": 20403.53857421875
}
2022-10-24 15:18:43,375 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval development set based on eval_every=2***************
2022-10-24 15:18:43,377 - trainer - INFO - {
  "dev_loss": 29583.87109375,
  "dev_best_score_for_loss": -10204.1376953125
}
2022-10-24 15:18:43,378 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:43,379 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,380 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,380 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,381 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_8
2022-10-24 15:18:43,383 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_14
2022-10-24 15:18:43,390 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_14
2022-10-24 15:18:43,392 - trainer - INFO - 
*****************[epoch: 14, global step: 15] eval training set at end of epoch***************
2022-10-24 15:18:43,392 - trainer - INFO - {
  "train_loss": 29840.15625
}
2022-10-24 15:18:43,394 - trainer - INFO - start training epoch 15
2022-10-24 15:18:43,394 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,396 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,397 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,408 - trainer - INFO - 
*****************[epoch: 15, global step: 16] eval training set at end of epoch***************
2022-10-24 15:18:43,408 - trainer - INFO - {
  "train_loss": 29583.87109375
}
2022-10-24 15:18:43,409 - trainer - INFO - start training epoch 16
2022-10-24 15:18:43,415 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,416 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,416 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,433 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval training set based on eval_every=2***************
2022-10-24 15:18:43,434 - trainer - INFO - {
  "train_loss": 20923.8916015625
}
2022-10-24 15:18:43,444 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval development set based on eval_every=2***************
2022-10-24 15:18:43,445 - trainer - INFO - {
  "dev_loss": 6480.91357421875,
  "dev_best_score_for_loss": -6480.91357421875
}
2022-10-24 15:18:43,447 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:43,449 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,449 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,449 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_10
2022-10-24 15:18:43,452 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:43,456 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:43,457 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:43,457 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,458 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:43,459 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_16
2022-10-24 15:18:43,464 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_16
2022-10-24 15:18:43,465 - trainer - INFO - 
*****************[epoch: 16, global step: 17] eval training set at end of epoch***************
2022-10-24 15:18:43,465 - trainer - INFO - {
  "train_loss": 12263.912109375
}
2022-10-24 15:18:43,467 - trainer - INFO - start training epoch 17
2022-10-24 15:18:43,467 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,468 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,469 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,482 - trainer - INFO - 
*****************[epoch: 17, global step: 18] eval training set at end of epoch***************
2022-10-24 15:18:43,483 - trainer - INFO - {
  "train_loss": 6480.91357421875
}
2022-10-24 15:18:43,485 - trainer - INFO - start training epoch 18
2022-10-24 15:18:43,486 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,486 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,487 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,496 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval training set based on eval_every=2***************
2022-10-24 15:18:43,496 - trainer - INFO - {
  "train_loss": 14184.277099609375
}
2022-10-24 15:18:43,505 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval development set based on eval_every=2***************
2022-10-24 15:18:43,506 - trainer - INFO - {
  "dev_loss": 18460.7578125,
  "dev_best_score_for_loss": -6480.91357421875
}
2022-10-24 15:18:43,506 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:43,507 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,509 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,509 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,510 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_12
2022-10-24 15:18:43,512 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_18
2022-10-24 15:18:43,518 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_18
2022-10-24 15:18:43,520 - trainer - INFO - 
*****************[epoch: 18, global step: 19] eval training set at end of epoch***************
2022-10-24 15:18:43,521 - trainer - INFO - {
  "train_loss": 21887.640625
}
2022-10-24 15:18:43,522 - trainer - INFO - start training epoch 19
2022-10-24 15:18:43,522 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,523 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,523 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,538 - trainer - INFO - 
*****************[epoch: 19, global step: 20] eval training set at end of epoch***************
2022-10-24 15:18:43,538 - trainer - INFO - {
  "train_loss": 18460.7578125
}
2022-10-24 15:18:43,539 - trainer - INFO - start training epoch 20
2022-10-24 15:18:43,540 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,540 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,541 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,555 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval training set based on eval_every=2***************
2022-10-24 15:18:43,555 - trainer - INFO - {
  "train_loss": 11965.65478515625
}
2022-10-24 15:18:43,566 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval development set based on eval_every=2***************
2022-10-24 15:18:43,567 - trainer - INFO - {
  "dev_loss": 10128.556640625,
  "dev_best_score_for_loss": -6480.91357421875
}
2022-10-24 15:18:43,568 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:43,569 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,570 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,570 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,571 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_14
2022-10-24 15:18:43,573 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_20
2022-10-24 15:18:43,578 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_20
2022-10-24 15:18:43,579 - trainer - INFO - 
*****************[epoch: 20, global step: 21] eval training set at end of epoch***************
2022-10-24 15:18:43,580 - trainer - INFO - {
  "train_loss": 5470.5517578125
}
2022-10-24 15:18:43,580 - trainer - INFO - start training epoch 21
2022-10-24 15:18:43,581 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,581 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,582 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,591 - trainer - INFO - 
*****************[epoch: 21, global step: 22] eval training set at end of epoch***************
2022-10-24 15:18:43,591 - trainer - INFO - {
  "train_loss": 10128.556640625
}
2022-10-24 15:18:43,595 - trainer - INFO - start training epoch 22
2022-10-24 15:18:43,595 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,596 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,597 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,607 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval training set based on eval_every=2***************
2022-10-24 15:18:43,610 - trainer - INFO - {
  "train_loss": 13715.189453125
}
2022-10-24 15:18:43,618 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval development set based on eval_every=2***************
2022-10-24 15:18:43,619 - trainer - INFO - {
  "dev_loss": 12892.2998046875,
  "dev_best_score_for_loss": -6480.91357421875
}
2022-10-24 15:18:43,620 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:43,620 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,622 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,622 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,623 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_16
2022-10-24 15:18:43,625 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_22
2022-10-24 15:18:43,630 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_22
2022-10-24 15:18:43,631 - trainer - INFO - 
*****************[epoch: 22, global step: 23] eval training set at end of epoch***************
2022-10-24 15:18:43,631 - trainer - INFO - {
  "train_loss": 17301.822265625
}
2022-10-24 15:18:43,632 - trainer - INFO - start training epoch 23
2022-10-24 15:18:43,633 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,633 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,634 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,645 - trainer - INFO - 
*****************[epoch: 23, global step: 24] eval training set at end of epoch***************
2022-10-24 15:18:43,646 - trainer - INFO - {
  "train_loss": 12892.2998046875
}
2022-10-24 15:18:43,646 - trainer - INFO - start training epoch 24
2022-10-24 15:18:43,647 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,647 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,648 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,658 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval training set based on eval_every=2***************
2022-10-24 15:18:43,658 - trainer - INFO - {
  "train_loss": 9008.7822265625
}
2022-10-24 15:18:43,670 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval development set based on eval_every=2***************
2022-10-24 15:18:43,671 - trainer - INFO - {
  "dev_loss": 9805.5791015625,
  "dev_best_score_for_loss": -6480.91357421875
}
2022-10-24 15:18:43,672 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:43,673 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,674 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,674 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,676 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_18
2022-10-24 15:18:43,677 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_24
2022-10-24 15:18:43,682 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_24
2022-10-24 15:18:43,684 - trainer - INFO - 
*****************[epoch: 24, global step: 25] eval training set at end of epoch***************
2022-10-24 15:18:43,684 - trainer - INFO - {
  "train_loss": 5125.2646484375
}
2022-10-24 15:18:43,687 - trainer - INFO - start training epoch 25
2022-10-24 15:18:43,687 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,687 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,688 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,697 - trainer - INFO - 
*****************[epoch: 25, global step: 26] eval training set at end of epoch***************
2022-10-24 15:18:43,697 - trainer - INFO - {
  "train_loss": 9805.578125
}
2022-10-24 15:18:43,701 - trainer - INFO - start training epoch 26
2022-10-24 15:18:43,701 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,702 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,703 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,710 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval training set based on eval_every=2***************
2022-10-24 15:18:43,711 - trainer - INFO - {
  "train_loss": 10973.462890625
}
2022-10-24 15:18:43,719 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval development set based on eval_every=2***************
2022-10-24 15:18:43,719 - trainer - INFO - {
  "dev_loss": 5758.7353515625,
  "dev_best_score_for_loss": -5758.7353515625
}
2022-10-24 15:18:43,720 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:43,721 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,722 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,722 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_20
2022-10-24 15:18:43,724 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:43,729 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:43,730 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:43,731 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,733 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:43,735 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_26
2022-10-24 15:18:43,741 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_26
2022-10-24 15:18:43,742 - trainer - INFO - 
*****************[epoch: 26, global step: 27] eval training set at end of epoch***************
2022-10-24 15:18:43,745 - trainer - INFO - {
  "train_loss": 12141.34765625
}
2022-10-24 15:18:43,746 - trainer - INFO - start training epoch 27
2022-10-24 15:18:43,748 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,749 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,749 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,758 - trainer - INFO - 
*****************[epoch: 27, global step: 28] eval training set at end of epoch***************
2022-10-24 15:18:43,758 - trainer - INFO - {
  "train_loss": 5758.73583984375
}
2022-10-24 15:18:43,759 - trainer - INFO - start training epoch 28
2022-10-24 15:18:43,760 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,761 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,762 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,774 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval training set based on eval_every=2***************
2022-10-24 15:18:43,774 - trainer - INFO - {
  "train_loss": 5630.178466796875
}
2022-10-24 15:18:43,783 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval development set based on eval_every=2***************
2022-10-24 15:18:43,784 - trainer - INFO - {
  "dev_loss": 8696.3193359375,
  "dev_best_score_for_loss": -5758.7353515625
}
2022-10-24 15:18:43,785 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:43,785 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,787 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,787 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,788 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_22
2022-10-24 15:18:43,790 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_28
2022-10-24 15:18:43,795 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_28
2022-10-24 15:18:43,796 - trainer - INFO - 
*****************[epoch: 28, global step: 29] eval training set at end of epoch***************
2022-10-24 15:18:43,796 - trainer - INFO - {
  "train_loss": 5501.62109375
}
2022-10-24 15:18:43,797 - trainer - INFO - start training epoch 29
2022-10-24 15:18:43,797 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,798 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,799 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,809 - trainer - INFO - 
*****************[epoch: 29, global step: 30] eval training set at end of epoch***************
2022-10-24 15:18:43,809 - trainer - INFO - {
  "train_loss": 8696.318359375
}
2022-10-24 15:18:43,811 - trainer - INFO - start training epoch 30
2022-10-24 15:18:43,811 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,811 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,812 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,825 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval training set based on eval_every=2***************
2022-10-24 15:18:43,826 - trainer - INFO - {
  "train_loss": 8199.39013671875
}
2022-10-24 15:18:43,838 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval development set based on eval_every=2***************
2022-10-24 15:18:43,839 - trainer - INFO - {
  "dev_loss": 4568.56396484375,
  "dev_best_score_for_loss": -4568.56396484375
}
2022-10-24 15:18:43,840 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:43,841 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,842 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,842 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_24
2022-10-24 15:18:43,846 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:43,851 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:43,851 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:43,852 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,853 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:43,854 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_30
2022-10-24 15:18:43,859 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_30
2022-10-24 15:18:43,864 - trainer - INFO - 
*****************[epoch: 30, global step: 31] eval training set at end of epoch***************
2022-10-24 15:18:43,864 - trainer - INFO - {
  "train_loss": 7702.4619140625
}
2022-10-24 15:18:43,865 - trainer - INFO - start training epoch 31
2022-10-24 15:18:43,865 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,866 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,867 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,878 - trainer - INFO - 
*****************[epoch: 31, global step: 32] eval training set at end of epoch***************
2022-10-24 15:18:43,878 - trainer - INFO - {
  "train_loss": 4568.564453125
}
2022-10-24 15:18:43,879 - trainer - INFO - start training epoch 32
2022-10-24 15:18:43,880 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,880 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,881 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,889 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval training set based on eval_every=2***************
2022-10-24 15:18:43,889 - trainer - INFO - {
  "train_loss": 4993.23828125
}
2022-10-24 15:18:43,902 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval development set based on eval_every=2***************
2022-10-24 15:18:43,903 - trainer - INFO - {
  "dev_loss": 7353.0791015625,
  "dev_best_score_for_loss": -4568.56396484375
}
2022-10-24 15:18:43,904 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:43,904 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,905 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,906 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,906 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_26
2022-10-24 15:18:43,908 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_32
2022-10-24 15:18:43,914 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_32
2022-10-24 15:18:43,914 - trainer - INFO - 
*****************[epoch: 32, global step: 33] eval training set at end of epoch***************
2022-10-24 15:18:43,916 - trainer - INFO - {
  "train_loss": 5417.912109375
}
2022-10-24 15:18:43,917 - trainer - INFO - start training epoch 33
2022-10-24 15:18:43,917 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,918 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,919 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,930 - trainer - INFO - 
*****************[epoch: 33, global step: 34] eval training set at end of epoch***************
2022-10-24 15:18:43,931 - trainer - INFO - {
  "train_loss": 7353.0791015625
}
2022-10-24 15:18:43,933 - trainer - INFO - start training epoch 34
2022-10-24 15:18:43,933 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,934 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,934 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,942 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval training set based on eval_every=2***************
2022-10-24 15:18:43,942 - trainer - INFO - {
  "train_loss": 6286.408203125
}
2022-10-24 15:18:43,952 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval development set based on eval_every=2***************
2022-10-24 15:18:43,952 - trainer - INFO - {
  "dev_loss": 4111.9677734375,
  "dev_best_score_for_loss": -4111.9677734375
}
2022-10-24 15:18:43,953 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:43,954 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:43,954 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:43,955 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_28
2022-10-24 15:18:43,956 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:43,960 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:43,961 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:43,963 - trainer - INFO -   patience: 200
2022-10-24 15:18:43,964 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:43,965 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_34
2022-10-24 15:18:43,970 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_34
2022-10-24 15:18:43,971 - trainer - INFO - 
*****************[epoch: 34, global step: 35] eval training set at end of epoch***************
2022-10-24 15:18:43,972 - trainer - INFO - {
  "train_loss": 5219.7373046875
}
2022-10-24 15:18:43,972 - trainer - INFO - start training epoch 35
2022-10-24 15:18:43,973 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,973 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,974 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,986 - trainer - INFO - 
*****************[epoch: 35, global step: 36] eval training set at end of epoch***************
2022-10-24 15:18:43,987 - trainer - INFO - {
  "train_loss": 4111.9677734375
}
2022-10-24 15:18:43,988 - trainer - INFO - start training epoch 36
2022-10-24 15:18:43,988 - trainer - INFO - training using device=cuda
2022-10-24 15:18:43,989 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:43,989 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:43,999 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval training set based on eval_every=2***************
2022-10-24 15:18:43,999 - trainer - INFO - {
  "train_loss": 4924.285400390625
}
2022-10-24 15:18:44,006 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval development set based on eval_every=2***************
2022-10-24 15:18:44,006 - trainer - INFO - {
  "dev_loss": 5717.76171875,
  "dev_best_score_for_loss": -4111.9677734375
}
2022-10-24 15:18:44,010 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:44,012 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,012 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,013 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,016 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_30
2022-10-24 15:18:44,017 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_36
2022-10-24 15:18:44,022 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_36
2022-10-24 15:18:44,023 - trainer - INFO - 
*****************[epoch: 36, global step: 37] eval training set at end of epoch***************
2022-10-24 15:18:44,025 - trainer - INFO - {
  "train_loss": 5736.60302734375
}
2022-10-24 15:18:44,026 - trainer - INFO - start training epoch 37
2022-10-24 15:18:44,028 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,029 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,030 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,041 - trainer - INFO - 
*****************[epoch: 37, global step: 38] eval training set at end of epoch***************
2022-10-24 15:18:44,041 - trainer - INFO - {
  "train_loss": 5717.76171875
}
2022-10-24 15:18:44,042 - trainer - INFO - start training epoch 38
2022-10-24 15:18:44,043 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,043 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,044 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,052 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval training set based on eval_every=2***************
2022-10-24 15:18:44,052 - trainer - INFO - {
  "train_loss": 4850.229248046875
}
2022-10-24 15:18:44,062 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval development set based on eval_every=2***************
2022-10-24 15:18:44,063 - trainer - INFO - {
  "dev_loss": 4182.50732421875,
  "dev_best_score_for_loss": -4111.9677734375
}
2022-10-24 15:18:44,064 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:44,065 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,066 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,067 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,067 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_32
2022-10-24 15:18:44,069 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_38
2022-10-24 15:18:44,075 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_38
2022-10-24 15:18:44,076 - trainer - INFO - 
*****************[epoch: 38, global step: 39] eval training set at end of epoch***************
2022-10-24 15:18:44,077 - trainer - INFO - {
  "train_loss": 3982.69677734375
}
2022-10-24 15:18:44,077 - trainer - INFO - start training epoch 39
2022-10-24 15:18:44,078 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,078 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,079 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,092 - trainer - INFO - 
*****************[epoch: 39, global step: 40] eval training set at end of epoch***************
2022-10-24 15:18:44,092 - trainer - INFO - {
  "train_loss": 4182.50732421875
}
2022-10-24 15:18:44,093 - trainer - INFO - start training epoch 40
2022-10-24 15:18:44,093 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,094 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,095 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,104 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval training set based on eval_every=2***************
2022-10-24 15:18:44,105 - trainer - INFO - {
  "train_loss": 4691.808349609375
}
2022-10-24 15:18:44,113 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval development set based on eval_every=2***************
2022-10-24 15:18:44,114 - trainer - INFO - {
  "dev_loss": 4086.69287109375,
  "dev_best_score_for_loss": -4086.69287109375
}
2022-10-24 15:18:44,115 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,117 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,118 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,118 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_34
2022-10-24 15:18:44,120 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,126 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,126 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,127 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,128 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,130 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_40
2022-10-24 15:18:44,136 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_40
2022-10-24 15:18:44,137 - trainer - INFO - 
*****************[epoch: 40, global step: 41] eval training set at end of epoch***************
2022-10-24 15:18:44,138 - trainer - INFO - {
  "train_loss": 5201.109375
}
2022-10-24 15:18:44,138 - trainer - INFO - start training epoch 41
2022-10-24 15:18:44,139 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,140 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,140 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,151 - trainer - INFO - 
*****************[epoch: 41, global step: 42] eval training set at end of epoch***************
2022-10-24 15:18:44,152 - trainer - INFO - {
  "train_loss": 4086.69287109375
}
2022-10-24 15:18:44,153 - trainer - INFO - start training epoch 42
2022-10-24 15:18:44,153 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,154 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,154 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,164 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval training set based on eval_every=2***************
2022-10-24 15:18:44,165 - trainer - INFO - {
  "train_loss": 3799.9976806640625
}
2022-10-24 15:18:44,173 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval development set based on eval_every=2***************
2022-10-24 15:18:44,173 - trainer - INFO - {
  "dev_loss": 4350.9296875,
  "dev_best_score_for_loss": -4086.69287109375
}
2022-10-24 15:18:44,174 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:44,175 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,176 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,177 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,177 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_36
2022-10-24 15:18:44,179 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_42
2022-10-24 15:18:44,184 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_42
2022-10-24 15:18:44,185 - trainer - INFO - 
*****************[epoch: 42, global step: 43] eval training set at end of epoch***************
2022-10-24 15:18:44,185 - trainer - INFO - {
  "train_loss": 3513.302490234375
}
2022-10-24 15:18:44,186 - trainer - INFO - start training epoch 43
2022-10-24 15:18:44,186 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,187 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,187 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,198 - trainer - INFO - 
*****************[epoch: 43, global step: 44] eval training set at end of epoch***************
2022-10-24 15:18:44,199 - trainer - INFO - {
  "train_loss": 4350.9296875
}
2022-10-24 15:18:44,199 - trainer - INFO - start training epoch 44
2022-10-24 15:18:44,200 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,200 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,201 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,208 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval training set based on eval_every=2***************
2022-10-24 15:18:44,209 - trainer - INFO - {
  "train_loss": 4226.50146484375
}
2022-10-24 15:18:44,220 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval development set based on eval_every=2***************
2022-10-24 15:18:44,221 - trainer - INFO - {
  "dev_loss": 3203.715576171875,
  "dev_best_score_for_loss": -3203.715576171875
}
2022-10-24 15:18:44,222 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,223 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,224 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,224 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_38
2022-10-24 15:18:44,227 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,233 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,233 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,234 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,235 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,235 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_44
2022-10-24 15:18:44,240 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_44
2022-10-24 15:18:44,242 - trainer - INFO - 
*****************[epoch: 44, global step: 45] eval training set at end of epoch***************
2022-10-24 15:18:44,243 - trainer - INFO - {
  "train_loss": 4102.0732421875
}
2022-10-24 15:18:44,244 - trainer - INFO - start training epoch 45
2022-10-24 15:18:44,245 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,245 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,246 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,255 - trainer - INFO - 
*****************[epoch: 45, global step: 46] eval training set at end of epoch***************
2022-10-24 15:18:44,255 - trainer - INFO - {
  "train_loss": 3203.715576171875
}
2022-10-24 15:18:44,257 - trainer - INFO - start training epoch 46
2022-10-24 15:18:44,257 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,258 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,259 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,271 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval training set based on eval_every=2***************
2022-10-24 15:18:44,272 - trainer - INFO - {
  "train_loss": 3386.2950439453125
}
2022-10-24 15:18:44,282 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval development set based on eval_every=2***************
2022-10-24 15:18:44,283 - trainer - INFO - {
  "dev_loss": 3787.19287109375,
  "dev_best_score_for_loss": -3203.715576171875
}
2022-10-24 15:18:44,284 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:44,285 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,286 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,286 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,287 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_40
2022-10-24 15:18:44,289 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_46
2022-10-24 15:18:44,295 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_46
2022-10-24 15:18:44,296 - trainer - INFO - 
*****************[epoch: 46, global step: 47] eval training set at end of epoch***************
2022-10-24 15:18:44,296 - trainer - INFO - {
  "train_loss": 3568.87451171875
}
2022-10-24 15:18:44,299 - trainer - INFO - start training epoch 47
2022-10-24 15:18:44,300 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,300 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,301 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,313 - trainer - INFO - 
*****************[epoch: 47, global step: 48] eval training set at end of epoch***************
2022-10-24 15:18:44,314 - trainer - INFO - {
  "train_loss": 3787.192138671875
}
2022-10-24 15:18:44,314 - trainer - INFO - start training epoch 48
2022-10-24 15:18:44,315 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,315 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,316 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,331 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval training set based on eval_every=2***************
2022-10-24 15:18:44,331 - trainer - INFO - {
  "train_loss": 3403.9354248046875
}
2022-10-24 15:18:44,342 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval development set based on eval_every=2***************
2022-10-24 15:18:44,344 - trainer - INFO - {
  "dev_loss": 3060.20068359375,
  "dev_best_score_for_loss": -3060.20068359375
}
2022-10-24 15:18:44,344 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,347 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,348 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,348 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_42
2022-10-24 15:18:44,350 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,354 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,354 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,355 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,356 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,357 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_48
2022-10-24 15:18:44,362 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_48
2022-10-24 15:18:44,363 - trainer - INFO - 
*****************[epoch: 48, global step: 49] eval training set at end of epoch***************
2022-10-24 15:18:44,363 - trainer - INFO - {
  "train_loss": 3020.6787109375
}
2022-10-24 15:18:44,364 - trainer - INFO - start training epoch 49
2022-10-24 15:18:44,364 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,365 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,366 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,375 - trainer - INFO - 
*****************[epoch: 49, global step: 50] eval training set at end of epoch***************
2022-10-24 15:18:44,376 - trainer - INFO - {
  "train_loss": 3060.20068359375
}
2022-10-24 15:18:44,378 - trainer - INFO - start training epoch 50
2022-10-24 15:18:44,379 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,379 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,380 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,389 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval training set based on eval_every=2***************
2022-10-24 15:18:44,390 - trainer - INFO - {
  "train_loss": 3211.8634033203125
}
2022-10-24 15:18:44,402 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval development set based on eval_every=2***************
2022-10-24 15:18:44,403 - trainer - INFO - {
  "dev_loss": 2859.640380859375,
  "dev_best_score_for_loss": -2859.640380859375
}
2022-10-24 15:18:44,403 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,404 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,405 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,405 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_44
2022-10-24 15:18:44,407 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,410 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,411 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,412 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,418 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,419 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_50
2022-10-24 15:18:44,423 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_50
2022-10-24 15:18:44,424 - trainer - INFO - 
*****************[epoch: 50, global step: 51] eval training set at end of epoch***************
2022-10-24 15:18:44,425 - trainer - INFO - {
  "train_loss": 3363.526123046875
}
2022-10-24 15:18:44,428 - trainer - INFO - start training epoch 51
2022-10-24 15:18:44,429 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,429 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,430 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,441 - trainer - INFO - 
*****************[epoch: 51, global step: 52] eval training set at end of epoch***************
2022-10-24 15:18:44,443 - trainer - INFO - {
  "train_loss": 2859.640380859375
}
2022-10-24 15:18:44,444 - trainer - INFO - start training epoch 52
2022-10-24 15:18:44,445 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,446 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,446 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,454 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval training set based on eval_every=2***************
2022-10-24 15:18:44,454 - trainer - INFO - {
  "train_loss": 2756.72802734375
}
2022-10-24 15:18:44,462 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval development set based on eval_every=2***************
2022-10-24 15:18:44,463 - trainer - INFO - {
  "dev_loss": 2942.2587890625,
  "dev_best_score_for_loss": -2859.640380859375
}
2022-10-24 15:18:44,464 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:44,464 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,466 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,466 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,467 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_46
2022-10-24 15:18:44,468 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_52
2022-10-24 15:18:44,473 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_52
2022-10-24 15:18:44,475 - trainer - INFO - 
*****************[epoch: 52, global step: 53] eval training set at end of epoch***************
2022-10-24 15:18:44,477 - trainer - INFO - {
  "train_loss": 2653.815673828125
}
2022-10-24 15:18:44,478 - trainer - INFO - start training epoch 53
2022-10-24 15:18:44,481 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,481 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,482 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,495 - trainer - INFO - 
*****************[epoch: 53, global step: 54] eval training set at end of epoch***************
2022-10-24 15:18:44,497 - trainer - INFO - {
  "train_loss": 2942.2587890625
}
2022-10-24 15:18:44,499 - trainer - INFO - start training epoch 54
2022-10-24 15:18:44,499 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,500 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,501 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,514 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval training set based on eval_every=2***************
2022-10-24 15:18:44,514 - trainer - INFO - {
  "train_loss": 2771.532470703125
}
2022-10-24 15:18:44,527 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval development set based on eval_every=2***************
2022-10-24 15:18:44,528 - trainer - INFO - {
  "dev_loss": 2361.84619140625,
  "dev_best_score_for_loss": -2361.84619140625
}
2022-10-24 15:18:44,530 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,531 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,532 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,533 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_48
2022-10-24 15:18:44,536 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,543 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,545 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,545 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,546 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,549 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_54
2022-10-24 15:18:44,555 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_54
2022-10-24 15:18:44,556 - trainer - INFO - 
*****************[epoch: 54, global step: 55] eval training set at end of epoch***************
2022-10-24 15:18:44,558 - trainer - INFO - {
  "train_loss": 2600.80615234375
}
2022-10-24 15:18:44,559 - trainer - INFO - start training epoch 55
2022-10-24 15:18:44,560 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,561 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,562 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,574 - trainer - INFO - 
*****************[epoch: 55, global step: 56] eval training set at end of epoch***************
2022-10-24 15:18:44,574 - trainer - INFO - {
  "train_loss": 2361.84619140625
}
2022-10-24 15:18:44,577 - trainer - INFO - start training epoch 56
2022-10-24 15:18:44,577 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,580 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,580 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,594 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval training set based on eval_every=2***************
2022-10-24 15:18:44,594 - trainer - INFO - {
  "train_loss": 2461.5654296875
}
2022-10-24 15:18:44,603 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval development set based on eval_every=2***************
2022-10-24 15:18:44,604 - trainer - INFO - {
  "dev_loss": 2332.603759765625,
  "dev_best_score_for_loss": -2332.603759765625
}
2022-10-24 15:18:44,605 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,607 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,607 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,609 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_50
2022-10-24 15:18:44,614 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,618 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,619 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,619 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,621 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,621 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_56
2022-10-24 15:18:44,627 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_56
2022-10-24 15:18:44,632 - trainer - INFO - 
*****************[epoch: 56, global step: 57] eval training set at end of epoch***************
2022-10-24 15:18:44,634 - trainer - INFO - {
  "train_loss": 2561.28466796875
}
2022-10-24 15:18:44,635 - trainer - INFO - start training epoch 57
2022-10-24 15:18:44,636 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,640 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,640 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,655 - trainer - INFO - 
*****************[epoch: 57, global step: 58] eval training set at end of epoch***************
2022-10-24 15:18:44,655 - trainer - INFO - {
  "train_loss": 2332.603759765625
}
2022-10-24 15:18:44,657 - trainer - INFO - start training epoch 58
2022-10-24 15:18:44,658 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,659 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,660 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,668 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval training set based on eval_every=2***************
2022-10-24 15:18:44,669 - trainer - INFO - {
  "train_loss": 2209.3768310546875
}
2022-10-24 15:18:44,679 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval development set based on eval_every=2***************
2022-10-24 15:18:44,680 - trainer - INFO - {
  "dev_loss": 2219.958251953125,
  "dev_best_score_for_loss": -2219.958251953125
}
2022-10-24 15:18:44,682 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,683 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,685 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,685 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_52
2022-10-24 15:18:44,687 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,692 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,693 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,695 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,698 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,698 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_58
2022-10-24 15:18:44,703 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_58
2022-10-24 15:18:44,704 - trainer - INFO - 
*****************[epoch: 58, global step: 59] eval training set at end of epoch***************
2022-10-24 15:18:44,704 - trainer - INFO - {
  "train_loss": 2086.14990234375
}
2022-10-24 15:18:44,704 - trainer - INFO - start training epoch 59
2022-10-24 15:18:44,705 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,707 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,708 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,718 - trainer - INFO - 
*****************[epoch: 59, global step: 60] eval training set at end of epoch***************
2022-10-24 15:18:44,718 - trainer - INFO - {
  "train_loss": 2219.95849609375
}
2022-10-24 15:18:44,719 - trainer - INFO - start training epoch 60
2022-10-24 15:18:44,719 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,719 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,720 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,731 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval training set based on eval_every=2***************
2022-10-24 15:18:44,732 - trainer - INFO - {
  "train_loss": 2124.9940795898438
}
2022-10-24 15:18:44,742 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval development set based on eval_every=2***************
2022-10-24 15:18:44,744 - trainer - INFO - {
  "dev_loss": 1834.3011474609375,
  "dev_best_score_for_loss": -1834.3011474609375
}
2022-10-24 15:18:44,746 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,748 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,749 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,749 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_54
2022-10-24 15:18:44,751 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,754 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,755 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,755 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,756 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,756 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_60
2022-10-24 15:18:44,761 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_60
2022-10-24 15:18:44,762 - trainer - INFO - 
*****************[epoch: 60, global step: 61] eval training set at end of epoch***************
2022-10-24 15:18:44,763 - trainer - INFO - {
  "train_loss": 2030.0296630859375
}
2022-10-24 15:18:44,763 - trainer - INFO - start training epoch 61
2022-10-24 15:18:44,764 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,764 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,765 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,775 - trainer - INFO - 
*****************[epoch: 61, global step: 62] eval training set at end of epoch***************
2022-10-24 15:18:44,775 - trainer - INFO - {
  "train_loss": 1834.3011474609375
}
2022-10-24 15:18:44,778 - trainer - INFO - start training epoch 62
2022-10-24 15:18:44,779 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,780 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,780 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,790 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval training set based on eval_every=2***************
2022-10-24 15:18:44,790 - trainer - INFO - {
  "train_loss": 1874.4010620117188
}
2022-10-24 15:18:44,801 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval development set based on eval_every=2***************
2022-10-24 15:18:44,802 - trainer - INFO - {
  "dev_loss": 1738.5150146484375,
  "dev_best_score_for_loss": -1738.5150146484375
}
2022-10-24 15:18:44,803 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,804 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,806 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,807 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_56
2022-10-24 15:18:44,809 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,812 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,813 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,813 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,814 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,819 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_62
2022-10-24 15:18:44,824 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_62
2022-10-24 15:18:44,825 - trainer - INFO - 
*****************[epoch: 62, global step: 63] eval training set at end of epoch***************
2022-10-24 15:18:44,825 - trainer - INFO - {
  "train_loss": 1914.5009765625
}
2022-10-24 15:18:44,827 - trainer - INFO - start training epoch 63
2022-10-24 15:18:44,829 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,829 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,831 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,840 - trainer - INFO - 
*****************[epoch: 63, global step: 64] eval training set at end of epoch***************
2022-10-24 15:18:44,841 - trainer - INFO - {
  "train_loss": 1738.5150146484375
}
2022-10-24 15:18:44,841 - trainer - INFO - start training epoch 64
2022-10-24 15:18:44,842 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,843 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,843 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,855 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval training set based on eval_every=2***************
2022-10-24 15:18:44,856 - trainer - INFO - {
  "train_loss": 1662.2643432617188
}
2022-10-24 15:18:44,864 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval development set based on eval_every=2***************
2022-10-24 15:18:44,865 - trainer - INFO - {
  "dev_loss": 1627.2293701171875,
  "dev_best_score_for_loss": -1627.2293701171875
}
2022-10-24 15:18:44,866 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,868 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,868 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,869 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_58
2022-10-24 15:18:44,870 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,875 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,876 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,879 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,880 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,880 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_64
2022-10-24 15:18:44,886 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_64
2022-10-24 15:18:44,887 - trainer - INFO - 
*****************[epoch: 64, global step: 65] eval training set at end of epoch***************
2022-10-24 15:18:44,888 - trainer - INFO - {
  "train_loss": 1586.013671875
}
2022-10-24 15:18:44,888 - trainer - INFO - start training epoch 65
2022-10-24 15:18:44,889 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,889 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,890 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,909 - trainer - INFO - 
*****************[epoch: 65, global step: 66] eval training set at end of epoch***************
2022-10-24 15:18:44,910 - trainer - INFO - {
  "train_loss": 1627.2294921875
}
2022-10-24 15:18:44,911 - trainer - INFO - start training epoch 66
2022-10-24 15:18:44,911 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,912 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,912 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,920 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval training set based on eval_every=2***************
2022-10-24 15:18:44,921 - trainer - INFO - {
  "train_loss": 1538.6278686523438
}
2022-10-24 15:18:44,932 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval development set based on eval_every=2***************
2022-10-24 15:18:44,933 - trainer - INFO - {
  "dev_loss": 1356.609619140625,
  "dev_best_score_for_loss": -1356.609619140625
}
2022-10-24 15:18:44,934 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,935 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,935 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,936 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_60
2022-10-24 15:18:44,937 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:44,941 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:44,942 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:44,945 - trainer - INFO -   patience: 200
2022-10-24 15:18:44,946 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:44,948 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_66
2022-10-24 15:18:44,953 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_66
2022-10-24 15:18:44,956 - trainer - INFO - 
*****************[epoch: 66, global step: 67] eval training set at end of epoch***************
2022-10-24 15:18:44,956 - trainer - INFO - {
  "train_loss": 1450.0262451171875
}
2022-10-24 15:18:44,957 - trainer - INFO - start training epoch 67
2022-10-24 15:18:44,958 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,958 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,960 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,970 - trainer - INFO - 
*****************[epoch: 67, global step: 68] eval training set at end of epoch***************
2022-10-24 15:18:44,970 - trainer - INFO - {
  "train_loss": 1356.609619140625
}
2022-10-24 15:18:44,971 - trainer - INFO - start training epoch 68
2022-10-24 15:18:44,971 - trainer - INFO - training using device=cuda
2022-10-24 15:18:44,972 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:44,973 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:44,983 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval training set based on eval_every=2***************
2022-10-24 15:18:44,983 - trainer - INFO - {
  "train_loss": 1356.6421508789062
}
2022-10-24 15:18:44,992 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval development set based on eval_every=2***************
2022-10-24 15:18:44,993 - trainer - INFO - {
  "dev_loss": 1187.52587890625,
  "dev_best_score_for_loss": -1187.52587890625
}
2022-10-24 15:18:44,994 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:44,996 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:44,996 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:44,997 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_62
2022-10-24 15:18:44,999 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,004 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,004 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,004 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,005 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,005 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_68
2022-10-24 15:18:45,010 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_68
2022-10-24 15:18:45,011 - trainer - INFO - 
*****************[epoch: 68, global step: 69] eval training set at end of epoch***************
2022-10-24 15:18:45,012 - trainer - INFO - {
  "train_loss": 1356.6746826171875
}
2022-10-24 15:18:45,012 - trainer - INFO - start training epoch 69
2022-10-24 15:18:45,013 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,013 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,014 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,025 - trainer - INFO - 
*****************[epoch: 69, global step: 70] eval training set at end of epoch***************
2022-10-24 15:18:45,026 - trainer - INFO - {
  "train_loss": 1187.5260009765625
}
2022-10-24 15:18:45,029 - trainer - INFO - start training epoch 70
2022-10-24 15:18:45,029 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,030 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,031 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,044 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval training set based on eval_every=2***************
2022-10-24 15:18:45,044 - trainer - INFO - {
  "train_loss": 1160.9388427734375
}
2022-10-24 15:18:45,053 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval development set based on eval_every=2***************
2022-10-24 15:18:45,053 - trainer - INFO - {
  "dev_loss": 1091.452880859375,
  "dev_best_score_for_loss": -1091.452880859375
}
2022-10-24 15:18:45,054 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,056 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,056 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,056 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_64
2022-10-24 15:18:45,059 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,064 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,065 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,067 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,069 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,072 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_70
2022-10-24 15:18:45,079 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_70
2022-10-24 15:18:45,080 - trainer - INFO - 
*****************[epoch: 70, global step: 71] eval training set at end of epoch***************
2022-10-24 15:18:45,081 - trainer - INFO - {
  "train_loss": 1134.3516845703125
}
2022-10-24 15:18:45,082 - trainer - INFO - start training epoch 71
2022-10-24 15:18:45,082 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,083 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,084 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,096 - trainer - INFO - 
*****************[epoch: 71, global step: 72] eval training set at end of epoch***************
2022-10-24 15:18:45,096 - trainer - INFO - {
  "train_loss": 1091.452880859375
}
2022-10-24 15:18:45,098 - trainer - INFO - start training epoch 72
2022-10-24 15:18:45,099 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,099 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,100 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,111 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval training set based on eval_every=2***************
2022-10-24 15:18:45,115 - trainer - INFO - {
  "train_loss": 1019.1983032226562
}
2022-10-24 15:18:45,125 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval development set based on eval_every=2***************
2022-10-24 15:18:45,126 - trainer - INFO - {
  "dev_loss": 928.6519775390625,
  "dev_best_score_for_loss": -928.6519775390625
}
2022-10-24 15:18:45,128 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,131 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,131 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,132 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_66
2022-10-24 15:18:45,134 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,138 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,138 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,139 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,143 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,144 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_72
2022-10-24 15:18:45,149 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_72
2022-10-24 15:18:45,150 - trainer - INFO - 
*****************[epoch: 72, global step: 73] eval training set at end of epoch***************
2022-10-24 15:18:45,151 - trainer - INFO - {
  "train_loss": 946.9437255859375
}
2022-10-24 15:18:45,151 - trainer - INFO - start training epoch 73
2022-10-24 15:18:45,152 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,153 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,153 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,173 - trainer - INFO - 
*****************[epoch: 73, global step: 74] eval training set at end of epoch***************
2022-10-24 15:18:45,174 - trainer - INFO - {
  "train_loss": 928.65185546875
}
2022-10-24 15:18:45,175 - trainer - INFO - start training epoch 74
2022-10-24 15:18:45,176 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,177 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,178 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,187 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval training set based on eval_every=2***************
2022-10-24 15:18:45,188 - trainer - INFO - {
  "train_loss": 885.3882751464844
}
2022-10-24 15:18:45,196 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval development set based on eval_every=2***************
2022-10-24 15:18:45,197 - trainer - INFO - {
  "dev_loss": 743.8958740234375,
  "dev_best_score_for_loss": -743.8958740234375
}
2022-10-24 15:18:45,198 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,199 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,199 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,200 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_68
2022-10-24 15:18:45,201 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,206 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,206 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,207 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,208 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,209 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_74
2022-10-24 15:18:45,214 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_74
2022-10-24 15:18:45,215 - trainer - INFO - 
*****************[epoch: 74, global step: 75] eval training set at end of epoch***************
2022-10-24 15:18:45,216 - trainer - INFO - {
  "train_loss": 842.1246948242188
}
2022-10-24 15:18:45,216 - trainer - INFO - start training epoch 75
2022-10-24 15:18:45,220 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,221 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,221 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,231 - trainer - INFO - 
*****************[epoch: 75, global step: 76] eval training set at end of epoch***************
2022-10-24 15:18:45,231 - trainer - INFO - {
  "train_loss": 743.8958740234375
}
2022-10-24 15:18:45,232 - trainer - INFO - start training epoch 76
2022-10-24 15:18:45,232 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,233 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,234 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,247 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval training set based on eval_every=2***************
2022-10-24 15:18:45,247 - trainer - INFO - {
  "train_loss": 733.7664794921875
}
2022-10-24 15:18:45,254 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval development set based on eval_every=2***************
2022-10-24 15:18:45,255 - trainer - INFO - {
  "dev_loss": 616.1427001953125,
  "dev_best_score_for_loss": -616.1427001953125
}
2022-10-24 15:18:45,255 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,257 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,257 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,257 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_70
2022-10-24 15:18:45,259 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,262 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,262 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,262 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,263 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,263 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_76
2022-10-24 15:18:45,270 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_76
2022-10-24 15:18:45,271 - trainer - INFO - 
*****************[epoch: 76, global step: 77] eval training set at end of epoch***************
2022-10-24 15:18:45,272 - trainer - INFO - {
  "train_loss": 723.6370849609375
}
2022-10-24 15:18:45,272 - trainer - INFO - start training epoch 77
2022-10-24 15:18:45,272 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,273 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,273 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,286 - trainer - INFO - 
*****************[epoch: 77, global step: 78] eval training set at end of epoch***************
2022-10-24 15:18:45,286 - trainer - INFO - {
  "train_loss": 616.1427001953125
}
2022-10-24 15:18:45,287 - trainer - INFO - start training epoch 78
2022-10-24 15:18:45,288 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,288 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,288 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,298 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval training set based on eval_every=2***************
2022-10-24 15:18:45,299 - trainer - INFO - {
  "train_loss": 595.9978637695312
}
2022-10-24 15:18:45,306 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval development set based on eval_every=2***************
2022-10-24 15:18:45,306 - trainer - INFO - {
  "dev_loss": 523.5262451171875,
  "dev_best_score_for_loss": -523.5262451171875
}
2022-10-24 15:18:45,307 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,308 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,309 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,309 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_72
2022-10-24 15:18:45,311 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,314 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,314 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,314 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,315 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,315 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_78
2022-10-24 15:18:45,319 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_78
2022-10-24 15:18:45,320 - trainer - INFO - 
*****************[epoch: 78, global step: 79] eval training set at end of epoch***************
2022-10-24 15:18:45,320 - trainer - INFO - {
  "train_loss": 575.85302734375
}
2022-10-24 15:18:45,320 - trainer - INFO - start training epoch 79
2022-10-24 15:18:45,320 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,321 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,321 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,332 - trainer - INFO - 
*****************[epoch: 79, global step: 80] eval training set at end of epoch***************
2022-10-24 15:18:45,333 - trainer - INFO - {
  "train_loss": 523.5262451171875
}
2022-10-24 15:18:45,333 - trainer - INFO - start training epoch 80
2022-10-24 15:18:45,334 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,334 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,335 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,343 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval training set based on eval_every=2***************
2022-10-24 15:18:45,344 - trainer - INFO - {
  "train_loss": 481.75592041015625
}
2022-10-24 15:18:45,351 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval development set based on eval_every=2***************
2022-10-24 15:18:45,351 - trainer - INFO - {
  "dev_loss": 419.9363708496094,
  "dev_best_score_for_loss": -419.9363708496094
}
2022-10-24 15:18:45,352 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,353 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,353 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,354 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_74
2022-10-24 15:18:45,355 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,358 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,359 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,359 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,360 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,360 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_80
2022-10-24 15:18:45,365 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_80
2022-10-24 15:18:45,366 - trainer - INFO - 
*****************[epoch: 80, global step: 81] eval training set at end of epoch***************
2022-10-24 15:18:45,366 - trainer - INFO - {
  "train_loss": 439.985595703125
}
2022-10-24 15:18:45,366 - trainer - INFO - start training epoch 81
2022-10-24 15:18:45,366 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,367 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,367 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,379 - trainer - INFO - 
*****************[epoch: 81, global step: 82] eval training set at end of epoch***************
2022-10-24 15:18:45,380 - trainer - INFO - {
  "train_loss": 419.9363708496094
}
2022-10-24 15:18:45,380 - trainer - INFO - start training epoch 82
2022-10-24 15:18:45,381 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,381 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,381 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,391 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval training set based on eval_every=2***************
2022-10-24 15:18:45,391 - trainer - INFO - {
  "train_loss": 381.73606872558594
}
2022-10-24 15:18:45,399 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval development set based on eval_every=2***************
2022-10-24 15:18:45,399 - trainer - INFO - {
  "dev_loss": 312.04901123046875,
  "dev_best_score_for_loss": -312.04901123046875
}
2022-10-24 15:18:45,400 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,402 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,402 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,402 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_76
2022-10-24 15:18:45,404 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,408 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,409 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,409 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,410 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,410 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_82
2022-10-24 15:18:45,415 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_82
2022-10-24 15:18:45,416 - trainer - INFO - 
*****************[epoch: 82, global step: 83] eval training set at end of epoch***************
2022-10-24 15:18:45,417 - trainer - INFO - {
  "train_loss": 343.5357666015625
}
2022-10-24 15:18:45,418 - trainer - INFO - start training epoch 83
2022-10-24 15:18:45,418 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,418 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,420 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,436 - trainer - INFO - 
*****************[epoch: 83, global step: 84] eval training set at end of epoch***************
2022-10-24 15:18:45,436 - trainer - INFO - {
  "train_loss": 312.04901123046875
}
2022-10-24 15:18:45,437 - trainer - INFO - start training epoch 84
2022-10-24 15:18:45,437 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,438 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,438 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,447 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval training set based on eval_every=2***************
2022-10-24 15:18:45,448 - trainer - INFO - {
  "train_loss": 290.9054260253906
}
2022-10-24 15:18:45,455 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval development set based on eval_every=2***************
2022-10-24 15:18:45,456 - trainer - INFO - {
  "dev_loss": 218.7360076904297,
  "dev_best_score_for_loss": -218.7360076904297
}
2022-10-24 15:18:45,457 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,459 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,459 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,459 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_78
2022-10-24 15:18:45,460 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,464 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,464 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,464 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,465 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,466 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_84
2022-10-24 15:18:45,471 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_84
2022-10-24 15:18:45,472 - trainer - INFO - 
*****************[epoch: 84, global step: 85] eval training set at end of epoch***************
2022-10-24 15:18:45,472 - trainer - INFO - {
  "train_loss": 269.7618408203125
}
2022-10-24 15:18:45,473 - trainer - INFO - start training epoch 85
2022-10-24 15:18:45,473 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,473 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,474 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,482 - trainer - INFO - 
*****************[epoch: 85, global step: 86] eval training set at end of epoch***************
2022-10-24 15:18:45,483 - trainer - INFO - {
  "train_loss": 218.7360076904297
}
2022-10-24 15:18:45,483 - trainer - INFO - start training epoch 86
2022-10-24 15:18:45,483 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,484 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,484 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,492 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval training set based on eval_every=2***************
2022-10-24 15:18:45,492 - trainer - INFO - {
  "train_loss": 209.80254364013672
}
2022-10-24 15:18:45,500 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval development set based on eval_every=2***************
2022-10-24 15:18:45,500 - trainer - INFO - {
  "dev_loss": 149.97308349609375,
  "dev_best_score_for_loss": -149.97308349609375
}
2022-10-24 15:18:45,501 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,502 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,502 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,503 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_80
2022-10-24 15:18:45,504 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,507 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,507 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,508 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,508 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,509 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_86
2022-10-24 15:18:45,513 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_86
2022-10-24 15:18:45,515 - trainer - INFO - 
*****************[epoch: 86, global step: 87] eval training set at end of epoch***************
2022-10-24 15:18:45,518 - trainer - INFO - {
  "train_loss": 200.86907958984375
}
2022-10-24 15:18:45,519 - trainer - INFO - start training epoch 87
2022-10-24 15:18:45,519 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,519 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,520 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,531 - trainer - INFO - 
*****************[epoch: 87, global step: 88] eval training set at end of epoch***************
2022-10-24 15:18:45,532 - trainer - INFO - {
  "train_loss": 149.97308349609375
}
2022-10-24 15:18:45,532 - trainer - INFO - start training epoch 88
2022-10-24 15:18:45,532 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,533 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,533 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,542 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval training set based on eval_every=2***************
2022-10-24 15:18:45,543 - trainer - INFO - {
  "train_loss": 144.14710998535156
}
2022-10-24 15:18:45,550 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval development set based on eval_every=2***************
2022-10-24 15:18:45,551 - trainer - INFO - {
  "dev_loss": 101.32159423828125,
  "dev_best_score_for_loss": -101.32159423828125
}
2022-10-24 15:18:45,551 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,552 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,553 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,553 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_82
2022-10-24 15:18:45,554 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,558 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,560 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,561 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,563 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,568 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_88
2022-10-24 15:18:45,573 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_88
2022-10-24 15:18:45,575 - trainer - INFO - 
*****************[epoch: 88, global step: 89] eval training set at end of epoch***************
2022-10-24 15:18:45,575 - trainer - INFO - {
  "train_loss": 138.32113647460938
}
2022-10-24 15:18:45,576 - trainer - INFO - start training epoch 89
2022-10-24 15:18:45,576 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,576 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,577 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,587 - trainer - INFO - 
*****************[epoch: 89, global step: 90] eval training set at end of epoch***************
2022-10-24 15:18:45,587 - trainer - INFO - {
  "train_loss": 101.32158660888672
}
2022-10-24 15:18:45,588 - trainer - INFO - start training epoch 90
2022-10-24 15:18:45,588 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,588 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,588 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,598 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval training set based on eval_every=2***************
2022-10-24 15:18:45,598 - trainer - INFO - {
  "train_loss": 93.40752792358398
}
2022-10-24 15:18:45,605 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval development set based on eval_every=2***************
2022-10-24 15:18:45,606 - trainer - INFO - {
  "dev_loss": 65.55488586425781,
  "dev_best_score_for_loss": -65.55488586425781
}
2022-10-24 15:18:45,608 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,610 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,612 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,612 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_84
2022-10-24 15:18:45,614 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,618 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,618 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,618 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,619 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,619 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_90
2022-10-24 15:18:45,624 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_90
2022-10-24 15:18:45,625 - trainer - INFO - 
*****************[epoch: 90, global step: 91] eval training set at end of epoch***************
2022-10-24 15:18:45,625 - trainer - INFO - {
  "train_loss": 85.49346923828125
}
2022-10-24 15:18:45,626 - trainer - INFO - start training epoch 91
2022-10-24 15:18:45,626 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,627 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,627 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,635 - trainer - INFO - 
*****************[epoch: 91, global step: 92] eval training set at end of epoch***************
2022-10-24 15:18:45,636 - trainer - INFO - {
  "train_loss": 65.55488586425781
}
2022-10-24 15:18:45,636 - trainer - INFO - start training epoch 92
2022-10-24 15:18:45,636 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,637 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,637 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,643 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval training set based on eval_every=2***************
2022-10-24 15:18:45,644 - trainer - INFO - {
  "train_loss": 56.43088150024414
}
2022-10-24 15:18:45,649 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval development set based on eval_every=2***************
2022-10-24 15:18:45,650 - trainer - INFO - {
  "dev_loss": 40.42017364501953,
  "dev_best_score_for_loss": -40.42017364501953
}
2022-10-24 15:18:45,650 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,652 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,653 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,654 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_86
2022-10-24 15:18:45,656 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,663 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,663 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,663 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,664 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,665 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_92
2022-10-24 15:18:45,669 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_92
2022-10-24 15:18:45,670 - trainer - INFO - 
*****************[epoch: 92, global step: 93] eval training set at end of epoch***************
2022-10-24 15:18:45,671 - trainer - INFO - {
  "train_loss": 47.30687713623047
}
2022-10-24 15:18:45,671 - trainer - INFO - start training epoch 93
2022-10-24 15:18:45,671 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,671 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,672 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,679 - trainer - INFO - 
*****************[epoch: 93, global step: 94] eval training set at end of epoch***************
2022-10-24 15:18:45,680 - trainer - INFO - {
  "train_loss": 40.420169830322266
}
2022-10-24 15:18:45,680 - trainer - INFO - start training epoch 94
2022-10-24 15:18:45,680 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,681 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,681 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,688 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval training set based on eval_every=2***************
2022-10-24 15:18:45,688 - trainer - INFO - {
  "train_loss": 31.438629150390625
}
2022-10-24 15:18:45,693 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval development set based on eval_every=2***************
2022-10-24 15:18:45,694 - trainer - INFO - {
  "dev_loss": 22.740875244140625,
  "dev_best_score_for_loss": -22.740875244140625
}
2022-10-24 15:18:45,694 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,695 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,696 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,696 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_88
2022-10-24 15:18:45,698 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,703 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,703 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,704 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,704 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,705 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_94
2022-10-24 15:18:45,709 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_94
2022-10-24 15:18:45,710 - trainer - INFO - 
*****************[epoch: 94, global step: 95] eval training set at end of epoch***************
2022-10-24 15:18:45,711 - trainer - INFO - {
  "train_loss": 22.457088470458984
}
2022-10-24 15:18:45,711 - trainer - INFO - start training epoch 95
2022-10-24 15:18:45,711 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,712 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,712 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,719 - trainer - INFO - 
*****************[epoch: 95, global step: 96] eval training set at end of epoch***************
2022-10-24 15:18:45,720 - trainer - INFO - {
  "train_loss": 22.74087905883789
}
2022-10-24 15:18:45,720 - trainer - INFO - start training epoch 96
2022-10-24 15:18:45,720 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,721 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,721 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,730 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval training set based on eval_every=2***************
2022-10-24 15:18:45,731 - trainer - INFO - {
  "train_loss": 15.657671928405762
}
2022-10-24 15:18:45,737 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval development set based on eval_every=2***************
2022-10-24 15:18:45,737 - trainer - INFO - {
  "dev_loss": 12.158907890319824,
  "dev_best_score_for_loss": -12.158907890319824
}
2022-10-24 15:18:45,737 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,739 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,739 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,739 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_90
2022-10-24 15:18:45,740 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,743 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,743 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,744 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,745 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,746 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_96
2022-10-24 15:18:45,752 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_96
2022-10-24 15:18:45,753 - trainer - INFO - 
*****************[epoch: 96, global step: 97] eval training set at end of epoch***************
2022-10-24 15:18:45,753 - trainer - INFO - {
  "train_loss": 8.574464797973633
}
2022-10-24 15:18:45,754 - trainer - INFO - start training epoch 97
2022-10-24 15:18:45,754 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,755 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,755 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,764 - trainer - INFO - 
*****************[epoch: 97, global step: 98] eval training set at end of epoch***************
2022-10-24 15:18:45,765 - trainer - INFO - {
  "train_loss": 12.158907890319824
}
2022-10-24 15:18:45,765 - trainer - INFO - start training epoch 98
2022-10-24 15:18:45,765 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,766 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,766 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,772 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval training set based on eval_every=2***************
2022-10-24 15:18:45,773 - trainer - INFO - {
  "train_loss": 7.332563281059265
}
2022-10-24 15:18:45,780 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval development set based on eval_every=2***************
2022-10-24 15:18:45,780 - trainer - INFO - {
  "dev_loss": 7.0225653648376465,
  "dev_best_score_for_loss": -7.0225653648376465
}
2022-10-24 15:18:45,781 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,782 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,783 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,783 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_92
2022-10-24 15:18:45,785 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,788 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,789 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,789 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,790 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,790 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_98
2022-10-24 15:18:45,796 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_98
2022-10-24 15:18:45,797 - trainer - INFO - 
*****************[epoch: 98, global step: 99] eval training set at end of epoch***************
2022-10-24 15:18:45,797 - trainer - INFO - {
  "train_loss": 2.506218671798706
}
2022-10-24 15:18:45,798 - trainer - INFO - start training epoch 99
2022-10-24 15:18:45,798 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,798 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,799 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,808 - trainer - INFO - 
*****************[epoch: 99, global step: 100] eval training set at end of epoch***************
2022-10-24 15:18:45,809 - trainer - INFO - {
  "train_loss": 7.0225653648376465
}
2022-10-24 15:18:45,809 - trainer - INFO - start training epoch 100
2022-10-24 15:18:45,809 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,810 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,810 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,818 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval training set based on eval_every=2***************
2022-10-24 15:18:45,819 - trainer - INFO - {
  "train_loss": 4.317236065864563
}
2022-10-24 15:18:45,827 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval development set based on eval_every=2***************
2022-10-24 15:18:45,827 - trainer - INFO - {
  "dev_loss": 6.140846252441406,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:45,828 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:45,830 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,830 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,831 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_94
2022-10-24 15:18:45,832 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:45,837 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:45,838 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:45,838 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,839 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:45,839 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_100
2022-10-24 15:18:45,844 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_100
2022-10-24 15:18:45,845 - trainer - INFO - 
*****************[epoch: 100, global step: 101] eval training set at end of epoch***************
2022-10-24 15:18:45,845 - trainer - INFO - {
  "train_loss": 1.6119067668914795
}
2022-10-24 15:18:45,845 - trainer - INFO - start training epoch 101
2022-10-24 15:18:45,846 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,846 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,846 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,854 - trainer - INFO - 
*****************[epoch: 101, global step: 102] eval training set at end of epoch***************
2022-10-24 15:18:45,856 - trainer - INFO - {
  "train_loss": 6.140846252441406
}
2022-10-24 15:18:45,859 - trainer - INFO - start training epoch 102
2022-10-24 15:18:45,859 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,860 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,860 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,869 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval training set based on eval_every=2***************
2022-10-24 15:18:45,870 - trainer - INFO - {
  "train_loss": 4.975453615188599
}
2022-10-24 15:18:45,875 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval development set based on eval_every=2***************
2022-10-24 15:18:45,875 - trainer - INFO - {
  "dev_loss": 7.6623382568359375,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:45,876 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:45,876 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,877 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,878 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,878 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_96
2022-10-24 15:18:45,879 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_102
2022-10-24 15:18:45,883 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_102
2022-10-24 15:18:45,885 - trainer - INFO - 
*****************[epoch: 102, global step: 103] eval training set at end of epoch***************
2022-10-24 15:18:45,885 - trainer - INFO - {
  "train_loss": 3.810060977935791
}
2022-10-24 15:18:45,885 - trainer - INFO - start training epoch 103
2022-10-24 15:18:45,885 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,886 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,886 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,896 - trainer - INFO - 
*****************[epoch: 103, global step: 104] eval training set at end of epoch***************
2022-10-24 15:18:45,897 - trainer - INFO - {
  "train_loss": 7.6623382568359375
}
2022-10-24 15:18:45,897 - trainer - INFO - start training epoch 104
2022-10-24 15:18:45,897 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,898 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,898 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,907 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval training set based on eval_every=2***************
2022-10-24 15:18:45,908 - trainer - INFO - {
  "train_loss": 7.408193588256836
}
2022-10-24 15:18:45,917 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval development set based on eval_every=2***************
2022-10-24 15:18:45,918 - trainer - INFO - {
  "dev_loss": 10.199088096618652,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:45,919 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:45,920 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,922 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,922 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,922 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_98
2022-10-24 15:18:45,924 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_104
2022-10-24 15:18:45,929 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_104
2022-10-24 15:18:45,930 - trainer - INFO - 
*****************[epoch: 104, global step: 105] eval training set at end of epoch***************
2022-10-24 15:18:45,931 - trainer - INFO - {
  "train_loss": 7.154048919677734
}
2022-10-24 15:18:45,932 - trainer - INFO - start training epoch 105
2022-10-24 15:18:45,932 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,932 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,933 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,941 - trainer - INFO - 
*****************[epoch: 105, global step: 106] eval training set at end of epoch***************
2022-10-24 15:18:45,942 - trainer - INFO - {
  "train_loss": 10.199088096618652
}
2022-10-24 15:18:45,942 - trainer - INFO - start training epoch 106
2022-10-24 15:18:45,942 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,942 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,943 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,953 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval training set based on eval_every=2***************
2022-10-24 15:18:45,954 - trainer - INFO - {
  "train_loss": 10.330002784729004
}
2022-10-24 15:18:45,963 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval development set based on eval_every=2***************
2022-10-24 15:18:45,964 - trainer - INFO - {
  "dev_loss": 12.55842113494873,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:45,965 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:45,965 - trainer - INFO -   patience: 200
2022-10-24 15:18:45,967 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:45,967 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:45,967 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_100
2022-10-24 15:18:45,968 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_106
2022-10-24 15:18:45,972 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_106
2022-10-24 15:18:45,973 - trainer - INFO - 
*****************[epoch: 106, global step: 107] eval training set at end of epoch***************
2022-10-24 15:18:45,973 - trainer - INFO - {
  "train_loss": 10.460917472839355
}
2022-10-24 15:18:45,974 - trainer - INFO - start training epoch 107
2022-10-24 15:18:45,974 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,975 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,975 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,983 - trainer - INFO - 
*****************[epoch: 107, global step: 108] eval training set at end of epoch***************
2022-10-24 15:18:45,983 - trainer - INFO - {
  "train_loss": 12.558420181274414
}
2022-10-24 15:18:45,984 - trainer - INFO - start training epoch 108
2022-10-24 15:18:45,984 - trainer - INFO - training using device=cuda
2022-10-24 15:18:45,984 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:45,984 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:45,991 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval training set based on eval_every=2***************
2022-10-24 15:18:45,991 - trainer - INFO - {
  "train_loss": 12.703176975250244
}
2022-10-24 15:18:46,003 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval development set based on eval_every=2***************
2022-10-24 15:18:46,003 - trainer - INFO - {
  "dev_loss": 14.070568084716797,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:46,004 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:46,004 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,005 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,006 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,006 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_102
2022-10-24 15:18:46,008 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_108
2022-10-24 15:18:46,013 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_108
2022-10-24 15:18:46,013 - trainer - INFO - 
*****************[epoch: 108, global step: 109] eval training set at end of epoch***************
2022-10-24 15:18:46,014 - trainer - INFO - {
  "train_loss": 12.847933769226074
}
2022-10-24 15:18:46,014 - trainer - INFO - start training epoch 109
2022-10-24 15:18:46,014 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,015 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,015 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,022 - trainer - INFO - 
*****************[epoch: 109, global step: 110] eval training set at end of epoch***************
2022-10-24 15:18:46,022 - trainer - INFO - {
  "train_loss": 14.070568084716797
}
2022-10-24 15:18:46,023 - trainer - INFO - start training epoch 110
2022-10-24 15:18:46,024 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,024 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,024 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,033 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval training set based on eval_every=2***************
2022-10-24 15:18:46,034 - trainer - INFO - {
  "train_loss": 14.03825855255127
}
2022-10-24 15:18:46,040 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval development set based on eval_every=2***************
2022-10-24 15:18:46,041 - trainer - INFO - {
  "dev_loss": 14.469791412353516,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:46,042 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:18:46,046 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,047 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,047 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,047 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_104
2022-10-24 15:18:46,049 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_110
2022-10-24 15:18:46,053 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_110
2022-10-24 15:18:46,056 - trainer - INFO - 
*****************[epoch: 110, global step: 111] eval training set at end of epoch***************
2022-10-24 15:18:46,056 - trainer - INFO - {
  "train_loss": 14.005949020385742
}
2022-10-24 15:18:46,056 - trainer - INFO - start training epoch 111
2022-10-24 15:18:46,057 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,057 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,058 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,068 - trainer - INFO - 
*****************[epoch: 111, global step: 112] eval training set at end of epoch***************
2022-10-24 15:18:46,068 - trainer - INFO - {
  "train_loss": 14.469793319702148
}
2022-10-24 15:18:46,068 - trainer - INFO - start training epoch 112
2022-10-24 15:18:46,069 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,069 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,069 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,077 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval training set based on eval_every=2***************
2022-10-24 15:18:46,077 - trainer - INFO - {
  "train_loss": 14.207058429718018
}
2022-10-24 15:18:46,083 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval development set based on eval_every=2***************
2022-10-24 15:18:46,083 - trainer - INFO - {
  "dev_loss": 13.8604154586792,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:46,084 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:18:46,084 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,085 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,085 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,086 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_106
2022-10-24 15:18:46,091 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_112
2022-10-24 15:18:46,096 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_112
2022-10-24 15:18:46,097 - trainer - INFO - 
*****************[epoch: 112, global step: 113] eval training set at end of epoch***************
2022-10-24 15:18:46,098 - trainer - INFO - {
  "train_loss": 13.944323539733887
}
2022-10-24 15:18:46,098 - trainer - INFO - start training epoch 113
2022-10-24 15:18:46,099 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,099 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,099 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,109 - trainer - INFO - 
*****************[epoch: 113, global step: 114] eval training set at end of epoch***************
2022-10-24 15:18:46,110 - trainer - INFO - {
  "train_loss": 13.860416412353516
}
2022-10-24 15:18:46,110 - trainer - INFO - start training epoch 114
2022-10-24 15:18:46,110 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,111 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,111 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,122 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval training set based on eval_every=2***************
2022-10-24 15:18:46,123 - trainer - INFO - {
  "train_loss": 13.401361465454102
}
2022-10-24 15:18:46,131 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval development set based on eval_every=2***************
2022-10-24 15:18:46,132 - trainer - INFO - {
  "dev_loss": 12.52003002166748,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:46,133 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:18:46,134 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,135 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,135 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,136 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_108
2022-10-24 15:18:46,137 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_114
2022-10-24 15:18:46,142 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_114
2022-10-24 15:18:46,143 - trainer - INFO - 
*****************[epoch: 114, global step: 115] eval training set at end of epoch***************
2022-10-24 15:18:46,143 - trainer - INFO - {
  "train_loss": 12.942306518554688
}
2022-10-24 15:18:46,144 - trainer - INFO - start training epoch 115
2022-10-24 15:18:46,144 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,144 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,144 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,153 - trainer - INFO - 
*****************[epoch: 115, global step: 116] eval training set at end of epoch***************
2022-10-24 15:18:46,153 - trainer - INFO - {
  "train_loss": 12.52003002166748
}
2022-10-24 15:18:46,154 - trainer - INFO - start training epoch 116
2022-10-24 15:18:46,154 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,154 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,155 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,166 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval training set based on eval_every=2***************
2022-10-24 15:18:46,169 - trainer - INFO - {
  "train_loss": 11.918134212493896
}
2022-10-24 15:18:46,176 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval development set based on eval_every=2***************
2022-10-24 15:18:46,177 - trainer - INFO - {
  "dev_loss": 10.73499584197998,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:46,177 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:18:46,178 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,180 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,180 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,180 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_110
2022-10-24 15:18:46,182 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_116
2022-10-24 15:18:46,188 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_116
2022-10-24 15:18:46,189 - trainer - INFO - 
*****************[epoch: 116, global step: 117] eval training set at end of epoch***************
2022-10-24 15:18:46,189 - trainer - INFO - {
  "train_loss": 11.316238403320312
}
2022-10-24 15:18:46,189 - trainer - INFO - start training epoch 117
2022-10-24 15:18:46,189 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,190 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,190 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,199 - trainer - INFO - 
*****************[epoch: 117, global step: 118] eval training set at end of epoch***************
2022-10-24 15:18:46,200 - trainer - INFO - {
  "train_loss": 10.73499584197998
}
2022-10-24 15:18:46,201 - trainer - INFO - start training epoch 118
2022-10-24 15:18:46,201 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,201 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,201 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,208 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval training set based on eval_every=2***************
2022-10-24 15:18:46,208 - trainer - INFO - {
  "train_loss": 10.092350006103516
}
2022-10-24 15:18:46,220 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval development set based on eval_every=2***************
2022-10-24 15:18:46,220 - trainer - INFO - {
  "dev_loss": 8.876440048217773,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:46,221 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:18:46,221 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,222 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,222 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,223 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_112
2022-10-24 15:18:46,224 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_118
2022-10-24 15:18:46,232 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_118
2022-10-24 15:18:46,233 - trainer - INFO - 
*****************[epoch: 118, global step: 119] eval training set at end of epoch***************
2022-10-24 15:18:46,233 - trainer - INFO - {
  "train_loss": 9.44970417022705
}
2022-10-24 15:18:46,234 - trainer - INFO - start training epoch 119
2022-10-24 15:18:46,234 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,234 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,235 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,245 - trainer - INFO - 
*****************[epoch: 119, global step: 120] eval training set at end of epoch***************
2022-10-24 15:18:46,246 - trainer - INFO - {
  "train_loss": 8.876440048217773
}
2022-10-24 15:18:46,246 - trainer - INFO - start training epoch 120
2022-10-24 15:18:46,247 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,247 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,247 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,257 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval training set based on eval_every=2***************
2022-10-24 15:18:46,259 - trainer - INFO - {
  "train_loss": 8.245641708374023
}
2022-10-24 15:18:46,272 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval development set based on eval_every=2***************
2022-10-24 15:18:46,273 - trainer - INFO - {
  "dev_loss": 7.145650863647461,
  "dev_best_score_for_loss": -6.140846252441406
}
2022-10-24 15:18:46,274 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:18:46,274 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,276 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,276 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,277 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_114
2022-10-24 15:18:46,278 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_120
2022-10-24 15:18:46,283 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_120
2022-10-24 15:18:46,284 - trainer - INFO - 
*****************[epoch: 120, global step: 121] eval training set at end of epoch***************
2022-10-24 15:18:46,285 - trainer - INFO - {
  "train_loss": 7.614843368530273
}
2022-10-24 15:18:46,285 - trainer - INFO - start training epoch 121
2022-10-24 15:18:46,285 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,285 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,286 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,297 - trainer - INFO - 
*****************[epoch: 121, global step: 122] eval training set at end of epoch***************
2022-10-24 15:18:46,298 - trainer - INFO - {
  "train_loss": 7.145651817321777
}
2022-10-24 15:18:46,298 - trainer - INFO - start training epoch 122
2022-10-24 15:18:46,299 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,299 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,299 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,308 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval training set based on eval_every=2***************
2022-10-24 15:18:46,309 - trainer - INFO - {
  "train_loss": 6.590051651000977
}
2022-10-24 15:18:46,317 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval development set based on eval_every=2***************
2022-10-24 15:18:46,317 - trainer - INFO - {
  "dev_loss": 5.713850975036621,
  "dev_best_score_for_loss": -5.713850975036621
}
2022-10-24 15:18:46,319 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,321 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,321 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,322 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_116
2022-10-24 15:18:46,323 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,364 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,364 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,365 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,367 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,369 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_122
2022-10-24 15:18:46,374 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_122
2022-10-24 15:18:46,375 - trainer - INFO - 
*****************[epoch: 122, global step: 123] eval training set at end of epoch***************
2022-10-24 15:18:46,376 - trainer - INFO - {
  "train_loss": 6.034451484680176
}
2022-10-24 15:18:46,377 - trainer - INFO - start training epoch 123
2022-10-24 15:18:46,377 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,378 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,378 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,387 - trainer - INFO - 
*****************[epoch: 123, global step: 124] eval training set at end of epoch***************
2022-10-24 15:18:46,387 - trainer - INFO - {
  "train_loss": 5.713850975036621
}
2022-10-24 15:18:46,388 - trainer - INFO - start training epoch 124
2022-10-24 15:18:46,388 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,388 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,389 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,399 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval training set based on eval_every=2***************
2022-10-24 15:18:46,402 - trainer - INFO - {
  "train_loss": 5.236529588699341
}
2022-10-24 15:18:46,410 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval development set based on eval_every=2***************
2022-10-24 15:18:46,410 - trainer - INFO - {
  "dev_loss": 4.578767776489258,
  "dev_best_score_for_loss": -4.578767776489258
}
2022-10-24 15:18:46,412 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,414 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,414 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,414 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_118
2022-10-24 15:18:46,416 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,419 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,419 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,419 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,420 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,420 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_124
2022-10-24 15:18:46,424 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_124
2022-10-24 15:18:46,425 - trainer - INFO - 
*****************[epoch: 124, global step: 125] eval training set at end of epoch***************
2022-10-24 15:18:46,425 - trainer - INFO - {
  "train_loss": 4.7592082023620605
}
2022-10-24 15:18:46,426 - trainer - INFO - start training epoch 125
2022-10-24 15:18:46,428 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,429 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,429 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,438 - trainer - INFO - 
*****************[epoch: 125, global step: 126] eval training set at end of epoch***************
2022-10-24 15:18:46,439 - trainer - INFO - {
  "train_loss": 4.578767776489258
}
2022-10-24 15:18:46,439 - trainer - INFO - start training epoch 126
2022-10-24 15:18:46,439 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,439 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,440 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,450 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval training set based on eval_every=2***************
2022-10-24 15:18:46,451 - trainer - INFO - {
  "train_loss": 4.1903464794158936
}
2022-10-24 15:18:46,459 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval development set based on eval_every=2***************
2022-10-24 15:18:46,460 - trainer - INFO - {
  "dev_loss": 3.7353529930114746,
  "dev_best_score_for_loss": -3.7353529930114746
}
2022-10-24 15:18:46,461 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,463 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,463 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,463 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_120
2022-10-24 15:18:46,465 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,469 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,469 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,469 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,470 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,470 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_126
2022-10-24 15:18:46,475 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_126
2022-10-24 15:18:46,476 - trainer - INFO - 
*****************[epoch: 126, global step: 127] eval training set at end of epoch***************
2022-10-24 15:18:46,476 - trainer - INFO - {
  "train_loss": 3.8019251823425293
}
2022-10-24 15:18:46,477 - trainer - INFO - start training epoch 127
2022-10-24 15:18:46,477 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,477 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,478 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,485 - trainer - INFO - 
*****************[epoch: 127, global step: 128] eval training set at end of epoch***************
2022-10-24 15:18:46,486 - trainer - INFO - {
  "train_loss": 3.7353529930114746
}
2022-10-24 15:18:46,486 - trainer - INFO - start training epoch 128
2022-10-24 15:18:46,486 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,487 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,487 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,497 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval training set based on eval_every=2***************
2022-10-24 15:18:46,497 - trainer - INFO - {
  "train_loss": 3.4198384284973145
}
2022-10-24 15:18:46,505 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval development set based on eval_every=2***************
2022-10-24 15:18:46,505 - trainer - INFO - {
  "dev_loss": 3.1068553924560547,
  "dev_best_score_for_loss": -3.1068553924560547
}
2022-10-24 15:18:46,506 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,507 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,507 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,508 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_122
2022-10-24 15:18:46,509 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,513 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,513 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,513 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,514 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,514 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_128
2022-10-24 15:18:46,518 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_128
2022-10-24 15:18:46,520 - trainer - INFO - 
*****************[epoch: 128, global step: 129] eval training set at end of epoch***************
2022-10-24 15:18:46,520 - trainer - INFO - {
  "train_loss": 3.1043238639831543
}
2022-10-24 15:18:46,520 - trainer - INFO - start training epoch 129
2022-10-24 15:18:46,521 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,521 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,521 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,530 - trainer - INFO - 
*****************[epoch: 129, global step: 130] eval training set at end of epoch***************
2022-10-24 15:18:46,530 - trainer - INFO - {
  "train_loss": 3.1068551540374756
}
2022-10-24 15:18:46,531 - trainer - INFO - start training epoch 130
2022-10-24 15:18:46,531 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,531 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,532 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,540 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval training set based on eval_every=2***************
2022-10-24 15:18:46,541 - trainer - INFO - {
  "train_loss": 2.8450783491134644
}
2022-10-24 15:18:46,547 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval development set based on eval_every=2***************
2022-10-24 15:18:46,547 - trainer - INFO - {
  "dev_loss": 2.6068673133850098,
  "dev_best_score_for_loss": -2.6068673133850098
}
2022-10-24 15:18:46,548 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,549 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,550 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,551 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_124
2022-10-24 15:18:46,552 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,555 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,556 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,556 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,557 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,557 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_130
2022-10-24 15:18:46,561 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_130
2022-10-24 15:18:46,562 - trainer - INFO - 
*****************[epoch: 130, global step: 131] eval training set at end of epoch***************
2022-10-24 15:18:46,563 - trainer - INFO - {
  "train_loss": 2.583301544189453
}
2022-10-24 15:18:46,563 - trainer - INFO - start training epoch 131
2022-10-24 15:18:46,563 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,564 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,564 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,574 - trainer - INFO - 
*****************[epoch: 131, global step: 132] eval training set at end of epoch***************
2022-10-24 15:18:46,574 - trainer - INFO - {
  "train_loss": 2.6068673133850098
}
2022-10-24 15:18:46,575 - trainer - INFO - start training epoch 132
2022-10-24 15:18:46,575 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,575 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,576 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,583 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval training set based on eval_every=2***************
2022-10-24 15:18:46,583 - trainer - INFO - {
  "train_loss": 2.3862192630767822
}
2022-10-24 15:18:46,589 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval development set based on eval_every=2***************
2022-10-24 15:18:46,589 - trainer - INFO - {
  "dev_loss": 2.1837451457977295,
  "dev_best_score_for_loss": -2.1837451457977295
}
2022-10-24 15:18:46,590 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,591 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,591 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,591 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_126
2022-10-24 15:18:46,593 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,596 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,597 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,597 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,599 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,599 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_132
2022-10-24 15:18:46,603 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_132
2022-10-24 15:18:46,604 - trainer - INFO - 
*****************[epoch: 132, global step: 133] eval training set at end of epoch***************
2022-10-24 15:18:46,605 - trainer - INFO - {
  "train_loss": 2.1655712127685547
}
2022-10-24 15:18:46,605 - trainer - INFO - start training epoch 133
2022-10-24 15:18:46,606 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,606 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,606 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,615 - trainer - INFO - 
*****************[epoch: 133, global step: 134] eval training set at end of epoch***************
2022-10-24 15:18:46,616 - trainer - INFO - {
  "train_loss": 2.1837451457977295
}
2022-10-24 15:18:46,616 - trainer - INFO - start training epoch 134
2022-10-24 15:18:46,616 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,616 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,617 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,623 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval training set based on eval_every=2***************
2022-10-24 15:18:46,623 - trainer - INFO - {
  "train_loss": 1.997043490409851
}
2022-10-24 15:18:46,630 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval development set based on eval_every=2***************
2022-10-24 15:18:46,631 - trainer - INFO - {
  "dev_loss": 1.809240698814392,
  "dev_best_score_for_loss": -1.809240698814392
}
2022-10-24 15:18:46,632 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,633 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,633 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,633 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_128
2022-10-24 15:18:46,635 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,638 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,638 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,638 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,639 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,639 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_134
2022-10-24 15:18:46,643 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_134
2022-10-24 15:18:46,645 - trainer - INFO - 
*****************[epoch: 134, global step: 135] eval training set at end of epoch***************
2022-10-24 15:18:46,646 - trainer - INFO - {
  "train_loss": 1.8103418350219727
}
2022-10-24 15:18:46,647 - trainer - INFO - start training epoch 135
2022-10-24 15:18:46,650 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,650 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,651 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,660 - trainer - INFO - 
*****************[epoch: 135, global step: 136] eval training set at end of epoch***************
2022-10-24 15:18:46,661 - trainer - INFO - {
  "train_loss": 1.8092408180236816
}
2022-10-24 15:18:46,661 - trainer - INFO - start training epoch 136
2022-10-24 15:18:46,662 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,662 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,662 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,670 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval training set based on eval_every=2***************
2022-10-24 15:18:46,671 - trainer - INFO - {
  "train_loss": 1.6684386134147644
}
2022-10-24 15:18:46,678 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval development set based on eval_every=2***************
2022-10-24 15:18:46,679 - trainer - INFO - {
  "dev_loss": 1.5734672546386719,
  "dev_best_score_for_loss": -1.5734672546386719
}
2022-10-24 15:18:46,679 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,681 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,681 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,681 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_130
2022-10-24 15:18:46,683 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,687 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,687 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,687 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,688 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,688 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_136
2022-10-24 15:18:46,696 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_136
2022-10-24 15:18:46,697 - trainer - INFO - 
*****************[epoch: 136, global step: 137] eval training set at end of epoch***************
2022-10-24 15:18:46,698 - trainer - INFO - {
  "train_loss": 1.5276364088058472
}
2022-10-24 15:18:46,698 - trainer - INFO - start training epoch 137
2022-10-24 15:18:46,699 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,699 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,699 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,708 - trainer - INFO - 
*****************[epoch: 137, global step: 138] eval training set at end of epoch***************
2022-10-24 15:18:46,709 - trainer - INFO - {
  "train_loss": 1.5734672546386719
}
2022-10-24 15:18:46,709 - trainer - INFO - start training epoch 138
2022-10-24 15:18:46,710 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,710 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,710 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,717 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval training set based on eval_every=2***************
2022-10-24 15:18:46,717 - trainer - INFO - {
  "train_loss": 1.4927427768707275
}
2022-10-24 15:18:46,723 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval development set based on eval_every=2***************
2022-10-24 15:18:46,724 - trainer - INFO - {
  "dev_loss": 1.4638946056365967,
  "dev_best_score_for_loss": -1.4638946056365967
}
2022-10-24 15:18:46,724 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,726 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,726 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,726 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_132
2022-10-24 15:18:46,728 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,733 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,733 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,733 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,734 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,734 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_138
2022-10-24 15:18:46,740 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_138
2022-10-24 15:18:46,744 - trainer - INFO - 
*****************[epoch: 138, global step: 139] eval training set at end of epoch***************
2022-10-24 15:18:46,744 - trainer - INFO - {
  "train_loss": 1.4120182991027832
}
2022-10-24 15:18:46,744 - trainer - INFO - start training epoch 139
2022-10-24 15:18:46,745 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,745 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,745 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,754 - trainer - INFO - 
*****************[epoch: 139, global step: 140] eval training set at end of epoch***************
2022-10-24 15:18:46,755 - trainer - INFO - {
  "train_loss": 1.4638946056365967
}
2022-10-24 15:18:46,755 - trainer - INFO - start training epoch 140
2022-10-24 15:18:46,756 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,756 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,756 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,766 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval training set based on eval_every=2***************
2022-10-24 15:18:46,768 - trainer - INFO - {
  "train_loss": 1.419405221939087
}
2022-10-24 15:18:46,776 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval development set based on eval_every=2***************
2022-10-24 15:18:46,777 - trainer - INFO - {
  "dev_loss": 1.3826102018356323,
  "dev_best_score_for_loss": -1.3826102018356323
}
2022-10-24 15:18:46,778 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,779 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,779 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,779 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_134
2022-10-24 15:18:46,781 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,786 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,790 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,791 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,792 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,792 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_140
2022-10-24 15:18:46,798 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_140
2022-10-24 15:18:46,800 - trainer - INFO - 
*****************[epoch: 140, global step: 141] eval training set at end of epoch***************
2022-10-24 15:18:46,800 - trainer - INFO - {
  "train_loss": 1.3749158382415771
}
2022-10-24 15:18:46,800 - trainer - INFO - start training epoch 141
2022-10-24 15:18:46,801 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,801 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,802 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,811 - trainer - INFO - 
*****************[epoch: 141, global step: 142] eval training set at end of epoch***************
2022-10-24 15:18:46,811 - trainer - INFO - {
  "train_loss": 1.3826102018356323
}
2022-10-24 15:18:46,812 - trainer - INFO - start training epoch 142
2022-10-24 15:18:46,812 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,812 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,813 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,822 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval training set based on eval_every=2***************
2022-10-24 15:18:46,822 - trainer - INFO - {
  "train_loss": 1.357275664806366
}
2022-10-24 15:18:46,835 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval development set based on eval_every=2***************
2022-10-24 15:18:46,836 - trainer - INFO - {
  "dev_loss": 1.2954320907592773,
  "dev_best_score_for_loss": -1.2954320907592773
}
2022-10-24 15:18:46,836 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,838 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,838 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,839 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_136
2022-10-24 15:18:46,840 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,844 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,845 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,846 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,847 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,847 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_142
2022-10-24 15:18:46,852 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_142
2022-10-24 15:18:46,853 - trainer - INFO - 
*****************[epoch: 142, global step: 143] eval training set at end of epoch***************
2022-10-24 15:18:46,854 - trainer - INFO - {
  "train_loss": 1.3319411277770996
}
2022-10-24 15:18:46,854 - trainer - INFO - start training epoch 143
2022-10-24 15:18:46,854 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,855 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,855 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,864 - trainer - INFO - 
*****************[epoch: 143, global step: 144] eval training set at end of epoch***************
2022-10-24 15:18:46,865 - trainer - INFO - {
  "train_loss": 1.2954320907592773
}
2022-10-24 15:18:46,865 - trainer - INFO - start training epoch 144
2022-10-24 15:18:46,865 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,866 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,866 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,873 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval training set based on eval_every=2***************
2022-10-24 15:18:46,873 - trainer - INFO - {
  "train_loss": 1.287081003189087
}
2022-10-24 15:18:46,881 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval development set based on eval_every=2***************
2022-10-24 15:18:46,882 - trainer - INFO - {
  "dev_loss": 1.2291898727416992,
  "dev_best_score_for_loss": -1.2291898727416992
}
2022-10-24 15:18:46,882 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,884 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,884 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,884 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_138
2022-10-24 15:18:46,886 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,890 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,890 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,890 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,891 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,892 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_144
2022-10-24 15:18:46,897 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_144
2022-10-24 15:18:46,898 - trainer - INFO - 
*****************[epoch: 144, global step: 145] eval training set at end of epoch***************
2022-10-24 15:18:46,898 - trainer - INFO - {
  "train_loss": 1.2787299156188965
}
2022-10-24 15:18:46,899 - trainer - INFO - start training epoch 145
2022-10-24 15:18:46,899 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,899 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,900 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,906 - trainer - INFO - 
*****************[epoch: 145, global step: 146] eval training set at end of epoch***************
2022-10-24 15:18:46,908 - trainer - INFO - {
  "train_loss": 1.2291897535324097
}
2022-10-24 15:18:46,909 - trainer - INFO - start training epoch 146
2022-10-24 15:18:46,910 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,910 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,914 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,921 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval training set based on eval_every=2***************
2022-10-24 15:18:46,922 - trainer - INFO - {
  "train_loss": 1.2360724806785583
}
2022-10-24 15:18:46,931 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval development set based on eval_every=2***************
2022-10-24 15:18:46,931 - trainer - INFO - {
  "dev_loss": 1.1713855266571045,
  "dev_best_score_for_loss": -1.1713855266571045
}
2022-10-24 15:18:46,932 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,933 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,933 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,933 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_140
2022-10-24 15:18:46,935 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,939 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,939 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,939 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,940 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,940 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_146
2022-10-24 15:18:46,945 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_146
2022-10-24 15:18:46,946 - trainer - INFO - 
*****************[epoch: 146, global step: 147] eval training set at end of epoch***************
2022-10-24 15:18:46,946 - trainer - INFO - {
  "train_loss": 1.242955207824707
}
2022-10-24 15:18:46,946 - trainer - INFO - start training epoch 147
2022-10-24 15:18:46,946 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,947 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,947 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,958 - trainer - INFO - 
*****************[epoch: 147, global step: 148] eval training set at end of epoch***************
2022-10-24 15:18:46,959 - trainer - INFO - {
  "train_loss": 1.1713855266571045
}
2022-10-24 15:18:46,959 - trainer - INFO - start training epoch 148
2022-10-24 15:18:46,960 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,960 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,960 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:46,970 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval training set based on eval_every=2***************
2022-10-24 15:18:46,971 - trainer - INFO - {
  "train_loss": 1.1782155632972717
}
2022-10-24 15:18:46,977 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval development set based on eval_every=2***************
2022-10-24 15:18:46,978 - trainer - INFO - {
  "dev_loss": 1.1188149452209473,
  "dev_best_score_for_loss": -1.1188149452209473
}
2022-10-24 15:18:46,978 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:46,979 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:46,980 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:46,980 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_142
2022-10-24 15:18:46,981 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:46,985 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:46,986 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:46,989 - trainer - INFO -   patience: 200
2022-10-24 15:18:46,989 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:46,990 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_148
2022-10-24 15:18:46,995 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_148
2022-10-24 15:18:46,995 - trainer - INFO - 
*****************[epoch: 148, global step: 149] eval training set at end of epoch***************
2022-10-24 15:18:46,996 - trainer - INFO - {
  "train_loss": 1.185045599937439
}
2022-10-24 15:18:46,996 - trainer - INFO - start training epoch 149
2022-10-24 15:18:46,997 - trainer - INFO - training using device=cuda
2022-10-24 15:18:46,997 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:46,997 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,005 - trainer - INFO - 
*****************[epoch: 149, global step: 150] eval training set at end of epoch***************
2022-10-24 15:18:47,006 - trainer - INFO - {
  "train_loss": 1.1188149452209473
}
2022-10-24 15:18:47,006 - trainer - INFO - start training epoch 150
2022-10-24 15:18:47,006 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,007 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,007 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,014 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval training set based on eval_every=2***************
2022-10-24 15:18:47,014 - trainer - INFO - {
  "train_loss": 1.1280136108398438
}
2022-10-24 15:18:47,022 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval development set based on eval_every=2***************
2022-10-24 15:18:47,022 - trainer - INFO - {
  "dev_loss": 1.089975118637085,
  "dev_best_score_for_loss": -1.089975118637085
}
2022-10-24 15:18:47,023 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,024 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,025 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,025 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_144
2022-10-24 15:18:47,027 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,031 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,032 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,032 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,033 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,033 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_150
2022-10-24 15:18:47,038 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_150
2022-10-24 15:18:47,038 - trainer - INFO - 
*****************[epoch: 150, global step: 151] eval training set at end of epoch***************
2022-10-24 15:18:47,039 - trainer - INFO - {
  "train_loss": 1.1372122764587402
}
2022-10-24 15:18:47,039 - trainer - INFO - start training epoch 151
2022-10-24 15:18:47,039 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,039 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,040 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,052 - trainer - INFO - 
*****************[epoch: 151, global step: 152] eval training set at end of epoch***************
2022-10-24 15:18:47,052 - trainer - INFO - {
  "train_loss": 1.089975118637085
}
2022-10-24 15:18:47,052 - trainer - INFO - start training epoch 152
2022-10-24 15:18:47,053 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,053 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,053 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,063 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval training set based on eval_every=2***************
2022-10-24 15:18:47,063 - trainer - INFO - {
  "train_loss": 1.0896961688995361
}
2022-10-24 15:18:47,069 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval development set based on eval_every=2***************
2022-10-24 15:18:47,069 - trainer - INFO - {
  "dev_loss": 1.061673641204834,
  "dev_best_score_for_loss": -1.061673641204834
}
2022-10-24 15:18:47,070 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,071 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,071 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,071 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_146
2022-10-24 15:18:47,073 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,076 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,076 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,076 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,077 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,078 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_152
2022-10-24 15:18:47,083 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_152
2022-10-24 15:18:47,084 - trainer - INFO - 
*****************[epoch: 152, global step: 153] eval training set at end of epoch***************
2022-10-24 15:18:47,084 - trainer - INFO - {
  "train_loss": 1.0894172191619873
}
2022-10-24 15:18:47,085 - trainer - INFO - start training epoch 153
2022-10-24 15:18:47,085 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,085 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,086 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,100 - trainer - INFO - 
*****************[epoch: 153, global step: 154] eval training set at end of epoch***************
2022-10-24 15:18:47,100 - trainer - INFO - {
  "train_loss": 1.0616735219955444
}
2022-10-24 15:18:47,101 - trainer - INFO - start training epoch 154
2022-10-24 15:18:47,101 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,101 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,102 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,109 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval training set based on eval_every=2***************
2022-10-24 15:18:47,110 - trainer - INFO - {
  "train_loss": 1.0521758794784546
}
2022-10-24 15:18:47,117 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval development set based on eval_every=2***************
2022-10-24 15:18:47,117 - trainer - INFO - {
  "dev_loss": 1.0361652374267578,
  "dev_best_score_for_loss": -1.0361652374267578
}
2022-10-24 15:18:47,118 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,119 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,119 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,120 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_148
2022-10-24 15:18:47,121 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,125 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,126 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,126 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,127 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,127 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_154
2022-10-24 15:18:47,133 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_154
2022-10-24 15:18:47,135 - trainer - INFO - 
*****************[epoch: 154, global step: 155] eval training set at end of epoch***************
2022-10-24 15:18:47,135 - trainer - INFO - {
  "train_loss": 1.0426782369613647
}
2022-10-24 15:18:47,135 - trainer - INFO - start training epoch 155
2022-10-24 15:18:47,136 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,136 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,136 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,146 - trainer - INFO - 
*****************[epoch: 155, global step: 156] eval training set at end of epoch***************
2022-10-24 15:18:47,147 - trainer - INFO - {
  "train_loss": 1.0361653566360474
}
2022-10-24 15:18:47,147 - trainer - INFO - start training epoch 156
2022-10-24 15:18:47,147 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,148 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,148 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,157 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval training set based on eval_every=2***************
2022-10-24 15:18:47,158 - trainer - INFO - {
  "train_loss": 1.026066541671753
}
2022-10-24 15:18:47,167 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval development set based on eval_every=2***************
2022-10-24 15:18:47,167 - trainer - INFO - {
  "dev_loss": 1.0243778228759766,
  "dev_best_score_for_loss": -1.0243778228759766
}
2022-10-24 15:18:47,168 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,170 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,171 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,171 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_150
2022-10-24 15:18:47,173 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,177 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,177 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,178 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,178 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,179 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_156
2022-10-24 15:18:47,183 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_156
2022-10-24 15:18:47,184 - trainer - INFO - 
*****************[epoch: 156, global step: 157] eval training set at end of epoch***************
2022-10-24 15:18:47,184 - trainer - INFO - {
  "train_loss": 1.0159677267074585
}
2022-10-24 15:18:47,185 - trainer - INFO - start training epoch 157
2022-10-24 15:18:47,185 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,185 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,186 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,204 - trainer - INFO - 
*****************[epoch: 157, global step: 158] eval training set at end of epoch***************
2022-10-24 15:18:47,204 - trainer - INFO - {
  "train_loss": 1.0243779420852661
}
2022-10-24 15:18:47,205 - trainer - INFO - start training epoch 158
2022-10-24 15:18:47,206 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,206 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,207 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,216 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval training set based on eval_every=2***************
2022-10-24 15:18:47,216 - trainer - INFO - {
  "train_loss": 1.012705385684967
}
2022-10-24 15:18:47,222 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval development set based on eval_every=2***************
2022-10-24 15:18:47,223 - trainer - INFO - {
  "dev_loss": 1.0084643363952637,
  "dev_best_score_for_loss": -1.0084643363952637
}
2022-10-24 15:18:47,224 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,225 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,225 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,225 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_152
2022-10-24 15:18:47,227 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,230 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,230 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,231 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,232 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,232 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_158
2022-10-24 15:18:47,238 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_158
2022-10-24 15:18:47,239 - trainer - INFO - 
*****************[epoch: 158, global step: 159] eval training set at end of epoch***************
2022-10-24 15:18:47,239 - trainer - INFO - {
  "train_loss": 1.001032829284668
}
2022-10-24 15:18:47,239 - trainer - INFO - start training epoch 159
2022-10-24 15:18:47,240 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,240 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,241 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,249 - trainer - INFO - 
*****************[epoch: 159, global step: 160] eval training set at end of epoch***************
2022-10-24 15:18:47,250 - trainer - INFO - {
  "train_loss": 1.0084643363952637
}
2022-10-24 15:18:47,250 - trainer - INFO - start training epoch 160
2022-10-24 15:18:47,250 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,251 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,251 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,258 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval training set based on eval_every=2***************
2022-10-24 15:18:47,259 - trainer - INFO - {
  "train_loss": 0.9998579919338226
}
2022-10-24 15:18:47,266 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval development set based on eval_every=2***************
2022-10-24 15:18:47,266 - trainer - INFO - {
  "dev_loss": 0.9916562438011169,
  "dev_best_score_for_loss": -0.9916562438011169
}
2022-10-24 15:18:47,267 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,268 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,268 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,268 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_154
2022-10-24 15:18:47,270 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,273 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,273 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,273 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,274 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,274 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_160
2022-10-24 15:18:47,278 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_160
2022-10-24 15:18:47,280 - trainer - INFO - 
*****************[epoch: 160, global step: 161] eval training set at end of epoch***************
2022-10-24 15:18:47,283 - trainer - INFO - {
  "train_loss": 0.9912516474723816
}
2022-10-24 15:18:47,283 - trainer - INFO - start training epoch 161
2022-10-24 15:18:47,284 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,284 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,284 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,292 - trainer - INFO - 
*****************[epoch: 161, global step: 162] eval training set at end of epoch***************
2022-10-24 15:18:47,292 - trainer - INFO - {
  "train_loss": 0.9916563034057617
}
2022-10-24 15:18:47,293 - trainer - INFO - start training epoch 162
2022-10-24 15:18:47,293 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,293 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,294 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,302 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval training set based on eval_every=2***************
2022-10-24 15:18:47,303 - trainer - INFO - {
  "train_loss": 0.9883268773555756
}
2022-10-24 15:18:47,309 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval development set based on eval_every=2***************
2022-10-24 15:18:47,309 - trainer - INFO - {
  "dev_loss": 0.9779221415519714,
  "dev_best_score_for_loss": -0.9779221415519714
}
2022-10-24 15:18:47,311 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,312 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,313 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,313 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_156
2022-10-24 15:18:47,314 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,317 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,317 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,318 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,318 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,319 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_162
2022-10-24 15:18:47,323 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_162
2022-10-24 15:18:47,323 - trainer - INFO - 
*****************[epoch: 162, global step: 163] eval training set at end of epoch***************
2022-10-24 15:18:47,323 - trainer - INFO - {
  "train_loss": 0.9849974513053894
}
2022-10-24 15:18:47,324 - trainer - INFO - start training epoch 163
2022-10-24 15:18:47,324 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,325 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,328 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,339 - trainer - INFO - 
*****************[epoch: 163, global step: 164] eval training set at end of epoch***************
2022-10-24 15:18:47,340 - trainer - INFO - {
  "train_loss": 0.9779222011566162
}
2022-10-24 15:18:47,340 - trainer - INFO - start training epoch 164
2022-10-24 15:18:47,340 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,341 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,342 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,350 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval training set based on eval_every=2***************
2022-10-24 15:18:47,351 - trainer - INFO - {
  "train_loss": 0.9788525998592377
}
2022-10-24 15:18:47,357 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval development set based on eval_every=2***************
2022-10-24 15:18:47,357 - trainer - INFO - {
  "dev_loss": 0.9695818424224854,
  "dev_best_score_for_loss": -0.9695818424224854
}
2022-10-24 15:18:47,357 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,359 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,360 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,360 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_158
2022-10-24 15:18:47,361 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,365 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,365 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,365 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,366 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,366 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_164
2022-10-24 15:18:47,370 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_164
2022-10-24 15:18:47,371 - trainer - INFO - 
*****************[epoch: 164, global step: 165] eval training set at end of epoch***************
2022-10-24 15:18:47,371 - trainer - INFO - {
  "train_loss": 0.9797829985618591
}
2022-10-24 15:18:47,371 - trainer - INFO - start training epoch 165
2022-10-24 15:18:47,372 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,372 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,373 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,386 - trainer - INFO - 
*****************[epoch: 165, global step: 166] eval training set at end of epoch***************
2022-10-24 15:18:47,386 - trainer - INFO - {
  "train_loss": 0.9695817828178406
}
2022-10-24 15:18:47,387 - trainer - INFO - start training epoch 166
2022-10-24 15:18:47,388 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,388 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,388 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,397 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval training set based on eval_every=2***************
2022-10-24 15:18:47,398 - trainer - INFO - {
  "train_loss": 0.9719508290290833
}
2022-10-24 15:18:47,404 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval development set based on eval_every=2***************
2022-10-24 15:18:47,404 - trainer - INFO - {
  "dev_loss": 0.9665948152542114,
  "dev_best_score_for_loss": -0.9665948152542114
}
2022-10-24 15:18:47,404 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,406 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,406 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,406 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_160
2022-10-24 15:18:47,408 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,411 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,411 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,411 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,412 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,413 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_166
2022-10-24 15:18:47,417 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_166
2022-10-24 15:18:47,417 - trainer - INFO - 
*****************[epoch: 166, global step: 167] eval training set at end of epoch***************
2022-10-24 15:18:47,418 - trainer - INFO - {
  "train_loss": 0.9743198752403259
}
2022-10-24 15:18:47,419 - trainer - INFO - start training epoch 167
2022-10-24 15:18:47,420 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,424 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,424 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,435 - trainer - INFO - 
*****************[epoch: 167, global step: 168] eval training set at end of epoch***************
2022-10-24 15:18:47,436 - trainer - INFO - {
  "train_loss": 0.9665948152542114
}
2022-10-24 15:18:47,436 - trainer - INFO - start training epoch 168
2022-10-24 15:18:47,437 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,437 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,437 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,444 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval training set based on eval_every=2***************
2022-10-24 15:18:47,444 - trainer - INFO - {
  "train_loss": 0.9677793085575104
}
2022-10-24 15:18:47,450 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval development set based on eval_every=2***************
2022-10-24 15:18:47,450 - trainer - INFO - {
  "dev_loss": 0.9670134782791138,
  "dev_best_score_for_loss": -0.9665948152542114
}
2022-10-24 15:18:47,451 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:47,452 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,453 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,453 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,453 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_162
2022-10-24 15:18:47,454 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_168
2022-10-24 15:18:47,458 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_168
2022-10-24 15:18:47,459 - trainer - INFO - 
*****************[epoch: 168, global step: 169] eval training set at end of epoch***************
2022-10-24 15:18:47,459 - trainer - INFO - {
  "train_loss": 0.9689638018608093
}
2022-10-24 15:18:47,460 - trainer - INFO - start training epoch 169
2022-10-24 15:18:47,460 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,460 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,461 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,469 - trainer - INFO - 
*****************[epoch: 169, global step: 170] eval training set at end of epoch***************
2022-10-24 15:18:47,470 - trainer - INFO - {
  "train_loss": 0.9670133590698242
}
2022-10-24 15:18:47,470 - trainer - INFO - start training epoch 170
2022-10-24 15:18:47,470 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,470 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,471 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,479 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval training set based on eval_every=2***************
2022-10-24 15:18:47,479 - trainer - INFO - {
  "train_loss": 0.9661215245723724
}
2022-10-24 15:18:47,488 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval development set based on eval_every=2***************
2022-10-24 15:18:47,488 - trainer - INFO - {
  "dev_loss": 0.9680346250534058,
  "dev_best_score_for_loss": -0.9665948152542114
}
2022-10-24 15:18:47,489 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:47,489 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,491 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,491 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,491 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_164
2022-10-24 15:18:47,493 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_170
2022-10-24 15:18:47,498 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_170
2022-10-24 15:18:47,499 - trainer - INFO - 
*****************[epoch: 170, global step: 171] eval training set at end of epoch***************
2022-10-24 15:18:47,500 - trainer - INFO - {
  "train_loss": 0.9652296900749207
}
2022-10-24 15:18:47,500 - trainer - INFO - start training epoch 171
2022-10-24 15:18:47,500 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,500 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,501 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,509 - trainer - INFO - 
*****************[epoch: 171, global step: 172] eval training set at end of epoch***************
2022-10-24 15:18:47,510 - trainer - INFO - {
  "train_loss": 0.9680344462394714
}
2022-10-24 15:18:47,510 - trainer - INFO - start training epoch 172
2022-10-24 15:18:47,510 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,511 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,512 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,529 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval training set based on eval_every=2***************
2022-10-24 15:18:47,530 - trainer - INFO - {
  "train_loss": 0.9661874175071716
}
2022-10-24 15:18:47,538 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval development set based on eval_every=2***************
2022-10-24 15:18:47,538 - trainer - INFO - {
  "dev_loss": 0.9679985046386719,
  "dev_best_score_for_loss": -0.9665948152542114
}
2022-10-24 15:18:47,539 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:47,539 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,540 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,541 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,541 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_166
2022-10-24 15:18:47,543 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_172
2022-10-24 15:18:47,547 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_172
2022-10-24 15:18:47,548 - trainer - INFO - 
*****************[epoch: 172, global step: 173] eval training set at end of epoch***************
2022-10-24 15:18:47,549 - trainer - INFO - {
  "train_loss": 0.9643403887748718
}
2022-10-24 15:18:47,549 - trainer - INFO - start training epoch 173
2022-10-24 15:18:47,550 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,550 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,550 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,561 - trainer - INFO - 
*****************[epoch: 173, global step: 174] eval training set at end of epoch***************
2022-10-24 15:18:47,562 - trainer - INFO - {
  "train_loss": 0.9679985046386719
}
2022-10-24 15:18:47,566 - trainer - INFO - start training epoch 174
2022-10-24 15:18:47,566 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,567 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,567 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,576 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval training set based on eval_every=2***************
2022-10-24 15:18:47,577 - trainer - INFO - {
  "train_loss": 0.9668622016906738
}
2022-10-24 15:18:47,585 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval development set based on eval_every=2***************
2022-10-24 15:18:47,585 - trainer - INFO - {
  "dev_loss": 0.9671499729156494,
  "dev_best_score_for_loss": -0.9665948152542114
}
2022-10-24 15:18:47,586 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:47,586 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,588 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,588 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,588 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_168
2022-10-24 15:18:47,591 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_174
2022-10-24 15:18:47,597 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_174
2022-10-24 15:18:47,598 - trainer - INFO - 
*****************[epoch: 174, global step: 175] eval training set at end of epoch***************
2022-10-24 15:18:47,598 - trainer - INFO - {
  "train_loss": 0.9657258987426758
}
2022-10-24 15:18:47,598 - trainer - INFO - start training epoch 175
2022-10-24 15:18:47,598 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,599 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,599 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,611 - trainer - INFO - 
*****************[epoch: 175, global step: 176] eval training set at end of epoch***************
2022-10-24 15:18:47,612 - trainer - INFO - {
  "train_loss": 0.9671499729156494
}
2022-10-24 15:18:47,612 - trainer - INFO - start training epoch 176
2022-10-24 15:18:47,612 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,613 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,613 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,622 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval training set based on eval_every=2***************
2022-10-24 15:18:47,623 - trainer - INFO - {
  "train_loss": 0.9674507975578308
}
2022-10-24 15:18:47,633 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval development set based on eval_every=2***************
2022-10-24 15:18:47,633 - trainer - INFO - {
  "dev_loss": 0.966526985168457,
  "dev_best_score_for_loss": -0.966526985168457
}
2022-10-24 15:18:47,634 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,637 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,637 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,638 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_170
2022-10-24 15:18:47,640 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,644 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,644 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,645 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,646 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,646 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_176
2022-10-24 15:18:47,650 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_176
2022-10-24 15:18:47,652 - trainer - INFO - 
*****************[epoch: 176, global step: 177] eval training set at end of epoch***************
2022-10-24 15:18:47,653 - trainer - INFO - {
  "train_loss": 0.9677516222000122
}
2022-10-24 15:18:47,654 - trainer - INFO - start training epoch 177
2022-10-24 15:18:47,655 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,658 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,659 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,669 - trainer - INFO - 
*****************[epoch: 177, global step: 178] eval training set at end of epoch***************
2022-10-24 15:18:47,669 - trainer - INFO - {
  "train_loss": 0.966526985168457
}
2022-10-24 15:18:47,670 - trainer - INFO - start training epoch 178
2022-10-24 15:18:47,670 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,670 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,671 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,678 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval training set based on eval_every=2***************
2022-10-24 15:18:47,678 - trainer - INFO - {
  "train_loss": 0.9676171243190765
}
2022-10-24 15:18:47,686 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval development set based on eval_every=2***************
2022-10-24 15:18:47,686 - trainer - INFO - {
  "dev_loss": 0.9669124484062195,
  "dev_best_score_for_loss": -0.966526985168457
}
2022-10-24 15:18:47,687 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:47,687 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,688 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,689 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,689 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_172
2022-10-24 15:18:47,690 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_178
2022-10-24 15:18:47,695 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_178
2022-10-24 15:18:47,696 - trainer - INFO - 
*****************[epoch: 178, global step: 179] eval training set at end of epoch***************
2022-10-24 15:18:47,697 - trainer - INFO - {
  "train_loss": 0.968707263469696
}
2022-10-24 15:18:47,698 - trainer - INFO - start training epoch 179
2022-10-24 15:18:47,699 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,700 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,701 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,715 - trainer - INFO - 
*****************[epoch: 179, global step: 180] eval training set at end of epoch***************
2022-10-24 15:18:47,716 - trainer - INFO - {
  "train_loss": 0.9669123888015747
}
2022-10-24 15:18:47,717 - trainer - INFO - start training epoch 180
2022-10-24 15:18:47,717 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,717 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,718 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,726 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval training set based on eval_every=2***************
2022-10-24 15:18:47,727 - trainer - INFO - {
  "train_loss": 0.9675875902175903
}
2022-10-24 15:18:47,734 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval development set based on eval_every=2***************
2022-10-24 15:18:47,735 - trainer - INFO - {
  "dev_loss": 0.9678305983543396,
  "dev_best_score_for_loss": -0.966526985168457
}
2022-10-24 15:18:47,735 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:47,736 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,737 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,737 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,738 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_174
2022-10-24 15:18:47,739 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_180
2022-10-24 15:18:47,746 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_180
2022-10-24 15:18:47,747 - trainer - INFO - 
*****************[epoch: 180, global step: 181] eval training set at end of epoch***************
2022-10-24 15:18:47,751 - trainer - INFO - {
  "train_loss": 0.968262791633606
}
2022-10-24 15:18:47,751 - trainer - INFO - start training epoch 181
2022-10-24 15:18:47,751 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,752 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,752 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,763 - trainer - INFO - 
*****************[epoch: 181, global step: 182] eval training set at end of epoch***************
2022-10-24 15:18:47,763 - trainer - INFO - {
  "train_loss": 0.9678306579589844
}
2022-10-24 15:18:47,764 - trainer - INFO - start training epoch 182
2022-10-24 15:18:47,764 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,764 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,765 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,772 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval training set based on eval_every=2***************
2022-10-24 15:18:47,772 - trainer - INFO - {
  "train_loss": 0.9675552248954773
}
2022-10-24 15:18:47,779 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval development set based on eval_every=2***************
2022-10-24 15:18:47,779 - trainer - INFO - {
  "dev_loss": 0.9680819511413574,
  "dev_best_score_for_loss": -0.966526985168457
}
2022-10-24 15:18:47,780 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:47,780 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,781 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,781 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,782 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_176
2022-10-24 15:18:47,783 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_182
2022-10-24 15:18:47,787 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_182
2022-10-24 15:18:47,788 - trainer - INFO - 
*****************[epoch: 182, global step: 183] eval training set at end of epoch***************
2022-10-24 15:18:47,788 - trainer - INFO - {
  "train_loss": 0.9672797918319702
}
2022-10-24 15:18:47,788 - trainer - INFO - start training epoch 183
2022-10-24 15:18:47,789 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,789 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,789 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,802 - trainer - INFO - 
*****************[epoch: 183, global step: 184] eval training set at end of epoch***************
2022-10-24 15:18:47,802 - trainer - INFO - {
  "train_loss": 0.968082070350647
}
2022-10-24 15:18:47,803 - trainer - INFO - start training epoch 184
2022-10-24 15:18:47,803 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,803 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,804 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,812 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval training set based on eval_every=2***************
2022-10-24 15:18:47,813 - trainer - INFO - {
  "train_loss": 0.9674167931079865
}
2022-10-24 15:18:47,819 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval development set based on eval_every=2***************
2022-10-24 15:18:47,819 - trainer - INFO - {
  "dev_loss": 0.9673529863357544,
  "dev_best_score_for_loss": -0.966526985168457
}
2022-10-24 15:18:47,820 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:47,820 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,821 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,821 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,822 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_178
2022-10-24 15:18:47,823 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_184
2022-10-24 15:18:47,828 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_184
2022-10-24 15:18:47,828 - trainer - INFO - 
*****************[epoch: 184, global step: 185] eval training set at end of epoch***************
2022-10-24 15:18:47,829 - trainer - INFO - {
  "train_loss": 0.9667515158653259
}
2022-10-24 15:18:47,829 - trainer - INFO - start training epoch 185
2022-10-24 15:18:47,830 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,830 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,830 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,842 - trainer - INFO - 
*****************[epoch: 185, global step: 186] eval training set at end of epoch***************
2022-10-24 15:18:47,843 - trainer - INFO - {
  "train_loss": 0.9673530459403992
}
2022-10-24 15:18:47,843 - trainer - INFO - start training epoch 186
2022-10-24 15:18:47,843 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,843 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,844 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,851 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval training set based on eval_every=2***************
2022-10-24 15:18:47,851 - trainer - INFO - {
  "train_loss": 0.9670316278934479
}
2022-10-24 15:18:47,860 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval development set based on eval_every=2***************
2022-10-24 15:18:47,860 - trainer - INFO - {
  "dev_loss": 0.966209352016449,
  "dev_best_score_for_loss": -0.966209352016449
}
2022-10-24 15:18:47,861 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,862 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,862 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,863 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_180
2022-10-24 15:18:47,864 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,868 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,868 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,868 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,869 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,869 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_186
2022-10-24 15:18:47,873 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_186
2022-10-24 15:18:47,874 - trainer - INFO - 
*****************[epoch: 186, global step: 187] eval training set at end of epoch***************
2022-10-24 15:18:47,874 - trainer - INFO - {
  "train_loss": 0.9667102098464966
}
2022-10-24 15:18:47,875 - trainer - INFO - start training epoch 187
2022-10-24 15:18:47,875 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,875 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,876 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,884 - trainer - INFO - 
*****************[epoch: 187, global step: 188] eval training set at end of epoch***************
2022-10-24 15:18:47,885 - trainer - INFO - {
  "train_loss": 0.9662094116210938
}
2022-10-24 15:18:47,886 - trainer - INFO - start training epoch 188
2022-10-24 15:18:47,887 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,891 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,891 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,900 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval training set based on eval_every=2***************
2022-10-24 15:18:47,901 - trainer - INFO - {
  "train_loss": 0.9663326740264893
}
2022-10-24 15:18:47,907 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval development set based on eval_every=2***************
2022-10-24 15:18:47,907 - trainer - INFO - {
  "dev_loss": 0.9654755592346191,
  "dev_best_score_for_loss": -0.9654755592346191
}
2022-10-24 15:18:47,908 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,909 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,910 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,910 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_182
2022-10-24 15:18:47,911 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,916 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,916 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,917 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,918 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,918 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_188
2022-10-24 15:18:47,922 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_188
2022-10-24 15:18:47,923 - trainer - INFO - 
*****************[epoch: 188, global step: 189] eval training set at end of epoch***************
2022-10-24 15:18:47,923 - trainer - INFO - {
  "train_loss": 0.9664559364318848
}
2022-10-24 15:18:47,924 - trainer - INFO - start training epoch 189
2022-10-24 15:18:47,924 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,925 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,925 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,934 - trainer - INFO - 
*****************[epoch: 189, global step: 190] eval training set at end of epoch***************
2022-10-24 15:18:47,935 - trainer - INFO - {
  "train_loss": 0.9654755592346191
}
2022-10-24 15:18:47,935 - trainer - INFO - start training epoch 190
2022-10-24 15:18:47,935 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,936 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,936 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,946 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval training set based on eval_every=2***************
2022-10-24 15:18:47,947 - trainer - INFO - {
  "train_loss": 0.965613603591919
}
2022-10-24 15:18:47,954 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval development set based on eval_every=2***************
2022-10-24 15:18:47,954 - trainer - INFO - {
  "dev_loss": 0.9652276039123535,
  "dev_best_score_for_loss": -0.9652276039123535
}
2022-10-24 15:18:47,955 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:47,956 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:47,957 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:47,957 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_184
2022-10-24 15:18:47,958 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:47,963 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:47,964 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:47,965 - trainer - INFO -   patience: 200
2022-10-24 15:18:47,966 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:47,970 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_190
2022-10-24 15:18:47,975 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_190
2022-10-24 15:18:47,975 - trainer - INFO - 
*****************[epoch: 190, global step: 191] eval training set at end of epoch***************
2022-10-24 15:18:47,976 - trainer - INFO - {
  "train_loss": 0.9657516479492188
}
2022-10-24 15:18:47,977 - trainer - INFO - start training epoch 191
2022-10-24 15:18:47,978 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,978 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,978 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,986 - trainer - INFO - 
*****************[epoch: 191, global step: 192] eval training set at end of epoch***************
2022-10-24 15:18:47,987 - trainer - INFO - {
  "train_loss": 0.9652276039123535
}
2022-10-24 15:18:47,987 - trainer - INFO - start training epoch 192
2022-10-24 15:18:47,987 - trainer - INFO - training using device=cuda
2022-10-24 15:18:47,987 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:47,988 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:47,997 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval training set based on eval_every=2***************
2022-10-24 15:18:47,998 - trainer - INFO - {
  "train_loss": 0.9650580883026123
}
2022-10-24 15:18:48,004 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval development set based on eval_every=2***************
2022-10-24 15:18:48,004 - trainer - INFO - {
  "dev_loss": 0.9650610685348511,
  "dev_best_score_for_loss": -0.9650610685348511
}
2022-10-24 15:18:48,005 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:48,006 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,006 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,006 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_186
2022-10-24 15:18:48,009 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:48,015 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:48,015 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:48,016 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,017 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:48,017 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_192
2022-10-24 15:18:48,022 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_192
2022-10-24 15:18:48,023 - trainer - INFO - 
*****************[epoch: 192, global step: 193] eval training set at end of epoch***************
2022-10-24 15:18:48,024 - trainer - INFO - {
  "train_loss": 0.9648885726928711
}
2022-10-24 15:18:48,024 - trainer - INFO - start training epoch 193
2022-10-24 15:18:48,024 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,025 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,025 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,036 - trainer - INFO - 
*****************[epoch: 193, global step: 194] eval training set at end of epoch***************
2022-10-24 15:18:48,036 - trainer - INFO - {
  "train_loss": 0.9650610685348511
}
2022-10-24 15:18:48,037 - trainer - INFO - start training epoch 194
2022-10-24 15:18:48,037 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,038 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,040 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,048 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval training set based on eval_every=2***************
2022-10-24 15:18:48,049 - trainer - INFO - {
  "train_loss": 0.9647496938705444
}
2022-10-24 15:18:48,055 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval development set based on eval_every=2***************
2022-10-24 15:18:48,057 - trainer - INFO - {
  "dev_loss": 0.9646224975585938,
  "dev_best_score_for_loss": -0.9646224975585938
}
2022-10-24 15:18:48,060 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:48,062 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,063 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,063 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_188
2022-10-24 15:18:48,065 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:48,069 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:48,071 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:48,071 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,072 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:48,072 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_194
2022-10-24 15:18:48,078 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_194
2022-10-24 15:18:48,079 - trainer - INFO - 
*****************[epoch: 194, global step: 195] eval training set at end of epoch***************
2022-10-24 15:18:48,079 - trainer - INFO - {
  "train_loss": 0.9644383192062378
}
2022-10-24 15:18:48,079 - trainer - INFO - start training epoch 195
2022-10-24 15:18:48,080 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,080 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,080 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,088 - trainer - INFO - 
*****************[epoch: 195, global step: 196] eval training set at end of epoch***************
2022-10-24 15:18:48,088 - trainer - INFO - {
  "train_loss": 0.9646224975585938
}
2022-10-24 15:18:48,089 - trainer - INFO - start training epoch 196
2022-10-24 15:18:48,089 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,089 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,090 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,097 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval training set based on eval_every=2***************
2022-10-24 15:18:48,098 - trainer - INFO - {
  "train_loss": 0.9645028114318848
}
2022-10-24 15:18:48,106 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval development set based on eval_every=2***************
2022-10-24 15:18:48,107 - trainer - INFO - {
  "dev_loss": 0.9642020463943481,
  "dev_best_score_for_loss": -0.9642020463943481
}
2022-10-24 15:18:48,108 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:48,109 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,110 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,110 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_190
2022-10-24 15:18:48,112 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:48,116 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:48,117 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:48,117 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,118 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:48,118 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_196
2022-10-24 15:18:48,122 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_196
2022-10-24 15:18:48,122 - trainer - INFO - 
*****************[epoch: 196, global step: 197] eval training set at end of epoch***************
2022-10-24 15:18:48,123 - trainer - INFO - {
  "train_loss": 0.9643831253051758
}
2022-10-24 15:18:48,123 - trainer - INFO - start training epoch 197
2022-10-24 15:18:48,123 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,123 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,124 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,138 - trainer - INFO - 
*****************[epoch: 197, global step: 198] eval training set at end of epoch***************
2022-10-24 15:18:48,138 - trainer - INFO - {
  "train_loss": 0.9642019271850586
}
2022-10-24 15:18:48,139 - trainer - INFO - start training epoch 198
2022-10-24 15:18:48,139 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,139 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,140 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,148 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval training set based on eval_every=2***************
2022-10-24 15:18:48,148 - trainer - INFO - {
  "train_loss": 0.9642924070358276
}
2022-10-24 15:18:48,154 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval development set based on eval_every=2***************
2022-10-24 15:18:48,154 - trainer - INFO - {
  "dev_loss": 0.9641158580780029,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,155 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:48,156 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,156 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,156 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_192
2022-10-24 15:18:48,158 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:48,161 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:48,161 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:48,162 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,164 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:48,164 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_198
2022-10-24 15:18:48,168 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_198
2022-10-24 15:18:48,169 - trainer - INFO - 
*****************[epoch: 198, global step: 199] eval training set at end of epoch***************
2022-10-24 15:18:48,169 - trainer - INFO - {
  "train_loss": 0.9643828868865967
}
2022-10-24 15:18:48,170 - trainer - INFO - start training epoch 199
2022-10-24 15:18:48,170 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,170 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,171 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,185 - trainer - INFO - 
*****************[epoch: 199, global step: 200] eval training set at end of epoch***************
2022-10-24 15:18:48,185 - trainer - INFO - {
  "train_loss": 0.9641159772872925
}
2022-10-24 15:18:48,186 - trainer - INFO - start training epoch 200
2022-10-24 15:18:48,186 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,186 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,187 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,196 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval training set based on eval_every=2***************
2022-10-24 15:18:48,196 - trainer - INFO - {
  "train_loss": 0.9641643166542053
}
2022-10-24 15:18:48,203 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval development set based on eval_every=2***************
2022-10-24 15:18:48,203 - trainer - INFO - {
  "dev_loss": 0.9642653465270996,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,204 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:48,204 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,205 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,205 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,206 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_194
2022-10-24 15:18:48,207 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_200
2022-10-24 15:18:48,211 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_200
2022-10-24 15:18:48,212 - trainer - INFO - 
*****************[epoch: 200, global step: 201] eval training set at end of epoch***************
2022-10-24 15:18:48,212 - trainer - INFO - {
  "train_loss": 0.9642126560211182
}
2022-10-24 15:18:48,213 - trainer - INFO - start training epoch 201
2022-10-24 15:18:48,213 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,213 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,213 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,220 - trainer - INFO - 
*****************[epoch: 201, global step: 202] eval training set at end of epoch***************
2022-10-24 15:18:48,220 - trainer - INFO - {
  "train_loss": 0.9642652869224548
}
2022-10-24 15:18:48,220 - trainer - INFO - start training epoch 202
2022-10-24 15:18:48,221 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,221 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,221 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,231 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval training set based on eval_every=2***************
2022-10-24 15:18:48,231 - trainer - INFO - {
  "train_loss": 0.964150995016098
}
2022-10-24 15:18:48,239 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval development set based on eval_every=2***************
2022-10-24 15:18:48,239 - trainer - INFO - {
  "dev_loss": 0.9642940759658813,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,241 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:48,242 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,243 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,243 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,244 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_196
2022-10-24 15:18:48,245 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_202
2022-10-24 15:18:48,250 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_202
2022-10-24 15:18:48,251 - trainer - INFO - 
*****************[epoch: 202, global step: 203] eval training set at end of epoch***************
2022-10-24 15:18:48,251 - trainer - INFO - {
  "train_loss": 0.9640367031097412
}
2022-10-24 15:18:48,251 - trainer - INFO - start training epoch 203
2022-10-24 15:18:48,252 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,252 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,252 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,263 - trainer - INFO - 
*****************[epoch: 203, global step: 204] eval training set at end of epoch***************
2022-10-24 15:18:48,263 - trainer - INFO - {
  "train_loss": 0.9642940759658813
}
2022-10-24 15:18:48,264 - trainer - INFO - start training epoch 204
2022-10-24 15:18:48,264 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,264 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,265 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,273 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval training set based on eval_every=2***************
2022-10-24 15:18:48,276 - trainer - INFO - {
  "train_loss": 0.9642216265201569
}
2022-10-24 15:18:48,283 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval development set based on eval_every=2***************
2022-10-24 15:18:48,283 - trainer - INFO - {
  "dev_loss": 0.9642033576965332,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,284 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:48,284 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,286 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,287 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,288 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_198
2022-10-24 15:18:48,289 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_204
2022-10-24 15:18:48,294 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_204
2022-10-24 15:18:48,294 - trainer - INFO - 
*****************[epoch: 204, global step: 205] eval training set at end of epoch***************
2022-10-24 15:18:48,295 - trainer - INFO - {
  "train_loss": 0.9641491770744324
}
2022-10-24 15:18:48,295 - trainer - INFO - start training epoch 205
2022-10-24 15:18:48,296 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,296 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,296 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,307 - trainer - INFO - 
*****************[epoch: 205, global step: 206] eval training set at end of epoch***************
2022-10-24 15:18:48,307 - trainer - INFO - {
  "train_loss": 0.9642033576965332
}
2022-10-24 15:18:48,308 - trainer - INFO - start training epoch 206
2022-10-24 15:18:48,308 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,308 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,309 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,316 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval training set based on eval_every=2***************
2022-10-24 15:18:48,317 - trainer - INFO - {
  "train_loss": 0.9642441868782043
}
2022-10-24 15:18:48,323 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval development set based on eval_every=2***************
2022-10-24 15:18:48,324 - trainer - INFO - {
  "dev_loss": 0.9641578197479248,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,325 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:48,325 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,326 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,327 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,327 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_200
2022-10-24 15:18:48,329 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_206
2022-10-24 15:18:48,333 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_206
2022-10-24 15:18:48,335 - trainer - INFO - 
*****************[epoch: 206, global step: 207] eval training set at end of epoch***************
2022-10-24 15:18:48,337 - trainer - INFO - {
  "train_loss": 0.9642850160598755
}
2022-10-24 15:18:48,338 - trainer - INFO - start training epoch 207
2022-10-24 15:18:48,338 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,338 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,339 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,349 - trainer - INFO - 
*****************[epoch: 207, global step: 208] eval training set at end of epoch***************
2022-10-24 15:18:48,350 - trainer - INFO - {
  "train_loss": 0.9641578197479248
}
2022-10-24 15:18:48,350 - trainer - INFO - start training epoch 208
2022-10-24 15:18:48,350 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,351 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,351 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,357 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval training set based on eval_every=2***************
2022-10-24 15:18:48,357 - trainer - INFO - {
  "train_loss": 0.9642184972763062
}
2022-10-24 15:18:48,368 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval development set based on eval_every=2***************
2022-10-24 15:18:48,369 - trainer - INFO - {
  "dev_loss": 0.9642488956451416,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,370 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:18:48,370 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,371 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,372 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,372 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_202
2022-10-24 15:18:48,373 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_208
2022-10-24 15:18:48,379 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_208
2022-10-24 15:18:48,380 - trainer - INFO - 
*****************[epoch: 208, global step: 209] eval training set at end of epoch***************
2022-10-24 15:18:48,381 - trainer - INFO - {
  "train_loss": 0.9642791748046875
}
2022-10-24 15:18:48,381 - trainer - INFO - start training epoch 209
2022-10-24 15:18:48,382 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,382 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,382 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,393 - trainer - INFO - 
*****************[epoch: 209, global step: 210] eval training set at end of epoch***************
2022-10-24 15:18:48,393 - trainer - INFO - {
  "train_loss": 0.9642488956451416
}
2022-10-24 15:18:48,394 - trainer - INFO - start training epoch 210
2022-10-24 15:18:48,394 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,396 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,398 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,413 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval training set based on eval_every=2***************
2022-10-24 15:18:48,414 - trainer - INFO - {
  "train_loss": 0.9642370939254761
}
2022-10-24 15:18:48,421 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval development set based on eval_every=2***************
2022-10-24 15:18:48,422 - trainer - INFO - {
  "dev_loss": 0.9642585515975952,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,422 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:18:48,423 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,424 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,424 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,425 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_204
2022-10-24 15:18:48,427 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_210
2022-10-24 15:18:48,431 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_210
2022-10-24 15:18:48,432 - trainer - INFO - 
*****************[epoch: 210, global step: 211] eval training set at end of epoch***************
2022-10-24 15:18:48,433 - trainer - INFO - {
  "train_loss": 0.9642252922058105
}
2022-10-24 15:18:48,434 - trainer - INFO - start training epoch 211
2022-10-24 15:18:48,434 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,434 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,435 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,448 - trainer - INFO - 
*****************[epoch: 211, global step: 212] eval training set at end of epoch***************
2022-10-24 15:18:48,448 - trainer - INFO - {
  "train_loss": 0.96425861120224
}
2022-10-24 15:18:48,449 - trainer - INFO - start training epoch 212
2022-10-24 15:18:48,449 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,449 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,450 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,459 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval training set based on eval_every=2***************
2022-10-24 15:18:48,459 - trainer - INFO - {
  "train_loss": 0.9642383456230164
}
2022-10-24 15:18:48,467 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval development set based on eval_every=2***************
2022-10-24 15:18:48,468 - trainer - INFO - {
  "dev_loss": 0.964207649230957,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,468 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:18:48,469 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,470 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,470 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,471 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_206
2022-10-24 15:18:48,472 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_212
2022-10-24 15:18:48,477 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_212
2022-10-24 15:18:48,478 - trainer - INFO - 
*****************[epoch: 212, global step: 213] eval training set at end of epoch***************
2022-10-24 15:18:48,479 - trainer - INFO - {
  "train_loss": 0.9642180800437927
}
2022-10-24 15:18:48,479 - trainer - INFO - start training epoch 213
2022-10-24 15:18:48,479 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,479 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,480 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,489 - trainer - INFO - 
*****************[epoch: 213, global step: 214] eval training set at end of epoch***************
2022-10-24 15:18:48,491 - trainer - INFO - {
  "train_loss": 0.9642075896263123
}
2022-10-24 15:18:48,495 - trainer - INFO - start training epoch 214
2022-10-24 15:18:48,495 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,496 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,496 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,506 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval training set based on eval_every=2***************
2022-10-24 15:18:48,507 - trainer - INFO - {
  "train_loss": 0.9642240703105927
}
2022-10-24 15:18:48,515 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval development set based on eval_every=2***************
2022-10-24 15:18:48,515 - trainer - INFO - {
  "dev_loss": 0.9641416072845459,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,516 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:18:48,516 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,517 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,518 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,518 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_208
2022-10-24 15:18:48,519 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_214
2022-10-24 15:18:48,524 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_214
2022-10-24 15:18:48,524 - trainer - INFO - 
*****************[epoch: 214, global step: 215] eval training set at end of epoch***************
2022-10-24 15:18:48,525 - trainer - INFO - {
  "train_loss": 0.964240550994873
}
2022-10-24 15:18:48,525 - trainer - INFO - start training epoch 215
2022-10-24 15:18:48,526 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,526 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,526 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,535 - trainer - INFO - 
*****************[epoch: 215, global step: 216] eval training set at end of epoch***************
2022-10-24 15:18:48,536 - trainer - INFO - {
  "train_loss": 0.9641417264938354
}
2022-10-24 15:18:48,540 - trainer - INFO - start training epoch 216
2022-10-24 15:18:48,540 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,541 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,541 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,550 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval training set based on eval_every=2***************
2022-10-24 15:18:48,551 - trainer - INFO - {
  "train_loss": 0.9641916751861572
}
2022-10-24 15:18:48,556 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval development set based on eval_every=2***************
2022-10-24 15:18:48,557 - trainer - INFO - {
  "dev_loss": 0.9641591906547546,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,557 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:18:48,558 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,559 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,559 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,559 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_210
2022-10-24 15:18:48,561 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_216
2022-10-24 15:18:48,565 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_216
2022-10-24 15:18:48,567 - trainer - INFO - 
*****************[epoch: 216, global step: 217] eval training set at end of epoch***************
2022-10-24 15:18:48,568 - trainer - INFO - {
  "train_loss": 0.964241623878479
}
2022-10-24 15:18:48,568 - trainer - INFO - start training epoch 217
2022-10-24 15:18:48,571 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,571 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,571 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,580 - trainer - INFO - 
*****************[epoch: 217, global step: 218] eval training set at end of epoch***************
2022-10-24 15:18:48,580 - trainer - INFO - {
  "train_loss": 0.9641592502593994
}
2022-10-24 15:18:48,581 - trainer - INFO - start training epoch 218
2022-10-24 15:18:48,582 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,582 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,582 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,590 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval training set based on eval_every=2***************
2022-10-24 15:18:48,590 - trainer - INFO - {
  "train_loss": 0.9641568660736084
}
2022-10-24 15:18:48,599 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval development set based on eval_every=2***************
2022-10-24 15:18:48,600 - trainer - INFO - {
  "dev_loss": 0.9641453623771667,
  "dev_best_score_for_loss": -0.9641158580780029
}
2022-10-24 15:18:48,601 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:18:48,603 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,604 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,604 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,605 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_212
2022-10-24 15:18:48,606 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_218
2022-10-24 15:18:48,611 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_218
2022-10-24 15:18:48,613 - trainer - INFO - 
*****************[epoch: 218, global step: 219] eval training set at end of epoch***************
2022-10-24 15:18:48,614 - trainer - INFO - {
  "train_loss": 0.9641544818878174
}
2022-10-24 15:18:48,614 - trainer - INFO - start training epoch 219
2022-10-24 15:18:48,614 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,614 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,615 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,622 - trainer - INFO - 
*****************[epoch: 219, global step: 220] eval training set at end of epoch***************
2022-10-24 15:18:48,622 - trainer - INFO - {
  "train_loss": 0.9641454219818115
}
2022-10-24 15:18:48,623 - trainer - INFO - start training epoch 220
2022-10-24 15:18:48,623 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,623 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,623 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,631 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval training set based on eval_every=2***************
2022-10-24 15:18:48,632 - trainer - INFO - {
  "train_loss": 0.9641508460044861
}
2022-10-24 15:18:48,637 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval development set based on eval_every=2***************
2022-10-24 15:18:48,638 - trainer - INFO - {
  "dev_loss": 0.964110255241394,
  "dev_best_score_for_loss": -0.964110255241394
}
2022-10-24 15:18:48,638 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:48,640 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,640 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,640 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_214
2022-10-24 15:18:48,641 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:48,645 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:48,648 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:48,648 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,649 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:48,649 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_220
2022-10-24 15:18:48,654 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_220
2022-10-24 15:18:48,655 - trainer - INFO - 
*****************[epoch: 220, global step: 221] eval training set at end of epoch***************
2022-10-24 15:18:48,655 - trainer - INFO - {
  "train_loss": 0.9641562700271606
}
2022-10-24 15:18:48,656 - trainer - INFO - start training epoch 221
2022-10-24 15:18:48,656 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,656 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,656 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,667 - trainer - INFO - 
*****************[epoch: 221, global step: 222] eval training set at end of epoch***************
2022-10-24 15:18:48,667 - trainer - INFO - {
  "train_loss": 0.964110255241394
}
2022-10-24 15:18:48,667 - trainer - INFO - start training epoch 222
2022-10-24 15:18:48,667 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,668 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,668 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,675 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval training set based on eval_every=2***************
2022-10-24 15:18:48,676 - trainer - INFO - {
  "train_loss": 0.9641114473342896
}
2022-10-24 15:18:48,682 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval development set based on eval_every=2***************
2022-10-24 15:18:48,682 - trainer - INFO - {
  "dev_loss": 0.9640836715698242,
  "dev_best_score_for_loss": -0.9640836715698242
}
2022-10-24 15:18:48,683 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:48,684 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,684 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,684 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_216
2022-10-24 15:18:48,685 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:48,688 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:48,688 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:48,689 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,690 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:48,694 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_222
2022-10-24 15:18:48,699 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_222
2022-10-24 15:18:48,700 - trainer - INFO - 
*****************[epoch: 222, global step: 223] eval training set at end of epoch***************
2022-10-24 15:18:48,700 - trainer - INFO - {
  "train_loss": 0.9641126394271851
}
2022-10-24 15:18:48,700 - trainer - INFO - start training epoch 223
2022-10-24 15:18:48,701 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,701 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,701 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,711 - trainer - INFO - 
*****************[epoch: 223, global step: 224] eval training set at end of epoch***************
2022-10-24 15:18:48,711 - trainer - INFO - {
  "train_loss": 0.9640836715698242
}
2022-10-24 15:18:48,712 - trainer - INFO - start training epoch 224
2022-10-24 15:18:48,712 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,712 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,713 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,722 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval training set based on eval_every=2***************
2022-10-24 15:18:48,722 - trainer - INFO - {
  "train_loss": 0.9640671014785767
}
2022-10-24 15:18:48,734 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval development set based on eval_every=2***************
2022-10-24 15:18:48,734 - trainer - INFO - {
  "dev_loss": 0.9640560150146484,
  "dev_best_score_for_loss": -0.9640560150146484
}
2022-10-24 15:18:48,735 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:48,737 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,738 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,738 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_218
2022-10-24 15:18:48,740 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:48,744 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:48,744 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:48,744 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,745 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:48,746 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_224
2022-10-24 15:18:48,751 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_224
2022-10-24 15:18:48,753 - trainer - INFO - 
*****************[epoch: 224, global step: 225] eval training set at end of epoch***************
2022-10-24 15:18:48,753 - trainer - INFO - {
  "train_loss": 0.9640505313873291
}
2022-10-24 15:18:48,753 - trainer - INFO - start training epoch 225
2022-10-24 15:18:48,754 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,754 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,754 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,764 - trainer - INFO - 
*****************[epoch: 225, global step: 226] eval training set at end of epoch***************
2022-10-24 15:18:48,764 - trainer - INFO - {
  "train_loss": 0.9640560150146484
}
2022-10-24 15:18:48,765 - trainer - INFO - start training epoch 226
2022-10-24 15:18:48,765 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,766 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,766 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,779 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval training set based on eval_every=2***************
2022-10-24 15:18:48,780 - trainer - INFO - {
  "train_loss": 0.964063972234726
}
2022-10-24 15:18:48,788 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval development set based on eval_every=2***************
2022-10-24 15:18:48,789 - trainer - INFO - {
  "dev_loss": 0.9640594720840454,
  "dev_best_score_for_loss": -0.9640560150146484
}
2022-10-24 15:18:48,789 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:48,790 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,791 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,791 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,791 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_220
2022-10-24 15:18:48,793 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_226
2022-10-24 15:18:48,800 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_226
2022-10-24 15:18:48,801 - trainer - INFO - 
*****************[epoch: 226, global step: 227] eval training set at end of epoch***************
2022-10-24 15:18:48,801 - trainer - INFO - {
  "train_loss": 0.9640719294548035
}
2022-10-24 15:18:48,802 - trainer - INFO - start training epoch 227
2022-10-24 15:18:48,802 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,802 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,803 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,811 - trainer - INFO - 
*****************[epoch: 227, global step: 228] eval training set at end of epoch***************
2022-10-24 15:18:48,812 - trainer - INFO - {
  "train_loss": 0.9640593528747559
}
2022-10-24 15:18:48,812 - trainer - INFO - start training epoch 228
2022-10-24 15:18:48,813 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,814 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,814 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,825 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval training set based on eval_every=2***************
2022-10-24 15:18:48,826 - trainer - INFO - {
  "train_loss": 0.9640785157680511
}
2022-10-24 15:18:48,836 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval development set based on eval_every=2***************
2022-10-24 15:18:48,837 - trainer - INFO - {
  "dev_loss": 0.964028537273407,
  "dev_best_score_for_loss": -0.964028537273407
}
2022-10-24 15:18:48,838 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:48,839 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,839 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,840 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_222
2022-10-24 15:18:48,842 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:48,845 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:48,846 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:48,846 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,847 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:48,847 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_228
2022-10-24 15:18:48,852 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_228
2022-10-24 15:18:48,853 - trainer - INFO - 
*****************[epoch: 228, global step: 229] eval training set at end of epoch***************
2022-10-24 15:18:48,853 - trainer - INFO - {
  "train_loss": 0.9640976786613464
}
2022-10-24 15:18:48,854 - trainer - INFO - start training epoch 229
2022-10-24 15:18:48,854 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,854 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,854 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,865 - trainer - INFO - 
*****************[epoch: 229, global step: 230] eval training set at end of epoch***************
2022-10-24 15:18:48,866 - trainer - INFO - {
  "train_loss": 0.9640285968780518
}
2022-10-24 15:18:48,867 - trainer - INFO - start training epoch 230
2022-10-24 15:18:48,867 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,867 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,868 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,877 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval training set based on eval_every=2***************
2022-10-24 15:18:48,877 - trainer - INFO - {
  "train_loss": 0.9640507698059082
}
2022-10-24 15:18:48,883 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval development set based on eval_every=2***************
2022-10-24 15:18:48,883 - trainer - INFO - {
  "dev_loss": 0.9640347957611084,
  "dev_best_score_for_loss": -0.964028537273407
}
2022-10-24 15:18:48,884 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:48,884 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,885 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,885 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,886 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_224
2022-10-24 15:18:48,887 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_230
2022-10-24 15:18:48,892 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_230
2022-10-24 15:18:48,895 - trainer - INFO - 
*****************[epoch: 230, global step: 231] eval training set at end of epoch***************
2022-10-24 15:18:48,898 - trainer - INFO - {
  "train_loss": 0.9640729427337646
}
2022-10-24 15:18:48,899 - trainer - INFO - start training epoch 231
2022-10-24 15:18:48,900 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,900 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,900 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,910 - trainer - INFO - 
*****************[epoch: 231, global step: 232] eval training set at end of epoch***************
2022-10-24 15:18:48,911 - trainer - INFO - {
  "train_loss": 0.9640347957611084
}
2022-10-24 15:18:48,911 - trainer - INFO - start training epoch 232
2022-10-24 15:18:48,911 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,912 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,912 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,920 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval training set based on eval_every=2***************
2022-10-24 15:18:48,920 - trainer - INFO - {
  "train_loss": 0.9640767276287079
}
2022-10-24 15:18:48,929 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval development set based on eval_every=2***************
2022-10-24 15:18:48,930 - trainer - INFO - {
  "dev_loss": 0.964078426361084,
  "dev_best_score_for_loss": -0.964028537273407
}
2022-10-24 15:18:48,931 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:48,931 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,933 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,933 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,934 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_226
2022-10-24 15:18:48,935 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_232
2022-10-24 15:18:48,941 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_232
2022-10-24 15:18:48,945 - trainer - INFO - 
*****************[epoch: 232, global step: 233] eval training set at end of epoch***************
2022-10-24 15:18:48,946 - trainer - INFO - {
  "train_loss": 0.9641186594963074
}
2022-10-24 15:18:48,946 - trainer - INFO - start training epoch 233
2022-10-24 15:18:48,947 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,947 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,947 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,958 - trainer - INFO - 
*****************[epoch: 233, global step: 234] eval training set at end of epoch***************
2022-10-24 15:18:48,959 - trainer - INFO - {
  "train_loss": 0.964078426361084
}
2022-10-24 15:18:48,960 - trainer - INFO - start training epoch 234
2022-10-24 15:18:48,960 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,961 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,961 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:48,969 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval training set based on eval_every=2***************
2022-10-24 15:18:48,969 - trainer - INFO - {
  "train_loss": 0.9641014933586121
}
2022-10-24 15:18:48,975 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval development set based on eval_every=2***************
2022-10-24 15:18:48,976 - trainer - INFO - {
  "dev_loss": 0.9640750885009766,
  "dev_best_score_for_loss": -0.964028537273407
}
2022-10-24 15:18:48,976 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:48,977 - trainer - INFO -   patience: 200
2022-10-24 15:18:48,978 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:48,978 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:48,978 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_228
2022-10-24 15:18:48,980 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_234
2022-10-24 15:18:48,985 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_234
2022-10-24 15:18:48,988 - trainer - INFO - 
*****************[epoch: 234, global step: 235] eval training set at end of epoch***************
2022-10-24 15:18:48,989 - trainer - INFO - {
  "train_loss": 0.9641245603561401
}
2022-10-24 15:18:48,989 - trainer - INFO - start training epoch 235
2022-10-24 15:18:48,989 - trainer - INFO - training using device=cuda
2022-10-24 15:18:48,990 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:48,990 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,001 - trainer - INFO - 
*****************[epoch: 235, global step: 236] eval training set at end of epoch***************
2022-10-24 15:18:49,002 - trainer - INFO - {
  "train_loss": 0.9640750288963318
}
2022-10-24 15:18:49,003 - trainer - INFO - start training epoch 236
2022-10-24 15:18:49,003 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,003 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,004 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,010 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval training set based on eval_every=2***************
2022-10-24 15:18:49,010 - trainer - INFO - {
  "train_loss": 0.9640530943870544
}
2022-10-24 15:18:49,016 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval development set based on eval_every=2***************
2022-10-24 15:18:49,017 - trainer - INFO - {
  "dev_loss": 0.9640726447105408,
  "dev_best_score_for_loss": -0.964028537273407
}
2022-10-24 15:18:49,018 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:49,021 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,022 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,022 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,022 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_230
2022-10-24 15:18:49,024 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_236
2022-10-24 15:18:49,029 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_236
2022-10-24 15:18:49,029 - trainer - INFO - 
*****************[epoch: 236, global step: 237] eval training set at end of epoch***************
2022-10-24 15:18:49,030 - trainer - INFO - {
  "train_loss": 0.9640311598777771
}
2022-10-24 15:18:49,032 - trainer - INFO - start training epoch 237
2022-10-24 15:18:49,032 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,032 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,033 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,040 - trainer - INFO - 
*****************[epoch: 237, global step: 238] eval training set at end of epoch***************
2022-10-24 15:18:49,041 - trainer - INFO - {
  "train_loss": 0.9640727043151855
}
2022-10-24 15:18:49,041 - trainer - INFO - start training epoch 238
2022-10-24 15:18:49,041 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,042 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,042 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,048 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval training set based on eval_every=2***************
2022-10-24 15:18:49,049 - trainer - INFO - {
  "train_loss": 0.9640706479549408
}
2022-10-24 15:18:49,054 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval development set based on eval_every=2***************
2022-10-24 15:18:49,054 - trainer - INFO - {
  "dev_loss": 0.9640846252441406,
  "dev_best_score_for_loss": -0.964028537273407
}
2022-10-24 15:18:49,054 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:18:49,055 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,056 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,056 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,056 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_232
2022-10-24 15:18:49,057 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_238
2022-10-24 15:18:49,062 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_238
2022-10-24 15:18:49,064 - trainer - INFO - 
*****************[epoch: 238, global step: 239] eval training set at end of epoch***************
2022-10-24 15:18:49,067 - trainer - INFO - {
  "train_loss": 0.964068591594696
}
2022-10-24 15:18:49,068 - trainer - INFO - start training epoch 239
2022-10-24 15:18:49,068 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,068 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,069 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,079 - trainer - INFO - 
*****************[epoch: 239, global step: 240] eval training set at end of epoch***************
2022-10-24 15:18:49,079 - trainer - INFO - {
  "train_loss": 0.9640845656394958
}
2022-10-24 15:18:49,079 - trainer - INFO - start training epoch 240
2022-10-24 15:18:49,080 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,080 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,081 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,087 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval training set based on eval_every=2***************
2022-10-24 15:18:49,088 - trainer - INFO - {
  "train_loss": 0.9640844762325287
}
2022-10-24 15:18:49,094 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval development set based on eval_every=2***************
2022-10-24 15:18:49,095 - trainer - INFO - {
  "dev_loss": 0.9640676379203796,
  "dev_best_score_for_loss": -0.964028537273407
}
2022-10-24 15:18:49,095 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:18:49,096 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,097 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,097 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,097 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_234
2022-10-24 15:18:49,098 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_240
2022-10-24 15:18:49,102 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_240
2022-10-24 15:18:49,103 - trainer - INFO - 
*****************[epoch: 240, global step: 241] eval training set at end of epoch***************
2022-10-24 15:18:49,103 - trainer - INFO - {
  "train_loss": 0.9640843868255615
}
2022-10-24 15:18:49,104 - trainer - INFO - start training epoch 241
2022-10-24 15:18:49,104 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,104 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,105 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,116 - trainer - INFO - 
*****************[epoch: 241, global step: 242] eval training set at end of epoch***************
2022-10-24 15:18:49,117 - trainer - INFO - {
  "train_loss": 0.9640676975250244
}
2022-10-24 15:18:49,117 - trainer - INFO - start training epoch 242
2022-10-24 15:18:49,117 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,118 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,118 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,127 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval training set based on eval_every=2***************
2022-10-24 15:18:49,128 - trainer - INFO - {
  "train_loss": 0.9640798270702362
}
2022-10-24 15:18:49,136 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval development set based on eval_every=2***************
2022-10-24 15:18:49,136 - trainer - INFO - {
  "dev_loss": 0.9641022086143494,
  "dev_best_score_for_loss": -0.964028537273407
}
2022-10-24 15:18:49,137 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:18:49,137 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,138 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,139 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,139 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_236
2022-10-24 15:18:49,140 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_242
2022-10-24 15:18:49,145 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_242
2022-10-24 15:18:49,145 - trainer - INFO - 
*****************[epoch: 242, global step: 243] eval training set at end of epoch***************
2022-10-24 15:18:49,146 - trainer - INFO - {
  "train_loss": 0.964091956615448
}
2022-10-24 15:18:49,146 - trainer - INFO - start training epoch 243
2022-10-24 15:18:49,146 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,146 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,147 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,153 - trainer - INFO - 
*****************[epoch: 243, global step: 244] eval training set at end of epoch***************
2022-10-24 15:18:49,154 - trainer - INFO - {
  "train_loss": 0.9641022086143494
}
2022-10-24 15:18:49,154 - trainer - INFO - start training epoch 244
2022-10-24 15:18:49,155 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,156 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,157 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,168 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval training set based on eval_every=2***************
2022-10-24 15:18:49,168 - trainer - INFO - {
  "train_loss": 0.964090496301651
}
2022-10-24 15:18:49,176 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval development set based on eval_every=2***************
2022-10-24 15:18:49,177 - trainer - INFO - {
  "dev_loss": 0.9641207456588745,
  "dev_best_score_for_loss": -0.964028537273407
}
2022-10-24 15:18:49,177 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:18:49,178 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,179 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,179 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,179 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_238
2022-10-24 15:18:49,181 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_244
2022-10-24 15:18:49,187 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_244
2022-10-24 15:18:49,188 - trainer - INFO - 
*****************[epoch: 244, global step: 245] eval training set at end of epoch***************
2022-10-24 15:18:49,188 - trainer - INFO - {
  "train_loss": 0.9640787839889526
}
2022-10-24 15:18:49,188 - trainer - INFO - start training epoch 245
2022-10-24 15:18:49,189 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,189 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,189 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,199 - trainer - INFO - 
*****************[epoch: 245, global step: 246] eval training set at end of epoch***************
2022-10-24 15:18:49,199 - trainer - INFO - {
  "train_loss": 0.9641208648681641
}
2022-10-24 15:18:49,200 - trainer - INFO - start training epoch 246
2022-10-24 15:18:49,200 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,200 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,201 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,215 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval training set based on eval_every=2***************
2022-10-24 15:18:49,217 - trainer - INFO - {
  "train_loss": 0.9641134440898895
}
2022-10-24 15:18:49,226 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval development set based on eval_every=2***************
2022-10-24 15:18:49,226 - trainer - INFO - {
  "dev_loss": 0.9640192985534668,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,227 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:49,228 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,229 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,229 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_240
2022-10-24 15:18:49,230 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:49,234 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:49,235 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:49,235 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,236 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:49,236 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_246
2022-10-24 15:18:49,241 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_246
2022-10-24 15:18:49,242 - trainer - INFO - 
*****************[epoch: 246, global step: 247] eval training set at end of epoch***************
2022-10-24 15:18:49,242 - trainer - INFO - {
  "train_loss": 0.964106023311615
}
2022-10-24 15:18:49,243 - trainer - INFO - start training epoch 247
2022-10-24 15:18:49,244 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,244 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,244 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,252 - trainer - INFO - 
*****************[epoch: 247, global step: 248] eval training set at end of epoch***************
2022-10-24 15:18:49,252 - trainer - INFO - {
  "train_loss": 0.9640192985534668
}
2022-10-24 15:18:49,253 - trainer - INFO - start training epoch 248
2022-10-24 15:18:49,253 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,253 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,254 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,264 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval training set based on eval_every=2***************
2022-10-24 15:18:49,264 - trainer - INFO - {
  "train_loss": 0.9640412926673889
}
2022-10-24 15:18:49,271 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval development set based on eval_every=2***************
2022-10-24 15:18:49,272 - trainer - INFO - {
  "dev_loss": 0.964087963104248,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,272 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:49,272 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,273 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,274 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,274 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_242
2022-10-24 15:18:49,276 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_248
2022-10-24 15:18:49,280 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_248
2022-10-24 15:18:49,281 - trainer - INFO - 
*****************[epoch: 248, global step: 249] eval training set at end of epoch***************
2022-10-24 15:18:49,281 - trainer - INFO - {
  "train_loss": 0.964063286781311
}
2022-10-24 15:18:49,281 - trainer - INFO - start training epoch 249
2022-10-24 15:18:49,281 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,282 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,282 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,289 - trainer - INFO - 
*****************[epoch: 249, global step: 250] eval training set at end of epoch***************
2022-10-24 15:18:49,290 - trainer - INFO - {
  "train_loss": 0.9640879034996033
}
2022-10-24 15:18:49,290 - trainer - INFO - start training epoch 250
2022-10-24 15:18:49,290 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,290 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,291 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,298 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval training set based on eval_every=2***************
2022-10-24 15:18:49,298 - trainer - INFO - {
  "train_loss": 0.9640873074531555
}
2022-10-24 15:18:49,305 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval development set based on eval_every=2***************
2022-10-24 15:18:49,305 - trainer - INFO - {
  "dev_loss": 0.9640597701072693,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,306 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:49,306 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,307 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,308 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,308 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_244
2022-10-24 15:18:49,311 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_250
2022-10-24 15:18:49,316 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_250
2022-10-24 15:18:49,317 - trainer - INFO - 
*****************[epoch: 250, global step: 251] eval training set at end of epoch***************
2022-10-24 15:18:49,318 - trainer - INFO - {
  "train_loss": 0.9640867114067078
}
2022-10-24 15:18:49,318 - trainer - INFO - start training epoch 251
2022-10-24 15:18:49,318 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,319 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,319 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,328 - trainer - INFO - 
*****************[epoch: 251, global step: 252] eval training set at end of epoch***************
2022-10-24 15:18:49,329 - trainer - INFO - {
  "train_loss": 0.9640597701072693
}
2022-10-24 15:18:49,330 - trainer - INFO - start training epoch 252
2022-10-24 15:18:49,330 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,330 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,331 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,339 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval training set based on eval_every=2***************
2022-10-24 15:18:49,339 - trainer - INFO - {
  "train_loss": 0.9640508592128754
}
2022-10-24 15:18:49,349 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval development set based on eval_every=2***************
2022-10-24 15:18:49,349 - trainer - INFO - {
  "dev_loss": 0.9640766382217407,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,350 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:49,350 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,352 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,352 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,352 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_246
2022-10-24 15:18:49,354 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_252
2022-10-24 15:18:49,361 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_252
2022-10-24 15:18:49,362 - trainer - INFO - 
*****************[epoch: 252, global step: 253] eval training set at end of epoch***************
2022-10-24 15:18:49,363 - trainer - INFO - {
  "train_loss": 0.9640419483184814
}
2022-10-24 15:18:49,364 - trainer - INFO - start training epoch 253
2022-10-24 15:18:49,364 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,364 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,365 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,373 - trainer - INFO - 
*****************[epoch: 253, global step: 254] eval training set at end of epoch***************
2022-10-24 15:18:49,373 - trainer - INFO - {
  "train_loss": 0.9640766382217407
}
2022-10-24 15:18:49,373 - trainer - INFO - start training epoch 254
2022-10-24 15:18:49,373 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,374 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,374 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,381 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval training set based on eval_every=2***************
2022-10-24 15:18:49,381 - trainer - INFO - {
  "train_loss": 0.964056134223938
}
2022-10-24 15:18:49,386 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval development set based on eval_every=2***************
2022-10-24 15:18:49,388 - trainer - INFO - {
  "dev_loss": 0.9640425443649292,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,390 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:49,393 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,395 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,395 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,395 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_248
2022-10-24 15:18:49,397 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_254
2022-10-24 15:18:49,402 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_254
2022-10-24 15:18:49,403 - trainer - INFO - 
*****************[epoch: 254, global step: 255] eval training set at end of epoch***************
2022-10-24 15:18:49,404 - trainer - INFO - {
  "train_loss": 0.9640356302261353
}
2022-10-24 15:18:49,404 - trainer - INFO - start training epoch 255
2022-10-24 15:18:49,405 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,405 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,405 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,414 - trainer - INFO - 
*****************[epoch: 255, global step: 256] eval training set at end of epoch***************
2022-10-24 15:18:49,414 - trainer - INFO - {
  "train_loss": 0.9640425443649292
}
2022-10-24 15:18:49,415 - trainer - INFO - start training epoch 256
2022-10-24 15:18:49,415 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,415 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,415 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,423 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval training set based on eval_every=2***************
2022-10-24 15:18:49,423 - trainer - INFO - {
  "train_loss": 0.964053750038147
}
2022-10-24 15:18:49,431 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval development set based on eval_every=2***************
2022-10-24 15:18:49,431 - trainer - INFO - {
  "dev_loss": 0.9640824198722839,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,432 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:18:49,433 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,434 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,435 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,436 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_250
2022-10-24 15:18:49,438 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_256
2022-10-24 15:18:49,445 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_256
2022-10-24 15:18:49,446 - trainer - INFO - 
*****************[epoch: 256, global step: 257] eval training set at end of epoch***************
2022-10-24 15:18:49,446 - trainer - INFO - {
  "train_loss": 0.9640649557113647
}
2022-10-24 15:18:49,446 - trainer - INFO - start training epoch 257
2022-10-24 15:18:49,446 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,447 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,447 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,456 - trainer - INFO - 
*****************[epoch: 257, global step: 258] eval training set at end of epoch***************
2022-10-24 15:18:49,456 - trainer - INFO - {
  "train_loss": 0.9640824794769287
}
2022-10-24 15:18:49,457 - trainer - INFO - start training epoch 258
2022-10-24 15:18:49,457 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,457 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,458 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,465 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval training set based on eval_every=2***************
2022-10-24 15:18:49,465 - trainer - INFO - {
  "train_loss": 0.9640790224075317
}
2022-10-24 15:18:49,471 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval development set based on eval_every=2***************
2022-10-24 15:18:49,472 - trainer - INFO - {
  "dev_loss": 0.9640724658966064,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,472 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:18:49,473 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,474 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,474 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,474 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_252
2022-10-24 15:18:49,476 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_258
2022-10-24 15:18:49,480 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_258
2022-10-24 15:18:49,481 - trainer - INFO - 
*****************[epoch: 258, global step: 259] eval training set at end of epoch***************
2022-10-24 15:18:49,483 - trainer - INFO - {
  "train_loss": 0.9640755653381348
}
2022-10-24 15:18:49,488 - trainer - INFO - start training epoch 259
2022-10-24 15:18:49,488 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,488 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,489 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,499 - trainer - INFO - 
*****************[epoch: 259, global step: 260] eval training set at end of epoch***************
2022-10-24 15:18:49,499 - trainer - INFO - {
  "train_loss": 0.9640724062919617
}
2022-10-24 15:18:49,500 - trainer - INFO - start training epoch 260
2022-10-24 15:18:49,500 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,500 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,500 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,507 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval training set based on eval_every=2***************
2022-10-24 15:18:49,507 - trainer - INFO - {
  "train_loss": 0.9640588164329529
}
2022-10-24 15:18:49,513 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval development set based on eval_every=2***************
2022-10-24 15:18:49,513 - trainer - INFO - {
  "dev_loss": 0.9640798568725586,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,514 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:18:49,514 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,515 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,515 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,515 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_254
2022-10-24 15:18:49,517 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_260
2022-10-24 15:18:49,521 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_260
2022-10-24 15:18:49,521 - trainer - INFO - 
*****************[epoch: 260, global step: 261] eval training set at end of epoch***************
2022-10-24 15:18:49,522 - trainer - INFO - {
  "train_loss": 0.9640452265739441
}
2022-10-24 15:18:49,522 - trainer - INFO - start training epoch 261
2022-10-24 15:18:49,522 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,522 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,522 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,534 - trainer - INFO - 
*****************[epoch: 261, global step: 262] eval training set at end of epoch***************
2022-10-24 15:18:49,534 - trainer - INFO - {
  "train_loss": 0.9640798568725586
}
2022-10-24 15:18:49,535 - trainer - INFO - start training epoch 262
2022-10-24 15:18:49,535 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,535 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,536 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,544 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval training set based on eval_every=2***************
2022-10-24 15:18:49,544 - trainer - INFO - {
  "train_loss": 0.9640605449676514
}
2022-10-24 15:18:49,552 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval development set based on eval_every=2***************
2022-10-24 15:18:49,552 - trainer - INFO - {
  "dev_loss": 0.9640264511108398,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,553 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:18:49,554 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,555 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,555 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,555 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_256
2022-10-24 15:18:49,556 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_262
2022-10-24 15:18:49,561 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_262
2022-10-24 15:18:49,562 - trainer - INFO - 
*****************[epoch: 262, global step: 263] eval training set at end of epoch***************
2022-10-24 15:18:49,562 - trainer - INFO - {
  "train_loss": 0.9640412330627441
}
2022-10-24 15:18:49,563 - trainer - INFO - start training epoch 263
2022-10-24 15:18:49,563 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,563 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,564 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,572 - trainer - INFO - 
*****************[epoch: 263, global step: 264] eval training set at end of epoch***************
2022-10-24 15:18:49,572 - trainer - INFO - {
  "train_loss": 0.9640264511108398
}
2022-10-24 15:18:49,572 - trainer - INFO - start training epoch 264
2022-10-24 15:18:49,573 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,573 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,574 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,586 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval training set based on eval_every=2***************
2022-10-24 15:18:49,586 - trainer - INFO - {
  "train_loss": 0.964031457901001
}
2022-10-24 15:18:49,594 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval development set based on eval_every=2***************
2022-10-24 15:18:49,595 - trainer - INFO - {
  "dev_loss": 0.9641095995903015,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,596 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:18:49,596 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,598 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,598 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,599 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_258
2022-10-24 15:18:49,601 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_264
2022-10-24 15:18:49,605 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_264
2022-10-24 15:18:49,606 - trainer - INFO - 
*****************[epoch: 264, global step: 265] eval training set at end of epoch***************
2022-10-24 15:18:49,606 - trainer - INFO - {
  "train_loss": 0.9640364646911621
}
2022-10-24 15:18:49,607 - trainer - INFO - start training epoch 265
2022-10-24 15:18:49,607 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,608 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,608 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,615 - trainer - INFO - 
*****************[epoch: 265, global step: 266] eval training set at end of epoch***************
2022-10-24 15:18:49,616 - trainer - INFO - {
  "train_loss": 0.9641095995903015
}
2022-10-24 15:18:49,616 - trainer - INFO - start training epoch 266
2022-10-24 15:18:49,616 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,616 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,617 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,624 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval training set based on eval_every=2***************
2022-10-24 15:18:49,625 - trainer - INFO - {
  "train_loss": 0.9641067087650299
}
2022-10-24 15:18:49,633 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval development set based on eval_every=2***************
2022-10-24 15:18:49,634 - trainer - INFO - {
  "dev_loss": 0.9640548229217529,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,634 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:18:49,635 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,636 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,637 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,637 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_260
2022-10-24 15:18:49,638 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_266
2022-10-24 15:18:49,642 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_266
2022-10-24 15:18:49,643 - trainer - INFO - 
*****************[epoch: 266, global step: 267] eval training set at end of epoch***************
2022-10-24 15:18:49,643 - trainer - INFO - {
  "train_loss": 0.9641038179397583
}
2022-10-24 15:18:49,644 - trainer - INFO - start training epoch 267
2022-10-24 15:18:49,644 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,644 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,645 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,655 - trainer - INFO - 
*****************[epoch: 267, global step: 268] eval training set at end of epoch***************
2022-10-24 15:18:49,655 - trainer - INFO - {
  "train_loss": 0.9640547633171082
}
2022-10-24 15:18:49,655 - trainer - INFO - start training epoch 268
2022-10-24 15:18:49,656 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,656 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,656 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,667 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval training set based on eval_every=2***************
2022-10-24 15:18:49,668 - trainer - INFO - {
  "train_loss": 0.9640676379203796
}
2022-10-24 15:18:49,674 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval development set based on eval_every=2***************
2022-10-24 15:18:49,674 - trainer - INFO - {
  "dev_loss": 0.9640290141105652,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,675 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:18:49,676 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,677 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,677 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,677 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_262
2022-10-24 15:18:49,679 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_268
2022-10-24 15:18:49,684 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_268
2022-10-24 15:18:49,687 - trainer - INFO - 
*****************[epoch: 268, global step: 269] eval training set at end of epoch***************
2022-10-24 15:18:49,687 - trainer - INFO - {
  "train_loss": 0.9640805125236511
}
2022-10-24 15:18:49,688 - trainer - INFO - start training epoch 269
2022-10-24 15:18:49,688 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,688 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,689 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,699 - trainer - INFO - 
*****************[epoch: 269, global step: 270] eval training set at end of epoch***************
2022-10-24 15:18:49,700 - trainer - INFO - {
  "train_loss": 0.96402907371521
}
2022-10-24 15:18:49,700 - trainer - INFO - start training epoch 270
2022-10-24 15:18:49,700 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,701 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,701 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,709 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval training set based on eval_every=2***************
2022-10-24 15:18:49,709 - trainer - INFO - {
  "train_loss": 0.9640406370162964
}
2022-10-24 15:18:49,719 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval development set based on eval_every=2***************
2022-10-24 15:18:49,719 - trainer - INFO - {
  "dev_loss": 0.9640669822692871,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,720 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:18:49,721 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,722 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,722 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,723 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_264
2022-10-24 15:18:49,724 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_270
2022-10-24 15:18:49,731 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_270
2022-10-24 15:18:49,734 - trainer - INFO - 
*****************[epoch: 270, global step: 271] eval training set at end of epoch***************
2022-10-24 15:18:49,735 - trainer - INFO - {
  "train_loss": 0.9640522003173828
}
2022-10-24 15:18:49,736 - trainer - INFO - start training epoch 271
2022-10-24 15:18:49,736 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,736 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,737 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,749 - trainer - INFO - 
*****************[epoch: 271, global step: 272] eval training set at end of epoch***************
2022-10-24 15:18:49,749 - trainer - INFO - {
  "train_loss": 0.9640671014785767
}
2022-10-24 15:18:49,750 - trainer - INFO - start training epoch 272
2022-10-24 15:18:49,750 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,750 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,750 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,758 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval training set based on eval_every=2***************
2022-10-24 15:18:49,758 - trainer - INFO - {
  "train_loss": 0.9640665650367737
}
2022-10-24 15:18:49,768 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval development set based on eval_every=2***************
2022-10-24 15:18:49,768 - trainer - INFO - {
  "dev_loss": 0.9640623331069946,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,769 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:18:49,769 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,770 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,770 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,771 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_266
2022-10-24 15:18:49,772 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_272
2022-10-24 15:18:49,778 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_272
2022-10-24 15:18:49,782 - trainer - INFO - 
*****************[epoch: 272, global step: 273] eval training set at end of epoch***************
2022-10-24 15:18:49,782 - trainer - INFO - {
  "train_loss": 0.9640660285949707
}
2022-10-24 15:18:49,783 - trainer - INFO - start training epoch 273
2022-10-24 15:18:49,783 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,783 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,784 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,793 - trainer - INFO - 
*****************[epoch: 273, global step: 274] eval training set at end of epoch***************
2022-10-24 15:18:49,793 - trainer - INFO - {
  "train_loss": 0.9640623331069946
}
2022-10-24 15:18:49,794 - trainer - INFO - start training epoch 274
2022-10-24 15:18:49,794 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,794 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,795 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,803 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval training set based on eval_every=2***************
2022-10-24 15:18:49,803 - trainer - INFO - {
  "train_loss": 0.9640522301197052
}
2022-10-24 15:18:49,811 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval development set based on eval_every=2***************
2022-10-24 15:18:49,811 - trainer - INFO - {
  "dev_loss": 0.964044451713562,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,812 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:18:49,812 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,814 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,814 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,814 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_268
2022-10-24 15:18:49,815 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_274
2022-10-24 15:18:49,819 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_274
2022-10-24 15:18:49,820 - trainer - INFO - 
*****************[epoch: 274, global step: 275] eval training set at end of epoch***************
2022-10-24 15:18:49,822 - trainer - INFO - {
  "train_loss": 0.9640421271324158
}
2022-10-24 15:18:49,822 - trainer - INFO - start training epoch 275
2022-10-24 15:18:49,826 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,826 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,827 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,837 - trainer - INFO - 
*****************[epoch: 275, global step: 276] eval training set at end of epoch***************
2022-10-24 15:18:49,837 - trainer - INFO - {
  "train_loss": 0.9640443325042725
}
2022-10-24 15:18:49,838 - trainer - INFO - start training epoch 276
2022-10-24 15:18:49,839 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,839 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,839 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,849 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval training set based on eval_every=2***************
2022-10-24 15:18:49,850 - trainer - INFO - {
  "train_loss": 0.9640645682811737
}
2022-10-24 15:18:49,859 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval development set based on eval_every=2***************
2022-10-24 15:18:49,860 - trainer - INFO - {
  "dev_loss": 0.9640827178955078,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,861 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:18:49,861 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,863 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,863 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,863 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_270
2022-10-24 15:18:49,865 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_276
2022-10-24 15:18:49,869 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_276
2022-10-24 15:18:49,870 - trainer - INFO - 
*****************[epoch: 276, global step: 277] eval training set at end of epoch***************
2022-10-24 15:18:49,870 - trainer - INFO - {
  "train_loss": 0.964084804058075
}
2022-10-24 15:18:49,871 - trainer - INFO - start training epoch 277
2022-10-24 15:18:49,871 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,871 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,871 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,879 - trainer - INFO - 
*****************[epoch: 277, global step: 278] eval training set at end of epoch***************
2022-10-24 15:18:49,880 - trainer - INFO - {
  "train_loss": 0.964082658290863
}
2022-10-24 15:18:49,880 - trainer - INFO - start training epoch 278
2022-10-24 15:18:49,880 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,881 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,881 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,890 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval training set based on eval_every=2***************
2022-10-24 15:18:49,890 - trainer - INFO - {
  "train_loss": 0.9640623927116394
}
2022-10-24 15:18:49,900 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval development set based on eval_every=2***************
2022-10-24 15:18:49,901 - trainer - INFO - {
  "dev_loss": 0.9640620946884155,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,903 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:18:49,906 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,907 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,907 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,908 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_272
2022-10-24 15:18:49,909 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_278
2022-10-24 15:18:49,915 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_278
2022-10-24 15:18:49,917 - trainer - INFO - 
*****************[epoch: 278, global step: 279] eval training set at end of epoch***************
2022-10-24 15:18:49,917 - trainer - INFO - {
  "train_loss": 0.9640421271324158
}
2022-10-24 15:18:49,917 - trainer - INFO - start training epoch 279
2022-10-24 15:18:49,918 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,918 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,918 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,931 - trainer - INFO - 
*****************[epoch: 279, global step: 280] eval training set at end of epoch***************
2022-10-24 15:18:49,931 - trainer - INFO - {
  "train_loss": 0.9640620946884155
}
2022-10-24 15:18:49,932 - trainer - INFO - start training epoch 280
2022-10-24 15:18:49,932 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,933 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,933 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,943 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval training set based on eval_every=2***************
2022-10-24 15:18:49,943 - trainer - INFO - {
  "train_loss": 0.9640364944934845
}
2022-10-24 15:18:49,954 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval development set based on eval_every=2***************
2022-10-24 15:18:49,954 - trainer - INFO - {
  "dev_loss": 0.9640603065490723,
  "dev_best_score_for_loss": -0.9640192985534668
}
2022-10-24 15:18:49,955 - trainer - INFO -   no_improve_count: 17
2022-10-24 15:18:49,956 - trainer - INFO -   patience: 200
2022-10-24 15:18:49,957 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:49,957 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:49,958 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_274
2022-10-24 15:18:49,959 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_280
2022-10-24 15:18:49,965 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_280
2022-10-24 15:18:49,967 - trainer - INFO - 
*****************[epoch: 280, global step: 281] eval training set at end of epoch***************
2022-10-24 15:18:49,967 - trainer - INFO - {
  "train_loss": 0.9640108942985535
}
2022-10-24 15:18:49,968 - trainer - INFO - start training epoch 281
2022-10-24 15:18:49,968 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,968 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,969 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,978 - trainer - INFO - 
*****************[epoch: 281, global step: 282] eval training set at end of epoch***************
2022-10-24 15:18:49,978 - trainer - INFO - {
  "train_loss": 0.9640602469444275
}
2022-10-24 15:18:49,979 - trainer - INFO - start training epoch 282
2022-10-24 15:18:49,979 - trainer - INFO - training using device=cuda
2022-10-24 15:18:49,979 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:49,980 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:49,986 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval training set based on eval_every=2***************
2022-10-24 15:18:49,987 - trainer - INFO - {
  "train_loss": 0.9640628695487976
}
2022-10-24 15:18:49,998 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval development set based on eval_every=2***************
2022-10-24 15:18:49,998 - trainer - INFO - {
  "dev_loss": 0.9640101790428162,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,000 - trainer - INFO -    save the model with best score so far
2022-10-24 15:18:50,001 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,002 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,002 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_276
2022-10-24 15:18:50,004 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd
2022-10-24 15:18:50,008 - trainer - INFO - save model to path: tmp/mlp_um_kdd
2022-10-24 15:18:50,009 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:18:50,009 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,010 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:18:50,010 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_282
2022-10-24 15:18:50,015 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_282
2022-10-24 15:18:50,016 - trainer - INFO - 
*****************[epoch: 282, global step: 283] eval training set at end of epoch***************
2022-10-24 15:18:50,016 - trainer - INFO - {
  "train_loss": 0.9640654921531677
}
2022-10-24 15:18:50,017 - trainer - INFO - start training epoch 283
2022-10-24 15:18:50,017 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,017 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,017 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,026 - trainer - INFO - 
*****************[epoch: 283, global step: 284] eval training set at end of epoch***************
2022-10-24 15:18:50,026 - trainer - INFO - {
  "train_loss": 0.9640101790428162
}
2022-10-24 15:18:50,026 - trainer - INFO - start training epoch 284
2022-10-24 15:18:50,027 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,027 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,027 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,034 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval training set based on eval_every=2***************
2022-10-24 15:18:50,035 - trainer - INFO - {
  "train_loss": 0.9640142619609833
}
2022-10-24 15:18:50,042 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval development set based on eval_every=2***************
2022-10-24 15:18:50,043 - trainer - INFO - {
  "dev_loss": 0.9640467166900635,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,044 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:18:50,044 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,046 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,046 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,046 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_278
2022-10-24 15:18:50,048 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_284
2022-10-24 15:18:50,053 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_284
2022-10-24 15:18:50,055 - trainer - INFO - 
*****************[epoch: 284, global step: 285] eval training set at end of epoch***************
2022-10-24 15:18:50,055 - trainer - INFO - {
  "train_loss": 0.9640183448791504
}
2022-10-24 15:18:50,056 - trainer - INFO - start training epoch 285
2022-10-24 15:18:50,056 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,056 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,057 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,066 - trainer - INFO - 
*****************[epoch: 285, global step: 286] eval training set at end of epoch***************
2022-10-24 15:18:50,066 - trainer - INFO - {
  "train_loss": 0.9640467166900635
}
2022-10-24 15:18:50,067 - trainer - INFO - start training epoch 286
2022-10-24 15:18:50,067 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,067 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,068 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,078 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval training set based on eval_every=2***************
2022-10-24 15:18:50,079 - trainer - INFO - {
  "train_loss": 0.9640481472015381
}
2022-10-24 15:18:50,086 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval development set based on eval_every=2***************
2022-10-24 15:18:50,086 - trainer - INFO - {
  "dev_loss": 0.9640754461288452,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,087 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:18:50,087 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,089 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,089 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,089 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_280
2022-10-24 15:18:50,091 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_286
2022-10-24 15:18:50,096 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_286
2022-10-24 15:18:50,097 - trainer - INFO - 
*****************[epoch: 286, global step: 287] eval training set at end of epoch***************
2022-10-24 15:18:50,097 - trainer - INFO - {
  "train_loss": 0.9640495777130127
}
2022-10-24 15:18:50,098 - trainer - INFO - start training epoch 287
2022-10-24 15:18:50,098 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,098 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,099 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,107 - trainer - INFO - 
*****************[epoch: 287, global step: 288] eval training set at end of epoch***************
2022-10-24 15:18:50,107 - trainer - INFO - {
  "train_loss": 0.9640753269195557
}
2022-10-24 15:18:50,107 - trainer - INFO - start training epoch 288
2022-10-24 15:18:50,108 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,108 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,108 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,118 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval training set based on eval_every=2***************
2022-10-24 15:18:50,120 - trainer - INFO - {
  "train_loss": 0.9640663266181946
}
2022-10-24 15:18:50,127 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval development set based on eval_every=2***************
2022-10-24 15:18:50,128 - trainer - INFO - {
  "dev_loss": 0.9640320539474487,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,129 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:18:50,131 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,133 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,133 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,133 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_282
2022-10-24 15:18:50,135 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_288
2022-10-24 15:18:50,140 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_288
2022-10-24 15:18:50,141 - trainer - INFO - 
*****************[epoch: 288, global step: 289] eval training set at end of epoch***************
2022-10-24 15:18:50,141 - trainer - INFO - {
  "train_loss": 0.9640573263168335
}
2022-10-24 15:18:50,142 - trainer - INFO - start training epoch 289
2022-10-24 15:18:50,142 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,142 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,143 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,152 - trainer - INFO - 
*****************[epoch: 289, global step: 290] eval training set at end of epoch***************
2022-10-24 15:18:50,152 - trainer - INFO - {
  "train_loss": 0.9640321135520935
}
2022-10-24 15:18:50,153 - trainer - INFO - start training epoch 290
2022-10-24 15:18:50,153 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,153 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,154 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,164 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval training set based on eval_every=2***************
2022-10-24 15:18:50,164 - trainer - INFO - {
  "train_loss": 0.9640429317951202
}
2022-10-24 15:18:50,170 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval development set based on eval_every=2***************
2022-10-24 15:18:50,170 - trainer - INFO - {
  "dev_loss": 0.9640567302703857,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,171 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:18:50,171 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,173 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,173 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,173 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_284
2022-10-24 15:18:50,174 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_290
2022-10-24 15:18:50,180 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_290
2022-10-24 15:18:50,184 - trainer - INFO - 
*****************[epoch: 290, global step: 291] eval training set at end of epoch***************
2022-10-24 15:18:50,185 - trainer - INFO - {
  "train_loss": 0.964053750038147
}
2022-10-24 15:18:50,185 - trainer - INFO - start training epoch 291
2022-10-24 15:18:50,186 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,186 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,186 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,195 - trainer - INFO - 
*****************[epoch: 291, global step: 292] eval training set at end of epoch***************
2022-10-24 15:18:50,196 - trainer - INFO - {
  "train_loss": 0.964056670665741
}
2022-10-24 15:18:50,196 - trainer - INFO - start training epoch 292
2022-10-24 15:18:50,196 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,197 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,197 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,204 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval training set based on eval_every=2***************
2022-10-24 15:18:50,204 - trainer - INFO - {
  "train_loss": 0.964051753282547
}
2022-10-24 15:18:50,211 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval development set based on eval_every=2***************
2022-10-24 15:18:50,211 - trainer - INFO - {
  "dev_loss": 0.9640178680419922,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,211 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:18:50,212 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,213 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,213 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,213 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_286
2022-10-24 15:18:50,215 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_292
2022-10-24 15:18:50,219 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_292
2022-10-24 15:18:50,220 - trainer - INFO - 
*****************[epoch: 292, global step: 293] eval training set at end of epoch***************
2022-10-24 15:18:50,220 - trainer - INFO - {
  "train_loss": 0.964046835899353
}
2022-10-24 15:18:50,220 - trainer - INFO - start training epoch 293
2022-10-24 15:18:50,221 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,221 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,221 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,230 - trainer - INFO - 
*****************[epoch: 293, global step: 294] eval training set at end of epoch***************
2022-10-24 15:18:50,231 - trainer - INFO - {
  "train_loss": 0.9640178680419922
}
2022-10-24 15:18:50,232 - trainer - INFO - start training epoch 294
2022-10-24 15:18:50,232 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,232 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,233 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,241 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval training set based on eval_every=2***************
2022-10-24 15:18:50,241 - trainer - INFO - {
  "train_loss": 0.9640440344810486
}
2022-10-24 15:18:50,248 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval development set based on eval_every=2***************
2022-10-24 15:18:50,249 - trainer - INFO - {
  "dev_loss": 0.9640685319900513,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,249 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:18:50,250 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,251 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,251 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,251 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_288
2022-10-24 15:18:50,252 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_294
2022-10-24 15:18:50,257 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_294
2022-10-24 15:18:50,260 - trainer - INFO - 
*****************[epoch: 294, global step: 295] eval training set at end of epoch***************
2022-10-24 15:18:50,261 - trainer - INFO - {
  "train_loss": 0.964070200920105
}
2022-10-24 15:18:50,261 - trainer - INFO - start training epoch 295
2022-10-24 15:18:50,261 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,262 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,262 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,272 - trainer - INFO - 
*****************[epoch: 295, global step: 296] eval training set at end of epoch***************
2022-10-24 15:18:50,272 - trainer - INFO - {
  "train_loss": 0.9640685319900513
}
2022-10-24 15:18:50,272 - trainer - INFO - start training epoch 296
2022-10-24 15:18:50,273 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,273 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,273 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,280 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval training set based on eval_every=2***************
2022-10-24 15:18:50,280 - trainer - INFO - {
  "train_loss": 0.9640663862228394
}
2022-10-24 15:18:50,286 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval development set based on eval_every=2***************
2022-10-24 15:18:50,287 - trainer - INFO - {
  "dev_loss": 0.9640743732452393,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,289 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:18:50,289 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,293 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,293 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,294 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_290
2022-10-24 15:18:50,296 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_296
2022-10-24 15:18:50,301 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_296
2022-10-24 15:18:50,302 - trainer - INFO - 
*****************[epoch: 296, global step: 297] eval training set at end of epoch***************
2022-10-24 15:18:50,303 - trainer - INFO - {
  "train_loss": 0.9640642404556274
}
2022-10-24 15:18:50,303 - trainer - INFO - start training epoch 297
2022-10-24 15:18:50,303 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,304 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,304 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,312 - trainer - INFO - 
*****************[epoch: 297, global step: 298] eval training set at end of epoch***************
2022-10-24 15:18:50,312 - trainer - INFO - {
  "train_loss": 0.9640743732452393
}
2022-10-24 15:18:50,312 - trainer - INFO - start training epoch 298
2022-10-24 15:18:50,313 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,313 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,313 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,320 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval training set based on eval_every=2***************
2022-10-24 15:18:50,320 - trainer - INFO - {
  "train_loss": 0.964057445526123
}
2022-10-24 15:18:50,326 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval development set based on eval_every=2***************
2022-10-24 15:18:50,326 - trainer - INFO - {
  "dev_loss": 0.9640674591064453,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,327 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:18:50,328 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,329 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,329 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,330 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_292
2022-10-24 15:18:50,331 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_298
2022-10-24 15:18:50,336 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_298
2022-10-24 15:18:50,337 - trainer - INFO - 
*****************[epoch: 298, global step: 299] eval training set at end of epoch***************
2022-10-24 15:18:50,338 - trainer - INFO - {
  "train_loss": 0.9640405178070068
}
2022-10-24 15:18:50,339 - trainer - INFO - start training epoch 299
2022-10-24 15:18:50,339 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,339 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,339 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,351 - trainer - INFO - 
*****************[epoch: 299, global step: 300] eval training set at end of epoch***************
2022-10-24 15:18:50,351 - trainer - INFO - {
  "train_loss": 0.9640674591064453
}
2022-10-24 15:18:50,351 - trainer - INFO - start training epoch 300
2022-10-24 15:18:50,352 - trainer - INFO - training using device=cuda
2022-10-24 15:18:50,352 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:18:50,352 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_um_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:18:50,360 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval training set based on eval_every=2***************
2022-10-24 15:18:50,361 - trainer - INFO - {
  "train_loss": 0.9640831351280212
}
2022-10-24 15:18:50,370 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval development set based on eval_every=2***************
2022-10-24 15:18:50,371 - trainer - INFO - {
  "dev_loss": 0.9640766978263855,
  "dev_best_score_for_loss": -0.9640101790428162
}
2022-10-24 15:18:50,372 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:18:50,372 - trainer - INFO -   patience: 200
2022-10-24 15:18:50,374 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:18:50,374 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:18:50,374 - trainer - INFO -   Remove checkpoint tmp/mlp_um_kdd\ck_294
2022-10-24 15:18:50,376 - trainer - INFO -   Save checkpoint to tmp/mlp_um_kdd\ck_300
2022-10-24 15:18:50,381 - trainer - INFO - save model to path: tmp/mlp_um_kdd\ck_300
2022-10-24 15:18:50,382 - trainer - INFO - 
*****************[epoch: 300, global step: 301] eval training set at end of epoch***************
2022-10-24 15:18:50,383 - trainer - INFO - {
  "train_loss": 0.9640988111495972
}
