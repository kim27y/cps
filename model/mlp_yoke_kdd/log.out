2022-10-24 15:19:07,784 - trainer - INFO - MLP(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=32, bias=True)
  )
  (activation_layers): ModuleList(
    (0): ReLU(inplace=True)
    (1): ReLU(inplace=True)
    (2): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (linear_output): Linear(in_features=32, out_features=1, bias=True)
)
2022-10-24 15:19:07,785 - trainer - INFO -   Total params: 10625
2022-10-24 15:19:07,786 - trainer - INFO -   Trainable params: 10625
2022-10-24 15:19:07,787 - trainer - INFO -   Non-trainable params: 0
2022-10-24 15:19:07,787 - trainer - INFO -   There are 11  training examples
2022-10-24 15:19:07,788 - trainer - INFO -   There are 11 examples for development
2022-10-24 15:19:07,918 - trainer - INFO - start training epoch 1
2022-10-24 15:19:07,919 - trainer - INFO - training using device=cuda
2022-10-24 15:19:07,919 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:07,920 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,333 - trainer - INFO - 
*****************[epoch: 1, global step: 2] eval training set at end of epoch***************
2022-10-24 15:19:09,333 - trainer - INFO - {
  "train_loss": 561756.5
}
2022-10-24 15:19:09,335 - trainer - INFO - start training epoch 2
2022-10-24 15:19:09,335 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,336 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,336 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,344 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval training set based on eval_every=2***************
2022-10-24 15:19:09,344 - trainer - INFO - {
  "train_loss": 532030.75
}
2022-10-24 15:19:09,354 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval development set based on eval_every=2***************
2022-10-24 15:19:09,354 - trainer - INFO - {
  "dev_loss": 292315.15625,
  "dev_best_score_for_loss": -292315.15625
}
2022-10-24 15:19:09,355 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:09,356 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:19:09,356 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:09,360 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:09,363 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:09,363 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,364 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:19:09,365 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_2
2022-10-24 15:19:09,370 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_2
2022-10-24 15:19:09,372 - trainer - INFO - 
*****************[epoch: 2, global step: 3] eval training set at end of epoch***************
2022-10-24 15:19:09,373 - trainer - INFO - {
  "train_loss": 502305.0
}
2022-10-24 15:19:09,373 - trainer - INFO - start training epoch 3
2022-10-24 15:19:09,374 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,375 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,375 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,390 - trainer - INFO - 
*****************[epoch: 3, global step: 4] eval training set at end of epoch***************
2022-10-24 15:19:09,391 - trainer - INFO - {
  "train_loss": 292315.15625
}
2022-10-24 15:19:09,391 - trainer - INFO - start training epoch 4
2022-10-24 15:19:09,392 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,393 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,393 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,409 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval training set based on eval_every=2***************
2022-10-24 15:19:09,411 - trainer - INFO - {
  "train_loss": 151701.4169921875
}
2022-10-24 15:19:09,423 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval development set based on eval_every=2***************
2022-10-24 15:19:09,423 - trainer - INFO - {
  "dev_loss": 803390.375,
  "dev_best_score_for_loss": -292315.15625
}
2022-10-24 15:19:09,424 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:09,425 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,426 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:19:09,427 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_4
2022-10-24 15:19:09,434 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_4
2022-10-24 15:19:09,434 - trainer - INFO - 
*****************[epoch: 4, global step: 5] eval training set at end of epoch***************
2022-10-24 15:19:09,435 - trainer - INFO - {
  "train_loss": 11087.677734375
}
2022-10-24 15:19:09,436 - trainer - INFO - start training epoch 5
2022-10-24 15:19:09,436 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,437 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,437 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,447 - trainer - INFO - 
*****************[epoch: 5, global step: 6] eval training set at end of epoch***************
2022-10-24 15:19:09,448 - trainer - INFO - {
  "train_loss": 803390.375
}
2022-10-24 15:19:09,449 - trainer - INFO - start training epoch 6
2022-10-24 15:19:09,450 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,450 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,451 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,463 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval training set based on eval_every=2***************
2022-10-24 15:19:09,464 - trainer - INFO - {
  "train_loss": 444412.87890625
}
2022-10-24 15:19:09,470 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval development set based on eval_every=2***************
2022-10-24 15:19:09,471 - trainer - INFO - {
  "dev_loss": 37385.421875,
  "dev_best_score_for_loss": -37385.421875
}
2022-10-24 15:19:09,472 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:09,473 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:09,473 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:09,477 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:09,478 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:09,480 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,481 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:09,482 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_6
2022-10-24 15:19:09,488 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_6
2022-10-24 15:19:09,489 - trainer - INFO - 
*****************[epoch: 6, global step: 7] eval training set at end of epoch***************
2022-10-24 15:19:09,492 - trainer - INFO - {
  "train_loss": 85435.3828125
}
2022-10-24 15:19:09,493 - trainer - INFO - start training epoch 7
2022-10-24 15:19:09,495 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,497 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,497 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,509 - trainer - INFO - 
*****************[epoch: 7, global step: 8] eval training set at end of epoch***************
2022-10-24 15:19:09,510 - trainer - INFO - {
  "train_loss": 37385.421875
}
2022-10-24 15:19:09,512 - trainer - INFO - start training epoch 8
2022-10-24 15:19:09,512 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,513 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,513 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,523 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval training set based on eval_every=2***************
2022-10-24 15:19:09,524 - trainer - INFO - {
  "train_loss": 104522.6875
}
2022-10-24 15:19:09,533 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval development set based on eval_every=2***************
2022-10-24 15:19:09,533 - trainer - INFO - {
  "dev_loss": 275075.8125,
  "dev_best_score_for_loss": -37385.421875
}
2022-10-24 15:19:09,534 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:09,535 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,536 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:09,536 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:09,538 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_2
2022-10-24 15:19:09,539 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_8
2022-10-24 15:19:09,545 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_8
2022-10-24 15:19:09,546 - trainer - INFO - 
*****************[epoch: 8, global step: 9] eval training set at end of epoch***************
2022-10-24 15:19:09,547 - trainer - INFO - {
  "train_loss": 171659.953125
}
2022-10-24 15:19:09,548 - trainer - INFO - start training epoch 9
2022-10-24 15:19:09,548 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,549 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,549 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,562 - trainer - INFO - 
*****************[epoch: 9, global step: 10] eval training set at end of epoch***************
2022-10-24 15:19:09,562 - trainer - INFO - {
  "train_loss": 275075.84375
}
2022-10-24 15:19:09,563 - trainer - INFO - start training epoch 10
2022-10-24 15:19:09,563 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,564 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,565 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,574 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval training set based on eval_every=2***************
2022-10-24 15:19:09,575 - trainer - INFO - {
  "train_loss": 304385.53125
}
2022-10-24 15:19:09,585 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval development set based on eval_every=2***************
2022-10-24 15:19:09,585 - trainer - INFO - {
  "dev_loss": 361177.25,
  "dev_best_score_for_loss": -37385.421875
}
2022-10-24 15:19:09,586 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:09,587 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,589 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:09,589 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:09,590 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_4
2022-10-24 15:19:09,591 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_10
2022-10-24 15:19:09,597 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_10
2022-10-24 15:19:09,597 - trainer - INFO - 
*****************[epoch: 10, global step: 11] eval training set at end of epoch***************
2022-10-24 15:19:09,598 - trainer - INFO - {
  "train_loss": 333695.21875
}
2022-10-24 15:19:09,599 - trainer - INFO - start training epoch 11
2022-10-24 15:19:09,600 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,601 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,602 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,613 - trainer - INFO - 
*****************[epoch: 11, global step: 12] eval training set at end of epoch***************
2022-10-24 15:19:09,613 - trainer - INFO - {
  "train_loss": 361177.25
}
2022-10-24 15:19:09,614 - trainer - INFO - start training epoch 12
2022-10-24 15:19:09,615 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,615 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,616 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,623 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval training set based on eval_every=2***************
2022-10-24 15:19:09,623 - trainer - INFO - {
  "train_loss": 364316.640625
}
2022-10-24 15:19:09,635 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval development set based on eval_every=2***************
2022-10-24 15:19:09,635 - trainer - INFO - {
  "dev_loss": 357027.8125,
  "dev_best_score_for_loss": -37385.421875
}
2022-10-24 15:19:09,637 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:09,637 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,639 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:09,639 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:09,640 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_6
2022-10-24 15:19:09,641 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_12
2022-10-24 15:19:09,647 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_12
2022-10-24 15:19:09,648 - trainer - INFO - 
*****************[epoch: 12, global step: 13] eval training set at end of epoch***************
2022-10-24 15:19:09,648 - trainer - INFO - {
  "train_loss": 367456.03125
}
2022-10-24 15:19:09,649 - trainer - INFO - start training epoch 13
2022-10-24 15:19:09,649 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,650 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,651 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,662 - trainer - INFO - 
*****************[epoch: 13, global step: 14] eval training set at end of epoch***************
2022-10-24 15:19:09,663 - trainer - INFO - {
  "train_loss": 357027.8125
}
2022-10-24 15:19:09,664 - trainer - INFO - start training epoch 14
2022-10-24 15:19:09,665 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,665 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,666 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,675 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval training set based on eval_every=2***************
2022-10-24 15:19:09,676 - trainer - INFO - {
  "train_loss": 343823.0625
}
2022-10-24 15:19:09,686 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval development set based on eval_every=2***************
2022-10-24 15:19:09,686 - trainer - INFO - {
  "dev_loss": 286744.0625,
  "dev_best_score_for_loss": -37385.421875
}
2022-10-24 15:19:09,687 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:19:09,688 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,689 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:09,689 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:09,689 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_8
2022-10-24 15:19:09,691 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_14
2022-10-24 15:19:09,697 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_14
2022-10-24 15:19:09,698 - trainer - INFO - 
*****************[epoch: 14, global step: 15] eval training set at end of epoch***************
2022-10-24 15:19:09,699 - trainer - INFO - {
  "train_loss": 330618.3125
}
2022-10-24 15:19:09,700 - trainer - INFO - start training epoch 15
2022-10-24 15:19:09,701 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,701 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,702 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,714 - trainer - INFO - 
*****************[epoch: 15, global step: 16] eval training set at end of epoch***************
2022-10-24 15:19:09,715 - trainer - INFO - {
  "train_loss": 286744.0625
}
2022-10-24 15:19:09,715 - trainer - INFO - start training epoch 16
2022-10-24 15:19:09,716 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,716 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,717 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,726 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval training set based on eval_every=2***************
2022-10-24 15:19:09,728 - trainer - INFO - {
  "train_loss": 255117.046875
}
2022-10-24 15:19:09,740 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval development set based on eval_every=2***************
2022-10-24 15:19:09,740 - trainer - INFO - {
  "dev_loss": 142406.03125,
  "dev_best_score_for_loss": -37385.421875
}
2022-10-24 15:19:09,742 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:19:09,742 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,744 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:09,745 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:09,746 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_10
2022-10-24 15:19:09,748 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_16
2022-10-24 15:19:09,753 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_16
2022-10-24 15:19:09,756 - trainer - INFO - 
*****************[epoch: 16, global step: 17] eval training set at end of epoch***************
2022-10-24 15:19:09,756 - trainer - INFO - {
  "train_loss": 223490.03125
}
2022-10-24 15:19:09,758 - trainer - INFO - start training epoch 17
2022-10-24 15:19:09,758 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,758 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,759 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,773 - trainer - INFO - 
*****************[epoch: 17, global step: 18] eval training set at end of epoch***************
2022-10-24 15:19:09,774 - trainer - INFO - {
  "train_loss": 142406.03125
}
2022-10-24 15:19:09,775 - trainer - INFO - start training epoch 18
2022-10-24 15:19:09,776 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,777 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,778 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,786 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval training set based on eval_every=2***************
2022-10-24 15:19:09,787 - trainer - INFO - {
  "train_loss": 99924.82421875
}
2022-10-24 15:19:09,795 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval development set based on eval_every=2***************
2022-10-24 15:19:09,796 - trainer - INFO - {
  "dev_loss": 8013.96044921875,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:09,798 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:09,800 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:09,801 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:09,801 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_12
2022-10-24 15:19:09,804 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:09,808 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:09,809 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:09,809 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,811 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:09,811 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_18
2022-10-24 15:19:09,816 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_18
2022-10-24 15:19:09,820 - trainer - INFO - 
*****************[epoch: 18, global step: 19] eval training set at end of epoch***************
2022-10-24 15:19:09,821 - trainer - INFO - {
  "train_loss": 57443.6171875
}
2022-10-24 15:19:09,821 - trainer - INFO - start training epoch 19
2022-10-24 15:19:09,822 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,822 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,823 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,836 - trainer - INFO - 
*****************[epoch: 19, global step: 20] eval training set at end of epoch***************
2022-10-24 15:19:09,836 - trainer - INFO - {
  "train_loss": 8013.96044921875
}
2022-10-24 15:19:09,837 - trainer - INFO - start training epoch 20
2022-10-24 15:19:09,838 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,838 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,839 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,849 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval training set based on eval_every=2***************
2022-10-24 15:19:09,849 - trainer - INFO - {
  "train_loss": 28145.921630859375
}
2022-10-24 15:19:09,857 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval development set based on eval_every=2***************
2022-10-24 15:19:09,861 - trainer - INFO - {
  "dev_loss": 147593.453125,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:09,862 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:09,864 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,866 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:09,866 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:09,867 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_14
2022-10-24 15:19:09,870 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_20
2022-10-24 15:19:09,875 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_20
2022-10-24 15:19:09,876 - trainer - INFO - 
*****************[epoch: 20, global step: 21] eval training set at end of epoch***************
2022-10-24 15:19:09,877 - trainer - INFO - {
  "train_loss": 48277.8828125
}
2022-10-24 15:19:09,881 - trainer - INFO - start training epoch 21
2022-10-24 15:19:09,882 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,882 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,883 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,896 - trainer - INFO - 
*****************[epoch: 21, global step: 22] eval training set at end of epoch***************
2022-10-24 15:19:09,896 - trainer - INFO - {
  "train_loss": 147593.453125
}
2022-10-24 15:19:09,899 - trainer - INFO - start training epoch 22
2022-10-24 15:19:09,900 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,900 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,901 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,914 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval training set based on eval_every=2***************
2022-10-24 15:19:09,914 - trainer - INFO - {
  "train_loss": 150954.328125
}
2022-10-24 15:19:09,922 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval development set based on eval_every=2***************
2022-10-24 15:19:09,922 - trainer - INFO - {
  "dev_loss": 68207.828125,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:09,923 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:09,925 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,926 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:09,931 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:09,931 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_16
2022-10-24 15:19:09,934 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_22
2022-10-24 15:19:09,939 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_22
2022-10-24 15:19:09,943 - trainer - INFO - 
*****************[epoch: 22, global step: 23] eval training set at end of epoch***************
2022-10-24 15:19:09,943 - trainer - INFO - {
  "train_loss": 154315.203125
}
2022-10-24 15:19:09,945 - trainer - INFO - start training epoch 23
2022-10-24 15:19:09,945 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,947 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,947 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,957 - trainer - INFO - 
*****************[epoch: 23, global step: 24] eval training set at end of epoch***************
2022-10-24 15:19:09,957 - trainer - INFO - {
  "train_loss": 68207.828125
}
2022-10-24 15:19:09,958 - trainer - INFO - start training epoch 24
2022-10-24 15:19:09,959 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,959 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,960 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:09,970 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval training set based on eval_every=2***************
2022-10-24 15:19:09,970 - trainer - INFO - {
  "train_loss": 39815.33740234375
}
2022-10-24 15:19:09,979 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval development set based on eval_every=2***************
2022-10-24 15:19:09,980 - trainer - INFO - {
  "dev_loss": 15473.865234375,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:09,981 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:09,981 - trainer - INFO -   patience: 200
2022-10-24 15:19:09,983 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:09,983 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:09,984 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_18
2022-10-24 15:19:09,985 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_24
2022-10-24 15:19:09,991 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_24
2022-10-24 15:19:09,992 - trainer - INFO - 
*****************[epoch: 24, global step: 25] eval training set at end of epoch***************
2022-10-24 15:19:09,992 - trainer - INFO - {
  "train_loss": 11422.8466796875
}
2022-10-24 15:19:09,994 - trainer - INFO - start training epoch 25
2022-10-24 15:19:09,994 - trainer - INFO - training using device=cuda
2022-10-24 15:19:09,995 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:09,996 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,006 - trainer - INFO - 
*****************[epoch: 25, global step: 26] eval training set at end of epoch***************
2022-10-24 15:19:10,007 - trainer - INFO - {
  "train_loss": 15473.8671875
}
2022-10-24 15:19:10,007 - trainer - INFO - start training epoch 26
2022-10-24 15:19:10,008 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,008 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,009 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,021 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval training set based on eval_every=2***************
2022-10-24 15:19:10,022 - trainer - INFO - {
  "train_loss": 30888.912109375
}
2022-10-24 15:19:10,032 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval development set based on eval_every=2***************
2022-10-24 15:19:10,035 - trainer - INFO - {
  "dev_loss": 74069.625,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:10,036 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:19:10,037 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,038 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,038 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,039 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_20
2022-10-24 15:19:10,040 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_26
2022-10-24 15:19:10,045 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_26
2022-10-24 15:19:10,049 - trainer - INFO - 
*****************[epoch: 26, global step: 27] eval training set at end of epoch***************
2022-10-24 15:19:10,050 - trainer - INFO - {
  "train_loss": 46303.95703125
}
2022-10-24 15:19:10,051 - trainer - INFO - start training epoch 27
2022-10-24 15:19:10,052 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,053 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,054 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,066 - trainer - INFO - 
*****************[epoch: 27, global step: 28] eval training set at end of epoch***************
2022-10-24 15:19:10,066 - trainer - INFO - {
  "train_loss": 74069.6328125
}
2022-10-24 15:19:10,067 - trainer - INFO - start training epoch 28
2022-10-24 15:19:10,067 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,068 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,069 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,078 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval training set based on eval_every=2***************
2022-10-24 15:19:10,079 - trainer - INFO - {
  "train_loss": 80215.1484375
}
2022-10-24 15:19:10,091 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval development set based on eval_every=2***************
2022-10-24 15:19:10,092 - trainer - INFO - {
  "dev_loss": 80961.328125,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:10,094 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:19:10,099 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,100 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,101 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,101 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_22
2022-10-24 15:19:10,103 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_28
2022-10-24 15:19:10,109 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_28
2022-10-24 15:19:10,113 - trainer - INFO - 
*****************[epoch: 28, global step: 29] eval training set at end of epoch***************
2022-10-24 15:19:10,114 - trainer - INFO - {
  "train_loss": 86360.6640625
}
2022-10-24 15:19:10,115 - trainer - INFO - start training epoch 29
2022-10-24 15:19:10,115 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,116 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,116 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,128 - trainer - INFO - 
*****************[epoch: 29, global step: 30] eval training set at end of epoch***************
2022-10-24 15:19:10,129 - trainer - INFO - {
  "train_loss": 80961.328125
}
2022-10-24 15:19:10,130 - trainer - INFO - start training epoch 30
2022-10-24 15:19:10,132 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,133 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,134 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,146 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval training set based on eval_every=2***************
2022-10-24 15:19:10,147 - trainer - INFO - {
  "train_loss": 70894.69921875
}
2022-10-24 15:19:10,155 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval development set based on eval_every=2***************
2022-10-24 15:19:10,156 - trainer - INFO - {
  "dev_loss": 33414.4140625,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:10,156 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:19:10,160 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,161 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,164 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,165 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_24
2022-10-24 15:19:10,167 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_30
2022-10-24 15:19:10,174 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_30
2022-10-24 15:19:10,175 - trainer - INFO - 
*****************[epoch: 30, global step: 31] eval training set at end of epoch***************
2022-10-24 15:19:10,175 - trainer - INFO - {
  "train_loss": 60828.0703125
}
2022-10-24 15:19:10,176 - trainer - INFO - start training epoch 31
2022-10-24 15:19:10,177 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,177 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,178 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,188 - trainer - INFO - 
*****************[epoch: 31, global step: 32] eval training set at end of epoch***************
2022-10-24 15:19:10,189 - trainer - INFO - {
  "train_loss": 33414.41015625
}
2022-10-24 15:19:10,189 - trainer - INFO - start training epoch 32
2022-10-24 15:19:10,190 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,191 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,192 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,204 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval training set based on eval_every=2***************
2022-10-24 15:19:10,205 - trainer - INFO - {
  "train_loss": 22391.70751953125
}
2022-10-24 15:19:10,221 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval development set based on eval_every=2***************
2022-10-24 15:19:10,221 - trainer - INFO - {
  "dev_loss": 8949.583984375,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:10,222 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:19:10,223 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,224 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,225 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,225 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_26
2022-10-24 15:19:10,227 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_32
2022-10-24 15:19:10,233 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_32
2022-10-24 15:19:10,236 - trainer - INFO - 
*****************[epoch: 32, global step: 33] eval training set at end of epoch***************
2022-10-24 15:19:10,237 - trainer - INFO - {
  "train_loss": 11369.0048828125
}
2022-10-24 15:19:10,238 - trainer - INFO - start training epoch 33
2022-10-24 15:19:10,238 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,239 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,239 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,252 - trainer - INFO - 
*****************[epoch: 33, global step: 34] eval training set at end of epoch***************
2022-10-24 15:19:10,254 - trainer - INFO - {
  "train_loss": 8949.583984375
}
2022-10-24 15:19:10,255 - trainer - INFO - start training epoch 34
2022-10-24 15:19:10,255 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,256 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,257 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,268 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval training set based on eval_every=2***************
2022-10-24 15:19:10,269 - trainer - INFO - {
  "train_loss": 18548.3388671875
}
2022-10-24 15:19:10,276 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval development set based on eval_every=2***************
2022-10-24 15:19:10,276 - trainer - INFO - {
  "dev_loss": 46567.421875,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:10,277 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:19:10,278 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,279 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,279 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,280 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_28
2022-10-24 15:19:10,281 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_34
2022-10-24 15:19:10,286 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_34
2022-10-24 15:19:10,287 - trainer - INFO - 
*****************[epoch: 34, global step: 35] eval training set at end of epoch***************
2022-10-24 15:19:10,288 - trainer - INFO - {
  "train_loss": 28147.09375
}
2022-10-24 15:19:10,288 - trainer - INFO - start training epoch 35
2022-10-24 15:19:10,289 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,290 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,290 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,302 - trainer - INFO - 
*****************[epoch: 35, global step: 36] eval training set at end of epoch***************
2022-10-24 15:19:10,302 - trainer - INFO - {
  "train_loss": 46567.421875
}
2022-10-24 15:19:10,303 - trainer - INFO - start training epoch 36
2022-10-24 15:19:10,304 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,304 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,305 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,314 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval training set based on eval_every=2***************
2022-10-24 15:19:10,317 - trainer - INFO - {
  "train_loss": 43645.4921875
}
2022-10-24 15:19:10,324 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval development set based on eval_every=2***************
2022-10-24 15:19:10,325 - trainer - INFO - {
  "dev_loss": 19462.955078125,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:10,326 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:19:10,332 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,334 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,334 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,335 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_30
2022-10-24 15:19:10,337 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_36
2022-10-24 15:19:10,341 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_36
2022-10-24 15:19:10,344 - trainer - INFO - 
*****************[epoch: 36, global step: 37] eval training set at end of epoch***************
2022-10-24 15:19:10,347 - trainer - INFO - {
  "train_loss": 40723.5625
}
2022-10-24 15:19:10,349 - trainer - INFO - start training epoch 37
2022-10-24 15:19:10,351 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,352 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,352 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,364 - trainer - INFO - 
*****************[epoch: 37, global step: 38] eval training set at end of epoch***************
2022-10-24 15:19:10,364 - trainer - INFO - {
  "train_loss": 19462.955078125
}
2022-10-24 15:19:10,365 - trainer - INFO - start training epoch 38
2022-10-24 15:19:10,366 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,366 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,367 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,376 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval training set based on eval_every=2***************
2022-10-24 15:19:10,376 - trainer - INFO - {
  "train_loss": 13264.88818359375
}
2022-10-24 15:19:10,382 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval development set based on eval_every=2***************
2022-10-24 15:19:10,382 - trainer - INFO - {
  "dev_loss": 11051.296875,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:10,383 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:19:10,384 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,385 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,385 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,385 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_32
2022-10-24 15:19:10,387 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_38
2022-10-24 15:19:10,392 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_38
2022-10-24 15:19:10,396 - trainer - INFO - 
*****************[epoch: 38, global step: 39] eval training set at end of epoch***************
2022-10-24 15:19:10,396 - trainer - INFO - {
  "train_loss": 7066.8212890625
}
2022-10-24 15:19:10,397 - trainer - INFO - start training epoch 39
2022-10-24 15:19:10,398 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,398 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,399 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,410 - trainer - INFO - 
*****************[epoch: 39, global step: 40] eval training set at end of epoch***************
2022-10-24 15:19:10,411 - trainer - INFO - {
  "train_loss": 11051.296875
}
2022-10-24 15:19:10,412 - trainer - INFO - start training epoch 40
2022-10-24 15:19:10,412 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,416 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,417 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,430 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval training set based on eval_every=2***************
2022-10-24 15:19:10,430 - trainer - INFO - {
  "train_loss": 15847.5986328125
}
2022-10-24 15:19:10,440 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval development set based on eval_every=2***************
2022-10-24 15:19:10,441 - trainer - INFO - {
  "dev_loss": 25857.25390625,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:10,442 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:19:10,443 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,444 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,445 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,445 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_34
2022-10-24 15:19:10,447 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_40
2022-10-24 15:19:10,453 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_40
2022-10-24 15:19:10,454 - trainer - INFO - 
*****************[epoch: 40, global step: 41] eval training set at end of epoch***************
2022-10-24 15:19:10,455 - trainer - INFO - {
  "train_loss": 20643.900390625
}
2022-10-24 15:19:10,456 - trainer - INFO - start training epoch 41
2022-10-24 15:19:10,456 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,457 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,457 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,470 - trainer - INFO - 
*****************[epoch: 41, global step: 42] eval training set at end of epoch***************
2022-10-24 15:19:10,470 - trainer - INFO - {
  "train_loss": 25857.25
}
2022-10-24 15:19:10,471 - trainer - INFO - start training epoch 42
2022-10-24 15:19:10,471 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,472 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,472 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,480 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval training set based on eval_every=2***************
2022-10-24 15:19:10,480 - trainer - INFO - {
  "train_loss": 25101.828125
}
2022-10-24 15:19:10,489 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval development set based on eval_every=2***************
2022-10-24 15:19:10,489 - trainer - INFO - {
  "dev_loss": 17925.451171875,
  "dev_best_score_for_loss": -8013.96044921875
}
2022-10-24 15:19:10,490 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:19:10,491 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,493 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,493 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,497 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_36
2022-10-24 15:19:10,498 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_42
2022-10-24 15:19:10,505 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_42
2022-10-24 15:19:10,506 - trainer - INFO - 
*****************[epoch: 42, global step: 43] eval training set at end of epoch***************
2022-10-24 15:19:10,507 - trainer - INFO - {
  "train_loss": 24346.40625
}
2022-10-24 15:19:10,507 - trainer - INFO - start training epoch 43
2022-10-24 15:19:10,508 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,508 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,509 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,518 - trainer - INFO - 
*****************[epoch: 43, global step: 44] eval training set at end of epoch***************
2022-10-24 15:19:10,518 - trainer - INFO - {
  "train_loss": 17925.453125
}
2022-10-24 15:19:10,519 - trainer - INFO - start training epoch 44
2022-10-24 15:19:10,519 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,519 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,520 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,531 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval training set based on eval_every=2***************
2022-10-24 15:19:10,533 - trainer - INFO - {
  "train_loss": 14057.43603515625
}
2022-10-24 15:19:10,541 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval development set based on eval_every=2***************
2022-10-24 15:19:10,542 - trainer - INFO - {
  "dev_loss": 6340.76171875,
  "dev_best_score_for_loss": -6340.76171875
}
2022-10-24 15:19:10,547 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:10,550 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,550 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,552 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_38
2022-10-24 15:19:10,554 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:10,557 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:10,557 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:10,558 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,559 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:10,560 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_44
2022-10-24 15:19:10,565 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_44
2022-10-24 15:19:10,566 - trainer - INFO - 
*****************[epoch: 44, global step: 45] eval training set at end of epoch***************
2022-10-24 15:19:10,567 - trainer - INFO - {
  "train_loss": 10189.4189453125
}
2022-10-24 15:19:10,568 - trainer - INFO - start training epoch 45
2022-10-24 15:19:10,568 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,568 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,569 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,580 - trainer - INFO - 
*****************[epoch: 45, global step: 46] eval training set at end of epoch***************
2022-10-24 15:19:10,581 - trainer - INFO - {
  "train_loss": 6340.76171875
}
2022-10-24 15:19:10,582 - trainer - INFO - start training epoch 46
2022-10-24 15:19:10,583 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,583 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,584 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,592 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval training set based on eval_every=2***************
2022-10-24 15:19:10,593 - trainer - INFO - {
  "train_loss": 7653.392578125
}
2022-10-24 15:19:10,605 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval development set based on eval_every=2***************
2022-10-24 15:19:10,605 - trainer - INFO - {
  "dev_loss": 14486.171875,
  "dev_best_score_for_loss": -6340.76171875
}
2022-10-24 15:19:10,606 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:10,607 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,609 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,609 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,610 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_40
2022-10-24 15:19:10,612 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_46
2022-10-24 15:19:10,617 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_46
2022-10-24 15:19:10,618 - trainer - INFO - 
*****************[epoch: 46, global step: 47] eval training set at end of epoch***************
2022-10-24 15:19:10,618 - trainer - INFO - {
  "train_loss": 8966.0234375
}
2022-10-24 15:19:10,619 - trainer - INFO - start training epoch 47
2022-10-24 15:19:10,620 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,620 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,621 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,638 - trainer - INFO - 
*****************[epoch: 47, global step: 48] eval training set at end of epoch***************
2022-10-24 15:19:10,638 - trainer - INFO - {
  "train_loss": 14486.171875
}
2022-10-24 15:19:10,639 - trainer - INFO - start training epoch 48
2022-10-24 15:19:10,639 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,640 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,640 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,650 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval training set based on eval_every=2***************
2022-10-24 15:19:10,650 - trainer - INFO - {
  "train_loss": 15307.8828125
}
2022-10-24 15:19:10,659 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval development set based on eval_every=2***************
2022-10-24 15:19:10,660 - trainer - INFO - {
  "dev_loss": 12123.2490234375,
  "dev_best_score_for_loss": -6340.76171875
}
2022-10-24 15:19:10,663 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:10,663 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,665 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,665 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,666 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_42
2022-10-24 15:19:10,667 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_48
2022-10-24 15:19:10,672 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_48
2022-10-24 15:19:10,673 - trainer - INFO - 
*****************[epoch: 48, global step: 49] eval training set at end of epoch***************
2022-10-24 15:19:10,674 - trainer - INFO - {
  "train_loss": 16129.59375
}
2022-10-24 15:19:10,675 - trainer - INFO - start training epoch 49
2022-10-24 15:19:10,676 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,678 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,679 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,690 - trainer - INFO - 
*****************[epoch: 49, global step: 50] eval training set at end of epoch***************
2022-10-24 15:19:10,690 - trainer - INFO - {
  "train_loss": 12123.2490234375
}
2022-10-24 15:19:10,691 - trainer - INFO - start training epoch 50
2022-10-24 15:19:10,692 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,692 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,693 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,703 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval training set based on eval_every=2***************
2022-10-24 15:19:10,704 - trainer - INFO - {
  "train_loss": 9706.828369140625
}
2022-10-24 15:19:10,710 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval development set based on eval_every=2***************
2022-10-24 15:19:10,710 - trainer - INFO - {
  "dev_loss": 6085.03369140625,
  "dev_best_score_for_loss": -6085.03369140625
}
2022-10-24 15:19:10,711 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:10,712 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,712 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,713 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_44
2022-10-24 15:19:10,715 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:10,719 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:10,719 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:10,719 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,720 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:10,721 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_50
2022-10-24 15:19:10,726 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_50
2022-10-24 15:19:10,728 - trainer - INFO - 
*****************[epoch: 50, global step: 51] eval training set at end of epoch***************
2022-10-24 15:19:10,729 - trainer - INFO - {
  "train_loss": 7290.40771484375
}
2022-10-24 15:19:10,732 - trainer - INFO - start training epoch 51
2022-10-24 15:19:10,733 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,734 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,734 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,743 - trainer - INFO - 
*****************[epoch: 51, global step: 52] eval training set at end of epoch***************
2022-10-24 15:19:10,743 - trainer - INFO - {
  "train_loss": 6085.0341796875
}
2022-10-24 15:19:10,744 - trainer - INFO - start training epoch 52
2022-10-24 15:19:10,747 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,748 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,749 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,757 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval training set based on eval_every=2***************
2022-10-24 15:19:10,757 - trainer - INFO - {
  "train_loss": 7165.74072265625
}
2022-10-24 15:19:10,769 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval development set based on eval_every=2***************
2022-10-24 15:19:10,769 - trainer - INFO - {
  "dev_loss": 10757.3935546875,
  "dev_best_score_for_loss": -6085.03369140625
}
2022-10-24 15:19:10,770 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:10,771 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,772 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,772 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,773 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_46
2022-10-24 15:19:10,774 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_52
2022-10-24 15:19:10,780 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_52
2022-10-24 15:19:10,781 - trainer - INFO - 
*****************[epoch: 52, global step: 53] eval training set at end of epoch***************
2022-10-24 15:19:10,782 - trainer - INFO - {
  "train_loss": 8246.447265625
}
2022-10-24 15:19:10,783 - trainer - INFO - start training epoch 53
2022-10-24 15:19:10,783 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,783 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,785 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,794 - trainer - INFO - 
*****************[epoch: 53, global step: 54] eval training set at end of epoch***************
2022-10-24 15:19:10,794 - trainer - INFO - {
  "train_loss": 10757.3935546875
}
2022-10-24 15:19:10,794 - trainer - INFO - start training epoch 54
2022-10-24 15:19:10,795 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,796 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,796 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,803 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval training set based on eval_every=2***************
2022-10-24 15:19:10,804 - trainer - INFO - {
  "train_loss": 10976.4990234375
}
2022-10-24 15:19:10,815 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval development set based on eval_every=2***************
2022-10-24 15:19:10,817 - trainer - INFO - {
  "dev_loss": 9297.607421875,
  "dev_best_score_for_loss": -6085.03369140625
}
2022-10-24 15:19:10,818 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:10,819 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,820 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,821 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,821 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_48
2022-10-24 15:19:10,823 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_54
2022-10-24 15:19:10,829 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_54
2022-10-24 15:19:10,830 - trainer - INFO - 
*****************[epoch: 54, global step: 55] eval training set at end of epoch***************
2022-10-24 15:19:10,831 - trainer - INFO - {
  "train_loss": 11195.6044921875
}
2022-10-24 15:19:10,832 - trainer - INFO - start training epoch 55
2022-10-24 15:19:10,832 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,833 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,833 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,841 - trainer - INFO - 
*****************[epoch: 55, global step: 56] eval training set at end of epoch***************
2022-10-24 15:19:10,841 - trainer - INFO - {
  "train_loss": 9297.607421875
}
2022-10-24 15:19:10,842 - trainer - INFO - start training epoch 56
2022-10-24 15:19:10,843 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,846 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,846 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,854 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval training set based on eval_every=2***************
2022-10-24 15:19:10,855 - trainer - INFO - {
  "train_loss": 8007.75732421875
}
2022-10-24 15:19:10,867 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval development set based on eval_every=2***************
2022-10-24 15:19:10,867 - trainer - INFO - {
  "dev_loss": 5588.87451171875,
  "dev_best_score_for_loss": -5588.87451171875
}
2022-10-24 15:19:10,868 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:10,870 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,870 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,871 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_50
2022-10-24 15:19:10,873 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:10,878 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:10,881 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:10,882 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,883 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:10,883 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_56
2022-10-24 15:19:10,889 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_56
2022-10-24 15:19:10,890 - trainer - INFO - 
*****************[epoch: 56, global step: 57] eval training set at end of epoch***************
2022-10-24 15:19:10,890 - trainer - INFO - {
  "train_loss": 6717.9072265625
}
2022-10-24 15:19:10,892 - trainer - INFO - start training epoch 57
2022-10-24 15:19:10,892 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,893 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,893 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,907 - trainer - INFO - 
*****************[epoch: 57, global step: 58] eval training set at end of epoch***************
2022-10-24 15:19:10,908 - trainer - INFO - {
  "train_loss": 5588.875
}
2022-10-24 15:19:10,909 - trainer - INFO - start training epoch 58
2022-10-24 15:19:10,909 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,911 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,911 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,922 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval training set based on eval_every=2***************
2022-10-24 15:19:10,923 - trainer - INFO - {
  "train_loss": 6094.669189453125
}
2022-10-24 15:19:10,933 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval development set based on eval_every=2***************
2022-10-24 15:19:10,934 - trainer - INFO - {
  "dev_loss": 8200.7822265625,
  "dev_best_score_for_loss": -5588.87451171875
}
2022-10-24 15:19:10,935 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:10,936 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,939 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,940 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,940 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_52
2022-10-24 15:19:10,943 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_58
2022-10-24 15:19:10,949 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_58
2022-10-24 15:19:10,950 - trainer - INFO - 
*****************[epoch: 58, global step: 59] eval training set at end of epoch***************
2022-10-24 15:19:10,951 - trainer - INFO - {
  "train_loss": 6600.46337890625
}
2022-10-24 15:19:10,952 - trainer - INFO - start training epoch 59
2022-10-24 15:19:10,952 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,953 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,953 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,964 - trainer - INFO - 
*****************[epoch: 59, global step: 60] eval training set at end of epoch***************
2022-10-24 15:19:10,965 - trainer - INFO - {
  "train_loss": 8200.7822265625
}
2022-10-24 15:19:10,965 - trainer - INFO - start training epoch 60
2022-10-24 15:19:10,966 - trainer - INFO - training using device=cuda
2022-10-24 15:19:10,966 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:10,967 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:10,974 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval training set based on eval_every=2***************
2022-10-24 15:19:10,974 - trainer - INFO - {
  "train_loss": 8258.43505859375
}
2022-10-24 15:19:10,984 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval development set based on eval_every=2***************
2022-10-24 15:19:10,984 - trainer - INFO - {
  "dev_loss": 6830.61376953125,
  "dev_best_score_for_loss": -5588.87451171875
}
2022-10-24 15:19:10,985 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:10,986 - trainer - INFO -   patience: 200
2022-10-24 15:19:10,989 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:10,989 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:10,989 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_54
2022-10-24 15:19:10,991 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_60
2022-10-24 15:19:10,996 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_60
2022-10-24 15:19:10,997 - trainer - INFO - 
*****************[epoch: 60, global step: 61] eval training set at end of epoch***************
2022-10-24 15:19:10,998 - trainer - INFO - {
  "train_loss": 8316.087890625
}
2022-10-24 15:19:10,999 - trainer - INFO - start training epoch 61
2022-10-24 15:19:10,999 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,000 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,000 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,008 - trainer - INFO - 
*****************[epoch: 61, global step: 62] eval training set at end of epoch***************
2022-10-24 15:19:11,009 - trainer - INFO - {
  "train_loss": 6830.61328125
}
2022-10-24 15:19:11,011 - trainer - INFO - start training epoch 62
2022-10-24 15:19:11,011 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,012 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,012 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,020 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval training set based on eval_every=2***************
2022-10-24 15:19:11,020 - trainer - INFO - {
  "train_loss": 6134.37646484375
}
2022-10-24 15:19:11,028 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval development set based on eval_every=2***************
2022-10-24 15:19:11,031 - trainer - INFO - {
  "dev_loss": 5355.52294921875,
  "dev_best_score_for_loss": -5355.52294921875
}
2022-10-24 15:19:11,032 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,035 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,035 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,036 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_56
2022-10-24 15:19:11,038 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,042 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,042 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,044 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,044 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,046 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_62
2022-10-24 15:19:11,051 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_62
2022-10-24 15:19:11,052 - trainer - INFO - 
*****************[epoch: 62, global step: 63] eval training set at end of epoch***************
2022-10-24 15:19:11,052 - trainer - INFO - {
  "train_loss": 5438.1396484375
}
2022-10-24 15:19:11,053 - trainer - INFO - start training epoch 63
2022-10-24 15:19:11,053 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,054 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,054 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,065 - trainer - INFO - 
*****************[epoch: 63, global step: 64] eval training set at end of epoch***************
2022-10-24 15:19:11,065 - trainer - INFO - {
  "train_loss": 5355.52294921875
}
2022-10-24 15:19:11,066 - trainer - INFO - start training epoch 64
2022-10-24 15:19:11,066 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,067 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,067 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,079 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval training set based on eval_every=2***************
2022-10-24 15:19:11,079 - trainer - INFO - {
  "train_loss": 5772.3310546875
}
2022-10-24 15:19:11,088 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval development set based on eval_every=2***************
2022-10-24 15:19:11,088 - trainer - INFO - {
  "dev_loss": 6775.68212890625,
  "dev_best_score_for_loss": -5355.52294921875
}
2022-10-24 15:19:11,089 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:11,090 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,091 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,092 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,092 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_58
2022-10-24 15:19:11,094 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_64
2022-10-24 15:19:11,100 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_64
2022-10-24 15:19:11,101 - trainer - INFO - 
*****************[epoch: 64, global step: 65] eval training set at end of epoch***************
2022-10-24 15:19:11,102 - trainer - INFO - {
  "train_loss": 6189.13916015625
}
2022-10-24 15:19:11,103 - trainer - INFO - start training epoch 65
2022-10-24 15:19:11,104 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,104 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,105 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,114 - trainer - INFO - 
*****************[epoch: 65, global step: 66] eval training set at end of epoch***************
2022-10-24 15:19:11,115 - trainer - INFO - {
  "train_loss": 6775.68212890625
}
2022-10-24 15:19:11,115 - trainer - INFO - start training epoch 66
2022-10-24 15:19:11,116 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,116 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,117 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,127 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval training set based on eval_every=2***************
2022-10-24 15:19:11,128 - trainer - INFO - {
  "train_loss": 6612.936279296875
}
2022-10-24 15:19:11,137 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval development set based on eval_every=2***************
2022-10-24 15:19:11,138 - trainer - INFO - {
  "dev_loss": 5531.296875,
  "dev_best_score_for_loss": -5355.52294921875
}
2022-10-24 15:19:11,138 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:11,139 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,140 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,140 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,141 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_60
2022-10-24 15:19:11,143 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_66
2022-10-24 15:19:11,147 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_66
2022-10-24 15:19:11,151 - trainer - INFO - 
*****************[epoch: 66, global step: 67] eval training set at end of epoch***************
2022-10-24 15:19:11,151 - trainer - INFO - {
  "train_loss": 6450.1904296875
}
2022-10-24 15:19:11,152 - trainer - INFO - start training epoch 67
2022-10-24 15:19:11,152 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,153 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,153 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,164 - trainer - INFO - 
*****************[epoch: 67, global step: 68] eval training set at end of epoch***************
2022-10-24 15:19:11,165 - trainer - INFO - {
  "train_loss": 5531.296875
}
2022-10-24 15:19:11,166 - trainer - INFO - start training epoch 68
2022-10-24 15:19:11,167 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,167 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,170 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,179 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval training set based on eval_every=2***************
2022-10-24 15:19:11,179 - trainer - INFO - {
  "train_loss": 5204.93603515625
}
2022-10-24 15:19:11,186 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval development set based on eval_every=2***************
2022-10-24 15:19:11,187 - trainer - INFO - {
  "dev_loss": 4995.0654296875,
  "dev_best_score_for_loss": -4995.0654296875
}
2022-10-24 15:19:11,188 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,189 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,190 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,190 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_62
2022-10-24 15:19:11,192 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,199 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,199 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,199 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,200 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,201 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_68
2022-10-24 15:19:11,205 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_68
2022-10-24 15:19:11,206 - trainer - INFO - 
*****************[epoch: 68, global step: 69] eval training set at end of epoch***************
2022-10-24 15:19:11,206 - trainer - INFO - {
  "train_loss": 4878.5751953125
}
2022-10-24 15:19:11,207 - trainer - INFO - start training epoch 69
2022-10-24 15:19:11,208 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,208 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,209 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,220 - trainer - INFO - 
*****************[epoch: 69, global step: 70] eval training set at end of epoch***************
2022-10-24 15:19:11,220 - trainer - INFO - {
  "train_loss": 4995.0654296875
}
2022-10-24 15:19:11,221 - trainer - INFO - start training epoch 70
2022-10-24 15:19:11,222 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,222 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,223 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,236 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval training set based on eval_every=2***************
2022-10-24 15:19:11,237 - trainer - INFO - {
  "train_loss": 5250.406005859375
}
2022-10-24 15:19:11,244 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval development set based on eval_every=2***************
2022-10-24 15:19:11,245 - trainer - INFO - {
  "dev_loss": 5633.92919921875,
  "dev_best_score_for_loss": -4995.0654296875
}
2022-10-24 15:19:11,246 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:11,246 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,247 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,247 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,248 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_64
2022-10-24 15:19:11,249 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_70
2022-10-24 15:19:11,253 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_70
2022-10-24 15:19:11,254 - trainer - INFO - 
*****************[epoch: 70, global step: 71] eval training set at end of epoch***************
2022-10-24 15:19:11,254 - trainer - INFO - {
  "train_loss": 5505.74658203125
}
2022-10-24 15:19:11,255 - trainer - INFO - start training epoch 71
2022-10-24 15:19:11,255 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,256 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,256 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,270 - trainer - INFO - 
*****************[epoch: 71, global step: 72] eval training set at end of epoch***************
2022-10-24 15:19:11,271 - trainer - INFO - {
  "train_loss": 5633.92919921875
}
2022-10-24 15:19:11,272 - trainer - INFO - start training epoch 72
2022-10-24 15:19:11,273 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,274 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,275 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,285 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval training set based on eval_every=2***************
2022-10-24 15:19:11,286 - trainer - INFO - {
  "train_loss": 5397.37060546875
}
2022-10-24 15:19:11,292 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval development set based on eval_every=2***************
2022-10-24 15:19:11,294 - trainer - INFO - {
  "dev_loss": 4607.06640625,
  "dev_best_score_for_loss": -4607.06640625
}
2022-10-24 15:19:11,297 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,298 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,299 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,299 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_66
2022-10-24 15:19:11,301 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,305 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,305 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,309 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,310 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,310 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_72
2022-10-24 15:19:11,315 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_72
2022-10-24 15:19:11,322 - trainer - INFO - 
*****************[epoch: 72, global step: 73] eval training set at end of epoch***************
2022-10-24 15:19:11,322 - trainer - INFO - {
  "train_loss": 5160.81201171875
}
2022-10-24 15:19:11,324 - trainer - INFO - start training epoch 73
2022-10-24 15:19:11,324 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,325 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,326 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,339 - trainer - INFO - 
*****************[epoch: 73, global step: 74] eval training set at end of epoch***************
2022-10-24 15:19:11,340 - trainer - INFO - {
  "train_loss": 4607.06689453125
}
2022-10-24 15:19:11,341 - trainer - INFO - start training epoch 74
2022-10-24 15:19:11,341 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,342 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,343 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,352 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval training set based on eval_every=2***************
2022-10-24 15:19:11,353 - trainer - INFO - {
  "train_loss": 4546.07958984375
}
2022-10-24 15:19:11,361 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval development set based on eval_every=2***************
2022-10-24 15:19:11,361 - trainer - INFO - {
  "dev_loss": 4724.22900390625,
  "dev_best_score_for_loss": -4607.06640625
}
2022-10-24 15:19:11,362 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:11,363 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,364 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,364 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,365 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_68
2022-10-24 15:19:11,369 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_74
2022-10-24 15:19:11,373 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_74
2022-10-24 15:19:11,374 - trainer - INFO - 
*****************[epoch: 74, global step: 75] eval training set at end of epoch***************
2022-10-24 15:19:11,375 - trainer - INFO - {
  "train_loss": 4485.09228515625
}
2022-10-24 15:19:11,376 - trainer - INFO - start training epoch 75
2022-10-24 15:19:11,377 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,377 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,378 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,387 - trainer - INFO - 
*****************[epoch: 75, global step: 76] eval training set at end of epoch***************
2022-10-24 15:19:11,387 - trainer - INFO - {
  "train_loss": 4724.22900390625
}
2022-10-24 15:19:11,388 - trainer - INFO - start training epoch 76
2022-10-24 15:19:11,388 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,389 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,389 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,400 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval training set based on eval_every=2***************
2022-10-24 15:19:11,401 - trainer - INFO - {
  "train_loss": 4805.268310546875
}
2022-10-24 15:19:11,406 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval development set based on eval_every=2***************
2022-10-24 15:19:11,407 - trainer - INFO - {
  "dev_loss": 4706.6689453125,
  "dev_best_score_for_loss": -4607.06640625
}
2022-10-24 15:19:11,408 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:11,408 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,409 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,413 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,414 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_70
2022-10-24 15:19:11,416 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_76
2022-10-24 15:19:11,420 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_76
2022-10-24 15:19:11,421 - trainer - INFO - 
*****************[epoch: 76, global step: 77] eval training set at end of epoch***************
2022-10-24 15:19:11,421 - trainer - INFO - {
  "train_loss": 4886.3076171875
}
2022-10-24 15:19:11,422 - trainer - INFO - start training epoch 77
2022-10-24 15:19:11,422 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,422 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,423 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,434 - trainer - INFO - 
*****************[epoch: 77, global step: 78] eval training set at end of epoch***************
2022-10-24 15:19:11,435 - trainer - INFO - {
  "train_loss": 4706.6689453125
}
2022-10-24 15:19:11,436 - trainer - INFO - start training epoch 78
2022-10-24 15:19:11,436 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,436 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,436 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,444 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval training set based on eval_every=2***************
2022-10-24 15:19:11,445 - trainer - INFO - {
  "train_loss": 4520.811279296875
}
2022-10-24 15:19:11,452 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval development set based on eval_every=2***************
2022-10-24 15:19:11,452 - trainer - INFO - {
  "dev_loss": 4111.7509765625,
  "dev_best_score_for_loss": -4111.7509765625
}
2022-10-24 15:19:11,453 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,455 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,455 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,455 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_72
2022-10-24 15:19:11,457 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,461 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,465 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,466 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,467 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,467 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_78
2022-10-24 15:19:11,472 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_78
2022-10-24 15:19:11,473 - trainer - INFO - 
*****************[epoch: 78, global step: 79] eval training set at end of epoch***************
2022-10-24 15:19:11,476 - trainer - INFO - {
  "train_loss": 4334.95361328125
}
2022-10-24 15:19:11,476 - trainer - INFO - start training epoch 79
2022-10-24 15:19:11,476 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,477 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,477 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,487 - trainer - INFO - 
*****************[epoch: 79, global step: 80] eval training set at end of epoch***************
2022-10-24 15:19:11,488 - trainer - INFO - {
  "train_loss": 4111.7509765625
}
2022-10-24 15:19:11,488 - trainer - INFO - start training epoch 80
2022-10-24 15:19:11,488 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,489 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,489 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,498 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval training set based on eval_every=2***************
2022-10-24 15:19:11,499 - trainer - INFO - {
  "train_loss": 4139.421875
}
2022-10-24 15:19:11,506 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval development set based on eval_every=2***************
2022-10-24 15:19:11,507 - trainer - INFO - {
  "dev_loss": 4289.38232421875,
  "dev_best_score_for_loss": -4111.7509765625
}
2022-10-24 15:19:11,509 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:11,513 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,514 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,514 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,515 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_74
2022-10-24 15:19:11,516 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_80
2022-10-24 15:19:11,522 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_80
2022-10-24 15:19:11,523 - trainer - INFO - 
*****************[epoch: 80, global step: 81] eval training set at end of epoch***************
2022-10-24 15:19:11,524 - trainer - INFO - {
  "train_loss": 4167.0927734375
}
2022-10-24 15:19:11,524 - trainer - INFO - start training epoch 81
2022-10-24 15:19:11,524 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,525 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,525 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,535 - trainer - INFO - 
*****************[epoch: 81, global step: 82] eval training set at end of epoch***************
2022-10-24 15:19:11,535 - trainer - INFO - {
  "train_loss": 4289.38232421875
}
2022-10-24 15:19:11,537 - trainer - INFO - start training epoch 82
2022-10-24 15:19:11,537 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,537 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,538 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,546 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval training set based on eval_every=2***************
2022-10-24 15:19:11,546 - trainer - INFO - {
  "train_loss": 4253.667724609375
}
2022-10-24 15:19:11,553 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval development set based on eval_every=2***************
2022-10-24 15:19:11,555 - trainer - INFO - {
  "dev_loss": 3971.832763671875,
  "dev_best_score_for_loss": -3971.832763671875
}
2022-10-24 15:19:11,556 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,560 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,560 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,561 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_76
2022-10-24 15:19:11,562 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,565 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,566 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,566 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,567 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,568 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_82
2022-10-24 15:19:11,573 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_82
2022-10-24 15:19:11,574 - trainer - INFO - 
*****************[epoch: 82, global step: 83] eval training set at end of epoch***************
2022-10-24 15:19:11,574 - trainer - INFO - {
  "train_loss": 4217.953125
}
2022-10-24 15:19:11,575 - trainer - INFO - start training epoch 83
2022-10-24 15:19:11,575 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,575 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,576 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,582 - trainer - INFO - 
*****************[epoch: 83, global step: 84] eval training set at end of epoch***************
2022-10-24 15:19:11,582 - trainer - INFO - {
  "train_loss": 3971.832763671875
}
2022-10-24 15:19:11,584 - trainer - INFO - start training epoch 84
2022-10-24 15:19:11,584 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,585 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,585 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,591 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval training set based on eval_every=2***************
2022-10-24 15:19:11,591 - trainer - INFO - {
  "train_loss": 3876.414306640625
}
2022-10-24 15:19:11,600 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval development set based on eval_every=2***************
2022-10-24 15:19:11,604 - trainer - INFO - {
  "dev_loss": 3766.251953125,
  "dev_best_score_for_loss": -3766.251953125
}
2022-10-24 15:19:11,605 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,607 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,607 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,607 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_78
2022-10-24 15:19:11,609 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,614 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,614 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,614 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,615 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,615 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_84
2022-10-24 15:19:11,620 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_84
2022-10-24 15:19:11,621 - trainer - INFO - 
*****************[epoch: 84, global step: 85] eval training set at end of epoch***************
2022-10-24 15:19:11,621 - trainer - INFO - {
  "train_loss": 3780.995849609375
}
2022-10-24 15:19:11,622 - trainer - INFO - start training epoch 85
2022-10-24 15:19:11,622 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,622 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,623 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,632 - trainer - INFO - 
*****************[epoch: 85, global step: 86] eval training set at end of epoch***************
2022-10-24 15:19:11,632 - trainer - INFO - {
  "train_loss": 3766.25146484375
}
2022-10-24 15:19:11,633 - trainer - INFO - start training epoch 86
2022-10-24 15:19:11,633 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,633 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,633 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,640 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval training set based on eval_every=2***************
2022-10-24 15:19:11,640 - trainer - INFO - {
  "train_loss": 3792.0439453125
}
2022-10-24 15:19:11,651 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval development set based on eval_every=2***************
2022-10-24 15:19:11,652 - trainer - INFO - {
  "dev_loss": 3771.29638671875,
  "dev_best_score_for_loss": -3766.251953125
}
2022-10-24 15:19:11,652 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:11,653 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,654 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,654 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,654 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_80
2022-10-24 15:19:11,656 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_86
2022-10-24 15:19:11,661 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_86
2022-10-24 15:19:11,661 - trainer - INFO - 
*****************[epoch: 86, global step: 87] eval training set at end of epoch***************
2022-10-24 15:19:11,662 - trainer - INFO - {
  "train_loss": 3817.83642578125
}
2022-10-24 15:19:11,662 - trainer - INFO - start training epoch 87
2022-10-24 15:19:11,663 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,663 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,663 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,670 - trainer - INFO - 
*****************[epoch: 87, global step: 88] eval training set at end of epoch***************
2022-10-24 15:19:11,671 - trainer - INFO - {
  "train_loss": 3771.29638671875
}
2022-10-24 15:19:11,671 - trainer - INFO - start training epoch 88
2022-10-24 15:19:11,671 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,671 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,672 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,678 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval training set based on eval_every=2***************
2022-10-24 15:19:11,678 - trainer - INFO - {
  "train_loss": 3689.9078369140625
}
2022-10-24 15:19:11,683 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval development set based on eval_every=2***************
2022-10-24 15:19:11,684 - trainer - INFO - {
  "dev_loss": 3453.841796875,
  "dev_best_score_for_loss": -3453.841796875
}
2022-10-24 15:19:11,684 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,686 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,686 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,686 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_82
2022-10-24 15:19:11,687 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,690 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,690 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,691 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,694 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,694 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_88
2022-10-24 15:19:11,701 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_88
2022-10-24 15:19:11,702 - trainer - INFO - 
*****************[epoch: 88, global step: 89] eval training set at end of epoch***************
2022-10-24 15:19:11,702 - trainer - INFO - {
  "train_loss": 3608.519287109375
}
2022-10-24 15:19:11,702 - trainer - INFO - start training epoch 89
2022-10-24 15:19:11,702 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,703 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,703 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,717 - trainer - INFO - 
*****************[epoch: 89, global step: 90] eval training set at end of epoch***************
2022-10-24 15:19:11,717 - trainer - INFO - {
  "train_loss": 3453.841796875
}
2022-10-24 15:19:11,718 - trainer - INFO - start training epoch 90
2022-10-24 15:19:11,718 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,718 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,719 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,727 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval training set based on eval_every=2***************
2022-10-24 15:19:11,728 - trainer - INFO - {
  "train_loss": 3428.99609375
}
2022-10-24 15:19:11,734 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval development set based on eval_every=2***************
2022-10-24 15:19:11,735 - trainer - INFO - {
  "dev_loss": 3412.5498046875,
  "dev_best_score_for_loss": -3412.5498046875
}
2022-10-24 15:19:11,735 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,737 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,737 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,737 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_84
2022-10-24 15:19:11,740 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,747 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,747 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,748 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,749 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,749 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_90
2022-10-24 15:19:11,755 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_90
2022-10-24 15:19:11,756 - trainer - INFO - 
*****************[epoch: 90, global step: 91] eval training set at end of epoch***************
2022-10-24 15:19:11,756 - trainer - INFO - {
  "train_loss": 3404.150390625
}
2022-10-24 15:19:11,757 - trainer - INFO - start training epoch 91
2022-10-24 15:19:11,757 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,757 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,757 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,766 - trainer - INFO - 
*****************[epoch: 91, global step: 92] eval training set at end of epoch***************
2022-10-24 15:19:11,767 - trainer - INFO - {
  "train_loss": 3412.5498046875
}
2022-10-24 15:19:11,767 - trainer - INFO - start training epoch 92
2022-10-24 15:19:11,767 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,768 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,768 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,774 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval training set based on eval_every=2***************
2022-10-24 15:19:11,774 - trainer - INFO - {
  "train_loss": 3389.4688720703125
}
2022-10-24 15:19:11,780 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval development set based on eval_every=2***************
2022-10-24 15:19:11,780 - trainer - INFO - {
  "dev_loss": 3242.311767578125,
  "dev_best_score_for_loss": -3242.311767578125
}
2022-10-24 15:19:11,781 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,782 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,782 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,782 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_86
2022-10-24 15:19:11,784 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,788 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,788 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,791 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,792 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,792 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_92
2022-10-24 15:19:11,798 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_92
2022-10-24 15:19:11,801 - trainer - INFO - 
*****************[epoch: 92, global step: 93] eval training set at end of epoch***************
2022-10-24 15:19:11,801 - trainer - INFO - {
  "train_loss": 3366.387939453125
}
2022-10-24 15:19:11,801 - trainer - INFO - start training epoch 93
2022-10-24 15:19:11,802 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,802 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,802 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,811 - trainer - INFO - 
*****************[epoch: 93, global step: 94] eval training set at end of epoch***************
2022-10-24 15:19:11,811 - trainer - INFO - {
  "train_loss": 3242.311767578125
}
2022-10-24 15:19:11,812 - trainer - INFO - start training epoch 94
2022-10-24 15:19:11,812 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,812 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,813 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,823 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval training set based on eval_every=2***************
2022-10-24 15:19:11,823 - trainer - INFO - {
  "train_loss": 3182.4163818359375
}
2022-10-24 15:19:11,832 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval development set based on eval_every=2***************
2022-10-24 15:19:11,833 - trainer - INFO - {
  "dev_loss": 3069.597412109375,
  "dev_best_score_for_loss": -3069.597412109375
}
2022-10-24 15:19:11,834 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,838 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,838 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,838 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_88
2022-10-24 15:19:11,840 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,844 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,845 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,845 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,846 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,847 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_94
2022-10-24 15:19:11,852 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_94
2022-10-24 15:19:11,852 - trainer - INFO - 
*****************[epoch: 94, global step: 95] eval training set at end of epoch***************
2022-10-24 15:19:11,853 - trainer - INFO - {
  "train_loss": 3122.52099609375
}
2022-10-24 15:19:11,853 - trainer - INFO - start training epoch 95
2022-10-24 15:19:11,854 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,854 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,854 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,863 - trainer - INFO - 
*****************[epoch: 95, global step: 96] eval training set at end of epoch***************
2022-10-24 15:19:11,864 - trainer - INFO - {
  "train_loss": 3069.597412109375
}
2022-10-24 15:19:11,864 - trainer - INFO - start training epoch 96
2022-10-24 15:19:11,865 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,865 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,865 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,871 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval training set based on eval_every=2***************
2022-10-24 15:19:11,872 - trainer - INFO - {
  "train_loss": 3059.781005859375
}
2022-10-24 15:19:11,878 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval development set based on eval_every=2***************
2022-10-24 15:19:11,879 - trainer - INFO - {
  "dev_loss": 2994.79052734375,
  "dev_best_score_for_loss": -2994.79052734375
}
2022-10-24 15:19:11,882 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,884 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,884 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,884 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_90
2022-10-24 15:19:11,885 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,889 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,889 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,889 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,890 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,890 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_96
2022-10-24 15:19:11,896 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_96
2022-10-24 15:19:11,897 - trainer - INFO - 
*****************[epoch: 96, global step: 97] eval training set at end of epoch***************
2022-10-24 15:19:11,897 - trainer - INFO - {
  "train_loss": 3049.964599609375
}
2022-10-24 15:19:11,898 - trainer - INFO - start training epoch 97
2022-10-24 15:19:11,898 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,898 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,899 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,905 - trainer - INFO - 
*****************[epoch: 97, global step: 98] eval training set at end of epoch***************
2022-10-24 15:19:11,905 - trainer - INFO - {
  "train_loss": 2994.791015625
}
2022-10-24 15:19:11,906 - trainer - INFO - start training epoch 98
2022-10-24 15:19:11,906 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,906 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,906 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,915 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval training set based on eval_every=2***************
2022-10-24 15:19:11,916 - trainer - INFO - {
  "train_loss": 2944.158203125
}
2022-10-24 15:19:11,922 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval development set based on eval_every=2***************
2022-10-24 15:19:11,922 - trainer - INFO - {
  "dev_loss": 2797.48046875,
  "dev_best_score_for_loss": -2797.48046875
}
2022-10-24 15:19:11,923 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,924 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,924 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,925 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_92
2022-10-24 15:19:11,926 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,929 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,929 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,930 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,931 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,931 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_98
2022-10-24 15:19:11,935 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_98
2022-10-24 15:19:11,935 - trainer - INFO - 
*****************[epoch: 98, global step: 99] eval training set at end of epoch***************
2022-10-24 15:19:11,936 - trainer - INFO - {
  "train_loss": 2893.525390625
}
2022-10-24 15:19:11,936 - trainer - INFO - start training epoch 99
2022-10-24 15:19:11,936 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,936 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,937 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,944 - trainer - INFO - 
*****************[epoch: 99, global step: 100] eval training set at end of epoch***************
2022-10-24 15:19:11,944 - trainer - INFO - {
  "train_loss": 2797.48046875
}
2022-10-24 15:19:11,944 - trainer - INFO - start training epoch 100
2022-10-24 15:19:11,945 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,945 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,945 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,951 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval training set based on eval_every=2***************
2022-10-24 15:19:11,951 - trainer - INFO - {
  "train_loss": 2770.5552978515625
}
2022-10-24 15:19:11,957 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval development set based on eval_every=2***************
2022-10-24 15:19:11,961 - trainer - INFO - {
  "dev_loss": 2707.593017578125,
  "dev_best_score_for_loss": -2707.593017578125
}
2022-10-24 15:19:11,962 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:11,963 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:11,963 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:11,963 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_94
2022-10-24 15:19:11,965 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:11,968 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:11,968 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:11,968 - trainer - INFO -   patience: 200
2022-10-24 15:19:11,969 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:11,970 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_100
2022-10-24 15:19:11,975 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_100
2022-10-24 15:19:11,976 - trainer - INFO - 
*****************[epoch: 100, global step: 101] eval training set at end of epoch***************
2022-10-24 15:19:11,976 - trainer - INFO - {
  "train_loss": 2743.630126953125
}
2022-10-24 15:19:11,976 - trainer - INFO - start training epoch 101
2022-10-24 15:19:11,977 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,977 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,977 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,984 - trainer - INFO - 
*****************[epoch: 101, global step: 102] eval training set at end of epoch***************
2022-10-24 15:19:11,984 - trainer - INFO - {
  "train_loss": 2707.593017578125
}
2022-10-24 15:19:11,984 - trainer - INFO - start training epoch 102
2022-10-24 15:19:11,984 - trainer - INFO - training using device=cuda
2022-10-24 15:19:11,985 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:11,985 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:11,992 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval training set based on eval_every=2***************
2022-10-24 15:19:11,993 - trainer - INFO - {
  "train_loss": 2676.550048828125
}
2022-10-24 15:19:11,999 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval development set based on eval_every=2***************
2022-10-24 15:19:11,999 - trainer - INFO - {
  "dev_loss": 2557.973388671875,
  "dev_best_score_for_loss": -2557.973388671875
}
2022-10-24 15:19:12,000 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,001 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,001 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,002 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_96
2022-10-24 15:19:12,007 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,010 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,011 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,011 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,012 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,012 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_102
2022-10-24 15:19:12,020 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_102
2022-10-24 15:19:12,021 - trainer - INFO - 
*****************[epoch: 102, global step: 103] eval training set at end of epoch***************
2022-10-24 15:19:12,021 - trainer - INFO - {
  "train_loss": 2645.507080078125
}
2022-10-24 15:19:12,022 - trainer - INFO - start training epoch 103
2022-10-24 15:19:12,022 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,022 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,023 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,033 - trainer - INFO - 
*****************[epoch: 103, global step: 104] eval training set at end of epoch***************
2022-10-24 15:19:12,033 - trainer - INFO - {
  "train_loss": 2557.97314453125
}
2022-10-24 15:19:12,034 - trainer - INFO - start training epoch 104
2022-10-24 15:19:12,035 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,038 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,038 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,047 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval training set based on eval_every=2***************
2022-10-24 15:19:12,048 - trainer - INFO - {
  "train_loss": 2519.1298828125
}
2022-10-24 15:19:12,054 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval development set based on eval_every=2***************
2022-10-24 15:19:12,055 - trainer - INFO - {
  "dev_loss": 2428.198974609375,
  "dev_best_score_for_loss": -2428.198974609375
}
2022-10-24 15:19:12,055 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,056 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,057 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,057 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_98
2022-10-24 15:19:12,059 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,063 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,063 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,064 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,065 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,065 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_104
2022-10-24 15:19:12,069 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_104
2022-10-24 15:19:12,069 - trainer - INFO - 
*****************[epoch: 104, global step: 105] eval training set at end of epoch***************
2022-10-24 15:19:12,070 - trainer - INFO - {
  "train_loss": 2480.28662109375
}
2022-10-24 15:19:12,070 - trainer - INFO - start training epoch 105
2022-10-24 15:19:12,070 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,070 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,071 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,078 - trainer - INFO - 
*****************[epoch: 105, global step: 106] eval training set at end of epoch***************
2022-10-24 15:19:12,078 - trainer - INFO - {
  "train_loss": 2428.198974609375
}
2022-10-24 15:19:12,078 - trainer - INFO - start training epoch 106
2022-10-24 15:19:12,079 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,080 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,082 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,091 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval training set based on eval_every=2***************
2022-10-24 15:19:12,092 - trainer - INFO - {
  "train_loss": 2404.2799072265625
}
2022-10-24 15:19:12,102 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval development set based on eval_every=2***************
2022-10-24 15:19:12,102 - trainer - INFO - {
  "dev_loss": 2314.404296875,
  "dev_best_score_for_loss": -2314.404296875
}
2022-10-24 15:19:12,103 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,104 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,104 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,105 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_100
2022-10-24 15:19:12,106 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,110 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,110 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,111 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,111 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,112 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_106
2022-10-24 15:19:12,116 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_106
2022-10-24 15:19:12,116 - trainer - INFO - 
*****************[epoch: 106, global step: 107] eval training set at end of epoch***************
2022-10-24 15:19:12,117 - trainer - INFO - {
  "train_loss": 2380.36083984375
}
2022-10-24 15:19:12,117 - trainer - INFO - start training epoch 107
2022-10-24 15:19:12,117 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,118 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,118 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,126 - trainer - INFO - 
*****************[epoch: 107, global step: 108] eval training set at end of epoch***************
2022-10-24 15:19:12,127 - trainer - INFO - {
  "train_loss": 2314.404296875
}
2022-10-24 15:19:12,128 - trainer - INFO - start training epoch 108
2022-10-24 15:19:12,129 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,132 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,132 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,140 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval training set based on eval_every=2***************
2022-10-24 15:19:12,140 - trainer - INFO - {
  "train_loss": 2276.2315673828125
}
2022-10-24 15:19:12,147 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval development set based on eval_every=2***************
2022-10-24 15:19:12,147 - trainer - INFO - {
  "dev_loss": 2172.33740234375,
  "dev_best_score_for_loss": -2172.33740234375
}
2022-10-24 15:19:12,148 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,149 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,149 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,149 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_102
2022-10-24 15:19:12,151 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,154 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,155 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,155 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,156 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,157 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_108
2022-10-24 15:19:12,162 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_108
2022-10-24 15:19:12,163 - trainer - INFO - 
*****************[epoch: 108, global step: 109] eval training set at end of epoch***************
2022-10-24 15:19:12,163 - trainer - INFO - {
  "train_loss": 2238.058837890625
}
2022-10-24 15:19:12,164 - trainer - INFO - start training epoch 109
2022-10-24 15:19:12,164 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,164 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,165 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,177 - trainer - INFO - 
*****************[epoch: 109, global step: 110] eval training set at end of epoch***************
2022-10-24 15:19:12,177 - trainer - INFO - {
  "train_loss": 2172.33740234375
}
2022-10-24 15:19:12,178 - trainer - INFO - start training epoch 110
2022-10-24 15:19:12,178 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,178 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,179 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,186 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval training set based on eval_every=2***************
2022-10-24 15:19:12,186 - trainer - INFO - {
  "train_loss": 2146.914306640625
}
2022-10-24 15:19:12,195 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval development set based on eval_every=2***************
2022-10-24 15:19:12,196 - trainer - INFO - {
  "dev_loss": 2067.85302734375,
  "dev_best_score_for_loss": -2067.85302734375
}
2022-10-24 15:19:12,197 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,198 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,198 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,199 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_104
2022-10-24 15:19:12,200 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,204 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,205 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,205 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,208 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,209 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_110
2022-10-24 15:19:12,213 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_110
2022-10-24 15:19:12,214 - trainer - INFO - 
*****************[epoch: 110, global step: 111] eval training set at end of epoch***************
2022-10-24 15:19:12,214 - trainer - INFO - {
  "train_loss": 2121.4912109375
}
2022-10-24 15:19:12,215 - trainer - INFO - start training epoch 111
2022-10-24 15:19:12,215 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,215 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,216 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,223 - trainer - INFO - 
*****************[epoch: 111, global step: 112] eval training set at end of epoch***************
2022-10-24 15:19:12,223 - trainer - INFO - {
  "train_loss": 2067.85302734375
}
2022-10-24 15:19:12,223 - trainer - INFO - start training epoch 112
2022-10-24 15:19:12,223 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,224 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,224 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,232 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval training set based on eval_every=2***************
2022-10-24 15:19:12,233 - trainer - INFO - {
  "train_loss": 2034.921142578125
}
2022-10-24 15:19:12,241 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval development set based on eval_every=2***************
2022-10-24 15:19:12,241 - trainer - INFO - {
  "dev_loss": 1934.544677734375,
  "dev_best_score_for_loss": -1934.544677734375
}
2022-10-24 15:19:12,242 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,243 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,243 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,244 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_106
2022-10-24 15:19:12,245 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,249 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,250 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,250 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,251 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,251 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_112
2022-10-24 15:19:12,255 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_112
2022-10-24 15:19:12,256 - trainer - INFO - 
*****************[epoch: 112, global step: 113] eval training set at end of epoch***************
2022-10-24 15:19:12,256 - trainer - INFO - {
  "train_loss": 2001.9892578125
}
2022-10-24 15:19:12,257 - trainer - INFO - start training epoch 113
2022-10-24 15:19:12,257 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,257 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,258 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,267 - trainer - INFO - 
*****************[epoch: 113, global step: 114] eval training set at end of epoch***************
2022-10-24 15:19:12,271 - trainer - INFO - {
  "train_loss": 1934.544677734375
}
2022-10-24 15:19:12,271 - trainer - INFO - start training epoch 114
2022-10-24 15:19:12,271 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,272 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,272 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,281 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval training set based on eval_every=2***************
2022-10-24 15:19:12,282 - trainer - INFO - {
  "train_loss": 1905.7322387695312
}
2022-10-24 15:19:12,288 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval development set based on eval_every=2***************
2022-10-24 15:19:12,288 - trainer - INFO - {
  "dev_loss": 1824.7659912109375,
  "dev_best_score_for_loss": -1824.7659912109375
}
2022-10-24 15:19:12,289 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,290 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,290 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,290 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_108
2022-10-24 15:19:12,292 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,295 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,295 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,297 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,298 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,298 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_114
2022-10-24 15:19:12,302 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_114
2022-10-24 15:19:12,303 - trainer - INFO - 
*****************[epoch: 114, global step: 115] eval training set at end of epoch***************
2022-10-24 15:19:12,303 - trainer - INFO - {
  "train_loss": 1876.9197998046875
}
2022-10-24 15:19:12,303 - trainer - INFO - start training epoch 115
2022-10-24 15:19:12,304 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,304 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,304 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,317 - trainer - INFO - 
*****************[epoch: 115, global step: 116] eval training set at end of epoch***************
2022-10-24 15:19:12,317 - trainer - INFO - {
  "train_loss": 1824.7659912109375
}
2022-10-24 15:19:12,317 - trainer - INFO - start training epoch 116
2022-10-24 15:19:12,318 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,318 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,318 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,326 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval training set based on eval_every=2***************
2022-10-24 15:19:12,326 - trainer - INFO - {
  "train_loss": 1796.3753662109375
}
2022-10-24 15:19:12,335 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval development set based on eval_every=2***************
2022-10-24 15:19:12,335 - trainer - INFO - {
  "dev_loss": 1705.4796142578125,
  "dev_best_score_for_loss": -1705.4796142578125
}
2022-10-24 15:19:12,336 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,337 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,337 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,337 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_110
2022-10-24 15:19:12,339 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,342 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,343 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,344 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,345 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,348 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_116
2022-10-24 15:19:12,351 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_116
2022-10-24 15:19:12,352 - trainer - INFO - 
*****************[epoch: 116, global step: 117] eval training set at end of epoch***************
2022-10-24 15:19:12,352 - trainer - INFO - {
  "train_loss": 1767.9847412109375
}
2022-10-24 15:19:12,353 - trainer - INFO - start training epoch 117
2022-10-24 15:19:12,353 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,353 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,354 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,364 - trainer - INFO - 
*****************[epoch: 117, global step: 118] eval training set at end of epoch***************
2022-10-24 15:19:12,364 - trainer - INFO - {
  "train_loss": 1705.4796142578125
}
2022-10-24 15:19:12,365 - trainer - INFO - start training epoch 118
2022-10-24 15:19:12,365 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,365 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,366 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,372 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval training set based on eval_every=2***************
2022-10-24 15:19:12,373 - trainer - INFO - {
  "train_loss": 1675.7664794921875
}
2022-10-24 15:19:12,379 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval development set based on eval_every=2***************
2022-10-24 15:19:12,379 - trainer - INFO - {
  "dev_loss": 1593.23193359375,
  "dev_best_score_for_loss": -1593.23193359375
}
2022-10-24 15:19:12,380 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,381 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,381 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,381 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_112
2022-10-24 15:19:12,383 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,386 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,386 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,386 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,387 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,387 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_118
2022-10-24 15:19:12,392 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_118
2022-10-24 15:19:12,396 - trainer - INFO - 
*****************[epoch: 118, global step: 119] eval training set at end of epoch***************
2022-10-24 15:19:12,396 - trainer - INFO - {
  "train_loss": 1646.0533447265625
}
2022-10-24 15:19:12,396 - trainer - INFO - start training epoch 119
2022-10-24 15:19:12,397 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,397 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,397 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,409 - trainer - INFO - 
*****************[epoch: 119, global step: 120] eval training set at end of epoch***************
2022-10-24 15:19:12,409 - trainer - INFO - {
  "train_loss": 1593.23193359375
}
2022-10-24 15:19:12,411 - trainer - INFO - start training epoch 120
2022-10-24 15:19:12,411 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,411 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,412 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,421 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval training set based on eval_every=2***************
2022-10-24 15:19:12,422 - trainer - INFO - {
  "train_loss": 1567.0195922851562
}
2022-10-24 15:19:12,431 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval development set based on eval_every=2***************
2022-10-24 15:19:12,431 - trainer - INFO - {
  "dev_loss": 1483.9744873046875,
  "dev_best_score_for_loss": -1483.9744873046875
}
2022-10-24 15:19:12,432 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,433 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,433 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,434 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_114
2022-10-24 15:19:12,435 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,439 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,439 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,440 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,440 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,441 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_120
2022-10-24 15:19:12,446 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_120
2022-10-24 15:19:12,447 - trainer - INFO - 
*****************[epoch: 120, global step: 121] eval training set at end of epoch***************
2022-10-24 15:19:12,448 - trainer - INFO - {
  "train_loss": 1540.8072509765625
}
2022-10-24 15:19:12,448 - trainer - INFO - start training epoch 121
2022-10-24 15:19:12,448 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,449 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,449 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,458 - trainer - INFO - 
*****************[epoch: 121, global step: 122] eval training set at end of epoch***************
2022-10-24 15:19:12,458 - trainer - INFO - {
  "train_loss": 1483.974365234375
}
2022-10-24 15:19:12,459 - trainer - INFO - start training epoch 122
2022-10-24 15:19:12,459 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,459 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,459 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,468 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval training set based on eval_every=2***************
2022-10-24 15:19:12,468 - trainer - INFO - {
  "train_loss": 1455.2960815429688
}
2022-10-24 15:19:12,473 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval development set based on eval_every=2***************
2022-10-24 15:19:12,473 - trainer - INFO - {
  "dev_loss": 1373.8658447265625,
  "dev_best_score_for_loss": -1373.8658447265625
}
2022-10-24 15:19:12,474 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,475 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,476 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,476 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_116
2022-10-24 15:19:12,477 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,480 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,480 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,481 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,482 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,482 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_122
2022-10-24 15:19:12,487 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_122
2022-10-24 15:19:12,488 - trainer - INFO - 
*****************[epoch: 122, global step: 123] eval training set at end of epoch***************
2022-10-24 15:19:12,488 - trainer - INFO - {
  "train_loss": 1426.6177978515625
}
2022-10-24 15:19:12,489 - trainer - INFO - start training epoch 123
2022-10-24 15:19:12,489 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,490 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,490 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,499 - trainer - INFO - 
*****************[epoch: 123, global step: 124] eval training set at end of epoch***************
2022-10-24 15:19:12,499 - trainer - INFO - {
  "train_loss": 1373.8658447265625
}
2022-10-24 15:19:12,500 - trainer - INFO - start training epoch 124
2022-10-24 15:19:12,500 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,500 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,501 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,508 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval training set based on eval_every=2***************
2022-10-24 15:19:12,509 - trainer - INFO - {
  "train_loss": 1348.8776245117188
}
2022-10-24 15:19:12,514 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval development set based on eval_every=2***************
2022-10-24 15:19:12,515 - trainer - INFO - {
  "dev_loss": 1272.32470703125,
  "dev_best_score_for_loss": -1272.32470703125
}
2022-10-24 15:19:12,515 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,517 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,517 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,517 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_118
2022-10-24 15:19:12,518 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,522 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,522 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,522 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,523 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,523 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_124
2022-10-24 15:19:12,527 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_124
2022-10-24 15:19:12,528 - trainer - INFO - 
*****************[epoch: 124, global step: 125] eval training set at end of epoch***************
2022-10-24 15:19:12,529 - trainer - INFO - {
  "train_loss": 1323.889404296875
}
2022-10-24 15:19:12,530 - trainer - INFO - start training epoch 125
2022-10-24 15:19:12,531 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,532 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,535 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,544 - trainer - INFO - 
*****************[epoch: 125, global step: 126] eval training set at end of epoch***************
2022-10-24 15:19:12,545 - trainer - INFO - {
  "train_loss": 1272.3248291015625
}
2022-10-24 15:19:12,546 - trainer - INFO - start training epoch 126
2022-10-24 15:19:12,546 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,546 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,547 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,554 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval training set based on eval_every=2***************
2022-10-24 15:19:12,555 - trainer - INFO - {
  "train_loss": 1245.5316772460938
}
2022-10-24 15:19:12,563 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval development set based on eval_every=2***************
2022-10-24 15:19:12,564 - trainer - INFO - {
  "dev_loss": 1167.642578125,
  "dev_best_score_for_loss": -1167.642578125
}
2022-10-24 15:19:12,564 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,566 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,566 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,567 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_120
2022-10-24 15:19:12,568 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,572 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,572 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,573 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,574 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,574 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_126
2022-10-24 15:19:12,579 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_126
2022-10-24 15:19:12,583 - trainer - INFO - 
*****************[epoch: 126, global step: 127] eval training set at end of epoch***************
2022-10-24 15:19:12,584 - trainer - INFO - {
  "train_loss": 1218.738525390625
}
2022-10-24 15:19:12,584 - trainer - INFO - start training epoch 127
2022-10-24 15:19:12,584 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,585 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,585 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,597 - trainer - INFO - 
*****************[epoch: 127, global step: 128] eval training set at end of epoch***************
2022-10-24 15:19:12,598 - trainer - INFO - {
  "train_loss": 1167.642578125
}
2022-10-24 15:19:12,598 - trainer - INFO - start training epoch 128
2022-10-24 15:19:12,599 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,599 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,599 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,607 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval training set based on eval_every=2***************
2022-10-24 15:19:12,607 - trainer - INFO - {
  "train_loss": 1144.0051879882812
}
2022-10-24 15:19:12,614 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval development set based on eval_every=2***************
2022-10-24 15:19:12,614 - trainer - INFO - {
  "dev_loss": 1072.5748291015625,
  "dev_best_score_for_loss": -1072.5748291015625
}
2022-10-24 15:19:12,615 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,616 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,617 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,617 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_122
2022-10-24 15:19:12,618 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,621 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,621 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,622 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,626 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,626 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_128
2022-10-24 15:19:12,631 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_128
2022-10-24 15:19:12,632 - trainer - INFO - 
*****************[epoch: 128, global step: 129] eval training set at end of epoch***************
2022-10-24 15:19:12,633 - trainer - INFO - {
  "train_loss": 1120.3677978515625
}
2022-10-24 15:19:12,633 - trainer - INFO - start training epoch 129
2022-10-24 15:19:12,633 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,633 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,634 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,644 - trainer - INFO - 
*****************[epoch: 129, global step: 130] eval training set at end of epoch***************
2022-10-24 15:19:12,644 - trainer - INFO - {
  "train_loss": 1072.5748291015625
}
2022-10-24 15:19:12,645 - trainer - INFO - start training epoch 130
2022-10-24 15:19:12,645 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,646 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,646 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,654 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval training set based on eval_every=2***************
2022-10-24 15:19:12,654 - trainer - INFO - {
  "train_loss": 1048.008056640625
}
2022-10-24 15:19:12,661 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval development set based on eval_every=2***************
2022-10-24 15:19:12,661 - trainer - INFO - {
  "dev_loss": 976.34423828125,
  "dev_best_score_for_loss": -976.34423828125
}
2022-10-24 15:19:12,662 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,664 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,664 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,665 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_124
2022-10-24 15:19:12,667 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,671 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,671 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,674 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,675 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,675 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_130
2022-10-24 15:19:12,681 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_130
2022-10-24 15:19:12,682 - trainer - INFO - 
*****************[epoch: 130, global step: 131] eval training set at end of epoch***************
2022-10-24 15:19:12,682 - trainer - INFO - {
  "train_loss": 1023.4412841796875
}
2022-10-24 15:19:12,683 - trainer - INFO - start training epoch 131
2022-10-24 15:19:12,685 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,685 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,686 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,695 - trainer - INFO - 
*****************[epoch: 131, global step: 132] eval training set at end of epoch***************
2022-10-24 15:19:12,696 - trainer - INFO - {
  "train_loss": 976.34423828125
}
2022-10-24 15:19:12,696 - trainer - INFO - start training epoch 132
2022-10-24 15:19:12,696 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,696 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,697 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,705 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval training set based on eval_every=2***************
2022-10-24 15:19:12,706 - trainer - INFO - {
  "train_loss": 954.0208435058594
}
2022-10-24 15:19:12,712 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval development set based on eval_every=2***************
2022-10-24 15:19:12,712 - trainer - INFO - {
  "dev_loss": 887.3508911132812,
  "dev_best_score_for_loss": -887.3508911132812
}
2022-10-24 15:19:12,713 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,714 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,715 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,717 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_126
2022-10-24 15:19:12,718 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,724 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,725 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,725 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,726 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,726 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_132
2022-10-24 15:19:12,732 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_132
2022-10-24 15:19:12,733 - trainer - INFO - 
*****************[epoch: 132, global step: 133] eval training set at end of epoch***************
2022-10-24 15:19:12,733 - trainer - INFO - {
  "train_loss": 931.6974487304688
}
2022-10-24 15:19:12,734 - trainer - INFO - start training epoch 133
2022-10-24 15:19:12,735 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,735 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,735 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,743 - trainer - INFO - 
*****************[epoch: 133, global step: 134] eval training set at end of epoch***************
2022-10-24 15:19:12,744 - trainer - INFO - {
  "train_loss": 887.3508911132812
}
2022-10-24 15:19:12,744 - trainer - INFO - start training epoch 134
2022-10-24 15:19:12,744 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,746 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,747 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,754 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval training set based on eval_every=2***************
2022-10-24 15:19:12,754 - trainer - INFO - {
  "train_loss": 865.3691711425781
}
2022-10-24 15:19:12,766 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval development set based on eval_every=2***************
2022-10-24 15:19:12,766 - trainer - INFO - {
  "dev_loss": 800.97802734375,
  "dev_best_score_for_loss": -800.97802734375
}
2022-10-24 15:19:12,767 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,768 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,769 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,769 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_128
2022-10-24 15:19:12,771 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,774 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,775 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,775 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,776 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,777 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_134
2022-10-24 15:19:12,781 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_134
2022-10-24 15:19:12,782 - trainer - INFO - 
*****************[epoch: 134, global step: 135] eval training set at end of epoch***************
2022-10-24 15:19:12,783 - trainer - INFO - {
  "train_loss": 843.387451171875
}
2022-10-24 15:19:12,783 - trainer - INFO - start training epoch 135
2022-10-24 15:19:12,783 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,783 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,784 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,790 - trainer - INFO - 
*****************[epoch: 135, global step: 136] eval training set at end of epoch***************
2022-10-24 15:19:12,791 - trainer - INFO - {
  "train_loss": 800.9779052734375
}
2022-10-24 15:19:12,791 - trainer - INFO - start training epoch 136
2022-10-24 15:19:12,793 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,793 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,794 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,802 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval training set based on eval_every=2***************
2022-10-24 15:19:12,802 - trainer - INFO - {
  "train_loss": 780.6873474121094
}
2022-10-24 15:19:12,809 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval development set based on eval_every=2***************
2022-10-24 15:19:12,810 - trainer - INFO - {
  "dev_loss": 720.9140625,
  "dev_best_score_for_loss": -720.9140625
}
2022-10-24 15:19:12,811 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,816 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,816 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,816 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_130
2022-10-24 15:19:12,818 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,826 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,827 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,827 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,829 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,829 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_136
2022-10-24 15:19:12,836 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_136
2022-10-24 15:19:12,837 - trainer - INFO - 
*****************[epoch: 136, global step: 137] eval training set at end of epoch***************
2022-10-24 15:19:12,837 - trainer - INFO - {
  "train_loss": 760.3967895507812
}
2022-10-24 15:19:12,837 - trainer - INFO - start training epoch 137
2022-10-24 15:19:12,838 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,839 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,839 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,848 - trainer - INFO - 
*****************[epoch: 137, global step: 138] eval training set at end of epoch***************
2022-10-24 15:19:12,848 - trainer - INFO - {
  "train_loss": 720.9141235351562
}
2022-10-24 15:19:12,849 - trainer - INFO - start training epoch 138
2022-10-24 15:19:12,849 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,849 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,849 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,860 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval training set based on eval_every=2***************
2022-10-24 15:19:12,860 - trainer - INFO - {
  "train_loss": 701.3087158203125
}
2022-10-24 15:19:12,868 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval development set based on eval_every=2***************
2022-10-24 15:19:12,868 - trainer - INFO - {
  "dev_loss": 643.3494262695312,
  "dev_best_score_for_loss": -643.3494262695312
}
2022-10-24 15:19:12,869 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,870 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,871 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,871 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_132
2022-10-24 15:19:12,872 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,876 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,876 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,876 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,877 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,878 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_138
2022-10-24 15:19:12,881 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_138
2022-10-24 15:19:12,882 - trainer - INFO - 
*****************[epoch: 138, global step: 139] eval training set at end of epoch***************
2022-10-24 15:19:12,882 - trainer - INFO - {
  "train_loss": 681.7033081054688
}
2022-10-24 15:19:12,883 - trainer - INFO - start training epoch 139
2022-10-24 15:19:12,883 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,883 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,884 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,891 - trainer - INFO - 
*****************[epoch: 139, global step: 140] eval training set at end of epoch***************
2022-10-24 15:19:12,891 - trainer - INFO - {
  "train_loss": 643.3494262695312
}
2022-10-24 15:19:12,892 - trainer - INFO - start training epoch 140
2022-10-24 15:19:12,892 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,892 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,893 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,899 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval training set based on eval_every=2***************
2022-10-24 15:19:12,901 - trainer - INFO - {
  "train_loss": 624.9846801757812
}
2022-10-24 15:19:12,909 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval development set based on eval_every=2***************
2022-10-24 15:19:12,910 - trainer - INFO - {
  "dev_loss": 571.6057739257812,
  "dev_best_score_for_loss": -571.6057739257812
}
2022-10-24 15:19:12,911 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,912 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,912 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,913 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_134
2022-10-24 15:19:12,914 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,918 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,919 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,919 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,920 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,920 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_140
2022-10-24 15:19:12,925 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_140
2022-10-24 15:19:12,926 - trainer - INFO - 
*****************[epoch: 140, global step: 141] eval training set at end of epoch***************
2022-10-24 15:19:12,926 - trainer - INFO - {
  "train_loss": 606.6199340820312
}
2022-10-24 15:19:12,927 - trainer - INFO - start training epoch 141
2022-10-24 15:19:12,927 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,927 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,928 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,936 - trainer - INFO - 
*****************[epoch: 141, global step: 142] eval training set at end of epoch***************
2022-10-24 15:19:12,937 - trainer - INFO - {
  "train_loss": 571.6057739257812
}
2022-10-24 15:19:12,937 - trainer - INFO - start training epoch 142
2022-10-24 15:19:12,937 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,938 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,938 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,946 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval training set based on eval_every=2***************
2022-10-24 15:19:12,946 - trainer - INFO - {
  "train_loss": 554.7375183105469
}
2022-10-24 15:19:12,954 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval development set based on eval_every=2***************
2022-10-24 15:19:12,955 - trainer - INFO - {
  "dev_loss": 504.7143249511719,
  "dev_best_score_for_loss": -504.7143249511719
}
2022-10-24 15:19:12,955 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:12,956 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:12,957 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:12,957 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_136
2022-10-24 15:19:12,959 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:12,964 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:12,964 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:12,965 - trainer - INFO -   patience: 200
2022-10-24 15:19:12,966 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:12,966 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_142
2022-10-24 15:19:12,970 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_142
2022-10-24 15:19:12,971 - trainer - INFO - 
*****************[epoch: 142, global step: 143] eval training set at end of epoch***************
2022-10-24 15:19:12,971 - trainer - INFO - {
  "train_loss": 537.8692626953125
}
2022-10-24 15:19:12,972 - trainer - INFO - start training epoch 143
2022-10-24 15:19:12,972 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,972 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,973 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,983 - trainer - INFO - 
*****************[epoch: 143, global step: 144] eval training set at end of epoch***************
2022-10-24 15:19:12,984 - trainer - INFO - {
  "train_loss": 504.7143249511719
}
2022-10-24 15:19:12,984 - trainer - INFO - start training epoch 144
2022-10-24 15:19:12,984 - trainer - INFO - training using device=cuda
2022-10-24 15:19:12,984 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:12,985 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:12,992 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval training set based on eval_every=2***************
2022-10-24 15:19:12,992 - trainer - INFO - {
  "train_loss": 488.8801574707031
}
2022-10-24 15:19:12,999 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval development set based on eval_every=2***************
2022-10-24 15:19:12,999 - trainer - INFO - {
  "dev_loss": 442.95611572265625,
  "dev_best_score_for_loss": -442.95611572265625
}
2022-10-24 15:19:13,000 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,001 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,001 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,001 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_138
2022-10-24 15:19:13,002 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,005 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,005 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,006 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,006 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,007 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_144
2022-10-24 15:19:13,011 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_144
2022-10-24 15:19:13,012 - trainer - INFO - 
*****************[epoch: 144, global step: 145] eval training set at end of epoch***************
2022-10-24 15:19:13,012 - trainer - INFO - {
  "train_loss": 473.0459899902344
}
2022-10-24 15:19:13,012 - trainer - INFO - start training epoch 145
2022-10-24 15:19:13,012 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,013 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,013 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,020 - trainer - INFO - 
*****************[epoch: 145, global step: 146] eval training set at end of epoch***************
2022-10-24 15:19:13,020 - trainer - INFO - {
  "train_loss": 442.95611572265625
}
2022-10-24 15:19:13,020 - trainer - INFO - start training epoch 146
2022-10-24 15:19:13,020 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,021 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,021 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,029 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval training set based on eval_every=2***************
2022-10-24 15:19:13,030 - trainer - INFO - {
  "train_loss": 428.22735595703125
}
2022-10-24 15:19:13,036 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval development set based on eval_every=2***************
2022-10-24 15:19:13,037 - trainer - INFO - {
  "dev_loss": 385.155029296875,
  "dev_best_score_for_loss": -385.155029296875
}
2022-10-24 15:19:13,037 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,038 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,039 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,039 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_140
2022-10-24 15:19:13,040 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,044 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,045 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,045 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,046 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,046 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_146
2022-10-24 15:19:13,050 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_146
2022-10-24 15:19:13,051 - trainer - INFO - 
*****************[epoch: 146, global step: 147] eval training set at end of epoch***************
2022-10-24 15:19:13,051 - trainer - INFO - {
  "train_loss": 413.49859619140625
}
2022-10-24 15:19:13,051 - trainer - INFO - start training epoch 147
2022-10-24 15:19:13,051 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,052 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,052 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,059 - trainer - INFO - 
*****************[epoch: 147, global step: 148] eval training set at end of epoch***************
2022-10-24 15:19:13,060 - trainer - INFO - {
  "train_loss": 385.155029296875
}
2022-10-24 15:19:13,060 - trainer - INFO - start training epoch 148
2022-10-24 15:19:13,060 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,061 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,061 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,068 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval training set based on eval_every=2***************
2022-10-24 15:19:13,069 - trainer - INFO - {
  "train_loss": 371.8504180908203
}
2022-10-24 15:19:13,074 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval development set based on eval_every=2***************
2022-10-24 15:19:13,075 - trainer - INFO - {
  "dev_loss": 332.9134826660156,
  "dev_best_score_for_loss": -332.9134826660156
}
2022-10-24 15:19:13,076 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,077 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,077 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,077 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_142
2022-10-24 15:19:13,079 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,081 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,082 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,082 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,083 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,083 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_148
2022-10-24 15:19:13,087 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_148
2022-10-24 15:19:13,088 - trainer - INFO - 
*****************[epoch: 148, global step: 149] eval training set at end of epoch***************
2022-10-24 15:19:13,091 - trainer - INFO - {
  "train_loss": 358.5458068847656
}
2022-10-24 15:19:13,091 - trainer - INFO - start training epoch 149
2022-10-24 15:19:13,092 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,092 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,092 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,102 - trainer - INFO - 
*****************[epoch: 149, global step: 150] eval training set at end of epoch***************
2022-10-24 15:19:13,102 - trainer - INFO - {
  "train_loss": 332.9134826660156
}
2022-10-24 15:19:13,103 - trainer - INFO - start training epoch 150
2022-10-24 15:19:13,103 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,103 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,104 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,111 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval training set based on eval_every=2***************
2022-10-24 15:19:13,112 - trainer - INFO - {
  "train_loss": 320.6919403076172
}
2022-10-24 15:19:13,120 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval development set based on eval_every=2***************
2022-10-24 15:19:13,123 - trainer - INFO - {
  "dev_loss": 285.35418701171875,
  "dev_best_score_for_loss": -285.35418701171875
}
2022-10-24 15:19:13,124 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,125 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,126 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,126 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_144
2022-10-24 15:19:13,128 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,132 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,132 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,132 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,134 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,134 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_150
2022-10-24 15:19:13,138 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_150
2022-10-24 15:19:13,139 - trainer - INFO - 
*****************[epoch: 150, global step: 151] eval training set at end of epoch***************
2022-10-24 15:19:13,139 - trainer - INFO - {
  "train_loss": 308.47039794921875
}
2022-10-24 15:19:13,139 - trainer - INFO - start training epoch 151
2022-10-24 15:19:13,139 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,140 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,140 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,150 - trainer - INFO - 
*****************[epoch: 151, global step: 152] eval training set at end of epoch***************
2022-10-24 15:19:13,150 - trainer - INFO - {
  "train_loss": 285.35418701171875
}
2022-10-24 15:19:13,151 - trainer - INFO - start training epoch 152
2022-10-24 15:19:13,151 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,151 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,152 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,158 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval training set based on eval_every=2***************
2022-10-24 15:19:13,159 - trainer - INFO - {
  "train_loss": 274.4376220703125
}
2022-10-24 15:19:13,168 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval development set based on eval_every=2***************
2022-10-24 15:19:13,168 - trainer - INFO - {
  "dev_loss": 242.60421752929688,
  "dev_best_score_for_loss": -242.60421752929688
}
2022-10-24 15:19:13,169 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,170 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,171 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,171 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_146
2022-10-24 15:19:13,172 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,176 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,177 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,177 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,178 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,178 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_152
2022-10-24 15:19:13,183 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_152
2022-10-24 15:19:13,183 - trainer - INFO - 
*****************[epoch: 152, global step: 153] eval training set at end of epoch***************
2022-10-24 15:19:13,184 - trainer - INFO - {
  "train_loss": 263.52105712890625
}
2022-10-24 15:19:13,184 - trainer - INFO - start training epoch 153
2022-10-24 15:19:13,184 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,184 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,185 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,192 - trainer - INFO - 
*****************[epoch: 153, global step: 154] eval training set at end of epoch***************
2022-10-24 15:19:13,193 - trainer - INFO - {
  "train_loss": 242.60421752929688
}
2022-10-24 15:19:13,193 - trainer - INFO - start training epoch 154
2022-10-24 15:19:13,194 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,194 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,194 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,205 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval training set based on eval_every=2***************
2022-10-24 15:19:13,205 - trainer - INFO - {
  "train_loss": 232.70172882080078
}
2022-10-24 15:19:13,211 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval development set based on eval_every=2***************
2022-10-24 15:19:13,212 - trainer - INFO - {
  "dev_loss": 204.13211059570312,
  "dev_best_score_for_loss": -204.13211059570312
}
2022-10-24 15:19:13,212 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,214 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,214 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,214 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_148
2022-10-24 15:19:13,215 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,218 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,219 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,219 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,220 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,220 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_154
2022-10-24 15:19:13,225 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_154
2022-10-24 15:19:13,227 - trainer - INFO - 
*****************[epoch: 154, global step: 155] eval training set at end of epoch***************
2022-10-24 15:19:13,227 - trainer - INFO - {
  "train_loss": 222.7992401123047
}
2022-10-24 15:19:13,228 - trainer - INFO - start training epoch 155
2022-10-24 15:19:13,228 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,228 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,229 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,236 - trainer - INFO - 
*****************[epoch: 155, global step: 156] eval training set at end of epoch***************
2022-10-24 15:19:13,236 - trainer - INFO - {
  "train_loss": 204.13211059570312
}
2022-10-24 15:19:13,236 - trainer - INFO - start training epoch 156
2022-10-24 15:19:13,236 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,237 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,237 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,247 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval training set based on eval_every=2***************
2022-10-24 15:19:13,247 - trainer - INFO - {
  "train_loss": 195.33538818359375
}
2022-10-24 15:19:13,253 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval development set based on eval_every=2***************
2022-10-24 15:19:13,254 - trainer - INFO - {
  "dev_loss": 169.99569702148438,
  "dev_best_score_for_loss": -169.99569702148438
}
2022-10-24 15:19:13,254 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,255 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,256 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,256 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_150
2022-10-24 15:19:13,257 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,260 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,261 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,261 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,262 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,262 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_156
2022-10-24 15:19:13,266 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_156
2022-10-24 15:19:13,266 - trainer - INFO - 
*****************[epoch: 156, global step: 157] eval training set at end of epoch***************
2022-10-24 15:19:13,267 - trainer - INFO - {
  "train_loss": 186.53866577148438
}
2022-10-24 15:19:13,267 - trainer - INFO - start training epoch 157
2022-10-24 15:19:13,267 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,267 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,268 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,278 - trainer - INFO - 
*****************[epoch: 157, global step: 158] eval training set at end of epoch***************
2022-10-24 15:19:13,279 - trainer - INFO - {
  "train_loss": 169.99569702148438
}
2022-10-24 15:19:13,279 - trainer - INFO - start training epoch 158
2022-10-24 15:19:13,279 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,279 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,280 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,289 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval training set based on eval_every=2***************
2022-10-24 15:19:13,289 - trainer - INFO - {
  "train_loss": 162.21976470947266
}
2022-10-24 15:19:13,299 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval development set based on eval_every=2***************
2022-10-24 15:19:13,299 - trainer - INFO - {
  "dev_loss": 140.0457000732422,
  "dev_best_score_for_loss": -140.0457000732422
}
2022-10-24 15:19:13,300 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,302 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,302 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,302 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_152
2022-10-24 15:19:13,304 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,313 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,314 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,314 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,316 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,316 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_158
2022-10-24 15:19:13,321 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_158
2022-10-24 15:19:13,322 - trainer - INFO - 
*****************[epoch: 158, global step: 159] eval training set at end of epoch***************
2022-10-24 15:19:13,322 - trainer - INFO - {
  "train_loss": 154.44383239746094
}
2022-10-24 15:19:13,323 - trainer - INFO - start training epoch 159
2022-10-24 15:19:13,323 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,324 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,324 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,336 - trainer - INFO - 
*****************[epoch: 159, global step: 160] eval training set at end of epoch***************
2022-10-24 15:19:13,337 - trainer - INFO - {
  "train_loss": 140.0457000732422
}
2022-10-24 15:19:13,338 - trainer - INFO - start training epoch 160
2022-10-24 15:19:13,338 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,339 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,339 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,351 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval training set based on eval_every=2***************
2022-10-24 15:19:13,351 - trainer - INFO - {
  "train_loss": 133.37068939208984
}
2022-10-24 15:19:13,357 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval development set based on eval_every=2***************
2022-10-24 15:19:13,358 - trainer - INFO - {
  "dev_loss": 114.20931243896484,
  "dev_best_score_for_loss": -114.20931243896484
}
2022-10-24 15:19:13,358 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,360 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,360 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,360 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_154
2022-10-24 15:19:13,362 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,366 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,367 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,367 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,368 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,368 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_160
2022-10-24 15:19:13,373 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_160
2022-10-24 15:19:13,373 - trainer - INFO - 
*****************[epoch: 160, global step: 161] eval training set at end of epoch***************
2022-10-24 15:19:13,374 - trainer - INFO - {
  "train_loss": 126.6956787109375
}
2022-10-24 15:19:13,374 - trainer - INFO - start training epoch 161
2022-10-24 15:19:13,374 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,374 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,375 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,386 - trainer - INFO - 
*****************[epoch: 161, global step: 162] eval training set at end of epoch***************
2022-10-24 15:19:13,386 - trainer - INFO - {
  "train_loss": 114.20931243896484
}
2022-10-24 15:19:13,386 - trainer - INFO - start training epoch 162
2022-10-24 15:19:13,387 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,387 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,387 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,398 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval training set based on eval_every=2***************
2022-10-24 15:19:13,398 - trainer - INFO - {
  "train_loss": 108.41582489013672
}
2022-10-24 15:19:13,405 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval development set based on eval_every=2***************
2022-10-24 15:19:13,405 - trainer - INFO - {
  "dev_loss": 91.95355224609375,
  "dev_best_score_for_loss": -91.95355224609375
}
2022-10-24 15:19:13,406 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,407 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,407 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,407 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_156
2022-10-24 15:19:13,409 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,413 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,414 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,415 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,416 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,418 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_162
2022-10-24 15:19:13,423 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_162
2022-10-24 15:19:13,424 - trainer - INFO - 
*****************[epoch: 162, global step: 163] eval training set at end of epoch***************
2022-10-24 15:19:13,424 - trainer - INFO - {
  "train_loss": 102.6223373413086
}
2022-10-24 15:19:13,424 - trainer - INFO - start training epoch 163
2022-10-24 15:19:13,425 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,425 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,425 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,434 - trainer - INFO - 
*****************[epoch: 163, global step: 164] eval training set at end of epoch***************
2022-10-24 15:19:13,435 - trainer - INFO - {
  "train_loss": 91.95355224609375
}
2022-10-24 15:19:13,435 - trainer - INFO - start training epoch 164
2022-10-24 15:19:13,436 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,436 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,436 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,444 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval training set based on eval_every=2***************
2022-10-24 15:19:13,444 - trainer - INFO - {
  "train_loss": 87.02432250976562
}
2022-10-24 15:19:13,450 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval development set based on eval_every=2***************
2022-10-24 15:19:13,451 - trainer - INFO - {
  "dev_loss": 73.0136489868164,
  "dev_best_score_for_loss": -73.0136489868164
}
2022-10-24 15:19:13,451 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,452 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,452 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,453 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_158
2022-10-24 15:19:13,454 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,457 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,457 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,457 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,458 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,459 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_164
2022-10-24 15:19:13,464 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_164
2022-10-24 15:19:13,465 - trainer - INFO - 
*****************[epoch: 164, global step: 165] eval training set at end of epoch***************
2022-10-24 15:19:13,465 - trainer - INFO - {
  "train_loss": 82.0950927734375
}
2022-10-24 15:19:13,466 - trainer - INFO - start training epoch 165
2022-10-24 15:19:13,466 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,466 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,467 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,476 - trainer - INFO - 
*****************[epoch: 165, global step: 166] eval training set at end of epoch***************
2022-10-24 15:19:13,477 - trainer - INFO - {
  "train_loss": 73.0136489868164
}
2022-10-24 15:19:13,478 - trainer - INFO - start training epoch 166
2022-10-24 15:19:13,478 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,478 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,479 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,486 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval training set based on eval_every=2***************
2022-10-24 15:19:13,486 - trainer - INFO - {
  "train_loss": 68.87123489379883
}
2022-10-24 15:19:13,493 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval development set based on eval_every=2***************
2022-10-24 15:19:13,493 - trainer - INFO - {
  "dev_loss": 57.16862487792969,
  "dev_best_score_for_loss": -57.16862487792969
}
2022-10-24 15:19:13,494 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,495 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,496 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,496 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_160
2022-10-24 15:19:13,498 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,501 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,501 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,502 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,502 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,503 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_166
2022-10-24 15:19:13,508 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_166
2022-10-24 15:19:13,512 - trainer - INFO - 
*****************[epoch: 166, global step: 167] eval training set at end of epoch***************
2022-10-24 15:19:13,512 - trainer - INFO - {
  "train_loss": 64.72882080078125
}
2022-10-24 15:19:13,513 - trainer - INFO - start training epoch 167
2022-10-24 15:19:13,513 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,513 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,513 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,522 - trainer - INFO - 
*****************[epoch: 167, global step: 168] eval training set at end of epoch***************
2022-10-24 15:19:13,523 - trainer - INFO - {
  "train_loss": 57.16862487792969
}
2022-10-24 15:19:13,523 - trainer - INFO - start training epoch 168
2022-10-24 15:19:13,523 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,524 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,524 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,534 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval training set based on eval_every=2***************
2022-10-24 15:19:13,535 - trainer - INFO - {
  "train_loss": 53.710960388183594
}
2022-10-24 15:19:13,544 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval development set based on eval_every=2***************
2022-10-24 15:19:13,544 - trainer - INFO - {
  "dev_loss": 43.97434997558594,
  "dev_best_score_for_loss": -43.97434997558594
}
2022-10-24 15:19:13,545 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,546 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,546 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,547 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_162
2022-10-24 15:19:13,548 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,553 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,553 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,554 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,556 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,559 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_168
2022-10-24 15:19:13,564 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_168
2022-10-24 15:19:13,565 - trainer - INFO - 
*****************[epoch: 168, global step: 169] eval training set at end of epoch***************
2022-10-24 15:19:13,566 - trainer - INFO - {
  "train_loss": 50.2532958984375
}
2022-10-24 15:19:13,568 - trainer - INFO - start training epoch 169
2022-10-24 15:19:13,568 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,568 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,569 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,576 - trainer - INFO - 
*****************[epoch: 169, global step: 170] eval training set at end of epoch***************
2022-10-24 15:19:13,577 - trainer - INFO - {
  "train_loss": 43.97434997558594
}
2022-10-24 15:19:13,577 - trainer - INFO - start training epoch 170
2022-10-24 15:19:13,578 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,578 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,578 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,586 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval training set based on eval_every=2***************
2022-10-24 15:19:13,587 - trainer - INFO - {
  "train_loss": 41.15475845336914
}
2022-10-24 15:19:13,594 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval development set based on eval_every=2***************
2022-10-24 15:19:13,595 - trainer - INFO - {
  "dev_loss": 33.234222412109375,
  "dev_best_score_for_loss": -33.234222412109375
}
2022-10-24 15:19:13,596 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,597 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,598 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,599 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_164
2022-10-24 15:19:13,604 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,608 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,609 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,609 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,610 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,610 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_170
2022-10-24 15:19:13,616 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_170
2022-10-24 15:19:13,617 - trainer - INFO - 
*****************[epoch: 170, global step: 171] eval training set at end of epoch***************
2022-10-24 15:19:13,617 - trainer - INFO - {
  "train_loss": 38.335166931152344
}
2022-10-24 15:19:13,617 - trainer - INFO - start training epoch 171
2022-10-24 15:19:13,618 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,618 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,618 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,627 - trainer - INFO - 
*****************[epoch: 171, global step: 172] eval training set at end of epoch***************
2022-10-24 15:19:13,627 - trainer - INFO - {
  "train_loss": 33.23421859741211
}
2022-10-24 15:19:13,628 - trainer - INFO - start training epoch 172
2022-10-24 15:19:13,628 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,629 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,630 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,641 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval training set based on eval_every=2***************
2022-10-24 15:19:13,642 - trainer - INFO - {
  "train_loss": 30.93312168121338
}
2022-10-24 15:19:13,649 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval development set based on eval_every=2***************
2022-10-24 15:19:13,650 - trainer - INFO - {
  "dev_loss": 24.557340621948242,
  "dev_best_score_for_loss": -24.557340621948242
}
2022-10-24 15:19:13,650 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,651 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,652 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,652 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_166
2022-10-24 15:19:13,653 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,657 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,657 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,657 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,658 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,658 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_172
2022-10-24 15:19:13,664 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_172
2022-10-24 15:19:13,665 - trainer - INFO - 
*****************[epoch: 172, global step: 173] eval training set at end of epoch***************
2022-10-24 15:19:13,665 - trainer - INFO - {
  "train_loss": 28.63202476501465
}
2022-10-24 15:19:13,666 - trainer - INFO - start training epoch 173
2022-10-24 15:19:13,666 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,666 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,667 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,677 - trainer - INFO - 
*****************[epoch: 173, global step: 174] eval training set at end of epoch***************
2022-10-24 15:19:13,678 - trainer - INFO - {
  "train_loss": 24.557340621948242
}
2022-10-24 15:19:13,682 - trainer - INFO - start training epoch 174
2022-10-24 15:19:13,682 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,682 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,683 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,692 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval training set based on eval_every=2***************
2022-10-24 15:19:13,693 - trainer - INFO - {
  "train_loss": 22.74867343902588
}
2022-10-24 15:19:13,702 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval development set based on eval_every=2***************
2022-10-24 15:19:13,702 - trainer - INFO - {
  "dev_loss": 17.73870849609375,
  "dev_best_score_for_loss": -17.73870849609375
}
2022-10-24 15:19:13,702 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,704 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,704 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,704 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_168
2022-10-24 15:19:13,706 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,710 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,710 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,711 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,711 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,712 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_174
2022-10-24 15:19:13,717 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_174
2022-10-24 15:19:13,718 - trainer - INFO - 
*****************[epoch: 174, global step: 175] eval training set at end of epoch***************
2022-10-24 15:19:13,718 - trainer - INFO - {
  "train_loss": 20.940006256103516
}
2022-10-24 15:19:13,718 - trainer - INFO - start training epoch 175
2022-10-24 15:19:13,719 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,719 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,719 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,731 - trainer - INFO - 
*****************[epoch: 175, global step: 176] eval training set at end of epoch***************
2022-10-24 15:19:13,732 - trainer - INFO - {
  "train_loss": 17.73870849609375
}
2022-10-24 15:19:13,732 - trainer - INFO - start training epoch 176
2022-10-24 15:19:13,733 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,733 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,733 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,740 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval training set based on eval_every=2***************
2022-10-24 15:19:13,741 - trainer - INFO - {
  "train_loss": 16.326904296875
}
2022-10-24 15:19:13,747 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval development set based on eval_every=2***************
2022-10-24 15:19:13,748 - trainer - INFO - {
  "dev_loss": 12.511096000671387,
  "dev_best_score_for_loss": -12.511096000671387
}
2022-10-24 15:19:13,748 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,749 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,749 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,750 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_170
2022-10-24 15:19:13,751 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,754 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,756 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,757 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,757 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,758 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_176
2022-10-24 15:19:13,763 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_176
2022-10-24 15:19:13,764 - trainer - INFO - 
*****************[epoch: 176, global step: 177] eval training set at end of epoch***************
2022-10-24 15:19:13,764 - trainer - INFO - {
  "train_loss": 14.91510009765625
}
2022-10-24 15:19:13,764 - trainer - INFO - start training epoch 177
2022-10-24 15:19:13,765 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,765 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,765 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,773 - trainer - INFO - 
*****************[epoch: 177, global step: 178] eval training set at end of epoch***************
2022-10-24 15:19:13,773 - trainer - INFO - {
  "train_loss": 12.511096000671387
}
2022-10-24 15:19:13,773 - trainer - INFO - start training epoch 178
2022-10-24 15:19:13,773 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,774 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,774 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,781 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval training set based on eval_every=2***************
2022-10-24 15:19:13,782 - trainer - INFO - {
  "train_loss": 11.459410190582275
}
2022-10-24 15:19:13,790 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval development set based on eval_every=2***************
2022-10-24 15:19:13,791 - trainer - INFO - {
  "dev_loss": 8.57330322265625,
  "dev_best_score_for_loss": -8.57330322265625
}
2022-10-24 15:19:13,791 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,793 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,793 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,793 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_172
2022-10-24 15:19:13,795 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,802 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,802 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,802 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,804 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,804 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_178
2022-10-24 15:19:13,809 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_178
2022-10-24 15:19:13,810 - trainer - INFO - 
*****************[epoch: 178, global step: 179] eval training set at end of epoch***************
2022-10-24 15:19:13,811 - trainer - INFO - {
  "train_loss": 10.407724380493164
}
2022-10-24 15:19:13,811 - trainer - INFO - start training epoch 179
2022-10-24 15:19:13,812 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,812 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,812 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,823 - trainer - INFO - 
*****************[epoch: 179, global step: 180] eval training set at end of epoch***************
2022-10-24 15:19:13,824 - trainer - INFO - {
  "train_loss": 8.57330322265625
}
2022-10-24 15:19:13,824 - trainer - INFO - start training epoch 180
2022-10-24 15:19:13,824 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,825 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,826 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,835 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval training set based on eval_every=2***************
2022-10-24 15:19:13,835 - trainer - INFO - {
  "train_loss": 7.784027576446533
}
2022-10-24 15:19:13,842 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval development set based on eval_every=2***************
2022-10-24 15:19:13,842 - trainer - INFO - {
  "dev_loss": 5.643126964569092,
  "dev_best_score_for_loss": -5.643126964569092
}
2022-10-24 15:19:13,843 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,844 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,845 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,845 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_174
2022-10-24 15:19:13,847 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,851 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,852 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,852 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,853 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,853 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_180
2022-10-24 15:19:13,858 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_180
2022-10-24 15:19:13,859 - trainer - INFO - 
*****************[epoch: 180, global step: 181] eval training set at end of epoch***************
2022-10-24 15:19:13,859 - trainer - INFO - {
  "train_loss": 6.994751930236816
}
2022-10-24 15:19:13,860 - trainer - INFO - start training epoch 181
2022-10-24 15:19:13,860 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,860 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,861 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,870 - trainer - INFO - 
*****************[epoch: 181, global step: 182] eval training set at end of epoch***************
2022-10-24 15:19:13,871 - trainer - INFO - {
  "train_loss": 5.643126487731934
}
2022-10-24 15:19:13,871 - trainer - INFO - start training epoch 182
2022-10-24 15:19:13,871 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,871 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,872 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,880 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval training set based on eval_every=2***************
2022-10-24 15:19:13,882 - trainer - INFO - {
  "train_loss": 5.067460775375366
}
2022-10-24 15:19:13,888 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval development set based on eval_every=2***************
2022-10-24 15:19:13,888 - trainer - INFO - {
  "dev_loss": 3.5306785106658936,
  "dev_best_score_for_loss": -3.5306785106658936
}
2022-10-24 15:19:13,889 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,890 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,891 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,891 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_176
2022-10-24 15:19:13,892 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,896 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,896 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,897 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,897 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,898 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_182
2022-10-24 15:19:13,902 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_182
2022-10-24 15:19:13,903 - trainer - INFO - 
*****************[epoch: 182, global step: 183] eval training set at end of epoch***************
2022-10-24 15:19:13,903 - trainer - INFO - {
  "train_loss": 4.491795063018799
}
2022-10-24 15:19:13,904 - trainer - INFO - start training epoch 183
2022-10-24 15:19:13,904 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,904 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,905 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,912 - trainer - INFO - 
*****************[epoch: 183, global step: 184] eval training set at end of epoch***************
2022-10-24 15:19:13,913 - trainer - INFO - {
  "train_loss": 3.5306785106658936
}
2022-10-24 15:19:13,914 - trainer - INFO - start training epoch 184
2022-10-24 15:19:13,914 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,914 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,915 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,921 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval training set based on eval_every=2***************
2022-10-24 15:19:13,922 - trainer - INFO - {
  "train_loss": 3.1362494230270386
}
2022-10-24 15:19:13,932 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval development set based on eval_every=2***************
2022-10-24 15:19:13,932 - trainer - INFO - {
  "dev_loss": 2.09765362739563,
  "dev_best_score_for_loss": -2.09765362739563
}
2022-10-24 15:19:13,933 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,934 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,934 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,935 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_178
2022-10-24 15:19:13,936 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,940 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,940 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,941 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,942 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,944 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_184
2022-10-24 15:19:13,949 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_184
2022-10-24 15:19:13,949 - trainer - INFO - 
*****************[epoch: 184, global step: 185] eval training set at end of epoch***************
2022-10-24 15:19:13,950 - trainer - INFO - {
  "train_loss": 2.7418203353881836
}
2022-10-24 15:19:13,950 - trainer - INFO - start training epoch 185
2022-10-24 15:19:13,950 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,951 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,951 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,958 - trainer - INFO - 
*****************[epoch: 185, global step: 186] eval training set at end of epoch***************
2022-10-24 15:19:13,959 - trainer - INFO - {
  "train_loss": 2.09765362739563
}
2022-10-24 15:19:13,959 - trainer - INFO - start training epoch 186
2022-10-24 15:19:13,959 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,960 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,960 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:13,968 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval training set based on eval_every=2***************
2022-10-24 15:19:13,969 - trainer - INFO - {
  "train_loss": 1.8352949023246765
}
2022-10-24 15:19:13,974 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval development set based on eval_every=2***************
2022-10-24 15:19:13,975 - trainer - INFO - {
  "dev_loss": 1.161562204360962,
  "dev_best_score_for_loss": -1.161562204360962
}
2022-10-24 15:19:13,975 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:13,976 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:13,977 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:13,977 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_180
2022-10-24 15:19:13,978 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:13,981 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:13,981 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:13,981 - trainer - INFO -   patience: 200
2022-10-24 15:19:13,982 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:13,982 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_186
2022-10-24 15:19:13,987 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_186
2022-10-24 15:19:13,988 - trainer - INFO - 
*****************[epoch: 186, global step: 187] eval training set at end of epoch***************
2022-10-24 15:19:13,988 - trainer - INFO - {
  "train_loss": 1.5729361772537231
}
2022-10-24 15:19:13,991 - trainer - INFO - start training epoch 187
2022-10-24 15:19:13,991 - trainer - INFO - training using device=cuda
2022-10-24 15:19:13,992 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:13,992 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,002 - trainer - INFO - 
*****************[epoch: 187, global step: 188] eval training set at end of epoch***************
2022-10-24 15:19:14,003 - trainer - INFO - {
  "train_loss": 1.161562204360962
}
2022-10-24 15:19:14,003 - trainer - INFO - start training epoch 188
2022-10-24 15:19:14,003 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,004 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,004 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,013 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval training set based on eval_every=2***************
2022-10-24 15:19:14,013 - trainer - INFO - {
  "train_loss": 1.0013049244880676
}
2022-10-24 15:19:14,020 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval development set based on eval_every=2***************
2022-10-24 15:19:14,020 - trainer - INFO - {
  "dev_loss": 0.58956378698349,
  "dev_best_score_for_loss": -0.58956378698349
}
2022-10-24 15:19:14,021 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:14,022 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,023 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,023 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_182
2022-10-24 15:19:14,025 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:14,029 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:14,029 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:14,030 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,031 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:14,031 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_188
2022-10-24 15:19:14,038 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_188
2022-10-24 15:19:14,039 - trainer - INFO - 
*****************[epoch: 188, global step: 189] eval training set at end of epoch***************
2022-10-24 15:19:14,039 - trainer - INFO - {
  "train_loss": 0.8410476446151733
}
2022-10-24 15:19:14,040 - trainer - INFO - start training epoch 189
2022-10-24 15:19:14,040 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,041 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,041 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,049 - trainer - INFO - 
*****************[epoch: 189, global step: 190] eval training set at end of epoch***************
2022-10-24 15:19:14,050 - trainer - INFO - {
  "train_loss": 0.5895637273788452
}
2022-10-24 15:19:14,050 - trainer - INFO - start training epoch 190
2022-10-24 15:19:14,051 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,051 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,051 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,058 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval training set based on eval_every=2***************
2022-10-24 15:19:14,059 - trainer - INFO - {
  "train_loss": 0.4969271868467331
}
2022-10-24 15:19:14,067 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval development set based on eval_every=2***************
2022-10-24 15:19:14,068 - trainer - INFO - {
  "dev_loss": 0.27680379152297974,
  "dev_best_score_for_loss": -0.27680379152297974
}
2022-10-24 15:19:14,068 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:14,070 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,070 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,070 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_184
2022-10-24 15:19:14,071 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:14,074 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:14,074 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:14,074 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,075 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:14,075 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_190
2022-10-24 15:19:14,080 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_190
2022-10-24 15:19:14,081 - trainer - INFO - 
*****************[epoch: 190, global step: 191] eval training set at end of epoch***************
2022-10-24 15:19:14,084 - trainer - INFO - {
  "train_loss": 0.40429064631462097
}
2022-10-24 15:19:14,085 - trainer - INFO - start training epoch 191
2022-10-24 15:19:14,085 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,085 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,085 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,096 - trainer - INFO - 
*****************[epoch: 191, global step: 192] eval training set at end of epoch***************
2022-10-24 15:19:14,097 - trainer - INFO - {
  "train_loss": 0.27680379152297974
}
2022-10-24 15:19:14,097 - trainer - INFO - start training epoch 192
2022-10-24 15:19:14,097 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,098 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,098 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,105 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval training set based on eval_every=2***************
2022-10-24 15:19:14,106 - trainer - INFO - {
  "train_loss": 0.23313112556934357
}
2022-10-24 15:19:14,113 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval development set based on eval_every=2***************
2022-10-24 15:19:14,113 - trainer - INFO - {
  "dev_loss": 0.13539567589759827,
  "dev_best_score_for_loss": -0.13539567589759827
}
2022-10-24 15:19:14,114 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:14,115 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,115 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,115 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_186
2022-10-24 15:19:14,117 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:14,120 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:14,120 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:14,120 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,121 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:14,121 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_192
2022-10-24 15:19:14,126 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_192
2022-10-24 15:19:14,128 - trainer - INFO - 
*****************[epoch: 192, global step: 193] eval training set at end of epoch***************
2022-10-24 15:19:14,128 - trainer - INFO - {
  "train_loss": 0.1894584596157074
}
2022-10-24 15:19:14,131 - trainer - INFO - start training epoch 193
2022-10-24 15:19:14,132 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,132 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,132 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,141 - trainer - INFO - 
*****************[epoch: 193, global step: 194] eval training set at end of epoch***************
2022-10-24 15:19:14,141 - trainer - INFO - {
  "train_loss": 0.13539567589759827
}
2022-10-24 15:19:14,142 - trainer - INFO - start training epoch 194
2022-10-24 15:19:14,142 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,142 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,143 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,151 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval training set based on eval_every=2***************
2022-10-24 15:19:14,151 - trainer - INFO - {
  "train_loss": 0.1237943172454834
}
2022-10-24 15:19:14,157 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval development set based on eval_every=2***************
2022-10-24 15:19:14,158 - trainer - INFO - {
  "dev_loss": 0.10983549803495407,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,158 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:14,160 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,160 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,160 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_188
2022-10-24 15:19:14,162 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:14,166 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:14,166 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:14,166 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,167 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:14,167 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_194
2022-10-24 15:19:14,173 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_194
2022-10-24 15:19:14,177 - trainer - INFO - 
*****************[epoch: 194, global step: 195] eval training set at end of epoch***************
2022-10-24 15:19:14,177 - trainer - INFO - {
  "train_loss": 0.11219295859336853
}
2022-10-24 15:19:14,177 - trainer - INFO - start training epoch 195
2022-10-24 15:19:14,178 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,178 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,178 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,187 - trainer - INFO - 
*****************[epoch: 195, global step: 196] eval training set at end of epoch***************
2022-10-24 15:19:14,188 - trainer - INFO - {
  "train_loss": 0.10983549803495407
}
2022-10-24 15:19:14,188 - trainer - INFO - start training epoch 196
2022-10-24 15:19:14,188 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,189 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,189 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,198 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval training set based on eval_every=2***************
2022-10-24 15:19:14,198 - trainer - INFO - {
  "train_loss": 0.1151912733912468
}
2022-10-24 15:19:14,207 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval development set based on eval_every=2***************
2022-10-24 15:19:14,208 - trainer - INFO - {
  "dev_loss": 0.14391598105430603,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,209 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:14,209 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,210 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,211 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,211 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_190
2022-10-24 15:19:14,212 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_196
2022-10-24 15:19:14,218 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_196
2022-10-24 15:19:14,219 - trainer - INFO - 
*****************[epoch: 196, global step: 197] eval training set at end of epoch***************
2022-10-24 15:19:14,219 - trainer - INFO - {
  "train_loss": 0.12054704874753952
}
2022-10-24 15:19:14,220 - trainer - INFO - start training epoch 197
2022-10-24 15:19:14,220 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,220 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,220 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,230 - trainer - INFO - 
*****************[epoch: 197, global step: 198] eval training set at end of epoch***************
2022-10-24 15:19:14,230 - trainer - INFO - {
  "train_loss": 0.14391598105430603
}
2022-10-24 15:19:14,230 - trainer - INFO - start training epoch 198
2022-10-24 15:19:14,231 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,231 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,231 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,240 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval training set based on eval_every=2***************
2022-10-24 15:19:14,240 - trainer - INFO - {
  "train_loss": 0.15987578779459
}
2022-10-24 15:19:14,248 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval development set based on eval_every=2***************
2022-10-24 15:19:14,249 - trainer - INFO - {
  "dev_loss": 0.20983481407165527,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,250 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:14,250 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,251 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,252 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,252 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_192
2022-10-24 15:19:14,253 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_198
2022-10-24 15:19:14,257 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_198
2022-10-24 15:19:14,258 - trainer - INFO - 
*****************[epoch: 198, global step: 199] eval training set at end of epoch***************
2022-10-24 15:19:14,258 - trainer - INFO - {
  "train_loss": 0.17583559453487396
}
2022-10-24 15:19:14,258 - trainer - INFO - start training epoch 199
2022-10-24 15:19:14,259 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,259 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,259 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,267 - trainer - INFO - 
*****************[epoch: 199, global step: 200] eval training set at end of epoch***************
2022-10-24 15:19:14,268 - trainer - INFO - {
  "train_loss": 0.20983481407165527
}
2022-10-24 15:19:14,268 - trainer - INFO - start training epoch 200
2022-10-24 15:19:14,268 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,269 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,269 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,275 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval training set based on eval_every=2***************
2022-10-24 15:19:14,276 - trainer - INFO - {
  "train_loss": 0.22763489931821823
}
2022-10-24 15:19:14,284 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval development set based on eval_every=2***************
2022-10-24 15:19:14,285 - trainer - INFO - {
  "dev_loss": 0.2822156846523285,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,285 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:14,286 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,287 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,287 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,287 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_194
2022-10-24 15:19:14,289 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_200
2022-10-24 15:19:14,293 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_200
2022-10-24 15:19:14,294 - trainer - INFO - 
*****************[epoch: 200, global step: 201] eval training set at end of epoch***************
2022-10-24 15:19:14,294 - trainer - INFO - {
  "train_loss": 0.2454349845647812
}
2022-10-24 15:19:14,296 - trainer - INFO - start training epoch 201
2022-10-24 15:19:14,296 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,296 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,297 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,305 - trainer - INFO - 
*****************[epoch: 201, global step: 202] eval training set at end of epoch***************
2022-10-24 15:19:14,305 - trainer - INFO - {
  "train_loss": 0.2822156846523285
}
2022-10-24 15:19:14,305 - trainer - INFO - start training epoch 202
2022-10-24 15:19:14,306 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,306 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,306 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,312 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval training set based on eval_every=2***************
2022-10-24 15:19:14,313 - trainer - INFO - {
  "train_loss": 0.2991301417350769
}
2022-10-24 15:19:14,318 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval development set based on eval_every=2***************
2022-10-24 15:19:14,318 - trainer - INFO - {
  "dev_loss": 0.3460741937160492,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,319 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:19:14,319 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,320 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,320 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,320 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_196
2022-10-24 15:19:14,321 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_202
2022-10-24 15:19:14,326 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_202
2022-10-24 15:19:14,328 - trainer - INFO - 
*****************[epoch: 202, global step: 203] eval training set at end of epoch***************
2022-10-24 15:19:14,330 - trainer - INFO - {
  "train_loss": 0.3160445988178253
}
2022-10-24 15:19:14,331 - trainer - INFO - start training epoch 203
2022-10-24 15:19:14,331 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,331 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,332 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,343 - trainer - INFO - 
*****************[epoch: 203, global step: 204] eval training set at end of epoch***************
2022-10-24 15:19:14,344 - trainer - INFO - {
  "train_loss": 0.3460741937160492
}
2022-10-24 15:19:14,344 - trainer - INFO - start training epoch 204
2022-10-24 15:19:14,345 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,345 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,345 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,353 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval training set based on eval_every=2***************
2022-10-24 15:19:14,353 - trainer - INFO - {
  "train_loss": 0.3597346097230911
}
2022-10-24 15:19:14,359 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval development set based on eval_every=2***************
2022-10-24 15:19:14,359 - trainer - INFO - {
  "dev_loss": 0.39622408151626587,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,360 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:19:14,360 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,361 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,361 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,362 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_198
2022-10-24 15:19:14,363 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_204
2022-10-24 15:19:14,368 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_204
2022-10-24 15:19:14,368 - trainer - INFO - 
*****************[epoch: 204, global step: 205] eval training set at end of epoch***************
2022-10-24 15:19:14,369 - trainer - INFO - {
  "train_loss": 0.37339502573013306
}
2022-10-24 15:19:14,369 - trainer - INFO - start training epoch 205
2022-10-24 15:19:14,370 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,370 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,370 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,379 - trainer - INFO - 
*****************[epoch: 205, global step: 206] eval training set at end of epoch***************
2022-10-24 15:19:14,379 - trainer - INFO - {
  "train_loss": 0.39622408151626587
}
2022-10-24 15:19:14,380 - trainer - INFO - start training epoch 206
2022-10-24 15:19:14,380 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,380 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,381 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,387 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval training set based on eval_every=2***************
2022-10-24 15:19:14,388 - trainer - INFO - {
  "train_loss": 0.40485385060310364
}
2022-10-24 15:19:14,396 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval development set based on eval_every=2***************
2022-10-24 15:19:14,396 - trainer - INFO - {
  "dev_loss": 0.42672500014305115,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,397 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:19:14,397 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,399 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,399 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,399 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_200
2022-10-24 15:19:14,401 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_206
2022-10-24 15:19:14,406 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_206
2022-10-24 15:19:14,407 - trainer - INFO - 
*****************[epoch: 206, global step: 207] eval training set at end of epoch***************
2022-10-24 15:19:14,407 - trainer - INFO - {
  "train_loss": 0.4134836196899414
}
2022-10-24 15:19:14,407 - trainer - INFO - start training epoch 207
2022-10-24 15:19:14,408 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,408 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,408 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,417 - trainer - INFO - 
*****************[epoch: 207, global step: 208] eval training set at end of epoch***************
2022-10-24 15:19:14,418 - trainer - INFO - {
  "train_loss": 0.42672500014305115
}
2022-10-24 15:19:14,418 - trainer - INFO - start training epoch 208
2022-10-24 15:19:14,418 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,420 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,420 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,434 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval training set based on eval_every=2***************
2022-10-24 15:19:14,434 - trainer - INFO - {
  "train_loss": 0.43123872578144073
}
2022-10-24 15:19:14,444 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval development set based on eval_every=2***************
2022-10-24 15:19:14,444 - trainer - INFO - {
  "dev_loss": 0.43970370292663574,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,445 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:19:14,445 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,446 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,447 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,447 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_202
2022-10-24 15:19:14,448 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_208
2022-10-24 15:19:14,453 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_208
2022-10-24 15:19:14,454 - trainer - INFO - 
*****************[epoch: 208, global step: 209] eval training set at end of epoch***************
2022-10-24 15:19:14,454 - trainer - INFO - {
  "train_loss": 0.4357524514198303
}
2022-10-24 15:19:14,455 - trainer - INFO - start training epoch 209
2022-10-24 15:19:14,455 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,455 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,455 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,464 - trainer - INFO - 
*****************[epoch: 209, global step: 210] eval training set at end of epoch***************
2022-10-24 15:19:14,465 - trainer - INFO - {
  "train_loss": 0.43970370292663574
}
2022-10-24 15:19:14,465 - trainer - INFO - start training epoch 210
2022-10-24 15:19:14,466 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,467 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,468 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,478 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval training set based on eval_every=2***************
2022-10-24 15:19:14,478 - trainer - INFO - {
  "train_loss": 0.4397210329771042
}
2022-10-24 15:19:14,484 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval development set based on eval_every=2***************
2022-10-24 15:19:14,485 - trainer - INFO - {
  "dev_loss": 0.43662315607070923,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,485 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:19:14,486 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,487 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,487 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,487 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_204
2022-10-24 15:19:14,488 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_210
2022-10-24 15:19:14,492 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_210
2022-10-24 15:19:14,493 - trainer - INFO - 
*****************[epoch: 210, global step: 211] eval training set at end of epoch***************
2022-10-24 15:19:14,493 - trainer - INFO - {
  "train_loss": 0.43973836302757263
}
2022-10-24 15:19:14,494 - trainer - INFO - start training epoch 211
2022-10-24 15:19:14,494 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,494 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,495 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,503 - trainer - INFO - 
*****************[epoch: 211, global step: 212] eval training set at end of epoch***************
2022-10-24 15:19:14,503 - trainer - INFO - {
  "train_loss": 0.43662315607070923
}
2022-10-24 15:19:14,504 - trainer - INFO - start training epoch 212
2022-10-24 15:19:14,504 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,504 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,504 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,510 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval training set based on eval_every=2***************
2022-10-24 15:19:14,510 - trainer - INFO - {
  "train_loss": 0.4332319498062134
}
2022-10-24 15:19:14,521 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval development set based on eval_every=2***************
2022-10-24 15:19:14,521 - trainer - INFO - {
  "dev_loss": 0.42012500762939453,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,522 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:19:14,522 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,523 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,523 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,524 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_206
2022-10-24 15:19:14,525 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_212
2022-10-24 15:19:14,531 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_212
2022-10-24 15:19:14,532 - trainer - INFO - 
*****************[epoch: 212, global step: 213] eval training set at end of epoch***************
2022-10-24 15:19:14,532 - trainer - INFO - {
  "train_loss": 0.42984074354171753
}
2022-10-24 15:19:14,533 - trainer - INFO - start training epoch 213
2022-10-24 15:19:14,533 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,533 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,533 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,541 - trainer - INFO - 
*****************[epoch: 213, global step: 214] eval training set at end of epoch***************
2022-10-24 15:19:14,542 - trainer - INFO - {
  "train_loss": 0.42012500762939453
}
2022-10-24 15:19:14,542 - trainer - INFO - start training epoch 214
2022-10-24 15:19:14,542 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,542 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,544 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,551 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval training set based on eval_every=2***************
2022-10-24 15:19:14,551 - trainer - INFO - {
  "train_loss": 0.41431406140327454
}
2022-10-24 15:19:14,556 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval development set based on eval_every=2***************
2022-10-24 15:19:14,557 - trainer - INFO - {
  "dev_loss": 0.3947848379611969,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,557 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:19:14,557 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,558 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,559 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,560 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_208
2022-10-24 15:19:14,561 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_214
2022-10-24 15:19:14,569 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_214
2022-10-24 15:19:14,569 - trainer - INFO - 
*****************[epoch: 214, global step: 215] eval training set at end of epoch***************
2022-10-24 15:19:14,570 - trainer - INFO - {
  "train_loss": 0.40850311517715454
}
2022-10-24 15:19:14,570 - trainer - INFO - start training epoch 215
2022-10-24 15:19:14,570 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,571 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,571 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,580 - trainer - INFO - 
*****************[epoch: 215, global step: 216] eval training set at end of epoch***************
2022-10-24 15:19:14,580 - trainer - INFO - {
  "train_loss": 0.3947848379611969
}
2022-10-24 15:19:14,581 - trainer - INFO - start training epoch 216
2022-10-24 15:19:14,581 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,581 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,582 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,589 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval training set based on eval_every=2***************
2022-10-24 15:19:14,589 - trainer - INFO - {
  "train_loss": 0.3870403617620468
}
2022-10-24 15:19:14,599 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval development set based on eval_every=2***************
2022-10-24 15:19:14,600 - trainer - INFO - {
  "dev_loss": 0.36301666498184204,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,601 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:19:14,601 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,602 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,603 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,603 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_210
2022-10-24 15:19:14,605 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_216
2022-10-24 15:19:14,610 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_216
2022-10-24 15:19:14,611 - trainer - INFO - 
*****************[epoch: 216, global step: 217] eval training set at end of epoch***************
2022-10-24 15:19:14,611 - trainer - INFO - {
  "train_loss": 0.37929588556289673
}
2022-10-24 15:19:14,612 - trainer - INFO - start training epoch 217
2022-10-24 15:19:14,612 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,613 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,613 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,623 - trainer - INFO - 
*****************[epoch: 217, global step: 218] eval training set at end of epoch***************
2022-10-24 15:19:14,623 - trainer - INFO - {
  "train_loss": 0.36301666498184204
}
2022-10-24 15:19:14,624 - trainer - INFO - start training epoch 218
2022-10-24 15:19:14,624 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,624 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,624 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,633 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval training set based on eval_every=2***************
2022-10-24 15:19:14,634 - trainer - INFO - {
  "train_loss": 0.3545420616865158
}
2022-10-24 15:19:14,641 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval development set based on eval_every=2***************
2022-10-24 15:19:14,641 - trainer - INFO - {
  "dev_loss": 0.32841756939888,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,642 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:19:14,643 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,643 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,644 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,644 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_212
2022-10-24 15:19:14,645 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_218
2022-10-24 15:19:14,649 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_218
2022-10-24 15:19:14,650 - trainer - INFO - 
*****************[epoch: 218, global step: 219] eval training set at end of epoch***************
2022-10-24 15:19:14,650 - trainer - INFO - {
  "train_loss": 0.3460674583911896
}
2022-10-24 15:19:14,651 - trainer - INFO - start training epoch 219
2022-10-24 15:19:14,651 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,651 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,653 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,668 - trainer - INFO - 
*****************[epoch: 219, global step: 220] eval training set at end of epoch***************
2022-10-24 15:19:14,668 - trainer - INFO - {
  "train_loss": 0.32841756939888
}
2022-10-24 15:19:14,669 - trainer - INFO - start training epoch 220
2022-10-24 15:19:14,669 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,669 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,670 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,678 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval training set based on eval_every=2***************
2022-10-24 15:19:14,678 - trainer - INFO - {
  "train_loss": 0.31962035596370697
}
2022-10-24 15:19:14,686 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval development set based on eval_every=2***************
2022-10-24 15:19:14,687 - trainer - INFO - {
  "dev_loss": 0.2934860289096832,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,687 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:19:14,688 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,689 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,689 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,690 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_214
2022-10-24 15:19:14,691 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_220
2022-10-24 15:19:14,696 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_220
2022-10-24 15:19:14,697 - trainer - INFO - 
*****************[epoch: 220, global step: 221] eval training set at end of epoch***************
2022-10-24 15:19:14,697 - trainer - INFO - {
  "train_loss": 0.31082314252853394
}
2022-10-24 15:19:14,698 - trainer - INFO - start training epoch 221
2022-10-24 15:19:14,699 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,699 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,700 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,713 - trainer - INFO - 
*****************[epoch: 221, global step: 222] eval training set at end of epoch***************
2022-10-24 15:19:14,714 - trainer - INFO - {
  "train_loss": 0.29348599910736084
}
2022-10-24 15:19:14,715 - trainer - INFO - start training epoch 222
2022-10-24 15:19:14,715 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,716 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,716 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,723 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval training set based on eval_every=2***************
2022-10-24 15:19:14,724 - trainer - INFO - {
  "train_loss": 0.2849087566137314
}
2022-10-24 15:19:14,735 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval development set based on eval_every=2***************
2022-10-24 15:19:14,735 - trainer - INFO - {
  "dev_loss": 0.25977084040641785,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,736 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:19:14,737 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,738 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,738 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,738 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_216
2022-10-24 15:19:14,740 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_222
2022-10-24 15:19:14,745 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_222
2022-10-24 15:19:14,746 - trainer - INFO - 
*****************[epoch: 222, global step: 223] eval training set at end of epoch***************
2022-10-24 15:19:14,748 - trainer - INFO - {
  "train_loss": 0.27633151412010193
}
2022-10-24 15:19:14,749 - trainer - INFO - start training epoch 223
2022-10-24 15:19:14,752 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,753 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,753 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,765 - trainer - INFO - 
*****************[epoch: 223, global step: 224] eval training set at end of epoch***************
2022-10-24 15:19:14,766 - trainer - INFO - {
  "train_loss": 0.25977084040641785
}
2022-10-24 15:19:14,766 - trainer - INFO - start training epoch 224
2022-10-24 15:19:14,766 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,767 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,767 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,777 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval training set based on eval_every=2***************
2022-10-24 15:19:14,777 - trainer - INFO - {
  "train_loss": 0.2518991529941559
}
2022-10-24 15:19:14,785 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval development set based on eval_every=2***************
2022-10-24 15:19:14,785 - trainer - INFO - {
  "dev_loss": 0.22903470695018768,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,786 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:19:14,786 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,788 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,788 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,788 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_218
2022-10-24 15:19:14,790 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_224
2022-10-24 15:19:14,795 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_224
2022-10-24 15:19:14,798 - trainer - INFO - 
*****************[epoch: 224, global step: 225] eval training set at end of epoch***************
2022-10-24 15:19:14,798 - trainer - INFO - {
  "train_loss": 0.24402746558189392
}
2022-10-24 15:19:14,803 - trainer - INFO - start training epoch 225
2022-10-24 15:19:14,803 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,803 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,804 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,815 - trainer - INFO - 
*****************[epoch: 225, global step: 226] eval training set at end of epoch***************
2022-10-24 15:19:14,815 - trainer - INFO - {
  "train_loss": 0.2290346920490265
}
2022-10-24 15:19:14,816 - trainer - INFO - start training epoch 226
2022-10-24 15:19:14,816 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,816 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,817 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,824 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval training set based on eval_every=2***************
2022-10-24 15:19:14,825 - trainer - INFO - {
  "train_loss": 0.22195513546466827
}
2022-10-24 15:19:14,839 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval development set based on eval_every=2***************
2022-10-24 15:19:14,840 - trainer - INFO - {
  "dev_loss": 0.20183081924915314,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,841 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:19:14,844 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,845 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,846 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,846 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_220
2022-10-24 15:19:14,847 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_226
2022-10-24 15:19:14,852 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_226
2022-10-24 15:19:14,854 - trainer - INFO - 
*****************[epoch: 226, global step: 227] eval training set at end of epoch***************
2022-10-24 15:19:14,855 - trainer - INFO - {
  "train_loss": 0.21487557888031006
}
2022-10-24 15:19:14,855 - trainer - INFO - start training epoch 227
2022-10-24 15:19:14,856 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,856 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,856 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,867 - trainer - INFO - 
*****************[epoch: 227, global step: 228] eval training set at end of epoch***************
2022-10-24 15:19:14,868 - trainer - INFO - {
  "train_loss": 0.20183081924915314
}
2022-10-24 15:19:14,868 - trainer - INFO - start training epoch 228
2022-10-24 15:19:14,870 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,870 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,871 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,879 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval training set based on eval_every=2***************
2022-10-24 15:19:14,879 - trainer - INFO - {
  "train_loss": 0.19576632976531982
}
2022-10-24 15:19:14,890 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval development set based on eval_every=2***************
2022-10-24 15:19:14,890 - trainer - INFO - {
  "dev_loss": 0.17854125797748566,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,891 - trainer - INFO -   no_improve_count: 17
2022-10-24 15:19:14,892 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,893 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,894 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,894 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_222
2022-10-24 15:19:14,896 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_228
2022-10-24 15:19:14,903 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_228
2022-10-24 15:19:14,904 - trainer - INFO - 
*****************[epoch: 228, global step: 229] eval training set at end of epoch***************
2022-10-24 15:19:14,905 - trainer - INFO - {
  "train_loss": 0.1897018402814865
}
2022-10-24 15:19:14,905 - trainer - INFO - start training epoch 229
2022-10-24 15:19:14,906 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,906 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,906 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,915 - trainer - INFO - 
*****************[epoch: 229, global step: 230] eval training set at end of epoch***************
2022-10-24 15:19:14,916 - trainer - INFO - {
  "train_loss": 0.17854125797748566
}
2022-10-24 15:19:14,916 - trainer - INFO - start training epoch 230
2022-10-24 15:19:14,916 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,916 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,917 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,922 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval training set based on eval_every=2***************
2022-10-24 15:19:14,923 - trainer - INFO - {
  "train_loss": 0.1735103279352188
}
2022-10-24 15:19:14,936 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval development set based on eval_every=2***************
2022-10-24 15:19:14,937 - trainer - INFO - {
  "dev_loss": 0.15939828753471375,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,937 - trainer - INFO -   no_improve_count: 18
2022-10-24 15:19:14,938 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,939 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,939 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,940 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_224
2022-10-24 15:19:14,941 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_230
2022-10-24 15:19:14,947 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_230
2022-10-24 15:19:14,948 - trainer - INFO - 
*****************[epoch: 230, global step: 231] eval training set at end of epoch***************
2022-10-24 15:19:14,948 - trainer - INFO - {
  "train_loss": 0.16847939789295197
}
2022-10-24 15:19:14,948 - trainer - INFO - start training epoch 231
2022-10-24 15:19:14,949 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,949 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,950 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,956 - trainer - INFO - 
*****************[epoch: 231, global step: 232] eval training set at end of epoch***************
2022-10-24 15:19:14,957 - trainer - INFO - {
  "train_loss": 0.15939828753471375
}
2022-10-24 15:19:14,957 - trainer - INFO - start training epoch 232
2022-10-24 15:19:14,958 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,958 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,958 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,966 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval training set based on eval_every=2***************
2022-10-24 15:19:14,967 - trainer - INFO - {
  "train_loss": 0.15530198067426682
}
2022-10-24 15:19:14,973 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval development set based on eval_every=2***************
2022-10-24 15:19:14,973 - trainer - INFO - {
  "dev_loss": 0.14397870004177094,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:14,974 - trainer - INFO -   no_improve_count: 19
2022-10-24 15:19:14,974 - trainer - INFO -   patience: 200
2022-10-24 15:19:14,975 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:14,976 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:14,976 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_226
2022-10-24 15:19:14,977 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_232
2022-10-24 15:19:14,983 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_232
2022-10-24 15:19:14,983 - trainer - INFO - 
*****************[epoch: 232, global step: 233] eval training set at end of epoch***************
2022-10-24 15:19:14,984 - trainer - INFO - {
  "train_loss": 0.15120567381381989
}
2022-10-24 15:19:14,984 - trainer - INFO - start training epoch 233
2022-10-24 15:19:14,984 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,984 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,985 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:14,995 - trainer - INFO - 
*****************[epoch: 233, global step: 234] eval training set at end of epoch***************
2022-10-24 15:19:14,996 - trainer - INFO - {
  "train_loss": 0.14397871494293213
}
2022-10-24 15:19:14,996 - trainer - INFO - start training epoch 234
2022-10-24 15:19:14,997 - trainer - INFO - training using device=cuda
2022-10-24 15:19:14,997 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:14,998 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,005 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval training set based on eval_every=2***************
2022-10-24 15:19:15,006 - trainer - INFO - {
  "train_loss": 0.14081645756959915
}
2022-10-24 15:19:15,013 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval development set based on eval_every=2***************
2022-10-24 15:19:15,014 - trainer - INFO - {
  "dev_loss": 0.13208037614822388,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:15,014 - trainer - INFO -   no_improve_count: 20
2022-10-24 15:19:15,015 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,016 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,017 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,017 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_228
2022-10-24 15:19:15,018 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_234
2022-10-24 15:19:15,022 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_234
2022-10-24 15:19:15,023 - trainer - INFO - 
*****************[epoch: 234, global step: 235] eval training set at end of epoch***************
2022-10-24 15:19:15,023 - trainer - INFO - {
  "train_loss": 0.13765420019626617
}
2022-10-24 15:19:15,024 - trainer - INFO - start training epoch 235
2022-10-24 15:19:15,025 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,025 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,025 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,034 - trainer - INFO - 
*****************[epoch: 235, global step: 236] eval training set at end of epoch***************
2022-10-24 15:19:15,034 - trainer - INFO - {
  "train_loss": 0.13208036124706268
}
2022-10-24 15:19:15,035 - trainer - INFO - start training epoch 236
2022-10-24 15:19:15,035 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,035 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,036 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,042 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval training set based on eval_every=2***************
2022-10-24 15:19:15,043 - trainer - INFO - {
  "train_loss": 0.12968453764915466
}
2022-10-24 15:19:15,050 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval development set based on eval_every=2***************
2022-10-24 15:19:15,050 - trainer - INFO - {
  "dev_loss": 0.12320395559072495,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:15,051 - trainer - INFO -   no_improve_count: 21
2022-10-24 15:19:15,052 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,053 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,053 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,054 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_230
2022-10-24 15:19:15,055 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_236
2022-10-24 15:19:15,061 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_236
2022-10-24 15:19:15,062 - trainer - INFO - 
*****************[epoch: 236, global step: 237] eval training set at end of epoch***************
2022-10-24 15:19:15,062 - trainer - INFO - {
  "train_loss": 0.12728871405124664
}
2022-10-24 15:19:15,063 - trainer - INFO - start training epoch 237
2022-10-24 15:19:15,063 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,063 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,064 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,078 - trainer - INFO - 
*****************[epoch: 237, global step: 238] eval training set at end of epoch***************
2022-10-24 15:19:15,078 - trainer - INFO - {
  "train_loss": 0.12320396304130554
}
2022-10-24 15:19:15,078 - trainer - INFO - start training epoch 238
2022-10-24 15:19:15,079 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,079 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,080 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,088 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval training set based on eval_every=2***************
2022-10-24 15:19:15,088 - trainer - INFO - {
  "train_loss": 0.12145492434501648
}
2022-10-24 15:19:15,099 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval development set based on eval_every=2***************
2022-10-24 15:19:15,099 - trainer - INFO - {
  "dev_loss": 0.1167900562286377,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:15,100 - trainer - INFO -   no_improve_count: 22
2022-10-24 15:19:15,100 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,102 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,102 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,102 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_232
2022-10-24 15:19:15,104 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_238
2022-10-24 15:19:15,108 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_238
2022-10-24 15:19:15,109 - trainer - INFO - 
*****************[epoch: 238, global step: 239] eval training set at end of epoch***************
2022-10-24 15:19:15,109 - trainer - INFO - {
  "train_loss": 0.11970588564872742
}
2022-10-24 15:19:15,110 - trainer - INFO - start training epoch 239
2022-10-24 15:19:15,110 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,110 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,111 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,122 - trainer - INFO - 
*****************[epoch: 239, global step: 240] eval training set at end of epoch***************
2022-10-24 15:19:15,122 - trainer - INFO - {
  "train_loss": 0.1167900562286377
}
2022-10-24 15:19:15,123 - trainer - INFO - start training epoch 240
2022-10-24 15:19:15,123 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,123 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,124 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,136 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval training set based on eval_every=2***************
2022-10-24 15:19:15,136 - trainer - INFO - {
  "train_loss": 0.11559014022350311
}
2022-10-24 15:19:15,145 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval development set based on eval_every=2***************
2022-10-24 15:19:15,145 - trainer - INFO - {
  "dev_loss": 0.11241704225540161,
  "dev_best_score_for_loss": -0.10983549803495407
}
2022-10-24 15:19:15,146 - trainer - INFO -   no_improve_count: 23
2022-10-24 15:19:15,146 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,148 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,148 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,149 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_234
2022-10-24 15:19:15,154 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_240
2022-10-24 15:19:15,159 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_240
2022-10-24 15:19:15,160 - trainer - INFO - 
*****************[epoch: 240, global step: 241] eval training set at end of epoch***************
2022-10-24 15:19:15,160 - trainer - INFO - {
  "train_loss": 0.11439022421836853
}
2022-10-24 15:19:15,161 - trainer - INFO - start training epoch 241
2022-10-24 15:19:15,161 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,161 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,162 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,172 - trainer - INFO - 
*****************[epoch: 241, global step: 242] eval training set at end of epoch***************
2022-10-24 15:19:15,172 - trainer - INFO - {
  "train_loss": 0.11241704970598221
}
2022-10-24 15:19:15,173 - trainer - INFO - start training epoch 242
2022-10-24 15:19:15,173 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,173 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,174 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,182 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval training set based on eval_every=2***************
2022-10-24 15:19:15,184 - trainer - INFO - {
  "train_loss": 0.11163054406642914
}
2022-10-24 15:19:15,191 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval development set based on eval_every=2***************
2022-10-24 15:19:15,191 - trainer - INFO - {
  "dev_loss": 0.10962621122598648,
  "dev_best_score_for_loss": -0.10962621122598648
}
2022-10-24 15:19:15,192 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:15,193 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,195 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,196 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_236
2022-10-24 15:19:15,197 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:15,201 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:15,201 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:15,202 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,202 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:15,202 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_242
2022-10-24 15:19:15,207 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_242
2022-10-24 15:19:15,208 - trainer - INFO - 
*****************[epoch: 242, global step: 243] eval training set at end of epoch***************
2022-10-24 15:19:15,208 - trainer - INFO - {
  "train_loss": 0.11084403842687607
}
2022-10-24 15:19:15,208 - trainer - INFO - start training epoch 243
2022-10-24 15:19:15,208 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,209 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,209 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,221 - trainer - INFO - 
*****************[epoch: 243, global step: 244] eval training set at end of epoch***************
2022-10-24 15:19:15,222 - trainer - INFO - {
  "train_loss": 0.10962621122598648
}
2022-10-24 15:19:15,222 - trainer - INFO - start training epoch 244
2022-10-24 15:19:15,222 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,222 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,223 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,234 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval training set based on eval_every=2***************
2022-10-24 15:19:15,234 - trainer - INFO - {
  "train_loss": 0.1091478019952774
}
2022-10-24 15:19:15,241 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval development set based on eval_every=2***************
2022-10-24 15:19:15,241 - trainer - INFO - {
  "dev_loss": 0.10795927047729492,
  "dev_best_score_for_loss": -0.10795927047729492
}
2022-10-24 15:19:15,242 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:15,243 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,244 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,244 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_238
2022-10-24 15:19:15,246 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:15,249 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:15,249 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:15,249 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,250 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:15,250 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_244
2022-10-24 15:19:15,254 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_244
2022-10-24 15:19:15,255 - trainer - INFO - 
*****************[epoch: 244, global step: 245] eval training set at end of epoch***************
2022-10-24 15:19:15,255 - trainer - INFO - {
  "train_loss": 0.10866939276456833
}
2022-10-24 15:19:15,255 - trainer - INFO - start training epoch 245
2022-10-24 15:19:15,255 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,257 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,258 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,271 - trainer - INFO - 
*****************[epoch: 245, global step: 246] eval training set at end of epoch***************
2022-10-24 15:19:15,272 - trainer - INFO - {
  "train_loss": 0.10795926302671432
}
2022-10-24 15:19:15,273 - trainer - INFO - start training epoch 246
2022-10-24 15:19:15,273 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,273 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,274 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,281 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval training set based on eval_every=2***************
2022-10-24 15:19:15,281 - trainer - INFO - {
  "train_loss": 0.10771390795707703
}
2022-10-24 15:19:15,288 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval development set based on eval_every=2***************
2022-10-24 15:19:15,288 - trainer - INFO - {
  "dev_loss": 0.10713691264390945,
  "dev_best_score_for_loss": -0.10713691264390945
}
2022-10-24 15:19:15,289 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:15,290 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,291 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,291 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_240
2022-10-24 15:19:15,292 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:15,296 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:15,296 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:15,297 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,297 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:15,298 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_246
2022-10-24 15:19:15,302 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_246
2022-10-24 15:19:15,305 - trainer - INFO - 
*****************[epoch: 246, global step: 247] eval training set at end of epoch***************
2022-10-24 15:19:15,308 - trainer - INFO - {
  "train_loss": 0.10746855288743973
}
2022-10-24 15:19:15,309 - trainer - INFO - start training epoch 247
2022-10-24 15:19:15,309 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,309 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,310 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,318 - trainer - INFO - 
*****************[epoch: 247, global step: 248] eval training set at end of epoch***************
2022-10-24 15:19:15,319 - trainer - INFO - {
  "train_loss": 0.10713692754507065
}
2022-10-24 15:19:15,320 - trainer - INFO - start training epoch 248
2022-10-24 15:19:15,320 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,320 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,320 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,330 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval training set based on eval_every=2***************
2022-10-24 15:19:15,331 - trainer - INFO - {
  "train_loss": 0.10702936723828316
}
2022-10-24 15:19:15,342 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval development set based on eval_every=2***************
2022-10-24 15:19:15,342 - trainer - INFO - {
  "dev_loss": 0.1068611741065979,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,343 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:15,345 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,345 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,346 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_242
2022-10-24 15:19:15,347 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:15,351 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:15,351 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:15,351 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,352 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:15,352 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_248
2022-10-24 15:19:15,356 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_248
2022-10-24 15:19:15,357 - trainer - INFO - 
*****************[epoch: 248, global step: 249] eval training set at end of epoch***************
2022-10-24 15:19:15,357 - trainer - INFO - {
  "train_loss": 0.10692180693149567
}
2022-10-24 15:19:15,358 - trainer - INFO - start training epoch 249
2022-10-24 15:19:15,358 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,358 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,358 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,371 - trainer - INFO - 
*****************[epoch: 249, global step: 250] eval training set at end of epoch***************
2022-10-24 15:19:15,371 - trainer - INFO - {
  "train_loss": 0.1068611741065979
}
2022-10-24 15:19:15,372 - trainer - INFO - start training epoch 250
2022-10-24 15:19:15,372 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,372 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,373 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,382 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval training set based on eval_every=2***************
2022-10-24 15:19:15,382 - trainer - INFO - {
  "train_loss": 0.10685139894485474
}
2022-10-24 15:19:15,388 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval development set based on eval_every=2***************
2022-10-24 15:19:15,388 - trainer - INFO - {
  "dev_loss": 0.10689689964056015,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,389 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:15,389 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,390 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,391 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,391 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_244
2022-10-24 15:19:15,393 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_250
2022-10-24 15:19:15,398 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_250
2022-10-24 15:19:15,402 - trainer - INFO - 
*****************[epoch: 250, global step: 251] eval training set at end of epoch***************
2022-10-24 15:19:15,402 - trainer - INFO - {
  "train_loss": 0.10684162378311157
}
2022-10-24 15:19:15,403 - trainer - INFO - start training epoch 251
2022-10-24 15:19:15,403 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,403 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,404 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,416 - trainer - INFO - 
*****************[epoch: 251, global step: 252] eval training set at end of epoch***************
2022-10-24 15:19:15,416 - trainer - INFO - {
  "train_loss": 0.10689689964056015
}
2022-10-24 15:19:15,417 - trainer - INFO - start training epoch 252
2022-10-24 15:19:15,418 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,418 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,418 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,428 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval training set based on eval_every=2***************
2022-10-24 15:19:15,429 - trainer - INFO - {
  "train_loss": 0.10694154351949692
}
2022-10-24 15:19:15,444 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval development set based on eval_every=2***************
2022-10-24 15:19:15,448 - trainer - INFO - {
  "dev_loss": 0.10712079703807831,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,449 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:15,450 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,451 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,452 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,452 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_246
2022-10-24 15:19:15,454 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_252
2022-10-24 15:19:15,460 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_252
2022-10-24 15:19:15,461 - trainer - INFO - 
*****************[epoch: 252, global step: 253] eval training set at end of epoch***************
2022-10-24 15:19:15,462 - trainer - INFO - {
  "train_loss": 0.10698618739843369
}
2022-10-24 15:19:15,462 - trainer - INFO - start training epoch 253
2022-10-24 15:19:15,463 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,463 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,464 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,475 - trainer - INFO - 
*****************[epoch: 253, global step: 254] eval training set at end of epoch***************
2022-10-24 15:19:15,475 - trainer - INFO - {
  "train_loss": 0.10712079703807831
}
2022-10-24 15:19:15,476 - trainer - INFO - start training epoch 254
2022-10-24 15:19:15,476 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,476 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,477 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,483 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval training set based on eval_every=2***************
2022-10-24 15:19:15,483 - trainer - INFO - {
  "train_loss": 0.10718073695898056
}
2022-10-24 15:19:15,489 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval development set based on eval_every=2***************
2022-10-24 15:19:15,490 - trainer - INFO - {
  "dev_loss": 0.1073848232626915,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,492 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:15,492 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,498 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,498 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,498 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_248
2022-10-24 15:19:15,500 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_254
2022-10-24 15:19:15,505 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_254
2022-10-24 15:19:15,506 - trainer - INFO - 
*****************[epoch: 254, global step: 255] eval training set at end of epoch***************
2022-10-24 15:19:15,507 - trainer - INFO - {
  "train_loss": 0.10724067687988281
}
2022-10-24 15:19:15,507 - trainer - INFO - start training epoch 255
2022-10-24 15:19:15,507 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,507 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,508 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,516 - trainer - INFO - 
*****************[epoch: 255, global step: 256] eval training set at end of epoch***************
2022-10-24 15:19:15,516 - trainer - INFO - {
  "train_loss": 0.1073848232626915
}
2022-10-24 15:19:15,517 - trainer - INFO - start training epoch 256
2022-10-24 15:19:15,517 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,517 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,517 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,528 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval training set based on eval_every=2***************
2022-10-24 15:19:15,529 - trainer - INFO - {
  "train_loss": 0.10745370015501976
}
2022-10-24 15:19:15,538 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval development set based on eval_every=2***************
2022-10-24 15:19:15,541 - trainer - INFO - {
  "dev_loss": 0.10765372961759567,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,542 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:19:15,543 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,544 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,544 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,544 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_250
2022-10-24 15:19:15,546 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_256
2022-10-24 15:19:15,551 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_256
2022-10-24 15:19:15,552 - trainer - INFO - 
*****************[epoch: 256, global step: 257] eval training set at end of epoch***************
2022-10-24 15:19:15,553 - trainer - INFO - {
  "train_loss": 0.10752257704734802
}
2022-10-24 15:19:15,553 - trainer - INFO - start training epoch 257
2022-10-24 15:19:15,554 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,554 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,555 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,566 - trainer - INFO - 
*****************[epoch: 257, global step: 258] eval training set at end of epoch***************
2022-10-24 15:19:15,567 - trainer - INFO - {
  "train_loss": 0.10765372961759567
}
2022-10-24 15:19:15,567 - trainer - INFO - start training epoch 258
2022-10-24 15:19:15,567 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,567 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,568 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,574 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval training set based on eval_every=2***************
2022-10-24 15:19:15,574 - trainer - INFO - {
  "train_loss": 0.10770153254270554
}
2022-10-24 15:19:15,581 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval development set based on eval_every=2***************
2022-10-24 15:19:15,581 - trainer - INFO - {
  "dev_loss": 0.10786780714988708,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,582 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:19:15,583 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,587 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,587 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,588 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_252
2022-10-24 15:19:15,589 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_258
2022-10-24 15:19:15,595 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_258
2022-10-24 15:19:15,596 - trainer - INFO - 
*****************[epoch: 258, global step: 259] eval training set at end of epoch***************
2022-10-24 15:19:15,596 - trainer - INFO - {
  "train_loss": 0.1077493354678154
}
2022-10-24 15:19:15,597 - trainer - INFO - start training epoch 259
2022-10-24 15:19:15,598 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,598 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,599 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,610 - trainer - INFO - 
*****************[epoch: 259, global step: 260] eval training set at end of epoch***************
2022-10-24 15:19:15,610 - trainer - INFO - {
  "train_loss": 0.10786780714988708
}
2022-10-24 15:19:15,610 - trainer - INFO - start training epoch 260
2022-10-24 15:19:15,611 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,611 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,611 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,622 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval training set based on eval_every=2***************
2022-10-24 15:19:15,623 - trainer - INFO - {
  "train_loss": 0.10790161788463593
}
2022-10-24 15:19:15,634 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval development set based on eval_every=2***************
2022-10-24 15:19:15,635 - trainer - INFO - {
  "dev_loss": 0.10798713564872742,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,635 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:19:15,636 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,637 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,638 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,638 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_254
2022-10-24 15:19:15,640 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_260
2022-10-24 15:19:15,645 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_260
2022-10-24 15:19:15,646 - trainer - INFO - 
*****************[epoch: 260, global step: 261] eval training set at end of epoch***************
2022-10-24 15:19:15,646 - trainer - INFO - {
  "train_loss": 0.10793542861938477
}
2022-10-24 15:19:15,646 - trainer - INFO - start training epoch 261
2022-10-24 15:19:15,647 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,647 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,647 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,656 - trainer - INFO - 
*****************[epoch: 261, global step: 262] eval training set at end of epoch***************
2022-10-24 15:19:15,657 - trainer - INFO - {
  "train_loss": 0.10798713564872742
}
2022-10-24 15:19:15,658 - trainer - INFO - start training epoch 262
2022-10-24 15:19:15,658 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,658 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,659 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,670 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval training set based on eval_every=2***************
2022-10-24 15:19:15,671 - trainer - INFO - {
  "train_loss": 0.10801271721720695
}
2022-10-24 15:19:15,679 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval development set based on eval_every=2***************
2022-10-24 15:19:15,679 - trainer - INFO - {
  "dev_loss": 0.10804718732833862,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,680 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:19:15,681 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,682 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,682 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,682 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_256
2022-10-24 15:19:15,684 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_262
2022-10-24 15:19:15,688 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_262
2022-10-24 15:19:15,689 - trainer - INFO - 
*****************[epoch: 262, global step: 263] eval training set at end of epoch***************
2022-10-24 15:19:15,689 - trainer - INFO - {
  "train_loss": 0.10803829878568649
}
2022-10-24 15:19:15,689 - trainer - INFO - start training epoch 263
2022-10-24 15:19:15,689 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,690 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,691 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,701 - trainer - INFO - 
*****************[epoch: 263, global step: 264] eval training set at end of epoch***************
2022-10-24 15:19:15,701 - trainer - INFO - {
  "train_loss": 0.10804718732833862
}
2022-10-24 15:19:15,702 - trainer - INFO - start training epoch 264
2022-10-24 15:19:15,702 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,702 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,702 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,709 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval training set based on eval_every=2***************
2022-10-24 15:19:15,710 - trainer - INFO - {
  "train_loss": 0.1080554947257042
}
2022-10-24 15:19:15,716 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval development set based on eval_every=2***************
2022-10-24 15:19:15,717 - trainer - INFO - {
  "dev_loss": 0.1080474704504013,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,717 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:19:15,718 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,719 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,719 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,719 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_258
2022-10-24 15:19:15,721 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_264
2022-10-24 15:19:15,727 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_264
2022-10-24 15:19:15,728 - trainer - INFO - 
*****************[epoch: 264, global step: 265] eval training set at end of epoch***************
2022-10-24 15:19:15,728 - trainer - INFO - {
  "train_loss": 0.10806380212306976
}
2022-10-24 15:19:15,729 - trainer - INFO - start training epoch 265
2022-10-24 15:19:15,729 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,729 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,729 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,742 - trainer - INFO - 
*****************[epoch: 265, global step: 266] eval training set at end of epoch***************
2022-10-24 15:19:15,742 - trainer - INFO - {
  "train_loss": 0.1080474704504013
}
2022-10-24 15:19:15,743 - trainer - INFO - start training epoch 266
2022-10-24 15:19:15,743 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,743 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,744 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,751 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval training set based on eval_every=2***************
2022-10-24 15:19:15,752 - trainer - INFO - {
  "train_loss": 0.10803379118442535
}
2022-10-24 15:19:15,759 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval development set based on eval_every=2***************
2022-10-24 15:19:15,760 - trainer - INFO - {
  "dev_loss": 0.10799352079629898,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,761 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:19:15,761 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,762 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,762 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,763 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_260
2022-10-24 15:19:15,764 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_266
2022-10-24 15:19:15,769 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_266
2022-10-24 15:19:15,773 - trainer - INFO - 
*****************[epoch: 266, global step: 267] eval training set at end of epoch***************
2022-10-24 15:19:15,774 - trainer - INFO - {
  "train_loss": 0.1080201119184494
}
2022-10-24 15:19:15,774 - trainer - INFO - start training epoch 267
2022-10-24 15:19:15,775 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,775 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,776 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,786 - trainer - INFO - 
*****************[epoch: 267, global step: 268] eval training set at end of epoch***************
2022-10-24 15:19:15,787 - trainer - INFO - {
  "train_loss": 0.10799352824687958
}
2022-10-24 15:19:15,787 - trainer - INFO - start training epoch 268
2022-10-24 15:19:15,787 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,788 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,788 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,797 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval training set based on eval_every=2***************
2022-10-24 15:19:15,798 - trainer - INFO - {
  "train_loss": 0.10796676576137543
}
2022-10-24 15:19:15,807 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval development set based on eval_every=2***************
2022-10-24 15:19:15,807 - trainer - INFO - {
  "dev_loss": 0.10788530111312866,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,808 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:19:15,808 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,809 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,809 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,810 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_262
2022-10-24 15:19:15,811 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_268
2022-10-24 15:19:15,816 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_268
2022-10-24 15:19:15,817 - trainer - INFO - 
*****************[epoch: 268, global step: 269] eval training set at end of epoch***************
2022-10-24 15:19:15,818 - trainer - INFO - {
  "train_loss": 0.10794000327587128
}
2022-10-24 15:19:15,821 - trainer - INFO - start training epoch 269
2022-10-24 15:19:15,821 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,821 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,822 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,833 - trainer - INFO - 
*****************[epoch: 269, global step: 270] eval training set at end of epoch***************
2022-10-24 15:19:15,833 - trainer - INFO - {
  "train_loss": 0.10788530111312866
}
2022-10-24 15:19:15,834 - trainer - INFO - start training epoch 270
2022-10-24 15:19:15,834 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,835 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,835 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,844 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval training set based on eval_every=2***************
2022-10-24 15:19:15,844 - trainer - INFO - {
  "train_loss": 0.10784842073917389
}
2022-10-24 15:19:15,852 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval development set based on eval_every=2***************
2022-10-24 15:19:15,852 - trainer - INFO - {
  "dev_loss": 0.10776728391647339,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,853 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:19:15,853 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,854 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,855 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,855 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_264
2022-10-24 15:19:15,857 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_270
2022-10-24 15:19:15,862 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_270
2022-10-24 15:19:15,864 - trainer - INFO - 
*****************[epoch: 270, global step: 271] eval training set at end of epoch***************
2022-10-24 15:19:15,869 - trainer - INFO - {
  "train_loss": 0.10781154036521912
}
2022-10-24 15:19:15,870 - trainer - INFO - start training epoch 271
2022-10-24 15:19:15,870 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,870 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,871 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,883 - trainer - INFO - 
*****************[epoch: 271, global step: 272] eval training set at end of epoch***************
2022-10-24 15:19:15,884 - trainer - INFO - {
  "train_loss": 0.10776729136705399
}
2022-10-24 15:19:15,884 - trainer - INFO - start training epoch 272
2022-10-24 15:19:15,885 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,885 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,885 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,896 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval training set based on eval_every=2***************
2022-10-24 15:19:15,897 - trainer - INFO - {
  "train_loss": 0.10772686824202538
}
2022-10-24 15:19:15,904 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval development set based on eval_every=2***************
2022-10-24 15:19:15,905 - trainer - INFO - {
  "dev_loss": 0.10762357711791992,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,906 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:19:15,906 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,907 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,908 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,909 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_266
2022-10-24 15:19:15,911 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_272
2022-10-24 15:19:15,918 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_272
2022-10-24 15:19:15,919 - trainer - INFO - 
*****************[epoch: 272, global step: 273] eval training set at end of epoch***************
2022-10-24 15:19:15,920 - trainer - INFO - {
  "train_loss": 0.10768644511699677
}
2022-10-24 15:19:15,920 - trainer - INFO - start training epoch 273
2022-10-24 15:19:15,921 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,921 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,921 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,933 - trainer - INFO - 
*****************[epoch: 273, global step: 274] eval training set at end of epoch***************
2022-10-24 15:19:15,934 - trainer - INFO - {
  "train_loss": 0.10762357711791992
}
2022-10-24 15:19:15,934 - trainer - INFO - start training epoch 274
2022-10-24 15:19:15,934 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,935 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,935 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,943 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval training set based on eval_every=2***************
2022-10-24 15:19:15,943 - trainer - INFO - {
  "train_loss": 0.10758768767118454
}
2022-10-24 15:19:15,950 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval development set based on eval_every=2***************
2022-10-24 15:19:15,950 - trainer - INFO - {
  "dev_loss": 0.10749126970767975,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:15,951 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:19:15,951 - trainer - INFO -   patience: 200
2022-10-24 15:19:15,952 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:15,953 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:15,953 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_268
2022-10-24 15:19:15,955 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_274
2022-10-24 15:19:15,961 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_274
2022-10-24 15:19:15,962 - trainer - INFO - 
*****************[epoch: 274, global step: 275] eval training set at end of epoch***************
2022-10-24 15:19:15,963 - trainer - INFO - {
  "train_loss": 0.10755179822444916
}
2022-10-24 15:19:15,963 - trainer - INFO - start training epoch 275
2022-10-24 15:19:15,963 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,964 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,964 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,973 - trainer - INFO - 
*****************[epoch: 275, global step: 276] eval training set at end of epoch***************
2022-10-24 15:19:15,974 - trainer - INFO - {
  "train_loss": 0.10749126970767975
}
2022-10-24 15:19:15,974 - trainer - INFO - start training epoch 276
2022-10-24 15:19:15,974 - trainer - INFO - training using device=cuda
2022-10-24 15:19:15,974 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:15,975 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:15,982 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval training set based on eval_every=2***************
2022-10-24 15:19:15,982 - trainer - INFO - {
  "train_loss": 0.10745605453848839
}
2022-10-24 15:19:16,001 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval development set based on eval_every=2***************
2022-10-24 15:19:16,002 - trainer - INFO - {
  "dev_loss": 0.10735458135604858,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:16,003 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:19:16,003 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,005 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,005 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,005 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_270
2022-10-24 15:19:16,007 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_276
2022-10-24 15:19:16,012 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_276
2022-10-24 15:19:16,013 - trainer - INFO - 
*****************[epoch: 276, global step: 277] eval training set at end of epoch***************
2022-10-24 15:19:16,013 - trainer - INFO - {
  "train_loss": 0.10742083936929703
}
2022-10-24 15:19:16,014 - trainer - INFO - start training epoch 277
2022-10-24 15:19:16,014 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,015 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,015 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,025 - trainer - INFO - 
*****************[epoch: 277, global step: 278] eval training set at end of epoch***************
2022-10-24 15:19:16,026 - trainer - INFO - {
  "train_loss": 0.10735458135604858
}
2022-10-24 15:19:16,026 - trainer - INFO - start training epoch 278
2022-10-24 15:19:16,026 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,027 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,027 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,038 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval training set based on eval_every=2***************
2022-10-24 15:19:16,038 - trainer - INFO - {
  "train_loss": 0.10732677206397057
}
2022-10-24 15:19:16,048 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval development set based on eval_every=2***************
2022-10-24 15:19:16,048 - trainer - INFO - {
  "dev_loss": 0.10723140090703964,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:16,049 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:19:16,049 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,051 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,051 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,051 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_272
2022-10-24 15:19:16,052 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_278
2022-10-24 15:19:16,057 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_278
2022-10-24 15:19:16,058 - trainer - INFO - 
*****************[epoch: 278, global step: 279] eval training set at end of epoch***************
2022-10-24 15:19:16,059 - trainer - INFO - {
  "train_loss": 0.10729896277189255
}
2022-10-24 15:19:16,059 - trainer - INFO - start training epoch 279
2022-10-24 15:19:16,060 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,060 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,060 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,073 - trainer - INFO - 
*****************[epoch: 279, global step: 280] eval training set at end of epoch***************
2022-10-24 15:19:16,073 - trainer - INFO - {
  "train_loss": 0.10723141580820084
}
2022-10-24 15:19:16,073 - trainer - INFO - start training epoch 280
2022-10-24 15:19:16,074 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,074 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,074 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,082 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval training set based on eval_every=2***************
2022-10-24 15:19:16,082 - trainer - INFO - {
  "train_loss": 0.10720571875572205
}
2022-10-24 15:19:16,088 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval development set based on eval_every=2***************
2022-10-24 15:19:16,088 - trainer - INFO - {
  "dev_loss": 0.1071382462978363,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:16,088 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:19:16,089 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,090 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,090 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,090 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_274
2022-10-24 15:19:16,092 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_280
2022-10-24 15:19:16,097 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_280
2022-10-24 15:19:16,101 - trainer - INFO - 
*****************[epoch: 280, global step: 281] eval training set at end of epoch***************
2022-10-24 15:19:16,101 - trainer - INFO - {
  "train_loss": 0.10718002170324326
}
2022-10-24 15:19:16,102 - trainer - INFO - start training epoch 281
2022-10-24 15:19:16,102 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,102 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,102 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,112 - trainer - INFO - 
*****************[epoch: 281, global step: 282] eval training set at end of epoch***************
2022-10-24 15:19:16,112 - trainer - INFO - {
  "train_loss": 0.1071382462978363
}
2022-10-24 15:19:16,113 - trainer - INFO - start training epoch 282
2022-10-24 15:19:16,113 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,113 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,114 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,120 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval training set based on eval_every=2***************
2022-10-24 15:19:16,120 - trainer - INFO - {
  "train_loss": 0.10711538419127464
}
2022-10-24 15:19:16,129 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval development set based on eval_every=2***************
2022-10-24 15:19:16,130 - trainer - INFO - {
  "dev_loss": 0.10705894976854324,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:16,130 - trainer - INFO -   no_improve_count: 17
2022-10-24 15:19:16,131 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,132 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,132 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,132 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_276
2022-10-24 15:19:16,134 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_282
2022-10-24 15:19:16,138 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_282
2022-10-24 15:19:16,139 - trainer - INFO - 
*****************[epoch: 282, global step: 283] eval training set at end of epoch***************
2022-10-24 15:19:16,139 - trainer - INFO - {
  "train_loss": 0.10709252208471298
}
2022-10-24 15:19:16,141 - trainer - INFO - start training epoch 283
2022-10-24 15:19:16,142 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,143 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,143 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,156 - trainer - INFO - 
*****************[epoch: 283, global step: 284] eval training set at end of epoch***************
2022-10-24 15:19:16,156 - trainer - INFO - {
  "train_loss": 0.10705894976854324
}
2022-10-24 15:19:16,157 - trainer - INFO - start training epoch 284
2022-10-24 15:19:16,157 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,157 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,158 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,172 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval training set based on eval_every=2***************
2022-10-24 15:19:16,173 - trainer - INFO - {
  "train_loss": 0.10703335702419281
}
2022-10-24 15:19:16,180 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval development set based on eval_every=2***************
2022-10-24 15:19:16,180 - trainer - INFO - {
  "dev_loss": 0.1069859191775322,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:16,181 - trainer - INFO -   no_improve_count: 18
2022-10-24 15:19:16,181 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,182 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,183 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,183 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_278
2022-10-24 15:19:16,184 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_284
2022-10-24 15:19:16,189 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_284
2022-10-24 15:19:16,193 - trainer - INFO - 
*****************[epoch: 284, global step: 285] eval training set at end of epoch***************
2022-10-24 15:19:16,193 - trainer - INFO - {
  "train_loss": 0.10700776427984238
}
2022-10-24 15:19:16,194 - trainer - INFO - start training epoch 285
2022-10-24 15:19:16,194 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,194 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,195 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,205 - trainer - INFO - 
*****************[epoch: 285, global step: 286] eval training set at end of epoch***************
2022-10-24 15:19:16,205 - trainer - INFO - {
  "train_loss": 0.1069859191775322
}
2022-10-24 15:19:16,206 - trainer - INFO - start training epoch 286
2022-10-24 15:19:16,206 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,206 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,207 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,215 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval training set based on eval_every=2***************
2022-10-24 15:19:16,215 - trainer - INFO - {
  "train_loss": 0.10696492344141006
}
2022-10-24 15:19:16,222 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval development set based on eval_every=2***************
2022-10-24 15:19:16,222 - trainer - INFO - {
  "dev_loss": 0.10693272948265076,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:16,223 - trainer - INFO -   no_improve_count: 19
2022-10-24 15:19:16,223 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,224 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,224 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,225 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_280
2022-10-24 15:19:16,226 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_286
2022-10-24 15:19:16,231 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_286
2022-10-24 15:19:16,233 - trainer - INFO - 
*****************[epoch: 286, global step: 287] eval training set at end of epoch***************
2022-10-24 15:19:16,234 - trainer - INFO - {
  "train_loss": 0.10694392770528793
}
2022-10-24 15:19:16,235 - trainer - INFO - start training epoch 287
2022-10-24 15:19:16,235 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,239 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,239 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,250 - trainer - INFO - 
*****************[epoch: 287, global step: 288] eval training set at end of epoch***************
2022-10-24 15:19:16,250 - trainer - INFO - {
  "train_loss": 0.10693272948265076
}
2022-10-24 15:19:16,251 - trainer - INFO - start training epoch 288
2022-10-24 15:19:16,251 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,252 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,252 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,259 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval training set based on eval_every=2***************
2022-10-24 15:19:16,260 - trainer - INFO - {
  "train_loss": 0.10691575706005096
}
2022-10-24 15:19:16,270 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval development set based on eval_every=2***************
2022-10-24 15:19:16,270 - trainer - INFO - {
  "dev_loss": 0.10689200460910797,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:16,271 - trainer - INFO -   no_improve_count: 20
2022-10-24 15:19:16,271 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,272 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,273 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,273 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_282
2022-10-24 15:19:16,275 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_288
2022-10-24 15:19:16,279 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_288
2022-10-24 15:19:16,280 - trainer - INFO - 
*****************[epoch: 288, global step: 289] eval training set at end of epoch***************
2022-10-24 15:19:16,281 - trainer - INFO - {
  "train_loss": 0.10689878463745117
}
2022-10-24 15:19:16,282 - trainer - INFO - start training epoch 289
2022-10-24 15:19:16,282 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,285 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,286 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,297 - trainer - INFO - 
*****************[epoch: 289, global step: 290] eval training set at end of epoch***************
2022-10-24 15:19:16,297 - trainer - INFO - {
  "train_loss": 0.10689200460910797
}
2022-10-24 15:19:16,298 - trainer - INFO - start training epoch 290
2022-10-24 15:19:16,299 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,299 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,299 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,308 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval training set based on eval_every=2***************
2022-10-24 15:19:16,308 - trainer - INFO - {
  "train_loss": 0.10688209533691406
}
2022-10-24 15:19:16,316 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval development set based on eval_every=2***************
2022-10-24 15:19:16,316 - trainer - INFO - {
  "dev_loss": 0.10686268657445908,
  "dev_best_score_for_loss": -0.1068611741065979
}
2022-10-24 15:19:16,317 - trainer - INFO -   no_improve_count: 21
2022-10-24 15:19:16,317 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,318 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,318 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,319 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_284
2022-10-24 15:19:16,320 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_290
2022-10-24 15:19:16,325 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_290
2022-10-24 15:19:16,326 - trainer - INFO - 
*****************[epoch: 290, global step: 291] eval training set at end of epoch***************
2022-10-24 15:19:16,328 - trainer - INFO - {
  "train_loss": 0.10687218606472015
}
2022-10-24 15:19:16,329 - trainer - INFO - start training epoch 291
2022-10-24 15:19:16,330 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,331 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,332 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,349 - trainer - INFO - 
*****************[epoch: 291, global step: 292] eval training set at end of epoch***************
2022-10-24 15:19:16,349 - trainer - INFO - {
  "train_loss": 0.10686268657445908
}
2022-10-24 15:19:16,350 - trainer - INFO - start training epoch 292
2022-10-24 15:19:16,350 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,350 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,351 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,359 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval training set based on eval_every=2***************
2022-10-24 15:19:16,360 - trainer - INFO - {
  "train_loss": 0.10686560347676277
}
2022-10-24 15:19:16,370 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval development set based on eval_every=2***************
2022-10-24 15:19:16,370 - trainer - INFO - {
  "dev_loss": 0.10684540122747421,
  "dev_best_score_for_loss": -0.10684540122747421
}
2022-10-24 15:19:16,371 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:16,373 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,373 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,374 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_286
2022-10-24 15:19:16,379 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:16,382 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:16,383 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:16,383 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,384 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:16,384 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_292
2022-10-24 15:19:16,389 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_292
2022-10-24 15:19:16,390 - trainer - INFO - 
*****************[epoch: 292, global step: 293] eval training set at end of epoch***************
2022-10-24 15:19:16,391 - trainer - INFO - {
  "train_loss": 0.10686852037906647
}
2022-10-24 15:19:16,391 - trainer - INFO - start training epoch 293
2022-10-24 15:19:16,391 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,392 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,392 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,401 - trainer - INFO - 
*****************[epoch: 293, global step: 294] eval training set at end of epoch***************
2022-10-24 15:19:16,401 - trainer - INFO - {
  "train_loss": 0.10684540122747421
}
2022-10-24 15:19:16,401 - trainer - INFO - start training epoch 294
2022-10-24 15:19:16,402 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,402 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,402 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,411 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval training set based on eval_every=2***************
2022-10-24 15:19:16,412 - trainer - INFO - {
  "train_loss": 0.10684464499354362
}
2022-10-24 15:19:16,419 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval development set based on eval_every=2***************
2022-10-24 15:19:16,420 - trainer - INFO - {
  "dev_loss": 0.10685242712497711,
  "dev_best_score_for_loss": -0.10684540122747421
}
2022-10-24 15:19:16,421 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:16,426 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,427 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,427 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,428 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_288
2022-10-24 15:19:16,430 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_294
2022-10-24 15:19:16,436 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_294
2022-10-24 15:19:16,437 - trainer - INFO - 
*****************[epoch: 294, global step: 295] eval training set at end of epoch***************
2022-10-24 15:19:16,438 - trainer - INFO - {
  "train_loss": 0.10684388875961304
}
2022-10-24 15:19:16,438 - trainer - INFO - start training epoch 295
2022-10-24 15:19:16,438 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,439 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,439 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,448 - trainer - INFO - 
*****************[epoch: 295, global step: 296] eval training set at end of epoch***************
2022-10-24 15:19:16,448 - trainer - INFO - {
  "train_loss": 0.10685242712497711
}
2022-10-24 15:19:16,449 - trainer - INFO - start training epoch 296
2022-10-24 15:19:16,449 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,450 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,450 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,456 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval training set based on eval_every=2***************
2022-10-24 15:19:16,457 - trainer - INFO - {
  "train_loss": 0.10684499144554138
}
2022-10-24 15:19:16,469 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval development set based on eval_every=2***************
2022-10-24 15:19:16,472 - trainer - INFO - {
  "dev_loss": 0.10684466361999512,
  "dev_best_score_for_loss": -0.10684466361999512
}
2022-10-24 15:19:16,473 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:16,475 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,475 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,475 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_290
2022-10-24 15:19:16,477 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:16,482 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:16,482 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:16,482 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,483 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:16,484 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_296
2022-10-24 15:19:16,488 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_296
2022-10-24 15:19:16,489 - trainer - INFO - 
*****************[epoch: 296, global step: 297] eval training set at end of epoch***************
2022-10-24 15:19:16,489 - trainer - INFO - {
  "train_loss": 0.10683755576610565
}
2022-10-24 15:19:16,489 - trainer - INFO - start training epoch 297
2022-10-24 15:19:16,489 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,490 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,490 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,501 - trainer - INFO - 
*****************[epoch: 297, global step: 298] eval training set at end of epoch***************
2022-10-24 15:19:16,501 - trainer - INFO - {
  "train_loss": 0.10684465616941452
}
2022-10-24 15:19:16,502 - trainer - INFO - start training epoch 298
2022-10-24 15:19:16,502 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,502 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,503 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,509 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval training set based on eval_every=2***************
2022-10-24 15:19:16,509 - trainer - INFO - {
  "train_loss": 0.10683765634894371
}
2022-10-24 15:19:16,519 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval development set based on eval_every=2***************
2022-10-24 15:19:16,520 - trainer - INFO - {
  "dev_loss": 0.10683383047580719,
  "dev_best_score_for_loss": -0.10683383047580719
}
2022-10-24 15:19:16,520 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:16,522 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,522 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,522 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_292
2022-10-24 15:19:16,524 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd
2022-10-24 15:19:16,529 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd
2022-10-24 15:19:16,529 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:16,529 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,530 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:16,531 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_298
2022-10-24 15:19:16,536 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_298
2022-10-24 15:19:16,537 - trainer - INFO - 
*****************[epoch: 298, global step: 299] eval training set at end of epoch***************
2022-10-24 15:19:16,537 - trainer - INFO - {
  "train_loss": 0.1068306565284729
}
2022-10-24 15:19:16,538 - trainer - INFO - start training epoch 299
2022-10-24 15:19:16,538 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,538 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,538 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,550 - trainer - INFO - 
*****************[epoch: 299, global step: 300] eval training set at end of epoch***************
2022-10-24 15:19:16,550 - trainer - INFO - {
  "train_loss": 0.10683383047580719
}
2022-10-24 15:19:16,551 - trainer - INFO - start training epoch 300
2022-10-24 15:19:16,551 - trainer - INFO - training using device=cuda
2022-10-24 15:19:16,551 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:16,552 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_yoke_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:16,559 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval training set based on eval_every=2***************
2022-10-24 15:19:16,560 - trainer - INFO - {
  "train_loss": 0.10683093219995499
}
2022-10-24 15:19:16,572 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval development set based on eval_every=2***************
2022-10-24 15:19:16,573 - trainer - INFO - {
  "dev_loss": 0.10684575140476227,
  "dev_best_score_for_loss": -0.10683383047580719
}
2022-10-24 15:19:16,574 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:16,575 - trainer - INFO -   patience: 200
2022-10-24 15:19:16,576 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:16,577 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:16,577 - trainer - INFO -   Remove checkpoint tmp/mlp_yoke_kdd\ck_294
2022-10-24 15:19:16,580 - trainer - INFO -   Save checkpoint to tmp/mlp_yoke_kdd\ck_300
2022-10-24 15:19:16,584 - trainer - INFO - save model to path: tmp/mlp_yoke_kdd\ck_300
2022-10-24 15:19:16,585 - trainer - INFO - 
*****************[epoch: 300, global step: 301] eval training set at end of epoch***************
2022-10-24 15:19:16,585 - trainer - INFO - {
  "train_loss": 0.10682803392410278
}
