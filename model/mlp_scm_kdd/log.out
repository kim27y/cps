2022-10-24 15:19:30,992 - trainer - INFO - MLP(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=32, bias=True)
  )
  (activation_layers): ModuleList(
    (0): ReLU(inplace=True)
    (1): ReLU(inplace=True)
    (2): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (linear_output): Linear(in_features=32, out_features=1, bias=True)
)
2022-10-24 15:19:30,993 - trainer - INFO -   Total params: 10625
2022-10-24 15:19:30,994 - trainer - INFO -   Trainable params: 10625
2022-10-24 15:19:30,994 - trainer - INFO -   Non-trainable params: 0
2022-10-24 15:19:30,994 - trainer - INFO -   There are 13  training examples
2022-10-24 15:19:30,995 - trainer - INFO -   There are 13 examples for development
2022-10-24 15:19:31,113 - trainer - INFO - start training epoch 1
2022-10-24 15:19:31,114 - trainer - INFO - training using device=cuda
2022-10-24 15:19:31,114 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:31,115 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,389 - trainer - INFO - 
*****************[epoch: 1, global step: 2] eval training set at end of epoch***************
2022-10-24 15:19:32,389 - trainer - INFO - {
  "train_loss": 599754.3125
}
2022-10-24 15:19:32,390 - trainer - INFO - start training epoch 2
2022-10-24 15:19:32,391 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,391 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,392 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,400 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval training set based on eval_every=2***************
2022-10-24 15:19:32,402 - trainer - INFO - {
  "train_loss": 571108.1875
}
2022-10-24 15:19:32,414 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval development set based on eval_every=2***************
2022-10-24 15:19:32,415 - trainer - INFO - {
  "dev_loss": 349279.625,
  "dev_best_score_for_loss": -349279.625
}
2022-10-24 15:19:32,416 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:32,417 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:19:32,418 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:32,423 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:32,423 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:32,424 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,425 - trainer - INFO -    Check 0 checkpoints already saved
2022-10-24 15:19:32,425 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_2
2022-10-24 15:19:32,431 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_2
2022-10-24 15:19:32,433 - trainer - INFO - 
*****************[epoch: 2, global step: 3] eval training set at end of epoch***************
2022-10-24 15:19:32,434 - trainer - INFO - {
  "train_loss": 542462.0625
}
2022-10-24 15:19:32,435 - trainer - INFO - start training epoch 3
2022-10-24 15:19:32,436 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,436 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,437 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,447 - trainer - INFO - 
*****************[epoch: 3, global step: 4] eval training set at end of epoch***************
2022-10-24 15:19:32,448 - trainer - INFO - {
  "train_loss": 349279.625
}
2022-10-24 15:19:32,449 - trainer - INFO - start training epoch 4
2022-10-24 15:19:32,449 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,450 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,450 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,459 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval training set based on eval_every=2***************
2022-10-24 15:19:32,459 - trainer - INFO - {
  "train_loss": 196675.833984375
}
2022-10-24 15:19:32,471 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval development set based on eval_every=2***************
2022-10-24 15:19:32,472 - trainer - INFO - {
  "dev_loss": 508167.3125,
  "dev_best_score_for_loss": -349279.625
}
2022-10-24 15:19:32,473 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:32,473 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,475 - trainer - INFO -    Check 1 checkpoints already saved
2022-10-24 15:19:32,475 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_4
2022-10-24 15:19:32,482 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_4
2022-10-24 15:19:32,486 - trainer - INFO - 
*****************[epoch: 4, global step: 5] eval training set at end of epoch***************
2022-10-24 15:19:32,488 - trainer - INFO - {
  "train_loss": 44072.04296875
}
2022-10-24 15:19:32,489 - trainer - INFO - start training epoch 5
2022-10-24 15:19:32,490 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,491 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,492 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,503 - trainer - INFO - 
*****************[epoch: 5, global step: 6] eval training set at end of epoch***************
2022-10-24 15:19:32,503 - trainer - INFO - {
  "train_loss": 508167.3125
}
2022-10-24 15:19:32,504 - trainer - INFO - start training epoch 6
2022-10-24 15:19:32,504 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,505 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,505 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,520 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval training set based on eval_every=2***************
2022-10-24 15:19:32,521 - trainer - INFO - {
  "train_loss": 293046.33984375
}
2022-10-24 15:19:32,532 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval development set based on eval_every=2***************
2022-10-24 15:19:32,532 - trainer - INFO - {
  "dev_loss": 25109.904296875,
  "dev_best_score_for_loss": -25109.904296875
}
2022-10-24 15:19:32,533 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:32,536 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:32,537 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:32,541 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:32,541 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:32,542 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,543 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:32,544 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_6
2022-10-24 15:19:32,550 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_6
2022-10-24 15:19:32,551 - trainer - INFO - 
*****************[epoch: 6, global step: 7] eval training set at end of epoch***************
2022-10-24 15:19:32,551 - trainer - INFO - {
  "train_loss": 77925.3671875
}
2022-10-24 15:19:32,552 - trainer - INFO - start training epoch 7
2022-10-24 15:19:32,552 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,553 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,554 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,568 - trainer - INFO - 
*****************[epoch: 7, global step: 8] eval training set at end of epoch***************
2022-10-24 15:19:32,568 - trainer - INFO - {
  "train_loss": 25109.904296875
}
2022-10-24 15:19:32,570 - trainer - INFO - start training epoch 8
2022-10-24 15:19:32,571 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,572 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,572 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,582 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval training set based on eval_every=2***************
2022-10-24 15:19:32,582 - trainer - INFO - {
  "train_loss": 76492.4365234375
}
2022-10-24 15:19:32,593 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval development set based on eval_every=2***************
2022-10-24 15:19:32,594 - trainer - INFO - {
  "dev_loss": 206887.53125,
  "dev_best_score_for_loss": -25109.904296875
}
2022-10-24 15:19:32,598 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:32,600 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,602 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:32,602 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:32,602 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_2
2022-10-24 15:19:32,604 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_8
2022-10-24 15:19:32,610 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_8
2022-10-24 15:19:32,611 - trainer - INFO - 
*****************[epoch: 8, global step: 9] eval training set at end of epoch***************
2022-10-24 15:19:32,614 - trainer - INFO - {
  "train_loss": 127874.96875
}
2022-10-24 15:19:32,615 - trainer - INFO - start training epoch 9
2022-10-24 15:19:32,615 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,616 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,616 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,631 - trainer - INFO - 
*****************[epoch: 9, global step: 10] eval training set at end of epoch***************
2022-10-24 15:19:32,632 - trainer - INFO - {
  "train_loss": 206887.53125
}
2022-10-24 15:19:32,633 - trainer - INFO - start training epoch 10
2022-10-24 15:19:32,634 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,634 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,635 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,644 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval training set based on eval_every=2***************
2022-10-24 15:19:32,645 - trainer - INFO - {
  "train_loss": 223518.609375
}
2022-10-24 15:19:32,654 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval development set based on eval_every=2***************
2022-10-24 15:19:32,654 - trainer - INFO - {
  "dev_loss": 234559.203125,
  "dev_best_score_for_loss": -25109.904296875
}
2022-10-24 15:19:32,655 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:32,656 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,657 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:32,657 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:32,658 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_4
2022-10-24 15:19:32,660 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_10
2022-10-24 15:19:32,666 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_10
2022-10-24 15:19:32,668 - trainer - INFO - 
*****************[epoch: 10, global step: 11] eval training set at end of epoch***************
2022-10-24 15:19:32,668 - trainer - INFO - {
  "train_loss": 240149.6875
}
2022-10-24 15:19:32,669 - trainer - INFO - start training epoch 11
2022-10-24 15:19:32,669 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,670 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,671 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,681 - trainer - INFO - 
*****************[epoch: 11, global step: 12] eval training set at end of epoch***************
2022-10-24 15:19:32,681 - trainer - INFO - {
  "train_loss": 234559.21875
}
2022-10-24 15:19:32,682 - trainer - INFO - start training epoch 12
2022-10-24 15:19:32,683 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,683 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,685 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,698 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval training set based on eval_every=2***************
2022-10-24 15:19:32,698 - trainer - INFO - {
  "train_loss": 215246.984375
}
2022-10-24 15:19:32,706 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval development set based on eval_every=2***************
2022-10-24 15:19:32,706 - trainer - INFO - {
  "dev_loss": 130011.078125,
  "dev_best_score_for_loss": -25109.904296875
}
2022-10-24 15:19:32,707 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:32,708 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,709 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:32,709 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:32,710 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_6
2022-10-24 15:19:32,714 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_12
2022-10-24 15:19:32,718 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_12
2022-10-24 15:19:32,719 - trainer - INFO - 
*****************[epoch: 12, global step: 13] eval training set at end of epoch***************
2022-10-24 15:19:32,720 - trainer - INFO - {
  "train_loss": 195934.75
}
2022-10-24 15:19:32,720 - trainer - INFO - start training epoch 13
2022-10-24 15:19:32,721 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,721 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,722 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,732 - trainer - INFO - 
*****************[epoch: 13, global step: 14] eval training set at end of epoch***************
2022-10-24 15:19:32,732 - trainer - INFO - {
  "train_loss": 130011.078125
}
2022-10-24 15:19:32,733 - trainer - INFO - start training epoch 14
2022-10-24 15:19:32,733 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,734 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,735 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,744 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval training set based on eval_every=2***************
2022-10-24 15:19:32,745 - trainer - INFO - {
  "train_loss": 91603.345703125
}
2022-10-24 15:19:32,755 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval development set based on eval_every=2***************
2022-10-24 15:19:32,755 - trainer - INFO - {
  "dev_loss": 8081.10009765625,
  "dev_best_score_for_loss": -8081.10009765625
}
2022-10-24 15:19:32,757 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:32,760 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:32,760 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:32,762 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_8
2022-10-24 15:19:32,766 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:32,771 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:32,772 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:32,773 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,774 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:32,774 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_14
2022-10-24 15:19:32,779 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_14
2022-10-24 15:19:32,780 - trainer - INFO - 
*****************[epoch: 14, global step: 15] eval training set at end of epoch***************
2022-10-24 15:19:32,781 - trainer - INFO - {
  "train_loss": 53195.61328125
}
2022-10-24 15:19:32,781 - trainer - INFO - start training epoch 15
2022-10-24 15:19:32,781 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,782 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,782 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,792 - trainer - INFO - 
*****************[epoch: 15, global step: 16] eval training set at end of epoch***************
2022-10-24 15:19:32,792 - trainer - INFO - {
  "train_loss": 8081.10009765625
}
2022-10-24 15:19:32,793 - trainer - INFO - start training epoch 16
2022-10-24 15:19:32,794 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,794 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,795 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,805 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval training set based on eval_every=2***************
2022-10-24 15:19:32,807 - trainer - INFO - {
  "train_loss": 28324.151611328125
}
2022-10-24 15:19:32,817 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval development set based on eval_every=2***************
2022-10-24 15:19:32,818 - trainer - INFO - {
  "dev_loss": 124804.8515625,
  "dev_best_score_for_loss": -8081.10009765625
}
2022-10-24 15:19:32,821 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:32,822 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,823 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:32,823 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:32,824 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_10
2022-10-24 15:19:32,826 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_16
2022-10-24 15:19:32,830 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_16
2022-10-24 15:19:32,831 - trainer - INFO - 
*****************[epoch: 16, global step: 17] eval training set at end of epoch***************
2022-10-24 15:19:32,832 - trainer - INFO - {
  "train_loss": 48567.203125
}
2022-10-24 15:19:32,833 - trainer - INFO - start training epoch 17
2022-10-24 15:19:32,833 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,836 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,837 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,848 - trainer - INFO - 
*****************[epoch: 17, global step: 18] eval training set at end of epoch***************
2022-10-24 15:19:32,848 - trainer - INFO - {
  "train_loss": 124804.8515625
}
2022-10-24 15:19:32,849 - trainer - INFO - start training epoch 18
2022-10-24 15:19:32,850 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,851 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,851 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,859 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval training set based on eval_every=2***************
2022-10-24 15:19:32,859 - trainer - INFO - {
  "train_loss": 112841.02734375
}
2022-10-24 15:19:32,868 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval development set based on eval_every=2***************
2022-10-24 15:19:32,869 - trainer - INFO - {
  "dev_loss": 29459.203125,
  "dev_best_score_for_loss": -8081.10009765625
}
2022-10-24 15:19:32,870 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:32,870 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,871 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:32,871 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:32,872 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_12
2022-10-24 15:19:32,873 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_18
2022-10-24 15:19:32,878 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_18
2022-10-24 15:19:32,882 - trainer - INFO - 
*****************[epoch: 18, global step: 19] eval training set at end of epoch***************
2022-10-24 15:19:32,884 - trainer - INFO - {
  "train_loss": 100877.203125
}
2022-10-24 15:19:32,886 - trainer - INFO - start training epoch 19
2022-10-24 15:19:32,887 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,887 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,888 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,899 - trainer - INFO - 
*****************[epoch: 19, global step: 20] eval training set at end of epoch***************
2022-10-24 15:19:32,900 - trainer - INFO - {
  "train_loss": 29459.203125
}
2022-10-24 15:19:32,900 - trainer - INFO - start training epoch 20
2022-10-24 15:19:32,901 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,901 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,902 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,910 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval training set based on eval_every=2***************
2022-10-24 15:19:32,910 - trainer - INFO - {
  "train_loss": 18811.276611328125
}
2022-10-24 15:19:32,921 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval development set based on eval_every=2***************
2022-10-24 15:19:32,921 - trainer - INFO - {
  "dev_loss": 30746.232421875,
  "dev_best_score_for_loss": -8081.10009765625
}
2022-10-24 15:19:32,922 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:32,923 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,924 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:32,924 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:32,924 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_14
2022-10-24 15:19:32,926 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_20
2022-10-24 15:19:32,932 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_20
2022-10-24 15:19:32,935 - trainer - INFO - 
*****************[epoch: 20, global step: 21] eval training set at end of epoch***************
2022-10-24 15:19:32,937 - trainer - INFO - {
  "train_loss": 8163.35009765625
}
2022-10-24 15:19:32,939 - trainer - INFO - start training epoch 21
2022-10-24 15:19:32,940 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,940 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,941 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,955 - trainer - INFO - 
*****************[epoch: 21, global step: 22] eval training set at end of epoch***************
2022-10-24 15:19:32,956 - trainer - INFO - {
  "train_loss": 30746.23046875
}
2022-10-24 15:19:32,957 - trainer - INFO - start training epoch 22
2022-10-24 15:19:32,957 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,958 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,959 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:32,969 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval training set based on eval_every=2***************
2022-10-24 15:19:32,969 - trainer - INFO - {
  "train_loss": 44866.759765625
}
2022-10-24 15:19:32,978 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval development set based on eval_every=2***************
2022-10-24 15:19:32,981 - trainer - INFO - {
  "dev_loss": 71885.7890625,
  "dev_best_score_for_loss": -8081.10009765625
}
2022-10-24 15:19:32,982 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:19:32,983 - trainer - INFO -   patience: 200
2022-10-24 15:19:32,984 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:32,984 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:32,985 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_16
2022-10-24 15:19:32,987 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_22
2022-10-24 15:19:32,993 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_22
2022-10-24 15:19:32,994 - trainer - INFO - 
*****************[epoch: 22, global step: 23] eval training set at end of epoch***************
2022-10-24 15:19:32,995 - trainer - INFO - {
  "train_loss": 58987.2890625
}
2022-10-24 15:19:32,996 - trainer - INFO - start training epoch 23
2022-10-24 15:19:32,997 - trainer - INFO - training using device=cuda
2022-10-24 15:19:32,998 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:32,998 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,008 - trainer - INFO - 
*****************[epoch: 23, global step: 24] eval training set at end of epoch***************
2022-10-24 15:19:33,009 - trainer - INFO - {
  "train_loss": 71885.78125
}
2022-10-24 15:19:33,010 - trainer - INFO - start training epoch 24
2022-10-24 15:19:33,010 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,012 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,013 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,025 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval training set based on eval_every=2***************
2022-10-24 15:19:33,026 - trainer - INFO - {
  "train_loss": 68327.732421875
}
2022-10-24 15:19:33,037 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval development set based on eval_every=2***************
2022-10-24 15:19:33,038 - trainer - INFO - {
  "dev_loss": 42401.68359375,
  "dev_best_score_for_loss": -8081.10009765625
}
2022-10-24 15:19:33,039 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:19:33,040 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,041 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,042 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,042 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_18
2022-10-24 15:19:33,045 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_24
2022-10-24 15:19:33,050 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_24
2022-10-24 15:19:33,051 - trainer - INFO - 
*****************[epoch: 24, global step: 25] eval training set at end of epoch***************
2022-10-24 15:19:33,052 - trainer - INFO - {
  "train_loss": 64769.68359375
}
2022-10-24 15:19:33,055 - trainer - INFO - start training epoch 25
2022-10-24 15:19:33,056 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,056 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,057 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,068 - trainer - INFO - 
*****************[epoch: 25, global step: 26] eval training set at end of epoch***************
2022-10-24 15:19:33,069 - trainer - INFO - {
  "train_loss": 42401.68359375
}
2022-10-24 15:19:33,069 - trainer - INFO - start training epoch 26
2022-10-24 15:19:33,070 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,071 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,071 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,081 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval training set based on eval_every=2***************
2022-10-24 15:19:33,081 - trainer - INFO - {
  "train_loss": 29850.107421875
}
2022-10-24 15:19:33,091 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval development set based on eval_every=2***************
2022-10-24 15:19:33,092 - trainer - INFO - {
  "dev_loss": 7505.50146484375,
  "dev_best_score_for_loss": -7505.50146484375
}
2022-10-24 15:19:33,093 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:33,095 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,098 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,100 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_20
2022-10-24 15:19:33,101 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:33,104 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:33,105 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:33,105 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,106 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:33,106 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_26
2022-10-24 15:19:33,111 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_26
2022-10-24 15:19:33,115 - trainer - INFO - 
*****************[epoch: 26, global step: 27] eval training set at end of epoch***************
2022-10-24 15:19:33,116 - trainer - INFO - {
  "train_loss": 17298.53125
}
2022-10-24 15:19:33,117 - trainer - INFO - start training epoch 27
2022-10-24 15:19:33,117 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,119 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,120 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,132 - trainer - INFO - 
*****************[epoch: 27, global step: 28] eval training set at end of epoch***************
2022-10-24 15:19:33,132 - trainer - INFO - {
  "train_loss": 7505.5009765625
}
2022-10-24 15:19:33,133 - trainer - INFO - start training epoch 28
2022-10-24 15:19:33,133 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,134 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,135 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,146 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval training set based on eval_every=2***************
2022-10-24 15:19:33,146 - trainer - INFO - {
  "train_loss": 14958.95068359375
}
2022-10-24 15:19:33,154 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval development set based on eval_every=2***************
2022-10-24 15:19:33,154 - trainer - INFO - {
  "dev_loss": 41826.6171875,
  "dev_best_score_for_loss": -7505.50146484375
}
2022-10-24 15:19:33,155 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:33,156 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,157 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,157 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,158 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_22
2022-10-24 15:19:33,160 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_28
2022-10-24 15:19:33,168 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_28
2022-10-24 15:19:33,170 - trainer - INFO - 
*****************[epoch: 28, global step: 29] eval training set at end of epoch***************
2022-10-24 15:19:33,170 - trainer - INFO - {
  "train_loss": 22412.400390625
}
2022-10-24 15:19:33,171 - trainer - INFO - start training epoch 29
2022-10-24 15:19:33,173 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,174 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,175 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,186 - trainer - INFO - 
*****************[epoch: 29, global step: 30] eval training set at end of epoch***************
2022-10-24 15:19:33,187 - trainer - INFO - {
  "train_loss": 41826.6171875
}
2022-10-24 15:19:33,187 - trainer - INFO - start training epoch 30
2022-10-24 15:19:33,188 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,188 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,189 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,201 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval training set based on eval_every=2***************
2022-10-24 15:19:33,202 - trainer - INFO - {
  "train_loss": 39232.2734375
}
2022-10-24 15:19:33,208 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval development set based on eval_every=2***************
2022-10-24 15:19:33,209 - trainer - INFO - {
  "dev_loss": 16249.173828125,
  "dev_best_score_for_loss": -7505.50146484375
}
2022-10-24 15:19:33,211 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:33,212 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,214 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,214 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,214 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_24
2022-10-24 15:19:33,216 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_30
2022-10-24 15:19:33,220 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_30
2022-10-24 15:19:33,220 - trainer - INFO - 
*****************[epoch: 30, global step: 31] eval training set at end of epoch***************
2022-10-24 15:19:33,221 - trainer - INFO - {
  "train_loss": 36637.9296875
}
2022-10-24 15:19:33,222 - trainer - INFO - start training epoch 31
2022-10-24 15:19:33,223 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,224 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,224 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,235 - trainer - INFO - 
*****************[epoch: 31, global step: 32] eval training set at end of epoch***************
2022-10-24 15:19:33,236 - trainer - INFO - {
  "train_loss": 16249.1708984375
}
2022-10-24 15:19:33,237 - trainer - INFO - start training epoch 32
2022-10-24 15:19:33,239 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,239 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,242 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,252 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval training set based on eval_every=2***************
2022-10-24 15:19:33,253 - trainer - INFO - {
  "train_loss": 11684.25830078125
}
2022-10-24 15:19:33,262 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval development set based on eval_every=2***************
2022-10-24 15:19:33,264 - trainer - INFO - {
  "dev_loss": 13407.9814453125,
  "dev_best_score_for_loss": -7505.50146484375
}
2022-10-24 15:19:33,264 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:33,265 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,266 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,267 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,267 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_26
2022-10-24 15:19:33,271 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_32
2022-10-24 15:19:33,276 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_32
2022-10-24 15:19:33,277 - trainer - INFO - 
*****************[epoch: 32, global step: 33] eval training set at end of epoch***************
2022-10-24 15:19:33,278 - trainer - INFO - {
  "train_loss": 7119.345703125
}
2022-10-24 15:19:33,278 - trainer - INFO - start training epoch 33
2022-10-24 15:19:33,279 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,280 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,280 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,294 - trainer - INFO - 
*****************[epoch: 33, global step: 34] eval training set at end of epoch***************
2022-10-24 15:19:33,294 - trainer - INFO - {
  "train_loss": 13407.9814453125
}
2022-10-24 15:19:33,299 - trainer - INFO - start training epoch 34
2022-10-24 15:19:33,299 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,300 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,301 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,312 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval training set based on eval_every=2***************
2022-10-24 15:19:33,313 - trainer - INFO - {
  "train_loss": 18405.69091796875
}
2022-10-24 15:19:33,323 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval development set based on eval_every=2***************
2022-10-24 15:19:33,323 - trainer - INFO - {
  "dev_loss": 27117.416015625,
  "dev_best_score_for_loss": -7505.50146484375
}
2022-10-24 15:19:33,324 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:19:33,324 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,325 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,326 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,326 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_28
2022-10-24 15:19:33,328 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_34
2022-10-24 15:19:33,333 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_34
2022-10-24 15:19:33,334 - trainer - INFO - 
*****************[epoch: 34, global step: 35] eval training set at end of epoch***************
2022-10-24 15:19:33,334 - trainer - INFO - {
  "train_loss": 23403.400390625
}
2022-10-24 15:19:33,335 - trainer - INFO - start training epoch 35
2022-10-24 15:19:33,335 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,336 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,336 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,346 - trainer - INFO - 
*****************[epoch: 35, global step: 36] eval training set at end of epoch***************
2022-10-24 15:19:33,347 - trainer - INFO - {
  "train_loss": 27117.416015625
}
2022-10-24 15:19:33,349 - trainer - INFO - start training epoch 36
2022-10-24 15:19:33,349 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,352 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,352 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,365 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval training set based on eval_every=2***************
2022-10-24 15:19:33,366 - trainer - INFO - {
  "train_loss": 24617.357421875
}
2022-10-24 15:19:33,374 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval development set based on eval_every=2***************
2022-10-24 15:19:33,375 - trainer - INFO - {
  "dev_loss": 12726.0517578125,
  "dev_best_score_for_loss": -7505.50146484375
}
2022-10-24 15:19:33,376 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:19:33,380 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,381 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,381 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,382 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_30
2022-10-24 15:19:33,383 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_36
2022-10-24 15:19:33,388 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_36
2022-10-24 15:19:33,388 - trainer - INFO - 
*****************[epoch: 36, global step: 37] eval training set at end of epoch***************
2022-10-24 15:19:33,389 - trainer - INFO - {
  "train_loss": 22117.298828125
}
2022-10-24 15:19:33,390 - trainer - INFO - start training epoch 37
2022-10-24 15:19:33,390 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,391 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,391 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,409 - trainer - INFO - 
*****************[epoch: 37, global step: 38] eval training set at end of epoch***************
2022-10-24 15:19:33,411 - trainer - INFO - {
  "train_loss": 12726.0517578125
}
2022-10-24 15:19:33,412 - trainer - INFO - start training epoch 38
2022-10-24 15:19:33,413 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,413 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,417 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,428 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval training set based on eval_every=2***************
2022-10-24 15:19:33,429 - trainer - INFO - {
  "train_loss": 9828.100830078125
}
2022-10-24 15:19:33,435 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval development set based on eval_every=2***************
2022-10-24 15:19:33,435 - trainer - INFO - {
  "dev_loss": 9907.0693359375,
  "dev_best_score_for_loss": -7505.50146484375
}
2022-10-24 15:19:33,436 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:19:33,436 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,438 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,439 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,439 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_32
2022-10-24 15:19:33,442 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_38
2022-10-24 15:19:33,448 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_38
2022-10-24 15:19:33,449 - trainer - INFO - 
*****************[epoch: 38, global step: 39] eval training set at end of epoch***************
2022-10-24 15:19:33,450 - trainer - INFO - {
  "train_loss": 6930.14990234375
}
2022-10-24 15:19:33,451 - trainer - INFO - start training epoch 39
2022-10-24 15:19:33,451 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,452 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,452 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,464 - trainer - INFO - 
*****************[epoch: 39, global step: 40] eval training set at end of epoch***************
2022-10-24 15:19:33,465 - trainer - INFO - {
  "train_loss": 9907.0693359375
}
2022-10-24 15:19:33,465 - trainer - INFO - start training epoch 40
2022-10-24 15:19:33,466 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,466 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,467 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,475 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval training set based on eval_every=2***************
2022-10-24 15:19:33,476 - trainer - INFO - {
  "train_loss": 13378.67919921875
}
2022-10-24 15:19:33,483 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval development set based on eval_every=2***************
2022-10-24 15:19:33,483 - trainer - INFO - {
  "dev_loss": 17519.59765625,
  "dev_best_score_for_loss": -7505.50146484375
}
2022-10-24 15:19:33,488 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:19:33,489 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,491 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,492 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,492 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_34
2022-10-24 15:19:33,493 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_40
2022-10-24 15:19:33,500 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_40
2022-10-24 15:19:33,502 - trainer - INFO - 
*****************[epoch: 40, global step: 41] eval training set at end of epoch***************
2022-10-24 15:19:33,503 - trainer - INFO - {
  "train_loss": 16850.2890625
}
2022-10-24 15:19:33,504 - trainer - INFO - start training epoch 41
2022-10-24 15:19:33,504 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,505 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,505 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,514 - trainer - INFO - 
*****************[epoch: 41, global step: 42] eval training set at end of epoch***************
2022-10-24 15:19:33,515 - trainer - INFO - {
  "train_loss": 17519.59765625
}
2022-10-24 15:19:33,515 - trainer - INFO - start training epoch 42
2022-10-24 15:19:33,516 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,518 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,518 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,525 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval training set based on eval_every=2***************
2022-10-24 15:19:33,525 - trainer - INFO - {
  "train_loss": 14393.04541015625
}
2022-10-24 15:19:33,535 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval development set based on eval_every=2***************
2022-10-24 15:19:33,536 - trainer - INFO - {
  "dev_loss": 6693.763671875,
  "dev_best_score_for_loss": -6693.763671875
}
2022-10-24 15:19:33,537 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:33,538 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,539 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,539 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_36
2022-10-24 15:19:33,541 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:33,545 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:33,547 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:33,548 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,550 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:33,550 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_42
2022-10-24 15:19:33,555 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_42
2022-10-24 15:19:33,556 - trainer - INFO - 
*****************[epoch: 42, global step: 43] eval training set at end of epoch***************
2022-10-24 15:19:33,556 - trainer - INFO - {
  "train_loss": 11266.4931640625
}
2022-10-24 15:19:33,557 - trainer - INFO - start training epoch 43
2022-10-24 15:19:33,557 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,558 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,558 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,577 - trainer - INFO - 
*****************[epoch: 43, global step: 44] eval training set at end of epoch***************
2022-10-24 15:19:33,580 - trainer - INFO - {
  "train_loss": 6693.763671875
}
2022-10-24 15:19:33,581 - trainer - INFO - start training epoch 44
2022-10-24 15:19:33,582 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,583 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,584 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,598 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval training set based on eval_every=2***************
2022-10-24 15:19:33,599 - trainer - INFO - {
  "train_loss": 7330.79052734375
}
2022-10-24 15:19:33,607 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval development set based on eval_every=2***************
2022-10-24 15:19:33,608 - trainer - INFO - {
  "dev_loss": 11676.2314453125,
  "dev_best_score_for_loss": -6693.763671875
}
2022-10-24 15:19:33,610 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:33,612 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,614 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,615 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,618 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_38
2022-10-24 15:19:33,619 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_44
2022-10-24 15:19:33,627 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_44
2022-10-24 15:19:33,630 - trainer - INFO - 
*****************[epoch: 44, global step: 45] eval training set at end of epoch***************
2022-10-24 15:19:33,630 - trainer - INFO - {
  "train_loss": 7967.8173828125
}
2022-10-24 15:19:33,632 - trainer - INFO - start training epoch 45
2022-10-24 15:19:33,632 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,633 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,633 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,642 - trainer - INFO - 
*****************[epoch: 45, global step: 46] eval training set at end of epoch***************
2022-10-24 15:19:33,643 - trainer - INFO - {
  "train_loss": 11676.2314453125
}
2022-10-24 15:19:33,643 - trainer - INFO - start training epoch 46
2022-10-24 15:19:33,645 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,645 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,646 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,654 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval training set based on eval_every=2***************
2022-10-24 15:19:33,654 - trainer - INFO - {
  "train_loss": 12395.330078125
}
2022-10-24 15:19:33,666 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval development set based on eval_every=2***************
2022-10-24 15:19:33,667 - trainer - INFO - {
  "dev_loss": 10906.1494140625,
  "dev_best_score_for_loss": -6693.763671875
}
2022-10-24 15:19:33,667 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:33,668 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,669 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,669 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,670 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_40
2022-10-24 15:19:33,675 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_46
2022-10-24 15:19:33,679 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_46
2022-10-24 15:19:33,680 - trainer - INFO - 
*****************[epoch: 46, global step: 47] eval training set at end of epoch***************
2022-10-24 15:19:33,680 - trainer - INFO - {
  "train_loss": 13114.4287109375
}
2022-10-24 15:19:33,681 - trainer - INFO - start training epoch 47
2022-10-24 15:19:33,681 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,682 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,682 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,691 - trainer - INFO - 
*****************[epoch: 47, global step: 48] eval training set at end of epoch***************
2022-10-24 15:19:33,691 - trainer - INFO - {
  "train_loss": 10906.1494140625
}
2022-10-24 15:19:33,692 - trainer - INFO - start training epoch 48
2022-10-24 15:19:33,693 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,693 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,695 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,705 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval training set based on eval_every=2***************
2022-10-24 15:19:33,705 - trainer - INFO - {
  "train_loss": 9160.052001953125
}
2022-10-24 15:19:33,712 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval development set based on eval_every=2***************
2022-10-24 15:19:33,713 - trainer - INFO - {
  "dev_loss": 6205.42431640625,
  "dev_best_score_for_loss": -6205.42431640625
}
2022-10-24 15:19:33,714 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:33,715 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,715 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,715 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_42
2022-10-24 15:19:33,717 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:33,720 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:33,721 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:33,721 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,722 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:33,723 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_48
2022-10-24 15:19:33,728 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_48
2022-10-24 15:19:33,730 - trainer - INFO - 
*****************[epoch: 48, global step: 49] eval training set at end of epoch***************
2022-10-24 15:19:33,730 - trainer - INFO - {
  "train_loss": 7413.95458984375
}
2022-10-24 15:19:33,731 - trainer - INFO - start training epoch 49
2022-10-24 15:19:33,731 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,732 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,736 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,744 - trainer - INFO - 
*****************[epoch: 49, global step: 50] eval training set at end of epoch***************
2022-10-24 15:19:33,745 - trainer - INFO - {
  "train_loss": 6205.42431640625
}
2022-10-24 15:19:33,746 - trainer - INFO - start training epoch 50
2022-10-24 15:19:33,746 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,747 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,748 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,756 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval training set based on eval_every=2***************
2022-10-24 15:19:33,757 - trainer - INFO - {
  "train_loss": 7172.84716796875
}
2022-10-24 15:19:33,767 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval development set based on eval_every=2***************
2022-10-24 15:19:33,770 - trainer - INFO - {
  "dev_loss": 10016.091796875,
  "dev_best_score_for_loss": -6205.42431640625
}
2022-10-24 15:19:33,772 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:33,772 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,774 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,774 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,775 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_44
2022-10-24 15:19:33,777 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_50
2022-10-24 15:19:33,783 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_50
2022-10-24 15:19:33,786 - trainer - INFO - 
*****************[epoch: 50, global step: 51] eval training set at end of epoch***************
2022-10-24 15:19:33,787 - trainer - INFO - {
  "train_loss": 8140.27001953125
}
2022-10-24 15:19:33,788 - trainer - INFO - start training epoch 51
2022-10-24 15:19:33,789 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,789 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,790 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,803 - trainer - INFO - 
*****************[epoch: 51, global step: 52] eval training set at end of epoch***************
2022-10-24 15:19:33,804 - trainer - INFO - {
  "train_loss": 10016.0908203125
}
2022-10-24 15:19:33,805 - trainer - INFO - start training epoch 52
2022-10-24 15:19:33,806 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,806 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,807 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,824 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval training set based on eval_every=2***************
2022-10-24 15:19:33,825 - trainer - INFO - {
  "train_loss": 9493.59521484375
}
2022-10-24 15:19:33,838 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval development set based on eval_every=2***************
2022-10-24 15:19:33,839 - trainer - INFO - {
  "dev_loss": 6583.93896484375,
  "dev_best_score_for_loss": -6205.42431640625
}
2022-10-24 15:19:33,840 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:33,840 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,842 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,843 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,843 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_46
2022-10-24 15:19:33,846 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_52
2022-10-24 15:19:33,850 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_52
2022-10-24 15:19:33,852 - trainer - INFO - 
*****************[epoch: 52, global step: 53] eval training set at end of epoch***************
2022-10-24 15:19:33,853 - trainer - INFO - {
  "train_loss": 8971.099609375
}
2022-10-24 15:19:33,854 - trainer - INFO - start training epoch 53
2022-10-24 15:19:33,855 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,856 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,859 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,876 - trainer - INFO - 
*****************[epoch: 53, global step: 54] eval training set at end of epoch***************
2022-10-24 15:19:33,876 - trainer - INFO - {
  "train_loss": 6583.93896484375
}
2022-10-24 15:19:33,880 - trainer - INFO - start training epoch 54
2022-10-24 15:19:33,881 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,883 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,884 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,895 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval training set based on eval_every=2***************
2022-10-24 15:19:33,896 - trainer - INFO - {
  "train_loss": 6229.4130859375
}
2022-10-24 15:19:33,906 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval development set based on eval_every=2***************
2022-10-24 15:19:33,907 - trainer - INFO - {
  "dev_loss": 7035.931640625,
  "dev_best_score_for_loss": -6205.42431640625
}
2022-10-24 15:19:33,907 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:33,909 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,911 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,911 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,911 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_48
2022-10-24 15:19:33,913 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_54
2022-10-24 15:19:33,917 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_54
2022-10-24 15:19:33,918 - trainer - INFO - 
*****************[epoch: 54, global step: 55] eval training set at end of epoch***************
2022-10-24 15:19:33,920 - trainer - INFO - {
  "train_loss": 5874.88720703125
}
2022-10-24 15:19:33,922 - trainer - INFO - start training epoch 55
2022-10-24 15:19:33,923 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,923 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,923 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,933 - trainer - INFO - 
*****************[epoch: 55, global step: 56] eval training set at end of epoch***************
2022-10-24 15:19:33,933 - trainer - INFO - {
  "train_loss": 7035.931640625
}
2022-10-24 15:19:33,934 - trainer - INFO - start training epoch 56
2022-10-24 15:19:33,935 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,935 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,936 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,943 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval training set based on eval_every=2***************
2022-10-24 15:19:33,943 - trainer - INFO - {
  "train_loss": 7545.625244140625
}
2022-10-24 15:19:33,951 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval development set based on eval_every=2***************
2022-10-24 15:19:33,954 - trainer - INFO - {
  "dev_loss": 7570.509765625,
  "dev_best_score_for_loss": -6205.42431640625
}
2022-10-24 15:19:33,955 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:19:33,955 - trainer - INFO -   patience: 200
2022-10-24 15:19:33,957 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:33,958 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:33,958 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_50
2022-10-24 15:19:33,960 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_56
2022-10-24 15:19:33,966 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_56
2022-10-24 15:19:33,967 - trainer - INFO - 
*****************[epoch: 56, global step: 57] eval training set at end of epoch***************
2022-10-24 15:19:33,967 - trainer - INFO - {
  "train_loss": 8055.31884765625
}
2022-10-24 15:19:33,968 - trainer - INFO - start training epoch 57
2022-10-24 15:19:33,969 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,969 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,970 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,978 - trainer - INFO - 
*****************[epoch: 57, global step: 58] eval training set at end of epoch***************
2022-10-24 15:19:33,979 - trainer - INFO - {
  "train_loss": 7570.509765625
}
2022-10-24 15:19:33,979 - trainer - INFO - start training epoch 58
2022-10-24 15:19:33,980 - trainer - INFO - training using device=cuda
2022-10-24 15:19:33,980 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:33,980 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:33,991 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval training set based on eval_every=2***************
2022-10-24 15:19:33,991 - trainer - INFO - {
  "train_loss": 6872.796142578125
}
2022-10-24 15:19:33,999 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval development set based on eval_every=2***************
2022-10-24 15:19:34,002 - trainer - INFO - {
  "dev_loss": 5474.275390625,
  "dev_best_score_for_loss": -5474.275390625
}
2022-10-24 15:19:34,003 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,004 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,005 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,005 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_52
2022-10-24 15:19:34,007 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,015 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,016 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,017 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,018 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,018 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_58
2022-10-24 15:19:34,022 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_58
2022-10-24 15:19:34,023 - trainer - INFO - 
*****************[epoch: 58, global step: 59] eval training set at end of epoch***************
2022-10-24 15:19:34,024 - trainer - INFO - {
  "train_loss": 6175.08251953125
}
2022-10-24 15:19:34,026 - trainer - INFO - start training epoch 59
2022-10-24 15:19:34,026 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,027 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,028 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,038 - trainer - INFO - 
*****************[epoch: 59, global step: 60] eval training set at end of epoch***************
2022-10-24 15:19:34,038 - trainer - INFO - {
  "train_loss": 5474.275390625
}
2022-10-24 15:19:34,039 - trainer - INFO - start training epoch 60
2022-10-24 15:19:34,039 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,040 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,040 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,056 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval training set based on eval_every=2***************
2022-10-24 15:19:34,056 - trainer - INFO - {
  "train_loss": 5773.034423828125
}
2022-10-24 15:19:34,067 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval development set based on eval_every=2***************
2022-10-24 15:19:34,067 - trainer - INFO - {
  "dev_loss": 6783.244140625,
  "dev_best_score_for_loss": -5474.275390625
}
2022-10-24 15:19:34,068 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:34,068 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,069 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,070 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,070 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_54
2022-10-24 15:19:34,072 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_60
2022-10-24 15:19:34,076 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_60
2022-10-24 15:19:34,077 - trainer - INFO - 
*****************[epoch: 60, global step: 61] eval training set at end of epoch***************
2022-10-24 15:19:34,077 - trainer - INFO - {
  "train_loss": 6071.79345703125
}
2022-10-24 15:19:34,078 - trainer - INFO - start training epoch 61
2022-10-24 15:19:34,078 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,079 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,080 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,088 - trainer - INFO - 
*****************[epoch: 61, global step: 62] eval training set at end of epoch***************
2022-10-24 15:19:34,088 - trainer - INFO - {
  "train_loss": 6783.244140625
}
2022-10-24 15:19:34,089 - trainer - INFO - start training epoch 62
2022-10-24 15:19:34,090 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,091 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,092 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,107 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval training set based on eval_every=2***************
2022-10-24 15:19:34,108 - trainer - INFO - {
  "train_loss": 6593.190185546875
}
2022-10-24 15:19:34,115 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval development set based on eval_every=2***************
2022-10-24 15:19:34,116 - trainer - INFO - {
  "dev_loss": 5461.89990234375,
  "dev_best_score_for_loss": -5461.89990234375
}
2022-10-24 15:19:34,117 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,118 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,119 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,119 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_56
2022-10-24 15:19:34,122 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,125 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,125 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,126 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,128 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,129 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_62
2022-10-24 15:19:34,134 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_62
2022-10-24 15:19:34,135 - trainer - INFO - 
*****************[epoch: 62, global step: 63] eval training set at end of epoch***************
2022-10-24 15:19:34,135 - trainer - INFO - {
  "train_loss": 6403.13623046875
}
2022-10-24 15:19:34,138 - trainer - INFO - start training epoch 63
2022-10-24 15:19:34,140 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,141 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,142 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,152 - trainer - INFO - 
*****************[epoch: 63, global step: 64] eval training set at end of epoch***************
2022-10-24 15:19:34,152 - trainer - INFO - {
  "train_loss": 5461.8994140625
}
2022-10-24 15:19:34,153 - trainer - INFO - start training epoch 64
2022-10-24 15:19:34,153 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,154 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,154 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,163 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval training set based on eval_every=2***************
2022-10-24 15:19:34,163 - trainer - INFO - {
  "train_loss": 5315.345458984375
}
2022-10-24 15:19:34,170 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval development set based on eval_every=2***************
2022-10-24 15:19:34,170 - trainer - INFO - {
  "dev_loss": 5605.7861328125,
  "dev_best_score_for_loss": -5461.89990234375
}
2022-10-24 15:19:34,171 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:34,171 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,172 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,173 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,173 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_58
2022-10-24 15:19:34,175 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_64
2022-10-24 15:19:34,179 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_64
2022-10-24 15:19:34,180 - trainer - INFO - 
*****************[epoch: 64, global step: 65] eval training set at end of epoch***************
2022-10-24 15:19:34,180 - trainer - INFO - {
  "train_loss": 5168.79150390625
}
2022-10-24 15:19:34,181 - trainer - INFO - start training epoch 65
2022-10-24 15:19:34,181 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,182 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,185 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,196 - trainer - INFO - 
*****************[epoch: 65, global step: 66] eval training set at end of epoch***************
2022-10-24 15:19:34,197 - trainer - INFO - {
  "train_loss": 5605.7861328125
}
2022-10-24 15:19:34,198 - trainer - INFO - start training epoch 66
2022-10-24 15:19:34,199 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,200 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,200 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,208 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval training set based on eval_every=2***************
2022-10-24 15:19:34,208 - trainer - INFO - {
  "train_loss": 5754.823974609375
}
2022-10-24 15:19:34,216 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval development set based on eval_every=2***************
2022-10-24 15:19:34,217 - trainer - INFO - {
  "dev_loss": 5557.234375,
  "dev_best_score_for_loss": -5461.89990234375
}
2022-10-24 15:19:34,218 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:34,218 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,219 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,220 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,220 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_60
2022-10-24 15:19:34,222 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_66
2022-10-24 15:19:34,225 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_66
2022-10-24 15:19:34,226 - trainer - INFO - 
*****************[epoch: 66, global step: 67] eval training set at end of epoch***************
2022-10-24 15:19:34,226 - trainer - INFO - {
  "train_loss": 5903.86181640625
}
2022-10-24 15:19:34,232 - trainer - INFO - start training epoch 67
2022-10-24 15:19:34,233 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,233 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,234 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,248 - trainer - INFO - 
*****************[epoch: 67, global step: 68] eval training set at end of epoch***************
2022-10-24 15:19:34,249 - trainer - INFO - {
  "train_loss": 5557.234375
}
2022-10-24 15:19:34,250 - trainer - INFO - start training epoch 68
2022-10-24 15:19:34,250 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,251 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,251 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,259 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval training set based on eval_every=2***************
2022-10-24 15:19:34,259 - trainer - INFO - {
  "train_loss": 5267.643798828125
}
2022-10-24 15:19:34,271 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval development set based on eval_every=2***************
2022-10-24 15:19:34,272 - trainer - INFO - {
  "dev_loss": 4835.66552734375,
  "dev_best_score_for_loss": -4835.66552734375
}
2022-10-24 15:19:34,273 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,274 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,274 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,276 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_62
2022-10-24 15:19:34,279 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,285 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,285 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,286 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,287 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,288 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_68
2022-10-24 15:19:34,293 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_68
2022-10-24 15:19:34,300 - trainer - INFO - 
*****************[epoch: 68, global step: 69] eval training set at end of epoch***************
2022-10-24 15:19:34,300 - trainer - INFO - {
  "train_loss": 4978.05322265625
}
2022-10-24 15:19:34,302 - trainer - INFO - start training epoch 69
2022-10-24 15:19:34,303 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,303 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,305 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,317 - trainer - INFO - 
*****************[epoch: 69, global step: 70] eval training set at end of epoch***************
2022-10-24 15:19:34,317 - trainer - INFO - {
  "train_loss": 4835.66552734375
}
2022-10-24 15:19:34,318 - trainer - INFO - start training epoch 70
2022-10-24 15:19:34,318 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,319 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,320 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,330 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval training set based on eval_every=2***************
2022-10-24 15:19:34,331 - trainer - INFO - {
  "train_loss": 4977.875732421875
}
2022-10-24 15:19:34,340 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval development set based on eval_every=2***************
2022-10-24 15:19:34,341 - trainer - INFO - {
  "dev_loss": 5210.5712890625,
  "dev_best_score_for_loss": -4835.66552734375
}
2022-10-24 15:19:34,341 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:34,342 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,344 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,345 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,345 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_64
2022-10-24 15:19:34,347 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_70
2022-10-24 15:19:34,351 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_70
2022-10-24 15:19:34,352 - trainer - INFO - 
*****************[epoch: 70, global step: 71] eval training set at end of epoch***************
2022-10-24 15:19:34,354 - trainer - INFO - {
  "train_loss": 5120.0859375
}
2022-10-24 15:19:34,354 - trainer - INFO - start training epoch 71
2022-10-24 15:19:34,355 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,355 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,355 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,364 - trainer - INFO - 
*****************[epoch: 71, global step: 72] eval training set at end of epoch***************
2022-10-24 15:19:34,364 - trainer - INFO - {
  "train_loss": 5210.5712890625
}
2022-10-24 15:19:34,365 - trainer - INFO - start training epoch 72
2022-10-24 15:19:34,366 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,366 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,367 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,378 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval training set based on eval_every=2***************
2022-10-24 15:19:34,379 - trainer - INFO - {
  "train_loss": 5037.561767578125
}
2022-10-24 15:19:34,387 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval development set based on eval_every=2***************
2022-10-24 15:19:34,388 - trainer - INFO - {
  "dev_loss": 4527.2021484375,
  "dev_best_score_for_loss": -4527.2021484375
}
2022-10-24 15:19:34,389 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,390 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,392 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,392 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_66
2022-10-24 15:19:34,394 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,398 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,399 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,400 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,401 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,402 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_72
2022-10-24 15:19:34,406 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_72
2022-10-24 15:19:34,407 - trainer - INFO - 
*****************[epoch: 72, global step: 73] eval training set at end of epoch***************
2022-10-24 15:19:34,407 - trainer - INFO - {
  "train_loss": 4864.55224609375
}
2022-10-24 15:19:34,408 - trainer - INFO - start training epoch 73
2022-10-24 15:19:34,409 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,409 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,410 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,432 - trainer - INFO - 
*****************[epoch: 73, global step: 74] eval training set at end of epoch***************
2022-10-24 15:19:34,432 - trainer - INFO - {
  "train_loss": 4527.2021484375
}
2022-10-24 15:19:34,433 - trainer - INFO - start training epoch 74
2022-10-24 15:19:34,434 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,434 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,435 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,443 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval training set based on eval_every=2***************
2022-10-24 15:19:34,444 - trainer - INFO - {
  "train_loss": 4536.7109375
}
2022-10-24 15:19:34,455 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval development set based on eval_every=2***************
2022-10-24 15:19:34,456 - trainer - INFO - {
  "dev_loss": 4696.3525390625,
  "dev_best_score_for_loss": -4527.2021484375
}
2022-10-24 15:19:34,457 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:34,457 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,459 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,459 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,459 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_68
2022-10-24 15:19:34,462 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_74
2022-10-24 15:19:34,470 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_74
2022-10-24 15:19:34,473 - trainer - INFO - 
*****************[epoch: 74, global step: 75] eval training set at end of epoch***************
2022-10-24 15:19:34,474 - trainer - INFO - {
  "train_loss": 4546.2197265625
}
2022-10-24 15:19:34,475 - trainer - INFO - start training epoch 75
2022-10-24 15:19:34,476 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,477 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,477 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,490 - trainer - INFO - 
*****************[epoch: 75, global step: 76] eval training set at end of epoch***************
2022-10-24 15:19:34,490 - trainer - INFO - {
  "train_loss": 4696.3525390625
}
2022-10-24 15:19:34,491 - trainer - INFO - start training epoch 76
2022-10-24 15:19:34,492 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,495 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,495 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,504 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval training set based on eval_every=2***************
2022-10-24 15:19:34,504 - trainer - INFO - {
  "train_loss": 4657.0537109375
}
2022-10-24 15:19:34,514 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval development set based on eval_every=2***************
2022-10-24 15:19:34,514 - trainer - INFO - {
  "dev_loss": 4338.44482421875,
  "dev_best_score_for_loss": -4338.44482421875
}
2022-10-24 15:19:34,515 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,517 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,517 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,518 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_70
2022-10-24 15:19:34,519 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,524 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,525 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,525 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,526 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,527 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_76
2022-10-24 15:19:34,532 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_76
2022-10-24 15:19:34,533 - trainer - INFO - 
*****************[epoch: 76, global step: 77] eval training set at end of epoch***************
2022-10-24 15:19:34,533 - trainer - INFO - {
  "train_loss": 4617.7548828125
}
2022-10-24 15:19:34,533 - trainer - INFO - start training epoch 77
2022-10-24 15:19:34,534 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,534 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,534 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,542 - trainer - INFO - 
*****************[epoch: 77, global step: 78] eval training set at end of epoch***************
2022-10-24 15:19:34,542 - trainer - INFO - {
  "train_loss": 4338.44482421875
}
2022-10-24 15:19:34,543 - trainer - INFO - start training epoch 78
2022-10-24 15:19:34,543 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,543 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,544 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,552 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval training set based on eval_every=2***************
2022-10-24 15:19:34,552 - trainer - INFO - {
  "train_loss": 4254.48291015625
}
2022-10-24 15:19:34,565 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval development set based on eval_every=2***************
2022-10-24 15:19:34,565 - trainer - INFO - {
  "dev_loss": 4220.578125,
  "dev_best_score_for_loss": -4220.578125
}
2022-10-24 15:19:34,566 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,567 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,568 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,568 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_72
2022-10-24 15:19:34,570 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,574 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,574 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,574 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,575 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,575 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_78
2022-10-24 15:19:34,580 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_78
2022-10-24 15:19:34,581 - trainer - INFO - 
*****************[epoch: 78, global step: 79] eval training set at end of epoch***************
2022-10-24 15:19:34,581 - trainer - INFO - {
  "train_loss": 4170.52099609375
}
2022-10-24 15:19:34,581 - trainer - INFO - start training epoch 79
2022-10-24 15:19:34,581 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,582 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,582 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,589 - trainer - INFO - 
*****************[epoch: 79, global step: 80] eval training set at end of epoch***************
2022-10-24 15:19:34,590 - trainer - INFO - {
  "train_loss": 4220.578125
}
2022-10-24 15:19:34,590 - trainer - INFO - start training epoch 80
2022-10-24 15:19:34,590 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,590 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,591 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,598 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval training set based on eval_every=2***************
2022-10-24 15:19:34,598 - trainer - INFO - {
  "train_loss": 4236.671142578125
}
2022-10-24 15:19:34,606 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval development set based on eval_every=2***************
2022-10-24 15:19:34,607 - trainer - INFO - {
  "dev_loss": 4095.81103515625,
  "dev_best_score_for_loss": -4095.81103515625
}
2022-10-24 15:19:34,607 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,608 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,609 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,609 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_74
2022-10-24 15:19:34,610 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,614 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,614 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,614 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,615 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,615 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_80
2022-10-24 15:19:34,620 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_80
2022-10-24 15:19:34,621 - trainer - INFO - 
*****************[epoch: 80, global step: 81] eval training set at end of epoch***************
2022-10-24 15:19:34,621 - trainer - INFO - {
  "train_loss": 4252.76416015625
}
2022-10-24 15:19:34,622 - trainer - INFO - start training epoch 81
2022-10-24 15:19:34,622 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,622 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,623 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,631 - trainer - INFO - 
*****************[epoch: 81, global step: 82] eval training set at end of epoch***************
2022-10-24 15:19:34,632 - trainer - INFO - {
  "train_loss": 4095.810791015625
}
2022-10-24 15:19:34,633 - trainer - INFO - start training epoch 82
2022-10-24 15:19:34,634 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,635 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,637 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,645 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval training set based on eval_every=2***************
2022-10-24 15:19:34,645 - trainer - INFO - {
  "train_loss": 3998.707763671875
}
2022-10-24 15:19:34,651 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval development set based on eval_every=2***************
2022-10-24 15:19:34,652 - trainer - INFO - {
  "dev_loss": 3852.6455078125,
  "dev_best_score_for_loss": -3852.6455078125
}
2022-10-24 15:19:34,652 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,653 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,653 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,654 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_76
2022-10-24 15:19:34,655 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,658 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,658 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,658 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,659 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,660 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_82
2022-10-24 15:19:34,665 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_82
2022-10-24 15:19:34,666 - trainer - INFO - 
*****************[epoch: 82, global step: 83] eval training set at end of epoch***************
2022-10-24 15:19:34,669 - trainer - INFO - {
  "train_loss": 3901.604736328125
}
2022-10-24 15:19:34,669 - trainer - INFO - start training epoch 83
2022-10-24 15:19:34,670 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,670 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,670 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,679 - trainer - INFO - 
*****************[epoch: 83, global step: 84] eval training set at end of epoch***************
2022-10-24 15:19:34,680 - trainer - INFO - {
  "train_loss": 3852.6455078125
}
2022-10-24 15:19:34,680 - trainer - INFO - start training epoch 84
2022-10-24 15:19:34,680 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,681 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,681 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,687 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval training set based on eval_every=2***************
2022-10-24 15:19:34,687 - trainer - INFO - {
  "train_loss": 3864.7506103515625
}
2022-10-24 15:19:34,693 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval development set based on eval_every=2***************
2022-10-24 15:19:34,693 - trainer - INFO - {
  "dev_loss": 3806.85888671875,
  "dev_best_score_for_loss": -3806.85888671875
}
2022-10-24 15:19:34,695 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,696 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,696 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,696 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_78
2022-10-24 15:19:34,698 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,701 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,701 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,701 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,702 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,702 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_84
2022-10-24 15:19:34,705 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_84
2022-10-24 15:19:34,706 - trainer - INFO - 
*****************[epoch: 84, global step: 85] eval training set at end of epoch***************
2022-10-24 15:19:34,706 - trainer - INFO - {
  "train_loss": 3876.855712890625
}
2022-10-24 15:19:34,707 - trainer - INFO - start training epoch 85
2022-10-24 15:19:34,707 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,707 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,707 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,717 - trainer - INFO - 
*****************[epoch: 85, global step: 86] eval training set at end of epoch***************
2022-10-24 15:19:34,717 - trainer - INFO - {
  "train_loss": 3806.85888671875
}
2022-10-24 15:19:34,718 - trainer - INFO - start training epoch 86
2022-10-24 15:19:34,718 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,718 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,719 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,727 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval training set based on eval_every=2***************
2022-10-24 15:19:34,728 - trainer - INFO - {
  "train_loss": 3727.866455078125
}
2022-10-24 15:19:34,735 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval development set based on eval_every=2***************
2022-10-24 15:19:34,736 - trainer - INFO - {
  "dev_loss": 3544.230224609375,
  "dev_best_score_for_loss": -3544.230224609375
}
2022-10-24 15:19:34,736 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,737 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,738 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,738 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_80
2022-10-24 15:19:34,739 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,743 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,743 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,743 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,744 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,744 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_86
2022-10-24 15:19:34,748 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_86
2022-10-24 15:19:34,749 - trainer - INFO - 
*****************[epoch: 86, global step: 87] eval training set at end of epoch***************
2022-10-24 15:19:34,749 - trainer - INFO - {
  "train_loss": 3648.8740234375
}
2022-10-24 15:19:34,750 - trainer - INFO - start training epoch 87
2022-10-24 15:19:34,750 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,750 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,751 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,762 - trainer - INFO - 
*****************[epoch: 87, global step: 88] eval training set at end of epoch***************
2022-10-24 15:19:34,762 - trainer - INFO - {
  "train_loss": 3544.230224609375
}
2022-10-24 15:19:34,763 - trainer - INFO - start training epoch 88
2022-10-24 15:19:34,763 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,763 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,764 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,773 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval training set based on eval_every=2***************
2022-10-24 15:19:34,773 - trainer - INFO - {
  "train_loss": 3537.0775146484375
}
2022-10-24 15:19:34,781 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval development set based on eval_every=2***************
2022-10-24 15:19:34,782 - trainer - INFO - {
  "dev_loss": 3493.56591796875,
  "dev_best_score_for_loss": -3493.56591796875
}
2022-10-24 15:19:34,783 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,784 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,784 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,784 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_82
2022-10-24 15:19:34,786 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,789 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,789 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,789 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,790 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,790 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_88
2022-10-24 15:19:34,795 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_88
2022-10-24 15:19:34,796 - trainer - INFO - 
*****************[epoch: 88, global step: 89] eval training set at end of epoch***************
2022-10-24 15:19:34,796 - trainer - INFO - {
  "train_loss": 3529.9248046875
}
2022-10-24 15:19:34,796 - trainer - INFO - start training epoch 89
2022-10-24 15:19:34,797 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,797 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,797 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,809 - trainer - INFO - 
*****************[epoch: 89, global step: 90] eval training set at end of epoch***************
2022-10-24 15:19:34,810 - trainer - INFO - {
  "train_loss": 3493.565673828125
}
2022-10-24 15:19:34,810 - trainer - INFO - start training epoch 90
2022-10-24 15:19:34,810 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,811 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,811 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,819 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval training set based on eval_every=2***************
2022-10-24 15:19:34,820 - trainer - INFO - {
  "train_loss": 3435.440673828125
}
2022-10-24 15:19:34,825 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval development set based on eval_every=2***************
2022-10-24 15:19:34,826 - trainer - INFO - {
  "dev_loss": 3264.322021484375,
  "dev_best_score_for_loss": -3264.322021484375
}
2022-10-24 15:19:34,826 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,827 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,828 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,828 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_84
2022-10-24 15:19:34,829 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,832 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,833 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,833 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,834 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,834 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_90
2022-10-24 15:19:34,839 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_90
2022-10-24 15:19:34,839 - trainer - INFO - 
*****************[epoch: 90, global step: 91] eval training set at end of epoch***************
2022-10-24 15:19:34,840 - trainer - INFO - {
  "train_loss": 3377.315673828125
}
2022-10-24 15:19:34,840 - trainer - INFO - start training epoch 91
2022-10-24 15:19:34,840 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,840 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,841 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,848 - trainer - INFO - 
*****************[epoch: 91, global step: 92] eval training set at end of epoch***************
2022-10-24 15:19:34,848 - trainer - INFO - {
  "train_loss": 3264.321533203125
}
2022-10-24 15:19:34,850 - trainer - INFO - start training epoch 92
2022-10-24 15:19:34,854 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,854 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,855 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,863 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval training set based on eval_every=2***************
2022-10-24 15:19:34,865 - trainer - INFO - {
  "train_loss": 3241.2581787109375
}
2022-10-24 15:19:34,877 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval development set based on eval_every=2***************
2022-10-24 15:19:34,877 - trainer - INFO - {
  "dev_loss": 3183.35888671875,
  "dev_best_score_for_loss": -3183.35888671875
}
2022-10-24 15:19:34,878 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,880 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,881 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,881 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_86
2022-10-24 15:19:34,882 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,886 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,887 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,887 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,888 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,888 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_92
2022-10-24 15:19:34,893 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_92
2022-10-24 15:19:34,894 - trainer - INFO - 
*****************[epoch: 92, global step: 93] eval training set at end of epoch***************
2022-10-24 15:19:34,895 - trainer - INFO - {
  "train_loss": 3218.19482421875
}
2022-10-24 15:19:34,896 - trainer - INFO - start training epoch 93
2022-10-24 15:19:34,897 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,899 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,900 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,914 - trainer - INFO - 
*****************[epoch: 93, global step: 94] eval training set at end of epoch***************
2022-10-24 15:19:34,915 - trainer - INFO - {
  "train_loss": 3183.35888671875
}
2022-10-24 15:19:34,915 - trainer - INFO - start training epoch 94
2022-10-24 15:19:34,916 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,916 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,916 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,923 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval training set based on eval_every=2***************
2022-10-24 15:19:34,923 - trainer - INFO - {
  "train_loss": 3140.0379638671875
}
2022-10-24 15:19:34,933 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval development set based on eval_every=2***************
2022-10-24 15:19:34,933 - trainer - INFO - {
  "dev_loss": 2990.908203125,
  "dev_best_score_for_loss": -2990.908203125
}
2022-10-24 15:19:34,934 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,935 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,936 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,936 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_88
2022-10-24 15:19:34,938 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,941 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,941 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,941 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,943 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,944 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_94
2022-10-24 15:19:34,949 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_94
2022-10-24 15:19:34,950 - trainer - INFO - 
*****************[epoch: 94, global step: 95] eval training set at end of epoch***************
2022-10-24 15:19:34,950 - trainer - INFO - {
  "train_loss": 3096.717041015625
}
2022-10-24 15:19:34,951 - trainer - INFO - start training epoch 95
2022-10-24 15:19:34,951 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,951 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,951 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,961 - trainer - INFO - 
*****************[epoch: 95, global step: 96] eval training set at end of epoch***************
2022-10-24 15:19:34,961 - trainer - INFO - {
  "train_loss": 2990.908203125
}
2022-10-24 15:19:34,962 - trainer - INFO - start training epoch 96
2022-10-24 15:19:34,962 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,962 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,962 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:34,969 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval training set based on eval_every=2***************
2022-10-24 15:19:34,969 - trainer - INFO - {
  "train_loss": 2958.12158203125
}
2022-10-24 15:19:34,974 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval development set based on eval_every=2***************
2022-10-24 15:19:34,975 - trainer - INFO - {
  "dev_loss": 2883.014892578125,
  "dev_best_score_for_loss": -2883.014892578125
}
2022-10-24 15:19:34,975 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:34,977 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:34,977 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:34,978 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_90
2022-10-24 15:19:34,979 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:34,982 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:34,982 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:34,982 - trainer - INFO -   patience: 200
2022-10-24 15:19:34,983 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:34,983 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_96
2022-10-24 15:19:34,987 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_96
2022-10-24 15:19:34,989 - trainer - INFO - 
*****************[epoch: 96, global step: 97] eval training set at end of epoch***************
2022-10-24 15:19:34,990 - trainer - INFO - {
  "train_loss": 2925.3349609375
}
2022-10-24 15:19:34,994 - trainer - INFO - start training epoch 97
2022-10-24 15:19:34,995 - trainer - INFO - training using device=cuda
2022-10-24 15:19:34,995 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:34,995 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,004 - trainer - INFO - 
*****************[epoch: 97, global step: 98] eval training set at end of epoch***************
2022-10-24 15:19:35,005 - trainer - INFO - {
  "train_loss": 2883.014892578125
}
2022-10-24 15:19:35,005 - trainer - INFO - start training epoch 98
2022-10-24 15:19:35,005 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,006 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,006 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,012 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval training set based on eval_every=2***************
2022-10-24 15:19:35,013 - trainer - INFO - {
  "train_loss": 2846.8612060546875
}
2022-10-24 15:19:35,018 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval development set based on eval_every=2***************
2022-10-24 15:19:35,018 - trainer - INFO - {
  "dev_loss": 2716.204345703125,
  "dev_best_score_for_loss": -2716.204345703125
}
2022-10-24 15:19:35,019 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,021 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,021 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,021 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_92
2022-10-24 15:19:35,023 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,026 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,026 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,026 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,027 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,027 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_98
2022-10-24 15:19:35,031 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_98
2022-10-24 15:19:35,032 - trainer - INFO - 
*****************[epoch: 98, global step: 99] eval training set at end of epoch***************
2022-10-24 15:19:35,032 - trainer - INFO - {
  "train_loss": 2810.70751953125
}
2022-10-24 15:19:35,033 - trainer - INFO - start training epoch 99
2022-10-24 15:19:35,033 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,033 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,033 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,043 - trainer - INFO - 
*****************[epoch: 99, global step: 100] eval training set at end of epoch***************
2022-10-24 15:19:35,043 - trainer - INFO - {
  "train_loss": 2716.204345703125
}
2022-10-24 15:19:35,044 - trainer - INFO - start training epoch 100
2022-10-24 15:19:35,044 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,044 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,044 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,053 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval training set based on eval_every=2***************
2022-10-24 15:19:35,054 - trainer - INFO - {
  "train_loss": 2680.5252685546875
}
2022-10-24 15:19:35,060 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval development set based on eval_every=2***************
2022-10-24 15:19:35,061 - trainer - INFO - {
  "dev_loss": 2594.7119140625,
  "dev_best_score_for_loss": -2594.7119140625
}
2022-10-24 15:19:35,062 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,063 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,063 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,063 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_94
2022-10-24 15:19:35,065 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,068 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,068 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,068 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,069 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,069 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_100
2022-10-24 15:19:35,072 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_100
2022-10-24 15:19:35,073 - trainer - INFO - 
*****************[epoch: 100, global step: 101] eval training set at end of epoch***************
2022-10-24 15:19:35,073 - trainer - INFO - {
  "train_loss": 2644.84619140625
}
2022-10-24 15:19:35,074 - trainer - INFO - start training epoch 101
2022-10-24 15:19:35,074 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,074 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,074 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,087 - trainer - INFO - 
*****************[epoch: 101, global step: 102] eval training set at end of epoch***************
2022-10-24 15:19:35,087 - trainer - INFO - {
  "train_loss": 2594.7119140625
}
2022-10-24 15:19:35,088 - trainer - INFO - start training epoch 102
2022-10-24 15:19:35,088 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,088 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,089 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,098 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval training set based on eval_every=2***************
2022-10-24 15:19:35,099 - trainer - INFO - {
  "train_loss": 2561.48388671875
}
2022-10-24 15:19:35,105 - trainer - INFO - 
*****************[epoch: 102, global step: 102] eval development set based on eval_every=2***************
2022-10-24 15:19:35,105 - trainer - INFO - {
  "dev_loss": 2443.165283203125,
  "dev_best_score_for_loss": -2443.165283203125
}
2022-10-24 15:19:35,106 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,107 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,107 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,107 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_96
2022-10-24 15:19:35,108 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,112 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,113 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,114 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,115 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,118 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_102
2022-10-24 15:19:35,122 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_102
2022-10-24 15:19:35,123 - trainer - INFO - 
*****************[epoch: 102, global step: 103] eval training set at end of epoch***************
2022-10-24 15:19:35,123 - trainer - INFO - {
  "train_loss": 2528.255859375
}
2022-10-24 15:19:35,124 - trainer - INFO - start training epoch 103
2022-10-24 15:19:35,124 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,124 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,125 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,133 - trainer - INFO - 
*****************[epoch: 103, global step: 104] eval training set at end of epoch***************
2022-10-24 15:19:35,133 - trainer - INFO - {
  "train_loss": 2443.165283203125
}
2022-10-24 15:19:35,134 - trainer - INFO - start training epoch 104
2022-10-24 15:19:35,134 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,134 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,135 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,140 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval training set based on eval_every=2***************
2022-10-24 15:19:35,140 - trainer - INFO - {
  "train_loss": 2406.838134765625
}
2022-10-24 15:19:35,149 - trainer - INFO - 
*****************[epoch: 104, global step: 104] eval development set based on eval_every=2***************
2022-10-24 15:19:35,150 - trainer - INFO - {
  "dev_loss": 2314.864013671875,
  "dev_best_score_for_loss": -2314.864013671875
}
2022-10-24 15:19:35,150 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,152 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,152 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,152 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_98
2022-10-24 15:19:35,154 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,157 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,158 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,158 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,159 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,160 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_104
2022-10-24 15:19:35,165 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_104
2022-10-24 15:19:35,165 - trainer - INFO - 
*****************[epoch: 104, global step: 105] eval training set at end of epoch***************
2022-10-24 15:19:35,166 - trainer - INFO - {
  "train_loss": 2370.510986328125
}
2022-10-24 15:19:35,166 - trainer - INFO - start training epoch 105
2022-10-24 15:19:35,166 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,167 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,167 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,173 - trainer - INFO - 
*****************[epoch: 105, global step: 106] eval training set at end of epoch***************
2022-10-24 15:19:35,175 - trainer - INFO - {
  "train_loss": 2314.86376953125
}
2022-10-24 15:19:35,175 - trainer - INFO - start training epoch 106
2022-10-24 15:19:35,175 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,175 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,176 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,182 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval training set based on eval_every=2***************
2022-10-24 15:19:35,183 - trainer - INFO - {
  "train_loss": 2282.7015380859375
}
2022-10-24 15:19:35,188 - trainer - INFO - 
*****************[epoch: 106, global step: 106] eval development set based on eval_every=2***************
2022-10-24 15:19:35,188 - trainer - INFO - {
  "dev_loss": 2172.48291015625,
  "dev_best_score_for_loss": -2172.48291015625
}
2022-10-24 15:19:35,189 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,190 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,191 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,192 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_100
2022-10-24 15:19:35,195 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,201 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,202 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,202 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,203 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,203 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_106
2022-10-24 15:19:35,208 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_106
2022-10-24 15:19:35,209 - trainer - INFO - 
*****************[epoch: 106, global step: 107] eval training set at end of epoch***************
2022-10-24 15:19:35,209 - trainer - INFO - {
  "train_loss": 2250.539306640625
}
2022-10-24 15:19:35,209 - trainer - INFO - start training epoch 107
2022-10-24 15:19:35,210 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,210 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,210 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,218 - trainer - INFO - 
*****************[epoch: 107, global step: 108] eval training set at end of epoch***************
2022-10-24 15:19:35,218 - trainer - INFO - {
  "train_loss": 2172.48291015625
}
2022-10-24 15:19:35,218 - trainer - INFO - start training epoch 108
2022-10-24 15:19:35,218 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,219 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,219 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,230 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval training set based on eval_every=2***************
2022-10-24 15:19:35,230 - trainer - INFO - {
  "train_loss": 2137.28369140625
}
2022-10-24 15:19:35,237 - trainer - INFO - 
*****************[epoch: 108, global step: 108] eval development set based on eval_every=2***************
2022-10-24 15:19:35,238 - trainer - INFO - {
  "dev_loss": 2044.040771484375,
  "dev_best_score_for_loss": -2044.040771484375
}
2022-10-24 15:19:35,241 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,242 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,242 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,243 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_102
2022-10-24 15:19:35,244 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,248 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,248 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,248 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,250 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,250 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_108
2022-10-24 15:19:35,260 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_108
2022-10-24 15:19:35,263 - trainer - INFO - 
*****************[epoch: 108, global step: 109] eval training set at end of epoch***************
2022-10-24 15:19:35,263 - trainer - INFO - {
  "train_loss": 2102.08447265625
}
2022-10-24 15:19:35,264 - trainer - INFO - start training epoch 109
2022-10-24 15:19:35,264 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,265 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,265 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,277 - trainer - INFO - 
*****************[epoch: 109, global step: 110] eval training set at end of epoch***************
2022-10-24 15:19:35,277 - trainer - INFO - {
  "train_loss": 2044.040771484375
}
2022-10-24 15:19:35,279 - trainer - INFO - start training epoch 110
2022-10-24 15:19:35,279 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,280 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,280 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,289 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval training set based on eval_every=2***************
2022-10-24 15:19:35,289 - trainer - INFO - {
  "train_loss": 2012.302490234375
}
2022-10-24 15:19:35,300 - trainer - INFO - 
*****************[epoch: 110, global step: 110] eval development set based on eval_every=2***************
2022-10-24 15:19:35,300 - trainer - INFO - {
  "dev_loss": 1907.7906494140625,
  "dev_best_score_for_loss": -1907.7906494140625
}
2022-10-24 15:19:35,301 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,303 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,303 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,303 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_104
2022-10-24 15:19:35,305 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,309 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,310 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,310 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,311 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,312 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_110
2022-10-24 15:19:35,317 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_110
2022-10-24 15:19:35,321 - trainer - INFO - 
*****************[epoch: 110, global step: 111] eval training set at end of epoch***************
2022-10-24 15:19:35,322 - trainer - INFO - {
  "train_loss": 1980.564208984375
}
2022-10-24 15:19:35,322 - trainer - INFO - start training epoch 111
2022-10-24 15:19:35,322 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,323 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,323 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,333 - trainer - INFO - 
*****************[epoch: 111, global step: 112] eval training set at end of epoch***************
2022-10-24 15:19:35,334 - trainer - INFO - {
  "train_loss": 1907.7906494140625
}
2022-10-24 15:19:35,334 - trainer - INFO - start training epoch 112
2022-10-24 15:19:35,334 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,335 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,335 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,341 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval training set based on eval_every=2***************
2022-10-24 15:19:35,342 - trainer - INFO - {
  "train_loss": 1874.1471557617188
}
2022-10-24 15:19:35,347 - trainer - INFO - 
*****************[epoch: 112, global step: 112] eval development set based on eval_every=2***************
2022-10-24 15:19:35,348 - trainer - INFO - {
  "dev_loss": 1781.607666015625,
  "dev_best_score_for_loss": -1781.607666015625
}
2022-10-24 15:19:35,348 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,349 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,349 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,350 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_106
2022-10-24 15:19:35,351 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,354 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,354 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,355 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,355 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,356 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_112
2022-10-24 15:19:35,360 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_112
2022-10-24 15:19:35,362 - trainer - INFO - 
*****************[epoch: 112, global step: 113] eval training set at end of epoch***************
2022-10-24 15:19:35,363 - trainer - INFO - {
  "train_loss": 1840.503662109375
}
2022-10-24 15:19:35,364 - trainer - INFO - start training epoch 113
2022-10-24 15:19:35,368 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,368 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,369 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,378 - trainer - INFO - 
*****************[epoch: 113, global step: 114] eval training set at end of epoch***************
2022-10-24 15:19:35,378 - trainer - INFO - {
  "train_loss": 1781.607666015625
}
2022-10-24 15:19:35,379 - trainer - INFO - start training epoch 114
2022-10-24 15:19:35,380 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,380 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,380 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,387 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval training set based on eval_every=2***************
2022-10-24 15:19:35,387 - trainer - INFO - {
  "train_loss": 1750.132568359375
}
2022-10-24 15:19:35,393 - trainer - INFO - 
*****************[epoch: 114, global step: 114] eval development set based on eval_every=2***************
2022-10-24 15:19:35,393 - trainer - INFO - {
  "dev_loss": 1650.265869140625,
  "dev_best_score_for_loss": -1650.265869140625
}
2022-10-24 15:19:35,394 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,395 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,395 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,395 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_108
2022-10-24 15:19:35,397 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,400 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,400 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,401 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,401 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,402 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_114
2022-10-24 15:19:35,405 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_114
2022-10-24 15:19:35,408 - trainer - INFO - 
*****************[epoch: 114, global step: 115] eval training set at end of epoch***************
2022-10-24 15:19:35,409 - trainer - INFO - {
  "train_loss": 1718.657470703125
}
2022-10-24 15:19:35,410 - trainer - INFO - start training epoch 115
2022-10-24 15:19:35,410 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,410 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,411 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,424 - trainer - INFO - 
*****************[epoch: 115, global step: 116] eval training set at end of epoch***************
2022-10-24 15:19:35,425 - trainer - INFO - {
  "train_loss": 1650.2659912109375
}
2022-10-24 15:19:35,425 - trainer - INFO - start training epoch 116
2022-10-24 15:19:35,425 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,426 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,426 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,434 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval training set based on eval_every=2***************
2022-10-24 15:19:35,435 - trainer - INFO - {
  "train_loss": 1618.6775512695312
}
2022-10-24 15:19:35,441 - trainer - INFO - 
*****************[epoch: 116, global step: 116] eval development set based on eval_every=2***************
2022-10-24 15:19:35,441 - trainer - INFO - {
  "dev_loss": 1529.214599609375,
  "dev_best_score_for_loss": -1529.214599609375
}
2022-10-24 15:19:35,442 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,443 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,443 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,444 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_110
2022-10-24 15:19:35,445 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,448 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,448 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,449 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,449 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,450 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_116
2022-10-24 15:19:35,454 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_116
2022-10-24 15:19:35,456 - trainer - INFO - 
*****************[epoch: 116, global step: 117] eval training set at end of epoch***************
2022-10-24 15:19:35,459 - trainer - INFO - {
  "train_loss": 1587.089111328125
}
2022-10-24 15:19:35,459 - trainer - INFO - start training epoch 117
2022-10-24 15:19:35,460 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,460 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,460 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,469 - trainer - INFO - 
*****************[epoch: 117, global step: 118] eval training set at end of epoch***************
2022-10-24 15:19:35,470 - trainer - INFO - {
  "train_loss": 1529.214599609375
}
2022-10-24 15:19:35,470 - trainer - INFO - start training epoch 118
2022-10-24 15:19:35,470 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,471 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,471 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,478 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval training set based on eval_every=2***************
2022-10-24 15:19:35,479 - trainer - INFO - {
  "train_loss": 1498.5648803710938
}
2022-10-24 15:19:35,485 - trainer - INFO - 
*****************[epoch: 118, global step: 118] eval development set based on eval_every=2***************
2022-10-24 15:19:35,485 - trainer - INFO - {
  "dev_loss": 1403.653564453125,
  "dev_best_score_for_loss": -1403.653564453125
}
2022-10-24 15:19:35,486 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,487 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,488 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,488 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_112
2022-10-24 15:19:35,489 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,493 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,493 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,493 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,494 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,494 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_118
2022-10-24 15:19:35,499 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_118
2022-10-24 15:19:35,500 - trainer - INFO - 
*****************[epoch: 118, global step: 119] eval training set at end of epoch***************
2022-10-24 15:19:35,501 - trainer - INFO - {
  "train_loss": 1467.9151611328125
}
2022-10-24 15:19:35,505 - trainer - INFO - start training epoch 119
2022-10-24 15:19:35,506 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,506 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,506 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,516 - trainer - INFO - 
*****************[epoch: 119, global step: 120] eval training set at end of epoch***************
2022-10-24 15:19:35,516 - trainer - INFO - {
  "train_loss": 1403.65380859375
}
2022-10-24 15:19:35,517 - trainer - INFO - start training epoch 120
2022-10-24 15:19:35,517 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,517 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,518 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,525 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval training set based on eval_every=2***************
2022-10-24 15:19:35,526 - trainer - INFO - {
  "train_loss": 1374.1476440429688
}
2022-10-24 15:19:35,536 - trainer - INFO - 
*****************[epoch: 120, global step: 120] eval development set based on eval_every=2***************
2022-10-24 15:19:35,536 - trainer - INFO - {
  "dev_loss": 1288.5794677734375,
  "dev_best_score_for_loss": -1288.5794677734375
}
2022-10-24 15:19:35,537 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,538 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,538 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,538 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_114
2022-10-24 15:19:35,540 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,543 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,543 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,544 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,545 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,545 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_120
2022-10-24 15:19:35,550 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_120
2022-10-24 15:19:35,550 - trainer - INFO - 
*****************[epoch: 120, global step: 121] eval training set at end of epoch***************
2022-10-24 15:19:35,551 - trainer - INFO - {
  "train_loss": 1344.6414794921875
}
2022-10-24 15:19:35,551 - trainer - INFO - start training epoch 121
2022-10-24 15:19:35,551 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,551 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,552 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,558 - trainer - INFO - 
*****************[epoch: 121, global step: 122] eval training set at end of epoch***************
2022-10-24 15:19:35,558 - trainer - INFO - {
  "train_loss": 1288.5794677734375
}
2022-10-24 15:19:35,559 - trainer - INFO - start training epoch 122
2022-10-24 15:19:35,559 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,559 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,560 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,571 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval training set based on eval_every=2***************
2022-10-24 15:19:35,572 - trainer - INFO - {
  "train_loss": 1259.009033203125
}
2022-10-24 15:19:35,580 - trainer - INFO - 
*****************[epoch: 122, global step: 122] eval development set based on eval_every=2***************
2022-10-24 15:19:35,580 - trainer - INFO - {
  "dev_loss": 1170.3851318359375,
  "dev_best_score_for_loss": -1170.3851318359375
}
2022-10-24 15:19:35,581 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,582 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,583 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,583 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_116
2022-10-24 15:19:35,584 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,588 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,588 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,589 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,590 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,590 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_122
2022-10-24 15:19:35,595 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_122
2022-10-24 15:19:35,596 - trainer - INFO - 
*****************[epoch: 122, global step: 123] eval training set at end of epoch***************
2022-10-24 15:19:35,597 - trainer - INFO - {
  "train_loss": 1229.4385986328125
}
2022-10-24 15:19:35,597 - trainer - INFO - start training epoch 123
2022-10-24 15:19:35,597 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,598 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,598 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,606 - trainer - INFO - 
*****************[epoch: 123, global step: 124] eval training set at end of epoch***************
2022-10-24 15:19:35,607 - trainer - INFO - {
  "train_loss": 1170.38525390625
}
2022-10-24 15:19:35,607 - trainer - INFO - start training epoch 124
2022-10-24 15:19:35,608 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,610 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,610 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,625 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval training set based on eval_every=2***************
2022-10-24 15:19:35,632 - trainer - INFO - {
  "train_loss": 1143.1265258789062
}
2022-10-24 15:19:35,663 - trainer - INFO - 
*****************[epoch: 124, global step: 124] eval development set based on eval_every=2***************
2022-10-24 15:19:35,664 - trainer - INFO - {
  "dev_loss": 1062.3387451171875,
  "dev_best_score_for_loss": -1062.3387451171875
}
2022-10-24 15:19:35,665 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,667 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,667 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,667 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_118
2022-10-24 15:19:35,669 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,673 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,674 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,674 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,675 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,675 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_124
2022-10-24 15:19:35,681 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_124
2022-10-24 15:19:35,682 - trainer - INFO - 
*****************[epoch: 124, global step: 125] eval training set at end of epoch***************
2022-10-24 15:19:35,682 - trainer - INFO - {
  "train_loss": 1115.8677978515625
}
2022-10-24 15:19:35,682 - trainer - INFO - start training epoch 125
2022-10-24 15:19:35,683 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,683 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,683 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,693 - trainer - INFO - 
*****************[epoch: 125, global step: 126] eval training set at end of epoch***************
2022-10-24 15:19:35,694 - trainer - INFO - {
  "train_loss": 1062.3387451171875
}
2022-10-24 15:19:35,694 - trainer - INFO - start training epoch 126
2022-10-24 15:19:35,695 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,695 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,695 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,707 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval training set based on eval_every=2***************
2022-10-24 15:19:35,707 - trainer - INFO - {
  "train_loss": 1034.6297912597656
}
2022-10-24 15:19:35,715 - trainer - INFO - 
*****************[epoch: 126, global step: 126] eval development set based on eval_every=2***************
2022-10-24 15:19:35,715 - trainer - INFO - {
  "dev_loss": 953.4063110351562,
  "dev_best_score_for_loss": -953.4063110351562
}
2022-10-24 15:19:35,716 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,717 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,718 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,718 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_120
2022-10-24 15:19:35,720 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,723 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,724 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,724 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,725 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,725 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_126
2022-10-24 15:19:35,729 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_126
2022-10-24 15:19:35,730 - trainer - INFO - 
*****************[epoch: 126, global step: 127] eval training set at end of epoch***************
2022-10-24 15:19:35,730 - trainer - INFO - {
  "train_loss": 1006.9208374023438
}
2022-10-24 15:19:35,731 - trainer - INFO - start training epoch 127
2022-10-24 15:19:35,731 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,731 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,733 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,741 - trainer - INFO - 
*****************[epoch: 127, global step: 128] eval training set at end of epoch***************
2022-10-24 15:19:35,741 - trainer - INFO - {
  "train_loss": 953.4063110351562
}
2022-10-24 15:19:35,742 - trainer - INFO - start training epoch 128
2022-10-24 15:19:35,742 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,742 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,742 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,753 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval training set based on eval_every=2***************
2022-10-24 15:19:35,754 - trainer - INFO - {
  "train_loss": 928.3884582519531
}
2022-10-24 15:19:35,765 - trainer - INFO - 
*****************[epoch: 128, global step: 128] eval development set based on eval_every=2***************
2022-10-24 15:19:35,766 - trainer - INFO - {
  "dev_loss": 853.186767578125,
  "dev_best_score_for_loss": -853.186767578125
}
2022-10-24 15:19:35,767 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,768 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,769 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,769 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_122
2022-10-24 15:19:35,770 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,773 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,774 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,774 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,775 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,775 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_128
2022-10-24 15:19:35,780 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_128
2022-10-24 15:19:35,781 - trainer - INFO - 
*****************[epoch: 128, global step: 129] eval training set at end of epoch***************
2022-10-24 15:19:35,781 - trainer - INFO - {
  "train_loss": 903.37060546875
}
2022-10-24 15:19:35,782 - trainer - INFO - start training epoch 129
2022-10-24 15:19:35,782 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,782 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,783 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,791 - trainer - INFO - 
*****************[epoch: 129, global step: 130] eval training set at end of epoch***************
2022-10-24 15:19:35,791 - trainer - INFO - {
  "train_loss": 853.1866455078125
}
2022-10-24 15:19:35,792 - trainer - INFO - start training epoch 130
2022-10-24 15:19:35,792 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,792 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,793 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,805 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval training set based on eval_every=2***************
2022-10-24 15:19:35,805 - trainer - INFO - {
  "train_loss": 828.0566711425781
}
2022-10-24 15:19:35,814 - trainer - INFO - 
*****************[epoch: 130, global step: 130] eval development set based on eval_every=2***************
2022-10-24 15:19:35,815 - trainer - INFO - {
  "dev_loss": 755.66943359375,
  "dev_best_score_for_loss": -755.66943359375
}
2022-10-24 15:19:35,816 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,818 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,818 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,818 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_124
2022-10-24 15:19:35,820 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,823 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,823 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,823 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,824 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,824 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_130
2022-10-24 15:19:35,830 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_130
2022-10-24 15:19:35,831 - trainer - INFO - 
*****************[epoch: 130, global step: 131] eval training set at end of epoch***************
2022-10-24 15:19:35,831 - trainer - INFO - {
  "train_loss": 802.9266967773438
}
2022-10-24 15:19:35,831 - trainer - INFO - start training epoch 131
2022-10-24 15:19:35,831 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,832 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,832 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,841 - trainer - INFO - 
*****************[epoch: 131, global step: 132] eval training set at end of epoch***************
2022-10-24 15:19:35,842 - trainer - INFO - {
  "train_loss": 755.66943359375
}
2022-10-24 15:19:35,846 - trainer - INFO - start training epoch 132
2022-10-24 15:19:35,847 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,847 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,847 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,854 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval training set based on eval_every=2***************
2022-10-24 15:19:35,855 - trainer - INFO - {
  "train_loss": 732.9767456054688
}
2022-10-24 15:19:35,863 - trainer - INFO - 
*****************[epoch: 132, global step: 132] eval development set based on eval_every=2***************
2022-10-24 15:19:35,864 - trainer - INFO - {
  "dev_loss": 664.9044799804688,
  "dev_best_score_for_loss": -664.9044799804688
}
2022-10-24 15:19:35,864 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,865 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,866 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,866 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_126
2022-10-24 15:19:35,867 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,871 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,871 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,871 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,873 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,873 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_132
2022-10-24 15:19:35,877 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_132
2022-10-24 15:19:35,878 - trainer - INFO - 
*****************[epoch: 132, global step: 133] eval training set at end of epoch***************
2022-10-24 15:19:35,879 - trainer - INFO - {
  "train_loss": 710.2840576171875
}
2022-10-24 15:19:35,879 - trainer - INFO - start training epoch 133
2022-10-24 15:19:35,879 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,879 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,880 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,886 - trainer - INFO - 
*****************[epoch: 133, global step: 134] eval training set at end of epoch***************
2022-10-24 15:19:35,887 - trainer - INFO - {
  "train_loss": 664.9044799804688
}
2022-10-24 15:19:35,889 - trainer - INFO - start training epoch 134
2022-10-24 15:19:35,890 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,891 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,896 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,904 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval training set based on eval_every=2***************
2022-10-24 15:19:35,904 - trainer - INFO - {
  "train_loss": 642.9486694335938
}
2022-10-24 15:19:35,911 - trainer - INFO - 
*****************[epoch: 134, global step: 134] eval development set based on eval_every=2***************
2022-10-24 15:19:35,912 - trainer - INFO - {
  "dev_loss": 579.6578369140625,
  "dev_best_score_for_loss": -579.6578369140625
}
2022-10-24 15:19:35,912 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,914 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,914 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,915 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_128
2022-10-24 15:19:35,916 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,919 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,920 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,920 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,921 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,921 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_134
2022-10-24 15:19:35,925 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_134
2022-10-24 15:19:35,926 - trainer - INFO - 
*****************[epoch: 134, global step: 135] eval training set at end of epoch***************
2022-10-24 15:19:35,926 - trainer - INFO - {
  "train_loss": 620.9928588867188
}
2022-10-24 15:19:35,927 - trainer - INFO - start training epoch 135
2022-10-24 15:19:35,927 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,927 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,927 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,941 - trainer - INFO - 
*****************[epoch: 135, global step: 136] eval training set at end of epoch***************
2022-10-24 15:19:35,941 - trainer - INFO - {
  "train_loss": 579.6578369140625
}
2022-10-24 15:19:35,941 - trainer - INFO - start training epoch 136
2022-10-24 15:19:35,942 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,942 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,942 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,950 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval training set based on eval_every=2***************
2022-10-24 15:19:35,951 - trainer - INFO - {
  "train_loss": 559.4976501464844
}
2022-10-24 15:19:35,957 - trainer - INFO - 
*****************[epoch: 136, global step: 136] eval development set based on eval_every=2***************
2022-10-24 15:19:35,958 - trainer - INFO - {
  "dev_loss": 499.90313720703125,
  "dev_best_score_for_loss": -499.90313720703125
}
2022-10-24 15:19:35,958 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:35,960 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:35,960 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:35,960 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_130
2022-10-24 15:19:35,962 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:35,965 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:35,966 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:35,966 - trainer - INFO -   patience: 200
2022-10-24 15:19:35,967 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:35,967 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_136
2022-10-24 15:19:35,971 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_136
2022-10-24 15:19:35,972 - trainer - INFO - 
*****************[epoch: 136, global step: 137] eval training set at end of epoch***************
2022-10-24 15:19:35,972 - trainer - INFO - {
  "train_loss": 539.3374633789062
}
2022-10-24 15:19:35,972 - trainer - INFO - start training epoch 137
2022-10-24 15:19:35,973 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,973 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,973 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,986 - trainer - INFO - 
*****************[epoch: 137, global step: 138] eval training set at end of epoch***************
2022-10-24 15:19:35,987 - trainer - INFO - {
  "train_loss": 499.9031066894531
}
2022-10-24 15:19:35,987 - trainer - INFO - start training epoch 138
2022-10-24 15:19:35,987 - trainer - INFO - training using device=cuda
2022-10-24 15:19:35,988 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:35,988 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:35,997 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval training set based on eval_every=2***************
2022-10-24 15:19:35,999 - trainer - INFO - {
  "train_loss": 481.3519744873047
}
2022-10-24 15:19:36,006 - trainer - INFO - 
*****************[epoch: 138, global step: 138] eval development set based on eval_every=2***************
2022-10-24 15:19:36,006 - trainer - INFO - {
  "dev_loss": 427.489501953125,
  "dev_best_score_for_loss": -427.489501953125
}
2022-10-24 15:19:36,007 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,009 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,009 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,009 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_132
2022-10-24 15:19:36,011 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,017 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,017 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,018 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,019 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,019 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_138
2022-10-24 15:19:36,024 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_138
2022-10-24 15:19:36,025 - trainer - INFO - 
*****************[epoch: 138, global step: 139] eval training set at end of epoch***************
2022-10-24 15:19:36,025 - trainer - INFO - {
  "train_loss": 462.80084228515625
}
2022-10-24 15:19:36,026 - trainer - INFO - start training epoch 139
2022-10-24 15:19:36,028 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,029 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,030 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,045 - trainer - INFO - 
*****************[epoch: 139, global step: 140] eval training set at end of epoch***************
2022-10-24 15:19:36,045 - trainer - INFO - {
  "train_loss": 427.489501953125
}
2022-10-24 15:19:36,046 - trainer - INFO - start training epoch 140
2022-10-24 15:19:36,046 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,046 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,046 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,055 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval training set based on eval_every=2***************
2022-10-24 15:19:36,055 - trainer - INFO - {
  "train_loss": 410.26271057128906
}
2022-10-24 15:19:36,066 - trainer - INFO - 
*****************[epoch: 140, global step: 140] eval development set based on eval_every=2***************
2022-10-24 15:19:36,066 - trainer - INFO - {
  "dev_loss": 360.58941650390625,
  "dev_best_score_for_loss": -360.58941650390625
}
2022-10-24 15:19:36,067 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,069 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,069 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,069 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_134
2022-10-24 15:19:36,071 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,074 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,074 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,075 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,076 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,079 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_140
2022-10-24 15:19:36,083 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_140
2022-10-24 15:19:36,083 - trainer - INFO - 
*****************[epoch: 140, global step: 141] eval training set at end of epoch***************
2022-10-24 15:19:36,083 - trainer - INFO - {
  "train_loss": 393.0359191894531
}
2022-10-24 15:19:36,084 - trainer - INFO - start training epoch 141
2022-10-24 15:19:36,084 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,084 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,085 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,094 - trainer - INFO - 
*****************[epoch: 141, global step: 142] eval training set at end of epoch***************
2022-10-24 15:19:36,095 - trainer - INFO - {
  "train_loss": 360.5894470214844
}
2022-10-24 15:19:36,095 - trainer - INFO - start training epoch 142
2022-10-24 15:19:36,095 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,095 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,096 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,102 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval training set based on eval_every=2***************
2022-10-24 15:19:36,102 - trainer - INFO - {
  "train_loss": 345.4121856689453
}
2022-10-24 15:19:36,109 - trainer - INFO - 
*****************[epoch: 142, global step: 142] eval development set based on eval_every=2***************
2022-10-24 15:19:36,110 - trainer - INFO - {
  "dev_loss": 300.99176025390625,
  "dev_best_score_for_loss": -300.99176025390625
}
2022-10-24 15:19:36,110 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,111 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,112 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,112 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_136
2022-10-24 15:19:36,113 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,116 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,116 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,116 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,117 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,117 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_142
2022-10-24 15:19:36,122 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_142
2022-10-24 15:19:36,125 - trainer - INFO - 
*****************[epoch: 142, global step: 143] eval training set at end of epoch***************
2022-10-24 15:19:36,126 - trainer - INFO - {
  "train_loss": 330.23492431640625
}
2022-10-24 15:19:36,127 - trainer - INFO - start training epoch 143
2022-10-24 15:19:36,127 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,127 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,128 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,137 - trainer - INFO - 
*****************[epoch: 143, global step: 144] eval training set at end of epoch***************
2022-10-24 15:19:36,138 - trainer - INFO - {
  "train_loss": 300.99176025390625
}
2022-10-24 15:19:36,138 - trainer - INFO - start training epoch 144
2022-10-24 15:19:36,138 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,139 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,139 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,146 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval training set based on eval_every=2***************
2022-10-24 15:19:36,147 - trainer - INFO - {
  "train_loss": 287.0569152832031
}
2022-10-24 15:19:36,153 - trainer - INFO - 
*****************[epoch: 144, global step: 144] eval development set based on eval_every=2***************
2022-10-24 15:19:36,153 - trainer - INFO - {
  "dev_loss": 247.38050842285156,
  "dev_best_score_for_loss": -247.38050842285156
}
2022-10-24 15:19:36,154 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,155 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,155 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,155 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_138
2022-10-24 15:19:36,157 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,160 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,160 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,160 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,161 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,161 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_144
2022-10-24 15:19:36,166 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_144
2022-10-24 15:19:36,167 - trainer - INFO - 
*****************[epoch: 144, global step: 145] eval training set at end of epoch***************
2022-10-24 15:19:36,169 - trainer - INFO - {
  "train_loss": 273.1220703125
}
2022-10-24 15:19:36,169 - trainer - INFO - start training epoch 145
2022-10-24 15:19:36,172 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,173 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,173 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,182 - trainer - INFO - 
*****************[epoch: 145, global step: 146] eval training set at end of epoch***************
2022-10-24 15:19:36,183 - trainer - INFO - {
  "train_loss": 247.38052368164062
}
2022-10-24 15:19:36,183 - trainer - INFO - start training epoch 146
2022-10-24 15:19:36,183 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,184 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,184 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,191 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval training set based on eval_every=2***************
2022-10-24 15:19:36,192 - trainer - INFO - {
  "train_loss": 235.3242645263672
}
2022-10-24 15:19:36,199 - trainer - INFO - 
*****************[epoch: 146, global step: 146] eval development set based on eval_every=2***************
2022-10-24 15:19:36,200 - trainer - INFO - {
  "dev_loss": 200.32876586914062,
  "dev_best_score_for_loss": -200.32876586914062
}
2022-10-24 15:19:36,200 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,201 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,201 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,202 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_140
2022-10-24 15:19:36,204 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,207 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,207 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,207 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,208 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,208 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_146
2022-10-24 15:19:36,213 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_146
2022-10-24 15:19:36,214 - trainer - INFO - 
*****************[epoch: 146, global step: 147] eval training set at end of epoch***************
2022-10-24 15:19:36,215 - trainer - INFO - {
  "train_loss": 223.26800537109375
}
2022-10-24 15:19:36,215 - trainer - INFO - start training epoch 147
2022-10-24 15:19:36,219 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,219 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,219 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,230 - trainer - INFO - 
*****************[epoch: 147, global step: 148] eval training set at end of epoch***************
2022-10-24 15:19:36,230 - trainer - INFO - {
  "train_loss": 200.32876586914062
}
2022-10-24 15:19:36,231 - trainer - INFO - start training epoch 148
2022-10-24 15:19:36,231 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,232 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,232 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,239 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval training set based on eval_every=2***************
2022-10-24 15:19:36,239 - trainer - INFO - {
  "train_loss": 189.73700714111328
}
2022-10-24 15:19:36,245 - trainer - INFO - 
*****************[epoch: 148, global step: 148] eval development set based on eval_every=2***************
2022-10-24 15:19:36,246 - trainer - INFO - {
  "dev_loss": 159.6835479736328,
  "dev_best_score_for_loss": -159.6835479736328
}
2022-10-24 15:19:36,246 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,247 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,248 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,248 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_142
2022-10-24 15:19:36,249 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,252 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,252 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,252 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,253 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,253 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_148
2022-10-24 15:19:36,257 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_148
2022-10-24 15:19:36,258 - trainer - INFO - 
*****************[epoch: 148, global step: 149] eval training set at end of epoch***************
2022-10-24 15:19:36,258 - trainer - INFO - {
  "train_loss": 179.14524841308594
}
2022-10-24 15:19:36,259 - trainer - INFO - start training epoch 149
2022-10-24 15:19:36,260 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,260 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,261 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,274 - trainer - INFO - 
*****************[epoch: 149, global step: 150] eval training set at end of epoch***************
2022-10-24 15:19:36,274 - trainer - INFO - {
  "train_loss": 159.68356323242188
}
2022-10-24 15:19:36,275 - trainer - INFO - start training epoch 150
2022-10-24 15:19:36,276 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,276 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,276 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,285 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval training set based on eval_every=2***************
2022-10-24 15:19:36,285 - trainer - INFO - {
  "train_loss": 150.53694915771484
}
2022-10-24 15:19:36,291 - trainer - INFO - 
*****************[epoch: 150, global step: 150] eval development set based on eval_every=2***************
2022-10-24 15:19:36,292 - trainer - INFO - {
  "dev_loss": 124.59654235839844,
  "dev_best_score_for_loss": -124.59654235839844
}
2022-10-24 15:19:36,292 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,294 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,294 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,294 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_144
2022-10-24 15:19:36,295 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,299 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,299 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,299 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,300 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,300 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_150
2022-10-24 15:19:36,304 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_150
2022-10-24 15:19:36,305 - trainer - INFO - 
*****************[epoch: 150, global step: 151] eval training set at end of epoch***************
2022-10-24 15:19:36,306 - trainer - INFO - {
  "train_loss": 141.3903350830078
}
2022-10-24 15:19:36,307 - trainer - INFO - start training epoch 151
2022-10-24 15:19:36,308 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,308 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,309 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,318 - trainer - INFO - 
*****************[epoch: 151, global step: 152] eval training set at end of epoch***************
2022-10-24 15:19:36,319 - trainer - INFO - {
  "train_loss": 124.59654998779297
}
2022-10-24 15:19:36,319 - trainer - INFO - start training epoch 152
2022-10-24 15:19:36,319 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,320 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,320 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,329 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval training set based on eval_every=2***************
2022-10-24 15:19:36,330 - trainer - INFO - {
  "train_loss": 117.00860595703125
}
2022-10-24 15:19:36,336 - trainer - INFO - 
*****************[epoch: 152, global step: 152] eval development set based on eval_every=2***************
2022-10-24 15:19:36,336 - trainer - INFO - {
  "dev_loss": 95.36046600341797,
  "dev_best_score_for_loss": -95.36046600341797
}
2022-10-24 15:19:36,337 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,339 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,339 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,339 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_146
2022-10-24 15:19:36,340 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,344 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,344 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,344 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,345 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,345 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_152
2022-10-24 15:19:36,349 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_152
2022-10-24 15:19:36,350 - trainer - INFO - 
*****************[epoch: 152, global step: 153] eval training set at end of epoch***************
2022-10-24 15:19:36,350 - trainer - INFO - {
  "train_loss": 109.42066192626953
}
2022-10-24 15:19:36,351 - trainer - INFO - start training epoch 153
2022-10-24 15:19:36,351 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,352 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,353 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,366 - trainer - INFO - 
*****************[epoch: 153, global step: 154] eval training set at end of epoch***************
2022-10-24 15:19:36,367 - trainer - INFO - {
  "train_loss": 95.36046600341797
}
2022-10-24 15:19:36,367 - trainer - INFO - start training epoch 154
2022-10-24 15:19:36,369 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,369 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,370 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,378 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval training set based on eval_every=2***************
2022-10-24 15:19:36,378 - trainer - INFO - {
  "train_loss": 88.96530151367188
}
2022-10-24 15:19:36,384 - trainer - INFO - 
*****************[epoch: 154, global step: 154] eval development set based on eval_every=2***************
2022-10-24 15:19:36,384 - trainer - INFO - {
  "dev_loss": 71.27505493164062,
  "dev_best_score_for_loss": -71.27505493164062
}
2022-10-24 15:19:36,385 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,387 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,387 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,387 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_148
2022-10-24 15:19:36,388 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,391 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,391 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,392 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,393 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,393 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_154
2022-10-24 15:19:36,397 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_154
2022-10-24 15:19:36,399 - trainer - INFO - 
*****************[epoch: 154, global step: 155] eval training set at end of epoch***************
2022-10-24 15:19:36,400 - trainer - INFO - {
  "train_loss": 82.57013702392578
}
2022-10-24 15:19:36,403 - trainer - INFO - start training epoch 155
2022-10-24 15:19:36,403 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,403 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,404 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,413 - trainer - INFO - 
*****************[epoch: 155, global step: 156] eval training set at end of epoch***************
2022-10-24 15:19:36,413 - trainer - INFO - {
  "train_loss": 71.27505493164062
}
2022-10-24 15:19:36,415 - trainer - INFO - start training epoch 156
2022-10-24 15:19:36,416 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,416 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,416 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,424 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval training set based on eval_every=2***************
2022-10-24 15:19:36,424 - trainer - INFO - {
  "train_loss": 66.18603324890137
}
2022-10-24 15:19:36,431 - trainer - INFO - 
*****************[epoch: 156, global step: 156] eval development set based on eval_every=2***************
2022-10-24 15:19:36,432 - trainer - INFO - {
  "dev_loss": 51.86778259277344,
  "dev_best_score_for_loss": -51.86778259277344
}
2022-10-24 15:19:36,433 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,437 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,437 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,437 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_150
2022-10-24 15:19:36,440 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,444 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,444 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,444 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,446 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,446 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_156
2022-10-24 15:19:36,451 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_156
2022-10-24 15:19:36,452 - trainer - INFO - 
*****************[epoch: 156, global step: 157] eval training set at end of epoch***************
2022-10-24 15:19:36,452 - trainer - INFO - {
  "train_loss": 61.09701156616211
}
2022-10-24 15:19:36,453 - trainer - INFO - start training epoch 157
2022-10-24 15:19:36,453 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,453 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,454 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,463 - trainer - INFO - 
*****************[epoch: 157, global step: 158] eval training set at end of epoch***************
2022-10-24 15:19:36,463 - trainer - INFO - {
  "train_loss": 51.867774963378906
}
2022-10-24 15:19:36,464 - trainer - INFO - start training epoch 158
2022-10-24 15:19:36,464 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,465 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,465 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,471 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval training set based on eval_every=2***************
2022-10-24 15:19:36,471 - trainer - INFO - {
  "train_loss": 47.843563079833984
}
2022-10-24 15:19:36,478 - trainer - INFO - 
*****************[epoch: 158, global step: 158] eval development set based on eval_every=2***************
2022-10-24 15:19:36,479 - trainer - INFO - {
  "dev_loss": 36.6805305480957,
  "dev_best_score_for_loss": -36.6805305480957
}
2022-10-24 15:19:36,481 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,485 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,486 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,486 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_152
2022-10-24 15:19:36,488 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,493 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,493 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,493 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,494 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,495 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_158
2022-10-24 15:19:36,502 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_158
2022-10-24 15:19:36,503 - trainer - INFO - 
*****************[epoch: 158, global step: 159] eval training set at end of epoch***************
2022-10-24 15:19:36,503 - trainer - INFO - {
  "train_loss": 43.81935119628906
}
2022-10-24 15:19:36,503 - trainer - INFO - start training epoch 159
2022-10-24 15:19:36,504 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,504 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,504 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,514 - trainer - INFO - 
*****************[epoch: 159, global step: 160] eval training set at end of epoch***************
2022-10-24 15:19:36,515 - trainer - INFO - {
  "train_loss": 36.68053436279297
}
2022-10-24 15:19:36,515 - trainer - INFO - start training epoch 160
2022-10-24 15:19:36,516 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,516 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,516 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,525 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval training set based on eval_every=2***************
2022-10-24 15:19:36,528 - trainer - INFO - {
  "train_loss": 33.481218338012695
}
2022-10-24 15:19:36,534 - trainer - INFO - 
*****************[epoch: 160, global step: 160] eval development set based on eval_every=2***************
2022-10-24 15:19:36,535 - trainer - INFO - {
  "dev_loss": 24.807693481445312,
  "dev_best_score_for_loss": -24.807693481445312
}
2022-10-24 15:19:36,535 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,537 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,537 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,537 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_154
2022-10-24 15:19:36,540 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,545 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,545 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,545 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,546 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,546 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_160
2022-10-24 15:19:36,551 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_160
2022-10-24 15:19:36,552 - trainer - INFO - 
*****************[epoch: 160, global step: 161] eval training set at end of epoch***************
2022-10-24 15:19:36,552 - trainer - INFO - {
  "train_loss": 30.281902313232422
}
2022-10-24 15:19:36,552 - trainer - INFO - start training epoch 161
2022-10-24 15:19:36,552 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,553 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,553 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,565 - trainer - INFO - 
*****************[epoch: 161, global step: 162] eval training set at end of epoch***************
2022-10-24 15:19:36,565 - trainer - INFO - {
  "train_loss": 24.807693481445312
}
2022-10-24 15:19:36,566 - trainer - INFO - start training epoch 162
2022-10-24 15:19:36,566 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,566 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,567 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,574 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval training set based on eval_every=2***************
2022-10-24 15:19:36,575 - trainer - INFO - {
  "train_loss": 22.451977729797363
}
2022-10-24 15:19:36,580 - trainer - INFO - 
*****************[epoch: 162, global step: 162] eval development set based on eval_every=2***************
2022-10-24 15:19:36,580 - trainer - INFO - {
  "dev_loss": 15.980420112609863,
  "dev_best_score_for_loss": -15.980420112609863
}
2022-10-24 15:19:36,581 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,582 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,582 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,583 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_156
2022-10-24 15:19:36,584 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,587 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,587 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,588 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,589 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,589 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_162
2022-10-24 15:19:36,593 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_162
2022-10-24 15:19:36,594 - trainer - INFO - 
*****************[epoch: 162, global step: 163] eval training set at end of epoch***************
2022-10-24 15:19:36,594 - trainer - INFO - {
  "train_loss": 20.096261978149414
}
2022-10-24 15:19:36,595 - trainer - INFO - start training epoch 163
2022-10-24 15:19:36,595 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,595 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,596 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,606 - trainer - INFO - 
*****************[epoch: 163, global step: 164] eval training set at end of epoch***************
2022-10-24 15:19:36,607 - trainer - INFO - {
  "train_loss": 15.980420112609863
}
2022-10-24 15:19:36,607 - trainer - INFO - start training epoch 164
2022-10-24 15:19:36,607 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,607 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,608 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,616 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval training set based on eval_every=2***************
2022-10-24 15:19:36,617 - trainer - INFO - {
  "train_loss": 14.2778902053833
}
2022-10-24 15:19:36,623 - trainer - INFO - 
*****************[epoch: 164, global step: 164] eval development set based on eval_every=2***************
2022-10-24 15:19:36,623 - trainer - INFO - {
  "dev_loss": 9.741147994995117,
  "dev_best_score_for_loss": -9.741147994995117
}
2022-10-24 15:19:36,624 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,625 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,625 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,625 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_158
2022-10-24 15:19:36,627 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,631 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,632 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,633 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,635 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,638 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_164
2022-10-24 15:19:36,642 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_164
2022-10-24 15:19:36,643 - trainer - INFO - 
*****************[epoch: 164, global step: 165] eval training set at end of epoch***************
2022-10-24 15:19:36,643 - trainer - INFO - {
  "train_loss": 12.575360298156738
}
2022-10-24 15:19:36,644 - trainer - INFO - start training epoch 165
2022-10-24 15:19:36,644 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,644 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,645 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,652 - trainer - INFO - 
*****************[epoch: 165, global step: 166] eval training set at end of epoch***************
2022-10-24 15:19:36,653 - trainer - INFO - {
  "train_loss": 9.741147994995117
}
2022-10-24 15:19:36,653 - trainer - INFO - start training epoch 166
2022-10-24 15:19:36,653 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,654 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,654 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,661 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval training set based on eval_every=2***************
2022-10-24 15:19:36,661 - trainer - INFO - {
  "train_loss": 8.536260604858398
}
2022-10-24 15:19:36,668 - trainer - INFO - 
*****************[epoch: 166, global step: 166] eval development set based on eval_every=2***************
2022-10-24 15:19:36,668 - trainer - INFO - {
  "dev_loss": 5.443458557128906,
  "dev_best_score_for_loss": -5.443458557128906
}
2022-10-24 15:19:36,669 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,670 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,671 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,671 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_160
2022-10-24 15:19:36,672 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,676 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,676 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,676 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,677 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,678 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_166
2022-10-24 15:19:36,684 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_166
2022-10-24 15:19:36,684 - trainer - INFO - 
*****************[epoch: 166, global step: 167] eval training set at end of epoch***************
2022-10-24 15:19:36,685 - trainer - INFO - {
  "train_loss": 7.33137321472168
}
2022-10-24 15:19:36,685 - trainer - INFO - start training epoch 167
2022-10-24 15:19:36,685 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,686 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,686 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,696 - trainer - INFO - 
*****************[epoch: 167, global step: 168] eval training set at end of epoch***************
2022-10-24 15:19:36,696 - trainer - INFO - {
  "train_loss": 5.443458557128906
}
2022-10-24 15:19:36,697 - trainer - INFO - start training epoch 168
2022-10-24 15:19:36,697 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,698 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,698 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,704 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval training set based on eval_every=2***************
2022-10-24 15:19:36,705 - trainer - INFO - {
  "train_loss": 4.693307638168335
}
2022-10-24 15:19:36,711 - trainer - INFO - 
*****************[epoch: 168, global step: 168] eval development set based on eval_every=2***************
2022-10-24 15:19:36,715 - trainer - INFO - {
  "dev_loss": 2.722292423248291,
  "dev_best_score_for_loss": -2.722292423248291
}
2022-10-24 15:19:36,715 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,716 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,717 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,717 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_162
2022-10-24 15:19:36,718 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,721 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,722 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,722 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,723 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,723 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_168
2022-10-24 15:19:36,728 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_168
2022-10-24 15:19:36,729 - trainer - INFO - 
*****************[epoch: 168, global step: 169] eval training set at end of epoch***************
2022-10-24 15:19:36,729 - trainer - INFO - {
  "train_loss": 3.9431567192077637
}
2022-10-24 15:19:36,730 - trainer - INFO - start training epoch 169
2022-10-24 15:19:36,730 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,730 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,731 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,738 - trainer - INFO - 
*****************[epoch: 169, global step: 170] eval training set at end of epoch***************
2022-10-24 15:19:36,738 - trainer - INFO - {
  "train_loss": 2.722292423248291
}
2022-10-24 15:19:36,738 - trainer - INFO - start training epoch 170
2022-10-24 15:19:36,739 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,739 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,740 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,747 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval training set based on eval_every=2***************
2022-10-24 15:19:36,748 - trainer - INFO - {
  "train_loss": 2.283798635005951
}
2022-10-24 15:19:36,753 - trainer - INFO - 
*****************[epoch: 170, global step: 170] eval development set based on eval_every=2***************
2022-10-24 15:19:36,753 - trainer - INFO - {
  "dev_loss": 1.2025974988937378,
  "dev_best_score_for_loss": -1.2025974988937378
}
2022-10-24 15:19:36,754 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,755 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,755 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,756 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_164
2022-10-24 15:19:36,759 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,765 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,765 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,765 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,766 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,766 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_170
2022-10-24 15:19:36,772 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_170
2022-10-24 15:19:36,773 - trainer - INFO - 
*****************[epoch: 170, global step: 171] eval training set at end of epoch***************
2022-10-24 15:19:36,773 - trainer - INFO - {
  "train_loss": 1.8453048467636108
}
2022-10-24 15:19:36,774 - trainer - INFO - start training epoch 171
2022-10-24 15:19:36,774 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,774 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,774 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,783 - trainer - INFO - 
*****************[epoch: 171, global step: 172] eval training set at end of epoch***************
2022-10-24 15:19:36,784 - trainer - INFO - {
  "train_loss": 1.2025974988937378
}
2022-10-24 15:19:36,784 - trainer - INFO - start training epoch 172
2022-10-24 15:19:36,784 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,784 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,785 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,792 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval training set based on eval_every=2***************
2022-10-24 15:19:36,792 - trainer - INFO - {
  "train_loss": 0.9613200128078461
}
2022-10-24 15:19:36,798 - trainer - INFO - 
*****************[epoch: 172, global step: 172] eval development set based on eval_every=2***************
2022-10-24 15:19:36,799 - trainer - INFO - {
  "dev_loss": 0.4480571448802948,
  "dev_best_score_for_loss": -0.4480571448802948
}
2022-10-24 15:19:36,799 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,800 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,801 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,801 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_166
2022-10-24 15:19:36,802 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,807 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,807 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,807 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,808 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,808 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_172
2022-10-24 15:19:36,813 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_172
2022-10-24 15:19:36,814 - trainer - INFO - 
*****************[epoch: 172, global step: 173] eval training set at end of epoch***************
2022-10-24 15:19:36,814 - trainer - INFO - {
  "train_loss": 0.7200425267219543
}
2022-10-24 15:19:36,815 - trainer - INFO - start training epoch 173
2022-10-24 15:19:36,815 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,816 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,816 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,827 - trainer - INFO - 
*****************[epoch: 173, global step: 174] eval training set at end of epoch***************
2022-10-24 15:19:36,827 - trainer - INFO - {
  "train_loss": 0.4480571448802948
}
2022-10-24 15:19:36,828 - trainer - INFO - start training epoch 174
2022-10-24 15:19:36,828 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,828 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,829 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,836 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval training set based on eval_every=2***************
2022-10-24 15:19:36,837 - trainer - INFO - {
  "train_loss": 0.37305159866809845
}
2022-10-24 15:19:36,843 - trainer - INFO - 
*****************[epoch: 174, global step: 174] eval development set based on eval_every=2***************
2022-10-24 15:19:36,844 - trainer - INFO - {
  "dev_loss": 0.22660189867019653,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:36,846 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:36,851 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,851 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,851 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_168
2022-10-24 15:19:36,853 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:36,858 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:36,858 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:36,858 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,859 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:36,860 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_174
2022-10-24 15:19:36,865 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_174
2022-10-24 15:19:36,866 - trainer - INFO - 
*****************[epoch: 174, global step: 175] eval training set at end of epoch***************
2022-10-24 15:19:36,869 - trainer - INFO - {
  "train_loss": 0.2980460524559021
}
2022-10-24 15:19:36,870 - trainer - INFO - start training epoch 175
2022-10-24 15:19:36,871 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,871 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,872 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,882 - trainer - INFO - 
*****************[epoch: 175, global step: 176] eval training set at end of epoch***************
2022-10-24 15:19:36,883 - trainer - INFO - {
  "train_loss": 0.22660191357135773
}
2022-10-24 15:19:36,883 - trainer - INFO - start training epoch 176
2022-10-24 15:19:36,883 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,884 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,884 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,892 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval training set based on eval_every=2***************
2022-10-24 15:19:36,893 - trainer - INFO - {
  "train_loss": 0.24773740023374557
}
2022-10-24 15:19:36,901 - trainer - INFO - 
*****************[epoch: 176, global step: 176] eval development set based on eval_every=2***************
2022-10-24 15:19:36,901 - trainer - INFO - {
  "dev_loss": 0.35355448722839355,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:36,902 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:36,902 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,904 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,904 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,904 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_170
2022-10-24 15:19:36,905 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_176
2022-10-24 15:19:36,909 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_176
2022-10-24 15:19:36,914 - trainer - INFO - 
*****************[epoch: 176, global step: 177] eval training set at end of epoch***************
2022-10-24 15:19:36,915 - trainer - INFO - {
  "train_loss": 0.2688728868961334
}
2022-10-24 15:19:36,916 - trainer - INFO - start training epoch 177
2022-10-24 15:19:36,920 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,920 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,921 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,931 - trainer - INFO - 
*****************[epoch: 177, global step: 178] eval training set at end of epoch***************
2022-10-24 15:19:36,932 - trainer - INFO - {
  "train_loss": 0.35355448722839355
}
2022-10-24 15:19:36,932 - trainer - INFO - start training epoch 178
2022-10-24 15:19:36,933 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,933 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,933 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,943 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval training set based on eval_every=2***************
2022-10-24 15:19:36,943 - trainer - INFO - {
  "train_loss": 0.4083346575498581
}
2022-10-24 15:19:36,951 - trainer - INFO - 
*****************[epoch: 178, global step: 178] eval development set based on eval_every=2***************
2022-10-24 15:19:36,951 - trainer - INFO - {
  "dev_loss": 0.6221757531166077,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:36,952 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:36,952 - trainer - INFO -   patience: 200
2022-10-24 15:19:36,953 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:36,954 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:36,954 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_172
2022-10-24 15:19:36,955 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_178
2022-10-24 15:19:36,964 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_178
2022-10-24 15:19:36,965 - trainer - INFO - 
*****************[epoch: 178, global step: 179] eval training set at end of epoch***************
2022-10-24 15:19:36,966 - trainer - INFO - {
  "train_loss": 0.46311482787132263
}
2022-10-24 15:19:36,966 - trainer - INFO - start training epoch 179
2022-10-24 15:19:36,966 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,967 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,967 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,979 - trainer - INFO - 
*****************[epoch: 179, global step: 180] eval training set at end of epoch***************
2022-10-24 15:19:36,979 - trainer - INFO - {
  "train_loss": 0.6221756935119629
}
2022-10-24 15:19:36,980 - trainer - INFO - start training epoch 180
2022-10-24 15:19:36,980 - trainer - INFO - training using device=cuda
2022-10-24 15:19:36,980 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:36,981 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:36,989 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval training set based on eval_every=2***************
2022-10-24 15:19:36,990 - trainer - INFO - {
  "train_loss": 0.6991218626499176
}
2022-10-24 15:19:36,998 - trainer - INFO - 
*****************[epoch: 180, global step: 180] eval development set based on eval_every=2***************
2022-10-24 15:19:36,999 - trainer - INFO - {
  "dev_loss": 0.9257960915565491,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:36,999 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:37,000 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,001 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,001 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,002 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_174
2022-10-24 15:19:37,003 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_180
2022-10-24 15:19:37,008 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_180
2022-10-24 15:19:37,009 - trainer - INFO - 
*****************[epoch: 180, global step: 181] eval training set at end of epoch***************
2022-10-24 15:19:37,009 - trainer - INFO - {
  "train_loss": 0.7760680317878723
}
2022-10-24 15:19:37,010 - trainer - INFO - start training epoch 181
2022-10-24 15:19:37,010 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,010 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,011 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,020 - trainer - INFO - 
*****************[epoch: 181, global step: 182] eval training set at end of epoch***************
2022-10-24 15:19:37,021 - trainer - INFO - {
  "train_loss": 0.9257960915565491
}
2022-10-24 15:19:37,021 - trainer - INFO - start training epoch 182
2022-10-24 15:19:37,021 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,021 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,022 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,030 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval training set based on eval_every=2***************
2022-10-24 15:19:37,031 - trainer - INFO - {
  "train_loss": 1.005790799856186
}
2022-10-24 15:19:37,041 - trainer - INFO - 
*****************[epoch: 182, global step: 182] eval development set based on eval_every=2***************
2022-10-24 15:19:37,041 - trainer - INFO - {
  "dev_loss": 1.2170614004135132,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,042 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:19:37,042 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,044 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,044 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,045 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_176
2022-10-24 15:19:37,046 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_182
2022-10-24 15:19:37,051 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_182
2022-10-24 15:19:37,052 - trainer - INFO - 
*****************[epoch: 182, global step: 183] eval training set at end of epoch***************
2022-10-24 15:19:37,052 - trainer - INFO - {
  "train_loss": 1.0857855081558228
}
2022-10-24 15:19:37,053 - trainer - INFO - start training epoch 183
2022-10-24 15:19:37,053 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,054 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,054 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,064 - trainer - INFO - 
*****************[epoch: 183, global step: 184] eval training set at end of epoch***************
2022-10-24 15:19:37,064 - trainer - INFO - {
  "train_loss": 1.2170614004135132
}
2022-10-24 15:19:37,065 - trainer - INFO - start training epoch 184
2022-10-24 15:19:37,066 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,067 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,067 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,074 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval training set based on eval_every=2***************
2022-10-24 15:19:37,074 - trainer - INFO - {
  "train_loss": 1.2747140526771545
}
2022-10-24 15:19:37,080 - trainer - INFO - 
*****************[epoch: 184, global step: 184] eval development set based on eval_every=2***************
2022-10-24 15:19:37,081 - trainer - INFO - {
  "dev_loss": 1.437866449356079,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,082 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:19:37,085 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,086 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,086 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,087 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_178
2022-10-24 15:19:37,088 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_184
2022-10-24 15:19:37,092 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_184
2022-10-24 15:19:37,093 - trainer - INFO - 
*****************[epoch: 184, global step: 185] eval training set at end of epoch***************
2022-10-24 15:19:37,094 - trainer - INFO - {
  "train_loss": 1.332366704940796
}
2022-10-24 15:19:37,094 - trainer - INFO - start training epoch 185
2022-10-24 15:19:37,094 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,095 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,096 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,105 - trainer - INFO - 
*****************[epoch: 185, global step: 186] eval training set at end of epoch***************
2022-10-24 15:19:37,105 - trainer - INFO - {
  "train_loss": 1.437866449356079
}
2022-10-24 15:19:37,106 - trainer - INFO - start training epoch 186
2022-10-24 15:19:37,106 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,106 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,106 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,114 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval training set based on eval_every=2***************
2022-10-24 15:19:37,117 - trainer - INFO - {
  "train_loss": 1.4733580350875854
}
2022-10-24 15:19:37,122 - trainer - INFO - 
*****************[epoch: 186, global step: 186] eval development set based on eval_every=2***************
2022-10-24 15:19:37,123 - trainer - INFO - {
  "dev_loss": 1.5629091262817383,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,123 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:19:37,124 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,125 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,125 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,126 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_180
2022-10-24 15:19:37,127 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_186
2022-10-24 15:19:37,133 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_186
2022-10-24 15:19:37,134 - trainer - INFO - 
*****************[epoch: 186, global step: 187] eval training set at end of epoch***************
2022-10-24 15:19:37,134 - trainer - INFO - {
  "train_loss": 1.5088496208190918
}
2022-10-24 15:19:37,135 - trainer - INFO - start training epoch 187
2022-10-24 15:19:37,135 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,135 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,136 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,148 - trainer - INFO - 
*****************[epoch: 187, global step: 188] eval training set at end of epoch***************
2022-10-24 15:19:37,148 - trainer - INFO - {
  "train_loss": 1.5629091262817383
}
2022-10-24 15:19:37,149 - trainer - INFO - start training epoch 188
2022-10-24 15:19:37,149 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,149 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,150 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,157 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval training set based on eval_every=2***************
2022-10-24 15:19:37,159 - trainer - INFO - {
  "train_loss": 1.5814204216003418
}
2022-10-24 15:19:37,165 - trainer - INFO - 
*****************[epoch: 188, global step: 188] eval development set based on eval_every=2***************
2022-10-24 15:19:37,165 - trainer - INFO - {
  "dev_loss": 1.6069245338439941,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,166 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:19:37,166 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,167 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,167 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,168 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_182
2022-10-24 15:19:37,169 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_188
2022-10-24 15:19:37,173 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_188
2022-10-24 15:19:37,176 - trainer - INFO - 
*****************[epoch: 188, global step: 189] eval training set at end of epoch***************
2022-10-24 15:19:37,176 - trainer - INFO - {
  "train_loss": 1.5999317169189453
}
2022-10-24 15:19:37,177 - trainer - INFO - start training epoch 189
2022-10-24 15:19:37,177 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,177 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,178 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,185 - trainer - INFO - 
*****************[epoch: 189, global step: 190] eval training set at end of epoch***************
2022-10-24 15:19:37,186 - trainer - INFO - {
  "train_loss": 1.6069245338439941
}
2022-10-24 15:19:37,186 - trainer - INFO - start training epoch 190
2022-10-24 15:19:37,186 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,187 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,187 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,194 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval training set based on eval_every=2***************
2022-10-24 15:19:37,195 - trainer - INFO - {
  "train_loss": 1.6042201519012451
}
2022-10-24 15:19:37,201 - trainer - INFO - 
*****************[epoch: 190, global step: 190] eval development set based on eval_every=2***************
2022-10-24 15:19:37,202 - trainer - INFO - {
  "dev_loss": 1.5795024633407593,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,202 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:19:37,203 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,204 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,204 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,204 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_184
2022-10-24 15:19:37,207 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_190
2022-10-24 15:19:37,211 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_190
2022-10-24 15:19:37,212 - trainer - INFO - 
*****************[epoch: 190, global step: 191] eval training set at end of epoch***************
2022-10-24 15:19:37,212 - trainer - INFO - {
  "train_loss": 1.601515769958496
}
2022-10-24 15:19:37,213 - trainer - INFO - start training epoch 191
2022-10-24 15:19:37,213 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,213 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,214 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,221 - trainer - INFO - 
*****************[epoch: 191, global step: 192] eval training set at end of epoch***************
2022-10-24 15:19:37,222 - trainer - INFO - {
  "train_loss": 1.5795024633407593
}
2022-10-24 15:19:37,225 - trainer - INFO - start training epoch 192
2022-10-24 15:19:37,225 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,226 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,226 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,237 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval training set based on eval_every=2***************
2022-10-24 15:19:37,238 - trainer - INFO - {
  "train_loss": 1.5582358241081238
}
2022-10-24 15:19:37,245 - trainer - INFO - 
*****************[epoch: 192, global step: 192] eval development set based on eval_every=2***************
2022-10-24 15:19:37,245 - trainer - INFO - {
  "dev_loss": 1.4879040718078613,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,246 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:19:37,247 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,248 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,248 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,249 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_186
2022-10-24 15:19:37,250 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_192
2022-10-24 15:19:37,255 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_192
2022-10-24 15:19:37,256 - trainer - INFO - 
*****************[epoch: 192, global step: 193] eval training set at end of epoch***************
2022-10-24 15:19:37,256 - trainer - INFO - {
  "train_loss": 1.5369691848754883
}
2022-10-24 15:19:37,257 - trainer - INFO - start training epoch 193
2022-10-24 15:19:37,257 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,257 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,258 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,272 - trainer - INFO - 
*****************[epoch: 193, global step: 194] eval training set at end of epoch***************
2022-10-24 15:19:37,272 - trainer - INFO - {
  "train_loss": 1.4879039525985718
}
2022-10-24 15:19:37,273 - trainer - INFO - start training epoch 194
2022-10-24 15:19:37,273 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,273 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,274 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,282 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval training set based on eval_every=2***************
2022-10-24 15:19:37,284 - trainer - INFO - {
  "train_loss": 1.457261085510254
}
2022-10-24 15:19:37,292 - trainer - INFO - 
*****************[epoch: 194, global step: 194] eval development set based on eval_every=2***************
2022-10-24 15:19:37,292 - trainer - INFO - {
  "dev_loss": 1.3547425270080566,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,293 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:19:37,294 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,296 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,296 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,296 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_188
2022-10-24 15:19:37,298 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_194
2022-10-24 15:19:37,304 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_194
2022-10-24 15:19:37,305 - trainer - INFO - 
*****************[epoch: 194, global step: 195] eval training set at end of epoch***************
2022-10-24 15:19:37,306 - trainer - INFO - {
  "train_loss": 1.426618218421936
}
2022-10-24 15:19:37,307 - trainer - INFO - start training epoch 195
2022-10-24 15:19:37,307 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,307 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,308 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,318 - trainer - INFO - 
*****************[epoch: 195, global step: 196] eval training set at end of epoch***************
2022-10-24 15:19:37,318 - trainer - INFO - {
  "train_loss": 1.354742407798767
}
2022-10-24 15:19:37,319 - trainer - INFO - start training epoch 196
2022-10-24 15:19:37,319 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,319 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,320 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,330 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval training set based on eval_every=2***************
2022-10-24 15:19:37,331 - trainer - INFO - {
  "train_loss": 1.3182427287101746
}
2022-10-24 15:19:37,337 - trainer - INFO - 
*****************[epoch: 196, global step: 196] eval development set based on eval_every=2***************
2022-10-24 15:19:37,337 - trainer - INFO - {
  "dev_loss": 1.2017790079116821,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,338 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:19:37,338 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,339 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,339 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,340 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_190
2022-10-24 15:19:37,341 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_196
2022-10-24 15:19:37,347 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_196
2022-10-24 15:19:37,350 - trainer - INFO - 
*****************[epoch: 196, global step: 197] eval training set at end of epoch***************
2022-10-24 15:19:37,351 - trainer - INFO - {
  "train_loss": 1.281743049621582
}
2022-10-24 15:19:37,351 - trainer - INFO - start training epoch 197
2022-10-24 15:19:37,351 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,352 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,352 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,361 - trainer - INFO - 
*****************[epoch: 197, global step: 198] eval training set at end of epoch***************
2022-10-24 15:19:37,362 - trainer - INFO - {
  "train_loss": 1.2017790079116821
}
2022-10-24 15:19:37,363 - trainer - INFO - start training epoch 198
2022-10-24 15:19:37,363 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,363 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,364 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,370 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval training set based on eval_every=2***************
2022-10-24 15:19:37,370 - trainer - INFO - {
  "train_loss": 1.1607586145401
}
2022-10-24 15:19:37,376 - trainer - INFO - 
*****************[epoch: 198, global step: 198] eval development set based on eval_every=2***************
2022-10-24 15:19:37,377 - trainer - INFO - {
  "dev_loss": 1.0400885343551636,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,377 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:19:37,378 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,379 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,379 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,380 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_192
2022-10-24 15:19:37,381 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_198
2022-10-24 15:19:37,384 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_198
2022-10-24 15:19:37,385 - trainer - INFO - 
*****************[epoch: 198, global step: 199] eval training set at end of epoch***************
2022-10-24 15:19:37,385 - trainer - INFO - {
  "train_loss": 1.119738221168518
}
2022-10-24 15:19:37,386 - trainer - INFO - start training epoch 199
2022-10-24 15:19:37,386 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,386 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,386 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,399 - trainer - INFO - 
*****************[epoch: 199, global step: 200] eval training set at end of epoch***************
2022-10-24 15:19:37,399 - trainer - INFO - {
  "train_loss": 1.0400885343551636
}
2022-10-24 15:19:37,399 - trainer - INFO - start training epoch 200
2022-10-24 15:19:37,400 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,400 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,400 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,408 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval training set based on eval_every=2***************
2022-10-24 15:19:37,408 - trainer - INFO - {
  "train_loss": 0.9992815554141998
}
2022-10-24 15:19:37,416 - trainer - INFO - 
*****************[epoch: 200, global step: 200] eval development set based on eval_every=2***************
2022-10-24 15:19:37,417 - trainer - INFO - {
  "dev_loss": 0.8803084492683411,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,417 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:19:37,418 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,418 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,419 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,419 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_194
2022-10-24 15:19:37,420 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_200
2022-10-24 15:19:37,424 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_200
2022-10-24 15:19:37,425 - trainer - INFO - 
*****************[epoch: 200, global step: 201] eval training set at end of epoch***************
2022-10-24 15:19:37,426 - trainer - INFO - {
  "train_loss": 0.9584745764732361
}
2022-10-24 15:19:37,427 - trainer - INFO - start training epoch 201
2022-10-24 15:19:37,427 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,427 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,428 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,435 - trainer - INFO - 
*****************[epoch: 201, global step: 202] eval training set at end of epoch***************
2022-10-24 15:19:37,435 - trainer - INFO - {
  "train_loss": 0.8803085684776306
}
2022-10-24 15:19:37,436 - trainer - INFO - start training epoch 202
2022-10-24 15:19:37,436 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,436 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,437 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,446 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval training set based on eval_every=2***************
2022-10-24 15:19:37,446 - trainer - INFO - {
  "train_loss": 0.8433462381362915
}
2022-10-24 15:19:37,454 - trainer - INFO - 
*****************[epoch: 202, global step: 202] eval development set based on eval_every=2***************
2022-10-24 15:19:37,455 - trainer - INFO - {
  "dev_loss": 0.7343109250068665,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,455 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:19:37,456 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,457 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,457 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,457 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_196
2022-10-24 15:19:37,459 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_202
2022-10-24 15:19:37,465 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_202
2022-10-24 15:19:37,466 - trainer - INFO - 
*****************[epoch: 202, global step: 203] eval training set at end of epoch***************
2022-10-24 15:19:37,466 - trainer - INFO - {
  "train_loss": 0.8063839077949524
}
2022-10-24 15:19:37,466 - trainer - INFO - start training epoch 203
2022-10-24 15:19:37,466 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,467 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,467 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,475 - trainer - INFO - 
*****************[epoch: 203, global step: 204] eval training set at end of epoch***************
2022-10-24 15:19:37,475 - trainer - INFO - {
  "train_loss": 0.7343109250068665
}
2022-10-24 15:19:37,476 - trainer - INFO - start training epoch 204
2022-10-24 15:19:37,476 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,476 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,477 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,483 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval training set based on eval_every=2***************
2022-10-24 15:19:37,485 - trainer - INFO - {
  "train_loss": 0.70137819647789
}
2022-10-24 15:19:37,492 - trainer - INFO - 
*****************[epoch: 204, global step: 204] eval development set based on eval_every=2***************
2022-10-24 15:19:37,493 - trainer - INFO - {
  "dev_loss": 0.607237696647644,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,493 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:19:37,494 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,495 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,495 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,495 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_198
2022-10-24 15:19:37,497 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_204
2022-10-24 15:19:37,503 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_204
2022-10-24 15:19:37,504 - trainer - INFO - 
*****************[epoch: 204, global step: 205] eval training set at end of epoch***************
2022-10-24 15:19:37,504 - trainer - INFO - {
  "train_loss": 0.6684454679489136
}
2022-10-24 15:19:37,504 - trainer - INFO - start training epoch 205
2022-10-24 15:19:37,504 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,505 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,505 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,512 - trainer - INFO - 
*****************[epoch: 205, global step: 206] eval training set at end of epoch***************
2022-10-24 15:19:37,512 - trainer - INFO - {
  "train_loss": 0.607237696647644
}
2022-10-24 15:19:37,512 - trainer - INFO - start training epoch 206
2022-10-24 15:19:37,513 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,513 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,513 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,522 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval training set based on eval_every=2***************
2022-10-24 15:19:37,523 - trainer - INFO - {
  "train_loss": 0.5785742402076721
}
2022-10-24 15:19:37,532 - trainer - INFO - 
*****************[epoch: 206, global step: 206] eval development set based on eval_every=2***************
2022-10-24 15:19:37,533 - trainer - INFO - {
  "dev_loss": 0.49954336881637573,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,533 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:19:37,534 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,535 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,535 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,535 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_200
2022-10-24 15:19:37,537 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_206
2022-10-24 15:19:37,541 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_206
2022-10-24 15:19:37,542 - trainer - INFO - 
*****************[epoch: 206, global step: 207] eval training set at end of epoch***************
2022-10-24 15:19:37,542 - trainer - INFO - {
  "train_loss": 0.5499107837677002
}
2022-10-24 15:19:37,543 - trainer - INFO - start training epoch 207
2022-10-24 15:19:37,543 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,543 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,544 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,555 - trainer - INFO - 
*****************[epoch: 207, global step: 208] eval training set at end of epoch***************
2022-10-24 15:19:37,555 - trainer - INFO - {
  "train_loss": 0.49954336881637573
}
2022-10-24 15:19:37,556 - trainer - INFO - start training epoch 208
2022-10-24 15:19:37,556 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,556 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,557 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,565 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval training set based on eval_every=2***************
2022-10-24 15:19:37,566 - trainer - INFO - {
  "train_loss": 0.4765944331884384
}
2022-10-24 15:19:37,571 - trainer - INFO - 
*****************[epoch: 208, global step: 208] eval development set based on eval_every=2***************
2022-10-24 15:19:37,572 - trainer - INFO - {
  "dev_loss": 0.4128248989582062,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,572 - trainer - INFO -   no_improve_count: 17
2022-10-24 15:19:37,573 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,574 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,574 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,574 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_202
2022-10-24 15:19:37,576 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_208
2022-10-24 15:19:37,580 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_208
2022-10-24 15:19:37,580 - trainer - INFO - 
*****************[epoch: 208, global step: 209] eval training set at end of epoch***************
2022-10-24 15:19:37,581 - trainer - INFO - {
  "train_loss": 0.4536454975605011
}
2022-10-24 15:19:37,581 - trainer - INFO - start training epoch 209
2022-10-24 15:19:37,581 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,581 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,582 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,589 - trainer - INFO - 
*****************[epoch: 209, global step: 210] eval training set at end of epoch***************
2022-10-24 15:19:37,589 - trainer - INFO - {
  "train_loss": 0.41282492876052856
}
2022-10-24 15:19:37,590 - trainer - INFO - start training epoch 210
2022-10-24 15:19:37,590 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,590 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,590 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,600 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval training set based on eval_every=2***************
2022-10-24 15:19:37,601 - trainer - INFO - {
  "train_loss": 0.39541923999786377
}
2022-10-24 15:19:37,607 - trainer - INFO - 
*****************[epoch: 210, global step: 210] eval development set based on eval_every=2***************
2022-10-24 15:19:37,609 - trainer - INFO - {
  "dev_loss": 0.3470173478126526,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,610 - trainer - INFO -   no_improve_count: 18
2022-10-24 15:19:37,610 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,612 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,612 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,612 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_204
2022-10-24 15:19:37,614 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_210
2022-10-24 15:19:37,618 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_210
2022-10-24 15:19:37,619 - trainer - INFO - 
*****************[epoch: 210, global step: 211] eval training set at end of epoch***************
2022-10-24 15:19:37,619 - trainer - INFO - {
  "train_loss": 0.378013551235199
}
2022-10-24 15:19:37,620 - trainer - INFO - start training epoch 211
2022-10-24 15:19:37,620 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,620 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,620 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,631 - trainer - INFO - 
*****************[epoch: 211, global step: 212] eval training set at end of epoch***************
2022-10-24 15:19:37,632 - trainer - INFO - {
  "train_loss": 0.3470173478126526
}
2022-10-24 15:19:37,632 - trainer - INFO - start training epoch 212
2022-10-24 15:19:37,632 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,633 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,633 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,642 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval training set based on eval_every=2***************
2022-10-24 15:19:37,643 - trainer - INFO - {
  "train_loss": 0.33396294713020325
}
2022-10-24 15:19:37,650 - trainer - INFO - 
*****************[epoch: 212, global step: 212] eval development set based on eval_every=2***************
2022-10-24 15:19:37,651 - trainer - INFO - {
  "dev_loss": 0.29924774169921875,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,651 - trainer - INFO -   no_improve_count: 19
2022-10-24 15:19:37,652 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,653 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,653 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,654 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_206
2022-10-24 15:19:37,656 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_212
2022-10-24 15:19:37,660 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_212
2022-10-24 15:19:37,662 - trainer - INFO - 
*****************[epoch: 212, global step: 213] eval training set at end of epoch***************
2022-10-24 15:19:37,662 - trainer - INFO - {
  "train_loss": 0.3209085464477539
}
2022-10-24 15:19:37,663 - trainer - INFO - start training epoch 213
2022-10-24 15:19:37,663 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,663 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,664 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,672 - trainer - INFO - 
*****************[epoch: 213, global step: 214] eval training set at end of epoch***************
2022-10-24 15:19:37,673 - trainer - INFO - {
  "train_loss": 0.29924774169921875
}
2022-10-24 15:19:37,673 - trainer - INFO - start training epoch 214
2022-10-24 15:19:37,673 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,673 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,674 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,681 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval training set based on eval_every=2***************
2022-10-24 15:19:37,682 - trainer - INFO - {
  "train_loss": 0.28998446464538574
}
2022-10-24 15:19:37,691 - trainer - INFO - 
*****************[epoch: 214, global step: 214] eval development set based on eval_every=2***************
2022-10-24 15:19:37,692 - trainer - INFO - {
  "dev_loss": 0.2661013603210449,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,692 - trainer - INFO -   no_improve_count: 20
2022-10-24 15:19:37,693 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,694 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,694 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,695 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_208
2022-10-24 15:19:37,696 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_214
2022-10-24 15:19:37,705 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_214
2022-10-24 15:19:37,706 - trainer - INFO - 
*****************[epoch: 214, global step: 215] eval training set at end of epoch***************
2022-10-24 15:19:37,707 - trainer - INFO - {
  "train_loss": 0.28072118759155273
}
2022-10-24 15:19:37,708 - trainer - INFO - start training epoch 215
2022-10-24 15:19:37,708 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,708 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,709 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,718 - trainer - INFO - 
*****************[epoch: 215, global step: 216] eval training set at end of epoch***************
2022-10-24 15:19:37,719 - trainer - INFO - {
  "train_loss": 0.2661013603210449
}
2022-10-24 15:19:37,719 - trainer - INFO - start training epoch 216
2022-10-24 15:19:37,719 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,720 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,720 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,729 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval training set based on eval_every=2***************
2022-10-24 15:19:37,730 - trainer - INFO - {
  "train_loss": 0.2602294385433197
}
2022-10-24 15:19:37,738 - trainer - INFO - 
*****************[epoch: 216, global step: 216] eval development set based on eval_every=2***************
2022-10-24 15:19:37,739 - trainer - INFO - {
  "dev_loss": 0.24493083357810974,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,740 - trainer - INFO -   no_improve_count: 21
2022-10-24 15:19:37,740 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,742 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,742 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,743 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_210
2022-10-24 15:19:37,744 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_216
2022-10-24 15:19:37,749 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_216
2022-10-24 15:19:37,750 - trainer - INFO - 
*****************[epoch: 216, global step: 217] eval training set at end of epoch***************
2022-10-24 15:19:37,750 - trainer - INFO - {
  "train_loss": 0.2543575167655945
}
2022-10-24 15:19:37,751 - trainer - INFO - start training epoch 217
2022-10-24 15:19:37,751 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,751 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,752 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,760 - trainer - INFO - 
*****************[epoch: 217, global step: 218] eval training set at end of epoch***************
2022-10-24 15:19:37,761 - trainer - INFO - {
  "train_loss": 0.24493084847927094
}
2022-10-24 15:19:37,761 - trainer - INFO - start training epoch 218
2022-10-24 15:19:37,761 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,761 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,762 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,769 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval training set based on eval_every=2***************
2022-10-24 15:19:37,769 - trainer - INFO - {
  "train_loss": 0.24159394204616547
}
2022-10-24 15:19:37,775 - trainer - INFO - 
*****************[epoch: 218, global step: 218] eval development set based on eval_every=2***************
2022-10-24 15:19:37,775 - trainer - INFO - {
  "dev_loss": 0.23316839337348938,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,775 - trainer - INFO -   no_improve_count: 22
2022-10-24 15:19:37,776 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,777 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,777 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,778 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_212
2022-10-24 15:19:37,780 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_218
2022-10-24 15:19:37,785 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_218
2022-10-24 15:19:37,786 - trainer - INFO - 
*****************[epoch: 218, global step: 219] eval training set at end of epoch***************
2022-10-24 15:19:37,786 - trainer - INFO - {
  "train_loss": 0.23825703561306
}
2022-10-24 15:19:37,787 - trainer - INFO - start training epoch 219
2022-10-24 15:19:37,787 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,787 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,788 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,797 - trainer - INFO - 
*****************[epoch: 219, global step: 220] eval training set at end of epoch***************
2022-10-24 15:19:37,798 - trainer - INFO - {
  "train_loss": 0.23316837847232819
}
2022-10-24 15:19:37,798 - trainer - INFO - start training epoch 220
2022-10-24 15:19:37,799 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,799 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,799 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,806 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval training set based on eval_every=2***************
2022-10-24 15:19:37,806 - trainer - INFO - {
  "train_loss": 0.23142580687999725
}
2022-10-24 15:19:37,812 - trainer - INFO - 
*****************[epoch: 220, global step: 220] eval development set based on eval_every=2***************
2022-10-24 15:19:37,812 - trainer - INFO - {
  "dev_loss": 0.2277534306049347,
  "dev_best_score_for_loss": -0.22660189867019653
}
2022-10-24 15:19:37,813 - trainer - INFO -   no_improve_count: 23
2022-10-24 15:19:37,813 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,814 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,814 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,815 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_214
2022-10-24 15:19:37,816 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_220
2022-10-24 15:19:37,820 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_220
2022-10-24 15:19:37,820 - trainer - INFO - 
*****************[epoch: 220, global step: 221] eval training set at end of epoch***************
2022-10-24 15:19:37,820 - trainer - INFO - {
  "train_loss": 0.22968323528766632
}
2022-10-24 15:19:37,821 - trainer - INFO - start training epoch 221
2022-10-24 15:19:37,821 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,821 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,822 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,830 - trainer - INFO - 
*****************[epoch: 221, global step: 222] eval training set at end of epoch***************
2022-10-24 15:19:37,831 - trainer - INFO - {
  "train_loss": 0.2277534455060959
}
2022-10-24 15:19:37,831 - trainer - INFO - start training epoch 222
2022-10-24 15:19:37,832 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,832 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,832 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,841 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval training set based on eval_every=2***************
2022-10-24 15:19:37,842 - trainer - INFO - {
  "train_loss": 0.2271648719906807
}
2022-10-24 15:19:37,849 - trainer - INFO - 
*****************[epoch: 222, global step: 222] eval development set based on eval_every=2***************
2022-10-24 15:19:37,849 - trainer - INFO - {
  "dev_loss": 0.2263488471508026,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:37,850 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:37,851 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,851 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,851 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_216
2022-10-24 15:19:37,853 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:37,856 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:37,856 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:37,856 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,858 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:37,858 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_222
2022-10-24 15:19:37,864 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_222
2022-10-24 15:19:37,865 - trainer - INFO - 
*****************[epoch: 222, global step: 223] eval training set at end of epoch***************
2022-10-24 15:19:37,865 - trainer - INFO - {
  "train_loss": 0.2265762984752655
}
2022-10-24 15:19:37,866 - trainer - INFO - start training epoch 223
2022-10-24 15:19:37,866 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,866 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,866 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,877 - trainer - INFO - 
*****************[epoch: 223, global step: 224] eval training set at end of epoch***************
2022-10-24 15:19:37,877 - trainer - INFO - {
  "train_loss": 0.2263488471508026
}
2022-10-24 15:19:37,878 - trainer - INFO - start training epoch 224
2022-10-24 15:19:37,878 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,878 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,879 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,886 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval training set based on eval_every=2***************
2022-10-24 15:19:37,887 - trainer - INFO - {
  "train_loss": 0.22656748443841934
}
2022-10-24 15:19:37,893 - trainer - INFO - 
*****************[epoch: 224, global step: 224] eval development set based on eval_every=2***************
2022-10-24 15:19:37,893 - trainer - INFO - {
  "dev_loss": 0.227457657456398,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:37,894 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:37,894 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,896 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,896 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,896 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_218
2022-10-24 15:19:37,898 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_224
2022-10-24 15:19:37,902 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_224
2022-10-24 15:19:37,903 - trainer - INFO - 
*****************[epoch: 224, global step: 225] eval training set at end of epoch***************
2022-10-24 15:19:37,903 - trainer - INFO - {
  "train_loss": 0.22678612172603607
}
2022-10-24 15:19:37,904 - trainer - INFO - start training epoch 225
2022-10-24 15:19:37,904 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,905 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,905 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,913 - trainer - INFO - 
*****************[epoch: 225, global step: 226] eval training set at end of epoch***************
2022-10-24 15:19:37,913 - trainer - INFO - {
  "train_loss": 0.227457657456398
}
2022-10-24 15:19:37,913 - trainer - INFO - start training epoch 226
2022-10-24 15:19:37,914 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,914 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,914 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,924 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval training set based on eval_every=2***************
2022-10-24 15:19:37,924 - trainer - INFO - {
  "train_loss": 0.22802790999412537
}
2022-10-24 15:19:37,932 - trainer - INFO - 
*****************[epoch: 226, global step: 226] eval development set based on eval_every=2***************
2022-10-24 15:19:37,932 - trainer - INFO - {
  "dev_loss": 0.2297646850347519,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:37,933 - trainer - INFO -   no_improve_count: 2
2022-10-24 15:19:37,934 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,935 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,935 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,936 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_220
2022-10-24 15:19:37,937 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_226
2022-10-24 15:19:37,942 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_226
2022-10-24 15:19:37,943 - trainer - INFO - 
*****************[epoch: 226, global step: 227] eval training set at end of epoch***************
2022-10-24 15:19:37,943 - trainer - INFO - {
  "train_loss": 0.22859816253185272
}
2022-10-24 15:19:37,943 - trainer - INFO - start training epoch 227
2022-10-24 15:19:37,944 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,944 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,944 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,953 - trainer - INFO - 
*****************[epoch: 227, global step: 228] eval training set at end of epoch***************
2022-10-24 15:19:37,954 - trainer - INFO - {
  "train_loss": 0.2297646850347519
}
2022-10-24 15:19:37,954 - trainer - INFO - start training epoch 228
2022-10-24 15:19:37,954 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,955 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,955 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:37,965 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval training set based on eval_every=2***************
2022-10-24 15:19:37,966 - trainer - INFO - {
  "train_loss": 0.23036403208971024
}
2022-10-24 15:19:37,976 - trainer - INFO - 
*****************[epoch: 228, global step: 228] eval development set based on eval_every=2***************
2022-10-24 15:19:37,977 - trainer - INFO - {
  "dev_loss": 0.23228147625923157,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:37,978 - trainer - INFO -   no_improve_count: 3
2022-10-24 15:19:37,979 - trainer - INFO -   patience: 200
2022-10-24 15:19:37,980 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:37,980 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:37,981 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_222
2022-10-24 15:19:37,982 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_228
2022-10-24 15:19:37,987 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_228
2022-10-24 15:19:37,989 - trainer - INFO - 
*****************[epoch: 228, global step: 229] eval training set at end of epoch***************
2022-10-24 15:19:37,989 - trainer - INFO - {
  "train_loss": 0.23096337914466858
}
2022-10-24 15:19:37,990 - trainer - INFO - start training epoch 229
2022-10-24 15:19:37,990 - trainer - INFO - training using device=cuda
2022-10-24 15:19:37,990 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:37,990 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,001 - trainer - INFO - 
*****************[epoch: 229, global step: 230] eval training set at end of epoch***************
2022-10-24 15:19:38,001 - trainer - INFO - {
  "train_loss": 0.23228147625923157
}
2022-10-24 15:19:38,002 - trainer - INFO - start training epoch 230
2022-10-24 15:19:38,002 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,002 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,003 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,013 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval training set based on eval_every=2***************
2022-10-24 15:19:38,016 - trainer - INFO - {
  "train_loss": 0.23280643671751022
}
2022-10-24 15:19:38,025 - trainer - INFO - 
*****************[epoch: 230, global step: 230] eval development set based on eval_every=2***************
2022-10-24 15:19:38,026 - trainer - INFO - {
  "dev_loss": 0.23432745039463043,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,028 - trainer - INFO -   no_improve_count: 4
2022-10-24 15:19:38,028 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,030 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,031 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,031 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_224
2022-10-24 15:19:38,033 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_230
2022-10-24 15:19:38,038 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_230
2022-10-24 15:19:38,039 - trainer - INFO - 
*****************[epoch: 230, global step: 231] eval training set at end of epoch***************
2022-10-24 15:19:38,040 - trainer - INFO - {
  "train_loss": 0.23333139717578888
}
2022-10-24 15:19:38,040 - trainer - INFO - start training epoch 231
2022-10-24 15:19:38,041 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,041 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,041 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,051 - trainer - INFO - 
*****************[epoch: 231, global step: 232] eval training set at end of epoch***************
2022-10-24 15:19:38,052 - trainer - INFO - {
  "train_loss": 0.23432745039463043
}
2022-10-24 15:19:38,052 - trainer - INFO - start training epoch 232
2022-10-24 15:19:38,052 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,052 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,053 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,065 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval training set based on eval_every=2***************
2022-10-24 15:19:38,066 - trainer - INFO - {
  "train_loss": 0.23476304858922958
}
2022-10-24 15:19:38,074 - trainer - INFO - 
*****************[epoch: 232, global step: 232] eval development set based on eval_every=2***************
2022-10-24 15:19:38,075 - trainer - INFO - {
  "dev_loss": 0.23577222228050232,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,075 - trainer - INFO -   no_improve_count: 5
2022-10-24 15:19:38,076 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,078 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,078 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,078 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_226
2022-10-24 15:19:38,080 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_232
2022-10-24 15:19:38,085 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_232
2022-10-24 15:19:38,086 - trainer - INFO - 
*****************[epoch: 232, global step: 233] eval training set at end of epoch***************
2022-10-24 15:19:38,087 - trainer - INFO - {
  "train_loss": 0.23519864678382874
}
2022-10-24 15:19:38,087 - trainer - INFO - start training epoch 233
2022-10-24 15:19:38,087 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,087 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,089 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,099 - trainer - INFO - 
*****************[epoch: 233, global step: 234] eval training set at end of epoch***************
2022-10-24 15:19:38,099 - trainer - INFO - {
  "train_loss": 0.2357722371816635
}
2022-10-24 15:19:38,100 - trainer - INFO - start training epoch 234
2022-10-24 15:19:38,100 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,100 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,101 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,108 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval training set based on eval_every=2***************
2022-10-24 15:19:38,109 - trainer - INFO - {
  "train_loss": 0.23601308465003967
}
2022-10-24 15:19:38,117 - trainer - INFO - 
*****************[epoch: 234, global step: 234] eval development set based on eval_every=2***************
2022-10-24 15:19:38,118 - trainer - INFO - {
  "dev_loss": 0.23650744557380676,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,119 - trainer - INFO -   no_improve_count: 6
2022-10-24 15:19:38,120 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,121 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,122 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,122 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_228
2022-10-24 15:19:38,124 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_234
2022-10-24 15:19:38,131 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_234
2022-10-24 15:19:38,132 - trainer - INFO - 
*****************[epoch: 234, global step: 235] eval training set at end of epoch***************
2022-10-24 15:19:38,132 - trainer - INFO - {
  "train_loss": 0.23625393211841583
}
2022-10-24 15:19:38,133 - trainer - INFO - start training epoch 235
2022-10-24 15:19:38,133 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,133 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,134 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,143 - trainer - INFO - 
*****************[epoch: 235, global step: 236] eval training set at end of epoch***************
2022-10-24 15:19:38,143 - trainer - INFO - {
  "train_loss": 0.23650744557380676
}
2022-10-24 15:19:38,144 - trainer - INFO - start training epoch 236
2022-10-24 15:19:38,144 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,145 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,145 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,156 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval training set based on eval_every=2***************
2022-10-24 15:19:38,157 - trainer - INFO - {
  "train_loss": 0.23653749376535416
}
2022-10-24 15:19:38,168 - trainer - INFO - 
*****************[epoch: 236, global step: 236] eval development set based on eval_every=2***************
2022-10-24 15:19:38,168 - trainer - INFO - {
  "dev_loss": 0.23654328286647797,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,169 - trainer - INFO -   no_improve_count: 7
2022-10-24 15:19:38,170 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,172 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,172 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,172 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_230
2022-10-24 15:19:38,174 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_236
2022-10-24 15:19:38,179 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_236
2022-10-24 15:19:38,180 - trainer - INFO - 
*****************[epoch: 236, global step: 237] eval training set at end of epoch***************
2022-10-24 15:19:38,181 - trainer - INFO - {
  "train_loss": 0.23656754195690155
}
2022-10-24 15:19:38,182 - trainer - INFO - start training epoch 237
2022-10-24 15:19:38,182 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,183 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,183 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,193 - trainer - INFO - 
*****************[epoch: 237, global step: 238] eval training set at end of epoch***************
2022-10-24 15:19:38,194 - trainer - INFO - {
  "train_loss": 0.23654328286647797
}
2022-10-24 15:19:38,195 - trainer - INFO - start training epoch 238
2022-10-24 15:19:38,195 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,195 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,196 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,211 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval training set based on eval_every=2***************
2022-10-24 15:19:38,212 - trainer - INFO - {
  "train_loss": 0.2363966479897499
}
2022-10-24 15:19:38,222 - trainer - INFO - 
*****************[epoch: 238, global step: 238] eval development set based on eval_every=2***************
2022-10-24 15:19:38,222 - trainer - INFO - {
  "dev_loss": 0.2359088808298111,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,223 - trainer - INFO -   no_improve_count: 8
2022-10-24 15:19:38,224 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,225 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,226 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,226 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_232
2022-10-24 15:19:38,228 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_238
2022-10-24 15:19:38,236 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_238
2022-10-24 15:19:38,238 - trainer - INFO - 
*****************[epoch: 238, global step: 239] eval training set at end of epoch***************
2022-10-24 15:19:38,238 - trainer - INFO - {
  "train_loss": 0.23625001311302185
}
2022-10-24 15:19:38,239 - trainer - INFO - start training epoch 239
2022-10-24 15:19:38,239 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,240 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,240 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,250 - trainer - INFO - 
*****************[epoch: 239, global step: 240] eval training set at end of epoch***************
2022-10-24 15:19:38,251 - trainer - INFO - {
  "train_loss": 0.2359088808298111
}
2022-10-24 15:19:38,251 - trainer - INFO - start training epoch 240
2022-10-24 15:19:38,251 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,252 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,252 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,261 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval training set based on eval_every=2***************
2022-10-24 15:19:38,261 - trainer - INFO - {
  "train_loss": 0.23568131774663925
}
2022-10-24 15:19:38,269 - trainer - INFO - 
*****************[epoch: 240, global step: 240] eval development set based on eval_every=2***************
2022-10-24 15:19:38,270 - trainer - INFO - {
  "dev_loss": 0.23486417531967163,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,270 - trainer - INFO -   no_improve_count: 9
2022-10-24 15:19:38,271 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,272 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,272 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,272 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_234
2022-10-24 15:19:38,273 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_240
2022-10-24 15:19:38,281 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_240
2022-10-24 15:19:38,282 - trainer - INFO - 
*****************[epoch: 240, global step: 241] eval training set at end of epoch***************
2022-10-24 15:19:38,282 - trainer - INFO - {
  "train_loss": 0.2354537546634674
}
2022-10-24 15:19:38,283 - trainer - INFO - start training epoch 241
2022-10-24 15:19:38,284 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,284 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,284 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,293 - trainer - INFO - 
*****************[epoch: 241, global step: 242] eval training set at end of epoch***************
2022-10-24 15:19:38,294 - trainer - INFO - {
  "train_loss": 0.23486417531967163
}
2022-10-24 15:19:38,294 - trainer - INFO - start training epoch 242
2022-10-24 15:19:38,295 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,295 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,296 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,304 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval training set based on eval_every=2***************
2022-10-24 15:19:38,305 - trainer - INFO - {
  "train_loss": 0.23456933349370956
}
2022-10-24 15:19:38,316 - trainer - INFO - 
*****************[epoch: 242, global step: 242] eval development set based on eval_every=2***************
2022-10-24 15:19:38,316 - trainer - INFO - {
  "dev_loss": 0.23359431326389313,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,317 - trainer - INFO -   no_improve_count: 10
2022-10-24 15:19:38,318 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,319 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,319 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,320 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_236
2022-10-24 15:19:38,321 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_242
2022-10-24 15:19:38,327 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_242
2022-10-24 15:19:38,328 - trainer - INFO - 
*****************[epoch: 242, global step: 243] eval training set at end of epoch***************
2022-10-24 15:19:38,328 - trainer - INFO - {
  "train_loss": 0.2342744916677475
}
2022-10-24 15:19:38,329 - trainer - INFO - start training epoch 243
2022-10-24 15:19:38,329 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,329 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,330 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,344 - trainer - INFO - 
*****************[epoch: 243, global step: 244] eval training set at end of epoch***************
2022-10-24 15:19:38,344 - trainer - INFO - {
  "train_loss": 0.23359431326389313
}
2022-10-24 15:19:38,345 - trainer - INFO - start training epoch 244
2022-10-24 15:19:38,345 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,345 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,346 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,355 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval training set based on eval_every=2***************
2022-10-24 15:19:38,355 - trainer - INFO - {
  "train_loss": 0.23325078934431076
}
2022-10-24 15:19:38,364 - trainer - INFO - 
*****************[epoch: 244, global step: 244] eval development set based on eval_every=2***************
2022-10-24 15:19:38,365 - trainer - INFO - {
  "dev_loss": 0.23220551013946533,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,366 - trainer - INFO -   no_improve_count: 11
2022-10-24 15:19:38,366 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,368 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,368 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,368 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_238
2022-10-24 15:19:38,370 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_244
2022-10-24 15:19:38,374 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_244
2022-10-24 15:19:38,375 - trainer - INFO - 
*****************[epoch: 244, global step: 245] eval training set at end of epoch***************
2022-10-24 15:19:38,376 - trainer - INFO - {
  "train_loss": 0.2329072654247284
}
2022-10-24 15:19:38,376 - trainer - INFO - start training epoch 245
2022-10-24 15:19:38,376 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,377 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,377 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,390 - trainer - INFO - 
*****************[epoch: 245, global step: 246] eval training set at end of epoch***************
2022-10-24 15:19:38,390 - trainer - INFO - {
  "train_loss": 0.23220551013946533
}
2022-10-24 15:19:38,391 - trainer - INFO - start training epoch 246
2022-10-24 15:19:38,391 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,391 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,392 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,402 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval training set based on eval_every=2***************
2022-10-24 15:19:38,403 - trainer - INFO - {
  "train_loss": 0.23185164481401443
}
2022-10-24 15:19:38,410 - trainer - INFO - 
*****************[epoch: 246, global step: 246] eval development set based on eval_every=2***************
2022-10-24 15:19:38,411 - trainer - INFO - {
  "dev_loss": 0.23082035779953003,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,412 - trainer - INFO -   no_improve_count: 12
2022-10-24 15:19:38,412 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,414 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,414 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,416 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_240
2022-10-24 15:19:38,417 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_246
2022-10-24 15:19:38,423 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_246
2022-10-24 15:19:38,424 - trainer - INFO - 
*****************[epoch: 246, global step: 247] eval training set at end of epoch***************
2022-10-24 15:19:38,424 - trainer - INFO - {
  "train_loss": 0.23149777948856354
}
2022-10-24 15:19:38,425 - trainer - INFO - start training epoch 247
2022-10-24 15:19:38,425 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,425 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,426 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,437 - trainer - INFO - 
*****************[epoch: 247, global step: 248] eval training set at end of epoch***************
2022-10-24 15:19:38,438 - trainer - INFO - {
  "train_loss": 0.23082032799720764
}
2022-10-24 15:19:38,438 - trainer - INFO - start training epoch 248
2022-10-24 15:19:38,438 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,439 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,439 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,450 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval training set based on eval_every=2***************
2022-10-24 15:19:38,450 - trainer - INFO - {
  "train_loss": 0.23049606382846832
}
2022-10-24 15:19:38,456 - trainer - INFO - 
*****************[epoch: 248, global step: 248] eval development set based on eval_every=2***************
2022-10-24 15:19:38,457 - trainer - INFO - {
  "dev_loss": 0.22951607406139374,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,457 - trainer - INFO -   no_improve_count: 13
2022-10-24 15:19:38,458 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,459 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,460 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,461 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_242
2022-10-24 15:19:38,463 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_248
2022-10-24 15:19:38,473 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_248
2022-10-24 15:19:38,478 - trainer - INFO - 
*****************[epoch: 248, global step: 249] eval training set at end of epoch***************
2022-10-24 15:19:38,478 - trainer - INFO - {
  "train_loss": 0.230171799659729
}
2022-10-24 15:19:38,479 - trainer - INFO - start training epoch 249
2022-10-24 15:19:38,479 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,479 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,480 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,490 - trainer - INFO - 
*****************[epoch: 249, global step: 250] eval training set at end of epoch***************
2022-10-24 15:19:38,490 - trainer - INFO - {
  "train_loss": 0.22951607406139374
}
2022-10-24 15:19:38,490 - trainer - INFO - start training epoch 250
2022-10-24 15:19:38,491 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,493 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,494 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,504 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval training set based on eval_every=2***************
2022-10-24 15:19:38,504 - trainer - INFO - {
  "train_loss": 0.2292320430278778
}
2022-10-24 15:19:38,517 - trainer - INFO - 
*****************[epoch: 250, global step: 250] eval development set based on eval_every=2***************
2022-10-24 15:19:38,517 - trainer - INFO - {
  "dev_loss": 0.22838996350765228,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,518 - trainer - INFO -   no_improve_count: 14
2022-10-24 15:19:38,519 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,520 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,520 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,521 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_244
2022-10-24 15:19:38,522 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_250
2022-10-24 15:19:38,530 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_250
2022-10-24 15:19:38,531 - trainer - INFO - 
*****************[epoch: 250, global step: 251] eval training set at end of epoch***************
2022-10-24 15:19:38,532 - trainer - INFO - {
  "train_loss": 0.22894801199436188
}
2022-10-24 15:19:38,533 - trainer - INFO - start training epoch 251
2022-10-24 15:19:38,533 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,534 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,534 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,542 - trainer - INFO - 
*****************[epoch: 251, global step: 252] eval training set at end of epoch***************
2022-10-24 15:19:38,542 - trainer - INFO - {
  "train_loss": 0.22838996350765228
}
2022-10-24 15:19:38,543 - trainer - INFO - start training epoch 252
2022-10-24 15:19:38,543 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,544 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,544 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,554 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval training set based on eval_every=2***************
2022-10-24 15:19:38,555 - trainer - INFO - {
  "train_loss": 0.2281404733657837
}
2022-10-24 15:19:38,565 - trainer - INFO - 
*****************[epoch: 252, global step: 252] eval development set based on eval_every=2***************
2022-10-24 15:19:38,565 - trainer - INFO - {
  "dev_loss": 0.22744150459766388,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,566 - trainer - INFO -   no_improve_count: 15
2022-10-24 15:19:38,567 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,568 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,569 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,569 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_246
2022-10-24 15:19:38,571 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_252
2022-10-24 15:19:38,577 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_252
2022-10-24 15:19:38,578 - trainer - INFO - 
*****************[epoch: 252, global step: 253] eval training set at end of epoch***************
2022-10-24 15:19:38,578 - trainer - INFO - {
  "train_loss": 0.2278909832239151
}
2022-10-24 15:19:38,579 - trainer - INFO - start training epoch 253
2022-10-24 15:19:38,579 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,580 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,580 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,589 - trainer - INFO - 
*****************[epoch: 253, global step: 254] eval training set at end of epoch***************
2022-10-24 15:19:38,589 - trainer - INFO - {
  "train_loss": 0.22744150459766388
}
2022-10-24 15:19:38,589 - trainer - INFO - start training epoch 254
2022-10-24 15:19:38,590 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,590 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,590 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,604 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval training set based on eval_every=2***************
2022-10-24 15:19:38,608 - trainer - INFO - {
  "train_loss": 0.2272372767329216
}
2022-10-24 15:19:38,617 - trainer - INFO - 
*****************[epoch: 254, global step: 254] eval development set based on eval_every=2***************
2022-10-24 15:19:38,618 - trainer - INFO - {
  "dev_loss": 0.22669561207294464,
  "dev_best_score_for_loss": -0.2263488471508026
}
2022-10-24 15:19:38,619 - trainer - INFO -   no_improve_count: 16
2022-10-24 15:19:38,619 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,621 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,621 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,621 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_248
2022-10-24 15:19:38,623 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_254
2022-10-24 15:19:38,628 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_254
2022-10-24 15:19:38,629 - trainer - INFO - 
*****************[epoch: 254, global step: 255] eval training set at end of epoch***************
2022-10-24 15:19:38,629 - trainer - INFO - {
  "train_loss": 0.22703304886817932
}
2022-10-24 15:19:38,630 - trainer - INFO - start training epoch 255
2022-10-24 15:19:38,630 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,632 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,632 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,640 - trainer - INFO - 
*****************[epoch: 255, global step: 256] eval training set at end of epoch***************
2022-10-24 15:19:38,641 - trainer - INFO - {
  "train_loss": 0.22669561207294464
}
2022-10-24 15:19:38,641 - trainer - INFO - start training epoch 256
2022-10-24 15:19:38,641 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,642 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,642 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,652 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval training set based on eval_every=2***************
2022-10-24 15:19:38,652 - trainer - INFO - {
  "train_loss": 0.2265295907855034
}
2022-10-24 15:19:38,660 - trainer - INFO - 
*****************[epoch: 256, global step: 256] eval development set based on eval_every=2***************
2022-10-24 15:19:38,661 - trainer - INFO - {
  "dev_loss": 0.22612829506397247,
  "dev_best_score_for_loss": -0.22612829506397247
}
2022-10-24 15:19:38,662 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:38,665 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,665 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,665 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_250
2022-10-24 15:19:38,667 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:38,671 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:38,671 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:38,671 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,672 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:38,672 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_256
2022-10-24 15:19:38,678 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_256
2022-10-24 15:19:38,680 - trainer - INFO - 
*****************[epoch: 256, global step: 257] eval training set at end of epoch***************
2022-10-24 15:19:38,680 - trainer - INFO - {
  "train_loss": 0.22636356949806213
}
2022-10-24 15:19:38,681 - trainer - INFO - start training epoch 257
2022-10-24 15:19:38,684 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,684 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,684 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,695 - trainer - INFO - 
*****************[epoch: 257, global step: 258] eval training set at end of epoch***************
2022-10-24 15:19:38,696 - trainer - INFO - {
  "train_loss": 0.22612829506397247
}
2022-10-24 15:19:38,696 - trainer - INFO - start training epoch 258
2022-10-24 15:19:38,696 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,697 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,697 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,706 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval training set based on eval_every=2***************
2022-10-24 15:19:38,706 - trainer - INFO - {
  "train_loss": 0.2260085493326187
}
2022-10-24 15:19:38,715 - trainer - INFO - 
*****************[epoch: 258, global step: 258] eval development set based on eval_every=2***************
2022-10-24 15:19:38,715 - trainer - INFO - {
  "dev_loss": 0.22569897770881653,
  "dev_best_score_for_loss": -0.22569897770881653
}
2022-10-24 15:19:38,716 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:38,717 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,717 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,718 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_252
2022-10-24 15:19:38,719 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:38,722 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:38,723 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:38,723 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,724 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:38,724 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_258
2022-10-24 15:19:38,732 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_258
2022-10-24 15:19:38,733 - trainer - INFO - 
*****************[epoch: 258, global step: 259] eval training set at end of epoch***************
2022-10-24 15:19:38,733 - trainer - INFO - {
  "train_loss": 0.22588880360126495
}
2022-10-24 15:19:38,734 - trainer - INFO - start training epoch 259
2022-10-24 15:19:38,734 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,734 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,735 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,744 - trainer - INFO - 
*****************[epoch: 259, global step: 260] eval training set at end of epoch***************
2022-10-24 15:19:38,744 - trainer - INFO - {
  "train_loss": 0.22569897770881653
}
2022-10-24 15:19:38,745 - trainer - INFO - start training epoch 260
2022-10-24 15:19:38,745 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,746 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,746 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,754 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval training set based on eval_every=2***************
2022-10-24 15:19:38,754 - trainer - INFO - {
  "train_loss": 0.22563400119543076
}
2022-10-24 15:19:38,764 - trainer - INFO - 
*****************[epoch: 260, global step: 260] eval development set based on eval_every=2***************
2022-10-24 15:19:38,765 - trainer - INFO - {
  "dev_loss": 0.22540877759456635,
  "dev_best_score_for_loss": -0.22540877759456635
}
2022-10-24 15:19:38,766 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:38,768 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,768 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,768 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_254
2022-10-24 15:19:38,770 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:38,774 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:38,774 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:38,774 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,775 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:38,776 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_260
2022-10-24 15:19:38,781 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_260
2022-10-24 15:19:38,782 - trainer - INFO - 
*****************[epoch: 260, global step: 261] eval training set at end of epoch***************
2022-10-24 15:19:38,783 - trainer - INFO - {
  "train_loss": 0.22556902468204498
}
2022-10-24 15:19:38,784 - trainer - INFO - start training epoch 261
2022-10-24 15:19:38,784 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,784 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,785 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,796 - trainer - INFO - 
*****************[epoch: 261, global step: 262] eval training set at end of epoch***************
2022-10-24 15:19:38,796 - trainer - INFO - {
  "train_loss": 0.22540876269340515
}
2022-10-24 15:19:38,797 - trainer - INFO - start training epoch 262
2022-10-24 15:19:38,797 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,797 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,798 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,807 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval training set based on eval_every=2***************
2022-10-24 15:19:38,808 - trainer - INFO - {
  "train_loss": 0.2253691405057907
}
2022-10-24 15:19:38,818 - trainer - INFO - 
*****************[epoch: 262, global step: 262] eval development set based on eval_every=2***************
2022-10-24 15:19:38,819 - trainer - INFO - {
  "dev_loss": 0.22524572908878326,
  "dev_best_score_for_loss": -0.22524572908878326
}
2022-10-24 15:19:38,820 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:38,821 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,822 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,822 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_256
2022-10-24 15:19:38,824 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:38,829 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:38,829 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:38,830 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,831 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:38,831 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_262
2022-10-24 15:19:38,836 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_262
2022-10-24 15:19:38,837 - trainer - INFO - 
*****************[epoch: 262, global step: 263] eval training set at end of epoch***************
2022-10-24 15:19:38,837 - trainer - INFO - {
  "train_loss": 0.22532951831817627
}
2022-10-24 15:19:38,838 - trainer - INFO - start training epoch 263
2022-10-24 15:19:38,838 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,838 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,839 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,849 - trainer - INFO - 
*****************[epoch: 263, global step: 264] eval training set at end of epoch***************
2022-10-24 15:19:38,849 - trainer - INFO - {
  "train_loss": 0.22524574398994446
}
2022-10-24 15:19:38,850 - trainer - INFO - start training epoch 264
2022-10-24 15:19:38,850 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,850 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,851 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,860 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval training set based on eval_every=2***************
2022-10-24 15:19:38,860 - trainer - INFO - {
  "train_loss": 0.22521577775478363
}
2022-10-24 15:19:38,871 - trainer - INFO - 
*****************[epoch: 264, global step: 264] eval development set based on eval_every=2***************
2022-10-24 15:19:38,871 - trainer - INFO - {
  "dev_loss": 0.2251608818769455,
  "dev_best_score_for_loss": -0.2251608818769455
}
2022-10-24 15:19:38,872 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:38,873 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,874 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,874 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_258
2022-10-24 15:19:38,876 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:38,883 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:38,883 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:38,883 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,885 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:38,885 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_264
2022-10-24 15:19:38,891 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_264
2022-10-24 15:19:38,891 - trainer - INFO - 
*****************[epoch: 264, global step: 265] eval training set at end of epoch***************
2022-10-24 15:19:38,892 - trainer - INFO - {
  "train_loss": 0.2251858115196228
}
2022-10-24 15:19:38,892 - trainer - INFO - start training epoch 265
2022-10-24 15:19:38,893 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,893 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,893 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,903 - trainer - INFO - 
*****************[epoch: 265, global step: 266] eval training set at end of epoch***************
2022-10-24 15:19:38,904 - trainer - INFO - {
  "train_loss": 0.2251608967781067
}
2022-10-24 15:19:38,905 - trainer - INFO - start training epoch 266
2022-10-24 15:19:38,905 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,905 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,905 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,916 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval training set based on eval_every=2***************
2022-10-24 15:19:38,916 - trainer - INFO - {
  "train_loss": 0.22513630241155624
}
2022-10-24 15:19:38,923 - trainer - INFO - 
*****************[epoch: 266, global step: 266] eval development set based on eval_every=2***************
2022-10-24 15:19:38,924 - trainer - INFO - {
  "dev_loss": 0.22507701814174652,
  "dev_best_score_for_loss": -0.22507701814174652
}
2022-10-24 15:19:38,924 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:38,929 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,930 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,930 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_260
2022-10-24 15:19:38,932 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:38,936 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:38,936 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:38,937 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,938 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:38,938 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_266
2022-10-24 15:19:38,943 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_266
2022-10-24 15:19:38,948 - trainer - INFO - 
*****************[epoch: 266, global step: 267] eval training set at end of epoch***************
2022-10-24 15:19:38,948 - trainer - INFO - {
  "train_loss": 0.2251117080450058
}
2022-10-24 15:19:38,949 - trainer - INFO - start training epoch 267
2022-10-24 15:19:38,949 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,949 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,950 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,960 - trainer - INFO - 
*****************[epoch: 267, global step: 268] eval training set at end of epoch***************
2022-10-24 15:19:38,960 - trainer - INFO - {
  "train_loss": 0.22507701814174652
}
2022-10-24 15:19:38,961 - trainer - INFO - start training epoch 268
2022-10-24 15:19:38,961 - trainer - INFO - training using device=cuda
2022-10-24 15:19:38,961 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:38,962 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:38,970 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval training set based on eval_every=2***************
2022-10-24 15:19:38,971 - trainer - INFO - {
  "train_loss": 0.22507347911596298
}
2022-10-24 15:19:38,981 - trainer - INFO - 
*****************[epoch: 268, global step: 268] eval development set based on eval_every=2***************
2022-10-24 15:19:38,981 - trainer - INFO - {
  "dev_loss": 0.2250644862651825,
  "dev_best_score_for_loss": -0.2250644862651825
}
2022-10-24 15:19:38,982 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:38,984 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:38,984 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:38,984 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_262
2022-10-24 15:19:38,986 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:38,994 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:38,995 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:38,995 - trainer - INFO -   patience: 200
2022-10-24 15:19:38,996 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:38,997 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_268
2022-10-24 15:19:39,002 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_268
2022-10-24 15:19:39,004 - trainer - INFO - 
*****************[epoch: 268, global step: 269] eval training set at end of epoch***************
2022-10-24 15:19:39,004 - trainer - INFO - {
  "train_loss": 0.22506994009017944
}
2022-10-24 15:19:39,005 - trainer - INFO - start training epoch 269
2022-10-24 15:19:39,005 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,005 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,006 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,015 - trainer - INFO - 
*****************[epoch: 269, global step: 270] eval training set at end of epoch***************
2022-10-24 15:19:39,016 - trainer - INFO - {
  "train_loss": 0.2250644862651825
}
2022-10-24 15:19:39,016 - trainer - INFO - start training epoch 270
2022-10-24 15:19:39,016 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,016 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,017 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,025 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval training set based on eval_every=2***************
2022-10-24 15:19:39,025 - trainer - INFO - {
  "train_loss": 0.2250550389289856
}
2022-10-24 15:19:39,033 - trainer - INFO - 
*****************[epoch: 270, global step: 270] eval development set based on eval_every=2***************
2022-10-24 15:19:39,035 - trainer - INFO - {
  "dev_loss": 0.22505372762680054,
  "dev_best_score_for_loss": -0.22505372762680054
}
2022-10-24 15:19:39,036 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,038 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,041 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,041 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_264
2022-10-24 15:19:39,043 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,047 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,048 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,048 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,049 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,050 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_270
2022-10-24 15:19:39,055 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_270
2022-10-24 15:19:39,056 - trainer - INFO - 
*****************[epoch: 270, global step: 271] eval training set at end of epoch***************
2022-10-24 15:19:39,057 - trainer - INFO - {
  "train_loss": 0.2250455915927887
}
2022-10-24 15:19:39,057 - trainer - INFO - start training epoch 271
2022-10-24 15:19:39,058 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,058 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,058 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,069 - trainer - INFO - 
*****************[epoch: 271, global step: 272] eval training set at end of epoch***************
2022-10-24 15:19:39,070 - trainer - INFO - {
  "train_loss": 0.22505369782447815
}
2022-10-24 15:19:39,070 - trainer - INFO - start training epoch 272
2022-10-24 15:19:39,070 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,071 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,071 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,079 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval training set based on eval_every=2***************
2022-10-24 15:19:39,080 - trainer - INFO - {
  "train_loss": 0.2250438779592514
}
2022-10-24 15:19:39,089 - trainer - INFO - 
*****************[epoch: 272, global step: 272] eval development set based on eval_every=2***************
2022-10-24 15:19:39,089 - trainer - INFO - {
  "dev_loss": 0.22506073117256165,
  "dev_best_score_for_loss": -0.22505372762680054
}
2022-10-24 15:19:39,090 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:39,091 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,092 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,092 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,093 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_266
2022-10-24 15:19:39,095 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_272
2022-10-24 15:19:39,101 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_272
2022-10-24 15:19:39,102 - trainer - INFO - 
*****************[epoch: 272, global step: 273] eval training set at end of epoch***************
2022-10-24 15:19:39,102 - trainer - INFO - {
  "train_loss": 0.22503405809402466
}
2022-10-24 15:19:39,103 - trainer - INFO - start training epoch 273
2022-10-24 15:19:39,103 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,103 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,104 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,118 - trainer - INFO - 
*****************[epoch: 273, global step: 274] eval training set at end of epoch***************
2022-10-24 15:19:39,118 - trainer - INFO - {
  "train_loss": 0.22506073117256165
}
2022-10-24 15:19:39,119 - trainer - INFO - start training epoch 274
2022-10-24 15:19:39,119 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,119 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,120 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,129 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval training set based on eval_every=2***************
2022-10-24 15:19:39,130 - trainer - INFO - {
  "train_loss": 0.2250407412648201
}
2022-10-24 15:19:39,138 - trainer - INFO - 
*****************[epoch: 274, global step: 274] eval development set based on eval_every=2***************
2022-10-24 15:19:39,138 - trainer - INFO - {
  "dev_loss": 0.22500860691070557,
  "dev_best_score_for_loss": -0.22500860691070557
}
2022-10-24 15:19:39,139 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,140 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,140 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,140 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_268
2022-10-24 15:19:39,142 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,147 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,147 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,148 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,149 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,149 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_274
2022-10-24 15:19:39,153 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_274
2022-10-24 15:19:39,154 - trainer - INFO - 
*****************[epoch: 274, global step: 275] eval training set at end of epoch***************
2022-10-24 15:19:39,154 - trainer - INFO - {
  "train_loss": 0.22502075135707855
}
2022-10-24 15:19:39,155 - trainer - INFO - start training epoch 275
2022-10-24 15:19:39,155 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,155 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,155 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,168 - trainer - INFO - 
*****************[epoch: 275, global step: 276] eval training set at end of epoch***************
2022-10-24 15:19:39,168 - trainer - INFO - {
  "train_loss": 0.22500860691070557
}
2022-10-24 15:19:39,169 - trainer - INFO - start training epoch 276
2022-10-24 15:19:39,169 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,170 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,170 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,181 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval training set based on eval_every=2***************
2022-10-24 15:19:39,182 - trainer - INFO - {
  "train_loss": 0.22499427944421768
}
2022-10-24 15:19:39,188 - trainer - INFO - 
*****************[epoch: 276, global step: 276] eval development set based on eval_every=2***************
2022-10-24 15:19:39,189 - trainer - INFO - {
  "dev_loss": 0.22500185668468475,
  "dev_best_score_for_loss": -0.22500185668468475
}
2022-10-24 15:19:39,190 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,192 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,192 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,193 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_270
2022-10-24 15:19:39,196 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,200 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,200 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,200 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,201 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,202 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_276
2022-10-24 15:19:39,207 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_276
2022-10-24 15:19:39,208 - trainer - INFO - 
*****************[epoch: 276, global step: 277] eval training set at end of epoch***************
2022-10-24 15:19:39,212 - trainer - INFO - {
  "train_loss": 0.2249799519777298
}
2022-10-24 15:19:39,212 - trainer - INFO - start training epoch 277
2022-10-24 15:19:39,212 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,213 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,213 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,223 - trainer - INFO - 
*****************[epoch: 277, global step: 278] eval training set at end of epoch***************
2022-10-24 15:19:39,223 - trainer - INFO - {
  "train_loss": 0.22500185668468475
}
2022-10-24 15:19:39,224 - trainer - INFO - start training epoch 278
2022-10-24 15:19:39,224 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,224 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,225 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,233 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval training set based on eval_every=2***************
2022-10-24 15:19:39,234 - trainer - INFO - {
  "train_loss": 0.22497893124818802
}
2022-10-24 15:19:39,242 - trainer - INFO - 
*****************[epoch: 278, global step: 278] eval development set based on eval_every=2***************
2022-10-24 15:19:39,243 - trainer - INFO - {
  "dev_loss": 0.2249612957239151,
  "dev_best_score_for_loss": -0.2249612957239151
}
2022-10-24 15:19:39,243 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,245 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,245 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,246 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_272
2022-10-24 15:19:39,247 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,253 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,254 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,255 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,260 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,260 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_278
2022-10-24 15:19:39,272 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_278
2022-10-24 15:19:39,273 - trainer - INFO - 
*****************[epoch: 278, global step: 279] eval training set at end of epoch***************
2022-10-24 15:19:39,274 - trainer - INFO - {
  "train_loss": 0.22495600581169128
}
2022-10-24 15:19:39,274 - trainer - INFO - start training epoch 279
2022-10-24 15:19:39,274 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,275 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,275 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,286 - trainer - INFO - 
*****************[epoch: 279, global step: 280] eval training set at end of epoch***************
2022-10-24 15:19:39,286 - trainer - INFO - {
  "train_loss": 0.2249612659215927
}
2022-10-24 15:19:39,287 - trainer - INFO - start training epoch 280
2022-10-24 15:19:39,287 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,287 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,288 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,300 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval training set based on eval_every=2***************
2022-10-24 15:19:39,304 - trainer - INFO - {
  "train_loss": 0.22495048493146896
}
2022-10-24 15:19:39,315 - trainer - INFO - 
*****************[epoch: 280, global step: 280] eval development set based on eval_every=2***************
2022-10-24 15:19:39,316 - trainer - INFO - {
  "dev_loss": 0.2248959094285965,
  "dev_best_score_for_loss": -0.2248959094285965
}
2022-10-24 15:19:39,317 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,319 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,319 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,320 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_274
2022-10-24 15:19:39,321 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,325 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,326 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,326 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,327 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,327 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_280
2022-10-24 15:19:39,334 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_280
2022-10-24 15:19:39,335 - trainer - INFO - 
*****************[epoch: 280, global step: 281] eval training set at end of epoch***************
2022-10-24 15:19:39,335 - trainer - INFO - {
  "train_loss": 0.22493970394134521
}
2022-10-24 15:19:39,336 - trainer - INFO - start training epoch 281
2022-10-24 15:19:39,336 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,336 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,337 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,350 - trainer - INFO - 
*****************[epoch: 281, global step: 282] eval training set at end of epoch***************
2022-10-24 15:19:39,351 - trainer - INFO - {
  "train_loss": 0.2248958945274353
}
2022-10-24 15:19:39,351 - trainer - INFO - start training epoch 282
2022-10-24 15:19:39,352 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,352 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,352 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,363 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval training set based on eval_every=2***************
2022-10-24 15:19:39,363 - trainer - INFO - {
  "train_loss": 0.2248809114098549
}
2022-10-24 15:19:39,370 - trainer - INFO - 
*****************[epoch: 282, global step: 282] eval development set based on eval_every=2***************
2022-10-24 15:19:39,371 - trainer - INFO - {
  "dev_loss": 0.2248518019914627,
  "dev_best_score_for_loss": -0.2248518019914627
}
2022-10-24 15:19:39,372 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,373 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,373 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,373 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_276
2022-10-24 15:19:39,376 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,380 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,380 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,381 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,381 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,382 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_282
2022-10-24 15:19:39,386 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_282
2022-10-24 15:19:39,386 - trainer - INFO - 
*****************[epoch: 282, global step: 283] eval training set at end of epoch***************
2022-10-24 15:19:39,387 - trainer - INFO - {
  "train_loss": 0.22486592829227448
}
2022-10-24 15:19:39,387 - trainer - INFO - start training epoch 283
2022-10-24 15:19:39,387 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,387 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,388 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,399 - trainer - INFO - 
*****************[epoch: 283, global step: 284] eval training set at end of epoch***************
2022-10-24 15:19:39,400 - trainer - INFO - {
  "train_loss": 0.2248518019914627
}
2022-10-24 15:19:39,400 - trainer - INFO - start training epoch 284
2022-10-24 15:19:39,401 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,401 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,401 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,411 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval training set based on eval_every=2***************
2022-10-24 15:19:39,411 - trainer - INFO - {
  "train_loss": 0.2248346507549286
}
2022-10-24 15:19:39,420 - trainer - INFO - 
*****************[epoch: 284, global step: 284] eval development set based on eval_every=2***************
2022-10-24 15:19:39,420 - trainer - INFO - {
  "dev_loss": 0.22481714189052582,
  "dev_best_score_for_loss": -0.22481714189052582
}
2022-10-24 15:19:39,423 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,424 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,425 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,425 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_278
2022-10-24 15:19:39,427 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,431 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,432 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,432 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,433 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,433 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_284
2022-10-24 15:19:39,438 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_284
2022-10-24 15:19:39,440 - trainer - INFO - 
*****************[epoch: 284, global step: 285] eval training set at end of epoch***************
2022-10-24 15:19:39,443 - trainer - INFO - {
  "train_loss": 0.22481749951839447
}
2022-10-24 15:19:39,444 - trainer - INFO - start training epoch 285
2022-10-24 15:19:39,444 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,445 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,445 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,455 - trainer - INFO - 
*****************[epoch: 285, global step: 286] eval training set at end of epoch***************
2022-10-24 15:19:39,456 - trainer - INFO - {
  "train_loss": 0.22481714189052582
}
2022-10-24 15:19:39,456 - trainer - INFO - start training epoch 286
2022-10-24 15:19:39,456 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,457 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,457 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,467 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval training set based on eval_every=2***************
2022-10-24 15:19:39,469 - trainer - INFO - {
  "train_loss": 0.2248205542564392
}
2022-10-24 15:19:39,478 - trainer - INFO - 
*****************[epoch: 286, global step: 286] eval development set based on eval_every=2***************
2022-10-24 15:19:39,479 - trainer - INFO - {
  "dev_loss": 0.22483059763908386,
  "dev_best_score_for_loss": -0.22481714189052582
}
2022-10-24 15:19:39,479 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:39,480 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,481 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,482 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,482 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_280
2022-10-24 15:19:39,484 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_286
2022-10-24 15:19:39,494 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_286
2022-10-24 15:19:39,495 - trainer - INFO - 
*****************[epoch: 286, global step: 287] eval training set at end of epoch***************
2022-10-24 15:19:39,496 - trainer - INFO - {
  "train_loss": 0.2248239666223526
}
2022-10-24 15:19:39,496 - trainer - INFO - start training epoch 287
2022-10-24 15:19:39,496 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,497 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,497 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,509 - trainer - INFO - 
*****************[epoch: 287, global step: 288] eval training set at end of epoch***************
2022-10-24 15:19:39,509 - trainer - INFO - {
  "train_loss": 0.22483058273792267
}
2022-10-24 15:19:39,510 - trainer - INFO - start training epoch 288
2022-10-24 15:19:39,510 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,510 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,511 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,521 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval training set based on eval_every=2***************
2022-10-24 15:19:39,522 - trainer - INFO - {
  "train_loss": 0.22482898831367493
}
2022-10-24 15:19:39,534 - trainer - INFO - 
*****************[epoch: 288, global step: 288] eval development set based on eval_every=2***************
2022-10-24 15:19:39,538 - trainer - INFO - {
  "dev_loss": 0.22478742897510529,
  "dev_best_score_for_loss": -0.22478742897510529
}
2022-10-24 15:19:39,539 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,541 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,542 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,542 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_282
2022-10-24 15:19:39,544 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,549 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,549 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,550 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,551 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,551 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_288
2022-10-24 15:19:39,557 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_288
2022-10-24 15:19:39,558 - trainer - INFO - 
*****************[epoch: 288, global step: 289] eval training set at end of epoch***************
2022-10-24 15:19:39,558 - trainer - INFO - {
  "train_loss": 0.22482739388942719
}
2022-10-24 15:19:39,559 - trainer - INFO - start training epoch 289
2022-10-24 15:19:39,559 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,560 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,560 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,574 - trainer - INFO - 
*****************[epoch: 289, global step: 290] eval training set at end of epoch***************
2022-10-24 15:19:39,574 - trainer - INFO - {
  "train_loss": 0.22478744387626648
}
2022-10-24 15:19:39,574 - trainer - INFO - start training epoch 290
2022-10-24 15:19:39,575 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,575 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,575 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,588 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval training set based on eval_every=2***************
2022-10-24 15:19:39,589 - trainer - INFO - {
  "train_loss": 0.22479360550642014
}
2022-10-24 15:19:39,600 - trainer - INFO - 
*****************[epoch: 290, global step: 290] eval development set based on eval_every=2***************
2022-10-24 15:19:39,601 - trainer - INFO - {
  "dev_loss": 0.2247953563928604,
  "dev_best_score_for_loss": -0.22478742897510529
}
2022-10-24 15:19:39,602 - trainer - INFO -   no_improve_count: 1
2022-10-24 15:19:39,602 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,603 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,604 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,604 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_284
2022-10-24 15:19:39,606 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_290
2022-10-24 15:19:39,611 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_290
2022-10-24 15:19:39,613 - trainer - INFO - 
*****************[epoch: 290, global step: 291] eval training set at end of epoch***************
2022-10-24 15:19:39,613 - trainer - INFO - {
  "train_loss": 0.2247997671365738
}
2022-10-24 15:19:39,614 - trainer - INFO - start training epoch 291
2022-10-24 15:19:39,614 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,614 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,615 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,625 - trainer - INFO - 
*****************[epoch: 291, global step: 292] eval training set at end of epoch***************
2022-10-24 15:19:39,630 - trainer - INFO - {
  "train_loss": 0.2247953563928604
}
2022-10-24 15:19:39,631 - trainer - INFO - start training epoch 292
2022-10-24 15:19:39,631 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,631 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,632 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,641 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval training set based on eval_every=2***************
2022-10-24 15:19:39,642 - trainer - INFO - {
  "train_loss": 0.22477952390909195
}
2022-10-24 15:19:39,650 - trainer - INFO - 
*****************[epoch: 292, global step: 292] eval development set based on eval_every=2***************
2022-10-24 15:19:39,651 - trainer - INFO - {
  "dev_loss": 0.22476567327976227,
  "dev_best_score_for_loss": -0.22476567327976227
}
2022-10-24 15:19:39,652 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,653 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,654 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,654 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_286
2022-10-24 15:19:39,656 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,661 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,661 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,661 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,662 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,663 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_292
2022-10-24 15:19:39,668 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_292
2022-10-24 15:19:39,670 - trainer - INFO - 
*****************[epoch: 292, global step: 293] eval training set at end of epoch***************
2022-10-24 15:19:39,671 - trainer - INFO - {
  "train_loss": 0.2247636914253235
}
2022-10-24 15:19:39,676 - trainer - INFO - start training epoch 293
2022-10-24 15:19:39,676 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,677 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,677 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,688 - trainer - INFO - 
*****************[epoch: 293, global step: 294] eval training set at end of epoch***************
2022-10-24 15:19:39,688 - trainer - INFO - {
  "train_loss": 0.22476568818092346
}
2022-10-24 15:19:39,688 - trainer - INFO - start training epoch 294
2022-10-24 15:19:39,689 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,689 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,689 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,698 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval training set based on eval_every=2***************
2022-10-24 15:19:39,699 - trainer - INFO - {
  "train_loss": 0.22476308047771454
}
2022-10-24 15:19:39,707 - trainer - INFO - 
*****************[epoch: 294, global step: 294] eval development set based on eval_every=2***************
2022-10-24 15:19:39,707 - trainer - INFO - {
  "dev_loss": 0.22476333379745483,
  "dev_best_score_for_loss": -0.22476333379745483
}
2022-10-24 15:19:39,708 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,709 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,709 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,710 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_288
2022-10-24 15:19:39,711 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,715 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,716 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,717 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,719 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,722 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_294
2022-10-24 15:19:39,728 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_294
2022-10-24 15:19:39,729 - trainer - INFO - 
*****************[epoch: 294, global step: 295] eval training set at end of epoch***************
2022-10-24 15:19:39,730 - trainer - INFO - {
  "train_loss": 0.22476047277450562
}
2022-10-24 15:19:39,730 - trainer - INFO - start training epoch 295
2022-10-24 15:19:39,732 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,733 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,733 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,743 - trainer - INFO - 
*****************[epoch: 295, global step: 296] eval training set at end of epoch***************
2022-10-24 15:19:39,744 - trainer - INFO - {
  "train_loss": 0.22476330399513245
}
2022-10-24 15:19:39,744 - trainer - INFO - start training epoch 296
2022-10-24 15:19:39,744 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,745 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,745 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,755 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval training set based on eval_every=2***************
2022-10-24 15:19:39,756 - trainer - INFO - {
  "train_loss": 0.2247593179345131
}
2022-10-24 15:19:39,766 - trainer - INFO - 
*****************[epoch: 296, global step: 296] eval development set based on eval_every=2***************
2022-10-24 15:19:39,770 - trainer - INFO - {
  "dev_loss": 0.22474466264247894,
  "dev_best_score_for_loss": -0.22474466264247894
}
2022-10-24 15:19:39,771 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,773 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,773 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,774 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_290
2022-10-24 15:19:39,776 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,780 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,780 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,781 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,782 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,782 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_296
2022-10-24 15:19:39,787 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_296
2022-10-24 15:19:39,788 - trainer - INFO - 
*****************[epoch: 296, global step: 297] eval training set at end of epoch***************
2022-10-24 15:19:39,788 - trainer - INFO - {
  "train_loss": 0.22475533187389374
}
2022-10-24 15:19:39,789 - trainer - INFO - start training epoch 297
2022-10-24 15:19:39,789 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,790 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,790 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,803 - trainer - INFO - 
*****************[epoch: 297, global step: 298] eval training set at end of epoch***************
2022-10-24 15:19:39,803 - trainer - INFO - {
  "train_loss": 0.22474466264247894
}
2022-10-24 15:19:39,804 - trainer - INFO - start training epoch 298
2022-10-24 15:19:39,804 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,805 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,805 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,818 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval training set based on eval_every=2***************
2022-10-24 15:19:39,819 - trainer - INFO - {
  "train_loss": 0.22474094480276108
}
2022-10-24 15:19:39,829 - trainer - INFO - 
*****************[epoch: 298, global step: 298] eval development set based on eval_every=2***************
2022-10-24 15:19:39,830 - trainer - INFO - {
  "dev_loss": 0.22473520040512085,
  "dev_best_score_for_loss": -0.22473520040512085
}
2022-10-24 15:19:39,832 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,834 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,834 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,835 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_292
2022-10-24 15:19:39,879 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,883 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,883 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,884 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,885 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,885 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_298
2022-10-24 15:19:39,890 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_298
2022-10-24 15:19:39,891 - trainer - INFO - 
*****************[epoch: 298, global step: 299] eval training set at end of epoch***************
2022-10-24 15:19:39,892 - trainer - INFO - {
  "train_loss": 0.2247372269630432
}
2022-10-24 15:19:39,892 - trainer - INFO - start training epoch 299
2022-10-24 15:19:39,892 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,893 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,893 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,903 - trainer - INFO - 
*****************[epoch: 299, global step: 300] eval training set at end of epoch***************
2022-10-24 15:19:39,904 - trainer - INFO - {
  "train_loss": 0.22473523020744324
}
2022-10-24 15:19:39,904 - trainer - INFO - start training epoch 300
2022-10-24 15:19:39,904 - trainer - INFO - training using device=cuda
2022-10-24 15:19:39,905 - trainer - INFO - 
*************hyperparam_dict**********

2022-10-24 15:19:39,905 - trainer - INFO - {
  "train_epochs": 300,
  "eval_batch_size": 64,
  "train_batch_size": 16,
  "no_improve_count": 0,
  "device": "cuda",
  "patience": 200,
  "save_path": "tmp/mlp_scm_kdd",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.1
}
2022-10-24 15:19:39,913 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval training set based on eval_every=2***************
2022-10-24 15:19:39,914 - trainer - INFO - {
  "train_loss": 0.22473320364952087
}
2022-10-24 15:19:39,925 - trainer - INFO - 
*****************[epoch: 300, global step: 300] eval development set based on eval_every=2***************
2022-10-24 15:19:39,926 - trainer - INFO - {
  "dev_loss": 0.2247026115655899,
  "dev_best_score_for_loss": -0.2247026115655899
}
2022-10-24 15:19:39,927 - trainer - INFO -    save the model with best score so far
2022-10-24 15:19:39,929 - trainer - INFO -    Check 3 checkpoints already saved
2022-10-24 15:19:39,930 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-10-24 15:19:39,930 - trainer - INFO -   Remove checkpoint tmp/mlp_scm_kdd\ck_294
2022-10-24 15:19:39,932 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd
2022-10-24 15:19:39,938 - trainer - INFO - save model to path: tmp/mlp_scm_kdd
2022-10-24 15:19:39,939 - trainer - INFO -   no_improve_count: 0
2022-10-24 15:19:39,939 - trainer - INFO -   patience: 200
2022-10-24 15:19:39,940 - trainer - INFO -    Check 2 checkpoints already saved
2022-10-24 15:19:39,940 - trainer - INFO -   Save checkpoint to tmp/mlp_scm_kdd\ck_300
2022-10-24 15:19:39,945 - trainer - INFO - save model to path: tmp/mlp_scm_kdd\ck_300
2022-10-24 15:19:39,946 - trainer - INFO - 
*****************[epoch: 300, global step: 301] eval training set at end of epoch***************
2022-10-24 15:19:39,946 - trainer - INFO - {
  "train_loss": 0.2247311770915985
}
