2022-09-23 14:23:17,825 - trainer - INFO - MLP(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=32, bias=True)
  )
  (activation_layers): ModuleList(
    (0): ReLU(inplace=True)
    (1): ReLU(inplace=True)
    (2): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (linear_output): Linear(in_features=32, out_features=1, bias=True)
)
2022-09-23 14:23:17,827 - trainer - INFO -   Total params: 10625
2022-09-23 14:23:17,827 - trainer - INFO -   Trainable params: 10625
2022-09-23 14:23:17,827 - trainer - INFO -   Non-trainable params: 0
2022-09-23 14:23:17,828 - trainer - INFO -   There are 12  training examples
2022-09-23 14:23:17,828 - trainer - INFO -   There are 12 examples for development
2022-09-23 14:23:17,829 - trainer - INFO - start training epoch 1
2022-09-23 14:23:17,829 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,829 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,830 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,862 - trainer - INFO - 
*****************[epoch: 1, global step: 2] eval training set at end of epoch***************
2022-09-23 14:23:17,862 - trainer - INFO - {
  "train_loss": 5224.357421875
}
2022-09-23 14:23:17,863 - trainer - INFO - start training epoch 2
2022-09-23 14:23:17,863 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,863 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,863 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,867 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval training set based on eval_every=2***************
2022-09-23 14:23:17,867 - trainer - INFO - {
  "train_loss": 5081.291748046875
}
2022-09-23 14:23:17,870 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval development set based on eval_every=2***************
2022-09-23 14:23:17,870 - trainer - INFO - {
  "dev_loss": 4241.16748046875,
  "dev_best_score_for_loss": -4241.16748046875
}
2022-09-23 14:23:17,871 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:17,871 - trainer - INFO -    Check 0 checkpoints already saved
2022-09-23 14:23:17,872 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:17,874 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:17,875 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:17,875 - trainer - INFO -   patience: 200
2022-09-23 14:23:17,876 - trainer - INFO -    Check 0 checkpoints already saved
2022-09-23 14:23:17,876 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_2
2022-09-23 14:23:17,879 - trainer - INFO - save model to path: model/mlp_pivot\ck_2
2022-09-23 14:23:17,879 - trainer - INFO - 
*****************[epoch: 2, global step: 3] eval training set at end of epoch***************
2022-09-23 14:23:17,879 - trainer - INFO - {
  "train_loss": 4938.22607421875
}
2022-09-23 14:23:17,880 - trainer - INFO - start training epoch 3
2022-09-23 14:23:17,880 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,880 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,881 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,884 - trainer - INFO - 
*****************[epoch: 3, global step: 4] eval training set at end of epoch***************
2022-09-23 14:23:17,884 - trainer - INFO - {
  "train_loss": 4241.16748046875
}
2022-09-23 14:23:17,885 - trainer - INFO - start training epoch 4
2022-09-23 14:23:17,885 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,885 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,886 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,888 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval training set based on eval_every=2***************
2022-09-23 14:23:17,889 - trainer - INFO - {
  "train_loss": 3625.321533203125
}
2022-09-23 14:23:17,891 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval development set based on eval_every=2***************
2022-09-23 14:23:17,891 - trainer - INFO - {
  "dev_loss": 1302.531494140625,
  "dev_best_score_for_loss": -1302.531494140625
}
2022-09-23 14:23:17,892 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:17,893 - trainer - INFO -    Check 1 checkpoints already saved
2022-09-23 14:23:17,893 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:17,895 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:17,895 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:17,895 - trainer - INFO -   patience: 200
2022-09-23 14:23:17,896 - trainer - INFO -    Check 1 checkpoints already saved
2022-09-23 14:23:17,896 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_4
2022-09-23 14:23:17,899 - trainer - INFO - save model to path: model/mlp_pivot\ck_4
2022-09-23 14:23:17,899 - trainer - INFO - 
*****************[epoch: 4, global step: 5] eval training set at end of epoch***************
2022-09-23 14:23:17,900 - trainer - INFO - {
  "train_loss": 3009.4755859375
}
2022-09-23 14:23:17,900 - trainer - INFO - start training epoch 5
2022-09-23 14:23:17,900 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,900 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,901 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,904 - trainer - INFO - 
*****************[epoch: 5, global step: 6] eval training set at end of epoch***************
2022-09-23 14:23:17,905 - trainer - INFO - {
  "train_loss": 1302.531494140625
}
2022-09-23 14:23:17,905 - trainer - INFO - start training epoch 6
2022-09-23 14:23:17,905 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,905 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,906 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,909 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval training set based on eval_every=2***************
2022-09-23 14:23:17,909 - trainer - INFO - {
  "train_loss": 675.8843441009521
}
2022-09-23 14:23:17,912 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval development set based on eval_every=2***************
2022-09-23 14:23:17,912 - trainer - INFO - {
  "dev_loss": 1739.9166259765625,
  "dev_best_score_for_loss": -1302.531494140625
}
2022-09-23 14:23:17,913 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:17,914 - trainer - INFO -   patience: 200
2022-09-23 14:23:17,915 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:17,915 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_6
2022-09-23 14:23:17,919 - trainer - INFO - save model to path: model/mlp_pivot\ck_6
2022-09-23 14:23:17,920 - trainer - INFO - 
*****************[epoch: 6, global step: 7] eval training set at end of epoch***************
2022-09-23 14:23:17,921 - trainer - INFO - {
  "train_loss": 49.2371940612793
}
2022-09-23 14:23:17,921 - trainer - INFO - start training epoch 7
2022-09-23 14:23:17,921 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,921 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,922 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,926 - trainer - INFO - 
*****************[epoch: 7, global step: 8] eval training set at end of epoch***************
2022-09-23 14:23:17,926 - trainer - INFO - {
  "train_loss": 1739.9166259765625
}
2022-09-23 14:23:17,927 - trainer - INFO - start training epoch 8
2022-09-23 14:23:17,927 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,927 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,928 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,932 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval training set based on eval_every=2***************
2022-09-23 14:23:17,932 - trainer - INFO - {
  "train_loss": 1530.4451904296875
}
2022-09-23 14:23:17,935 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval development set based on eval_every=2***************
2022-09-23 14:23:17,935 - trainer - INFO - {
  "dev_loss": 291.3945617675781,
  "dev_best_score_for_loss": -291.3945617675781
}
2022-09-23 14:23:17,936 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:17,937 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:17,937 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:17,937 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_2
2022-09-23 14:23:17,939 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:17,941 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:17,941 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:17,942 - trainer - INFO -   patience: 200
2022-09-23 14:23:17,943 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:17,943 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_8
2022-09-23 14:23:17,946 - trainer - INFO - save model to path: model/mlp_pivot\ck_8
2022-09-23 14:23:17,947 - trainer - INFO - 
*****************[epoch: 8, global step: 9] eval training set at end of epoch***************
2022-09-23 14:23:17,947 - trainer - INFO - {
  "train_loss": 1320.9737548828125
}
2022-09-23 14:23:17,947 - trainer - INFO - start training epoch 9
2022-09-23 14:23:17,948 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,948 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,948 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,952 - trainer - INFO - 
*****************[epoch: 9, global step: 10] eval training set at end of epoch***************
2022-09-23 14:23:17,952 - trainer - INFO - {
  "train_loss": 291.3945617675781
}
2022-09-23 14:23:17,953 - trainer - INFO - start training epoch 10
2022-09-23 14:23:17,953 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,953 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,954 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,958 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval training set based on eval_every=2***************
2022-09-23 14:23:17,958 - trainer - INFO - {
  "train_loss": 171.966796875
}
2022-09-23 14:23:17,961 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval development set based on eval_every=2***************
2022-09-23 14:23:17,961 - trainer - INFO - {
  "dev_loss": 338.7489013671875,
  "dev_best_score_for_loss": -291.3945617675781
}
2022-09-23 14:23:17,962 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:17,963 - trainer - INFO -   patience: 200
2022-09-23 14:23:17,963 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:17,964 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:17,964 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_4
2022-09-23 14:23:17,965 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_10
2022-09-23 14:23:17,968 - trainer - INFO - save model to path: model/mlp_pivot\ck_10
2022-09-23 14:23:17,968 - trainer - INFO - 
*****************[epoch: 10, global step: 11] eval training set at end of epoch***************
2022-09-23 14:23:17,969 - trainer - INFO - {
  "train_loss": 52.539031982421875
}
2022-09-23 14:23:17,969 - trainer - INFO - start training epoch 11
2022-09-23 14:23:17,969 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,969 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,970 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,973 - trainer - INFO - 
*****************[epoch: 11, global step: 12] eval training set at end of epoch***************
2022-09-23 14:23:17,974 - trainer - INFO - {
  "train_loss": 338.7488708496094
}
2022-09-23 14:23:17,974 - trainer - INFO - start training epoch 12
2022-09-23 14:23:17,974 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,975 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,975 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,978 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval training set based on eval_every=2***************
2022-09-23 14:23:17,978 - trainer - INFO - {
  "train_loss": 500.71934509277344
}
2022-09-23 14:23:17,981 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval development set based on eval_every=2***************
2022-09-23 14:23:17,981 - trainer - INFO - {
  "dev_loss": 821.4901733398438,
  "dev_best_score_for_loss": -291.3945617675781
}
2022-09-23 14:23:17,982 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:23:17,982 - trainer - INFO -   patience: 200
2022-09-23 14:23:17,983 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:17,983 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:17,983 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_6
2022-09-23 14:23:17,984 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_12
2022-09-23 14:23:17,987 - trainer - INFO - save model to path: model/mlp_pivot\ck_12
2022-09-23 14:23:17,987 - trainer - INFO - 
*****************[epoch: 12, global step: 13] eval training set at end of epoch***************
2022-09-23 14:23:17,988 - trainer - INFO - {
  "train_loss": 662.6898193359375
}
2022-09-23 14:23:17,988 - trainer - INFO - start training epoch 13
2022-09-23 14:23:17,988 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,989 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,989 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,992 - trainer - INFO - 
*****************[epoch: 13, global step: 14] eval training set at end of epoch***************
2022-09-23 14:23:17,992 - trainer - INFO - {
  "train_loss": 821.4901733398438
}
2022-09-23 14:23:17,993 - trainer - INFO - start training epoch 14
2022-09-23 14:23:17,993 - trainer - INFO - training using device=cpu
2022-09-23 14:23:17,993 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:17,994 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:17,997 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval training set based on eval_every=2***************
2022-09-23 14:23:17,997 - trainer - INFO - {
  "train_loss": 804.0865173339844
}
2022-09-23 14:23:17,999 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval development set based on eval_every=2***************
2022-09-23 14:23:17,999 - trainer - INFO - {
  "dev_loss": 595.8993530273438,
  "dev_best_score_for_loss": -291.3945617675781
}
2022-09-23 14:23:18,000 - trainer - INFO -   no_improve_count: 3
2022-09-23 14:23:18,000 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,001 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,001 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,001 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_8
2022-09-23 14:23:18,002 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_14
2022-09-23 14:23:18,005 - trainer - INFO - save model to path: model/mlp_pivot\ck_14
2022-09-23 14:23:18,006 - trainer - INFO - 
*****************[epoch: 14, global step: 15] eval training set at end of epoch***************
2022-09-23 14:23:18,006 - trainer - INFO - {
  "train_loss": 786.682861328125
}
2022-09-23 14:23:18,006 - trainer - INFO - start training epoch 15
2022-09-23 14:23:18,006 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,007 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,007 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,011 - trainer - INFO - 
*****************[epoch: 15, global step: 16] eval training set at end of epoch***************
2022-09-23 14:23:18,011 - trainer - INFO - {
  "train_loss": 595.8992309570312
}
2022-09-23 14:23:18,011 - trainer - INFO - start training epoch 16
2022-09-23 14:23:18,011 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,012 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,012 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,015 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval training set based on eval_every=2***************
2022-09-23 14:23:18,015 - trainer - INFO - {
  "train_loss": 461.086181640625
}
2022-09-23 14:23:18,017 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval development set based on eval_every=2***************
2022-09-23 14:23:18,018 - trainer - INFO - {
  "dev_loss": 97.65188598632812,
  "dev_best_score_for_loss": -97.65188598632812
}
2022-09-23 14:23:18,018 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,019 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,019 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,019 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_10
2022-09-23 14:23:18,020 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,022 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,022 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,022 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,023 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,023 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_16
2022-09-23 14:23:18,026 - trainer - INFO - save model to path: model/mlp_pivot\ck_16
2022-09-23 14:23:18,027 - trainer - INFO - 
*****************[epoch: 16, global step: 17] eval training set at end of epoch***************
2022-09-23 14:23:18,027 - trainer - INFO - {
  "train_loss": 326.27313232421875
}
2022-09-23 14:23:18,028 - trainer - INFO - start training epoch 17
2022-09-23 14:23:18,028 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,028 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,029 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,032 - trainer - INFO - 
*****************[epoch: 17, global step: 18] eval training set at end of epoch***************
2022-09-23 14:23:18,033 - trainer - INFO - {
  "train_loss": 97.65189361572266
}
2022-09-23 14:23:18,033 - trainer - INFO - start training epoch 18
2022-09-23 14:23:18,033 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,034 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,034 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,037 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval training set based on eval_every=2***************
2022-09-23 14:23:18,037 - trainer - INFO - {
  "train_loss": 72.32021522521973
}
2022-09-23 14:23:18,040 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval development set based on eval_every=2***************
2022-09-23 14:23:18,040 - trainer - INFO - {
  "dev_loss": 212.2302703857422,
  "dev_best_score_for_loss": -97.65188598632812
}
2022-09-23 14:23:18,041 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,041 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,042 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,042 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,042 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_12
2022-09-23 14:23:18,043 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_18
2022-09-23 14:23:18,046 - trainer - INFO - save model to path: model/mlp_pivot\ck_18
2022-09-23 14:23:18,047 - trainer - INFO - 
*****************[epoch: 18, global step: 19] eval training set at end of epoch***************
2022-09-23 14:23:18,047 - trainer - INFO - {
  "train_loss": 46.9885368347168
}
2022-09-23 14:23:18,048 - trainer - INFO - start training epoch 19
2022-09-23 14:23:18,048 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,048 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,048 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,052 - trainer - INFO - 
*****************[epoch: 19, global step: 20] eval training set at end of epoch***************
2022-09-23 14:23:18,052 - trainer - INFO - {
  "train_loss": 212.2302703857422
}
2022-09-23 14:23:18,052 - trainer - INFO - start training epoch 20
2022-09-23 14:23:18,053 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,053 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,053 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,056 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval training set based on eval_every=2***************
2022-09-23 14:23:18,057 - trainer - INFO - {
  "train_loss": 307.4950637817383
}
2022-09-23 14:23:18,059 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval development set based on eval_every=2***************
2022-09-23 14:23:18,060 - trainer - INFO - {
  "dev_loss": 383.015625,
  "dev_best_score_for_loss": -97.65188598632812
}
2022-09-23 14:23:18,060 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:23:18,060 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,061 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,062 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,062 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_14
2022-09-23 14:23:18,063 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_20
2022-09-23 14:23:18,066 - trainer - INFO - save model to path: model/mlp_pivot\ck_20
2022-09-23 14:23:18,066 - trainer - INFO - 
*****************[epoch: 20, global step: 21] eval training set at end of epoch***************
2022-09-23 14:23:18,067 - trainer - INFO - {
  "train_loss": 402.7598571777344
}
2022-09-23 14:23:18,067 - trainer - INFO - start training epoch 21
2022-09-23 14:23:18,067 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,067 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,068 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,072 - trainer - INFO - 
*****************[epoch: 21, global step: 22] eval training set at end of epoch***************
2022-09-23 14:23:18,072 - trainer - INFO - {
  "train_loss": 383.015625
}
2022-09-23 14:23:18,072 - trainer - INFO - start training epoch 22
2022-09-23 14:23:18,073 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,073 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,073 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,077 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval training set based on eval_every=2***************
2022-09-23 14:23:18,078 - trainer - INFO - {
  "train_loss": 291.07489013671875
}
2022-09-23 14:23:18,080 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval development set based on eval_every=2***************
2022-09-23 14:23:18,080 - trainer - INFO - {
  "dev_loss": 56.49314498901367,
  "dev_best_score_for_loss": -56.49314498901367
}
2022-09-23 14:23:18,081 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,082 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,082 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,082 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_16
2022-09-23 14:23:18,083 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,085 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,085 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,085 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,086 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,086 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_22
2022-09-23 14:23:18,089 - trainer - INFO - save model to path: model/mlp_pivot\ck_22
2022-09-23 14:23:18,089 - trainer - INFO - 
*****************[epoch: 22, global step: 23] eval training set at end of epoch***************
2022-09-23 14:23:18,090 - trainer - INFO - {
  "train_loss": 199.1341552734375
}
2022-09-23 14:23:18,090 - trainer - INFO - start training epoch 23
2022-09-23 14:23:18,090 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,090 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,091 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,094 - trainer - INFO - 
*****************[epoch: 23, global step: 24] eval training set at end of epoch***************
2022-09-23 14:23:18,095 - trainer - INFO - {
  "train_loss": 56.49314498901367
}
2022-09-23 14:23:18,095 - trainer - INFO - start training epoch 24
2022-09-23 14:23:18,095 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,095 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,096 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,099 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval training set based on eval_every=2***************
2022-09-23 14:23:18,099 - trainer - INFO - {
  "train_loss": 51.82722473144531
}
2022-09-23 14:23:18,102 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval development set based on eval_every=2***************
2022-09-23 14:23:18,102 - trainer - INFO - {
  "dev_loss": 123.37342071533203,
  "dev_best_score_for_loss": -56.49314498901367
}
2022-09-23 14:23:18,102 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,103 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,104 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,104 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,104 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_18
2022-09-23 14:23:18,105 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_24
2022-09-23 14:23:18,108 - trainer - INFO - save model to path: model/mlp_pivot\ck_24
2022-09-23 14:23:18,108 - trainer - INFO - 
*****************[epoch: 24, global step: 25] eval training set at end of epoch***************
2022-09-23 14:23:18,109 - trainer - INFO - {
  "train_loss": 47.16130447387695
}
2022-09-23 14:23:18,109 - trainer - INFO - start training epoch 25
2022-09-23 14:23:18,109 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,109 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,110 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,115 - trainer - INFO - 
*****************[epoch: 25, global step: 26] eval training set at end of epoch***************
2022-09-23 14:23:18,115 - trainer - INFO - {
  "train_loss": 123.3734359741211
}
2022-09-23 14:23:18,116 - trainer - INFO - start training epoch 26
2022-09-23 14:23:18,116 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,116 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,116 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,120 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval training set based on eval_every=2***************
2022-09-23 14:23:18,120 - trainer - INFO - {
  "train_loss": 162.50325393676758
}
2022-09-23 14:23:18,123 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval development set based on eval_every=2***************
2022-09-23 14:23:18,124 - trainer - INFO - {
  "dev_loss": 229.1063690185547,
  "dev_best_score_for_loss": -56.49314498901367
}
2022-09-23 14:23:18,124 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:23:18,125 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,126 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,126 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,126 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_20
2022-09-23 14:23:18,128 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_26
2022-09-23 14:23:18,131 - trainer - INFO - save model to path: model/mlp_pivot\ck_26
2022-09-23 14:23:18,132 - trainer - INFO - 
*****************[epoch: 26, global step: 27] eval training set at end of epoch***************
2022-09-23 14:23:18,133 - trainer - INFO - {
  "train_loss": 201.63307189941406
}
2022-09-23 14:23:18,133 - trainer - INFO - start training epoch 27
2022-09-23 14:23:18,133 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,134 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,134 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,138 - trainer - INFO - 
*****************[epoch: 27, global step: 28] eval training set at end of epoch***************
2022-09-23 14:23:18,139 - trainer - INFO - {
  "train_loss": 229.10638427734375
}
2022-09-23 14:23:18,139 - trainer - INFO - start training epoch 28
2022-09-23 14:23:18,140 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,140 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,140 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,144 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval training set based on eval_every=2***************
2022-09-23 14:23:18,144 - trainer - INFO - {
  "train_loss": 212.08220672607422
}
2022-09-23 14:23:18,147 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval development set based on eval_every=2***************
2022-09-23 14:23:18,147 - trainer - INFO - {
  "dev_loss": 122.3826904296875,
  "dev_best_score_for_loss": -56.49314498901367
}
2022-09-23 14:23:18,148 - trainer - INFO -   no_improve_count: 3
2022-09-23 14:23:18,148 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,149 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,149 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,150 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_22
2022-09-23 14:23:18,151 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_28
2022-09-23 14:23:18,154 - trainer - INFO - save model to path: model/mlp_pivot\ck_28
2022-09-23 14:23:18,155 - trainer - INFO - 
*****************[epoch: 28, global step: 29] eval training set at end of epoch***************
2022-09-23 14:23:18,155 - trainer - INFO - {
  "train_loss": 195.0580291748047
}
2022-09-23 14:23:18,155 - trainer - INFO - start training epoch 29
2022-09-23 14:23:18,156 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,156 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,156 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,160 - trainer - INFO - 
*****************[epoch: 29, global step: 30] eval training set at end of epoch***************
2022-09-23 14:23:18,161 - trainer - INFO - {
  "train_loss": 122.38269805908203
}
2022-09-23 14:23:18,161 - trainer - INFO - start training epoch 30
2022-09-23 14:23:18,161 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,162 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,162 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,165 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval training set based on eval_every=2***************
2022-09-23 14:23:18,166 - trainer - INFO - {
  "train_loss": 88.53984642028809
}
2022-09-23 14:23:18,168 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval development set based on eval_every=2***************
2022-09-23 14:23:18,168 - trainer - INFO - {
  "dev_loss": 34.69367218017578,
  "dev_best_score_for_loss": -34.69367218017578
}
2022-09-23 14:23:18,169 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,170 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,170 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,170 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_24
2022-09-23 14:23:18,171 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,174 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,174 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,174 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,175 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,175 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_30
2022-09-23 14:23:18,179 - trainer - INFO - save model to path: model/mlp_pivot\ck_30
2022-09-23 14:23:18,179 - trainer - INFO - 
*****************[epoch: 30, global step: 31] eval training set at end of epoch***************
2022-09-23 14:23:18,180 - trainer - INFO - {
  "train_loss": 54.69699478149414
}
2022-09-23 14:23:18,180 - trainer - INFO - start training epoch 31
2022-09-23 14:23:18,180 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,181 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,181 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,185 - trainer - INFO - 
*****************[epoch: 31, global step: 32] eval training set at end of epoch***************
2022-09-23 14:23:18,186 - trainer - INFO - {
  "train_loss": 34.693668365478516
}
2022-09-23 14:23:18,186 - trainer - INFO - start training epoch 32
2022-09-23 14:23:18,186 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,186 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,187 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,191 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval training set based on eval_every=2***************
2022-09-23 14:23:18,191 - trainer - INFO - {
  "train_loss": 53.01267051696777
}
2022-09-23 14:23:18,193 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval development set based on eval_every=2***************
2022-09-23 14:23:18,194 - trainer - INFO - {
  "dev_loss": 121.86865234375,
  "dev_best_score_for_loss": -34.69367218017578
}
2022-09-23 14:23:18,194 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,195 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,195 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,196 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,196 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_26
2022-09-23 14:23:18,197 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_32
2022-09-23 14:23:18,200 - trainer - INFO - save model to path: model/mlp_pivot\ck_32
2022-09-23 14:23:18,201 - trainer - INFO - 
*****************[epoch: 32, global step: 33] eval training set at end of epoch***************
2022-09-23 14:23:18,201 - trainer - INFO - {
  "train_loss": 71.33167266845703
}
2022-09-23 14:23:18,201 - trainer - INFO - start training epoch 33
2022-09-23 14:23:18,202 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,202 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,202 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,206 - trainer - INFO - 
*****************[epoch: 33, global step: 34] eval training set at end of epoch***************
2022-09-23 14:23:18,206 - trainer - INFO - {
  "train_loss": 121.86865997314453
}
2022-09-23 14:23:18,207 - trainer - INFO - start training epoch 34
2022-09-23 14:23:18,207 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,207 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,207 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,210 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval training set based on eval_every=2***************
2022-09-23 14:23:18,211 - trainer - INFO - {
  "train_loss": 125.91549301147461
}
2022-09-23 14:23:18,213 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval development set based on eval_every=2***************
2022-09-23 14:23:18,213 - trainer - INFO - {
  "dev_loss": 89.60592651367188,
  "dev_best_score_for_loss": -34.69367218017578
}
2022-09-23 14:23:18,214 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:23:18,214 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,215 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,215 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,216 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_28
2022-09-23 14:23:18,217 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_34
2022-09-23 14:23:18,219 - trainer - INFO - save model to path: model/mlp_pivot\ck_34
2022-09-23 14:23:18,220 - trainer - INFO - 
*****************[epoch: 34, global step: 35] eval training set at end of epoch***************
2022-09-23 14:23:18,220 - trainer - INFO - {
  "train_loss": 129.9623260498047
}
2022-09-23 14:23:18,221 - trainer - INFO - start training epoch 35
2022-09-23 14:23:18,221 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,221 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,222 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,225 - trainer - INFO - 
*****************[epoch: 35, global step: 36] eval training set at end of epoch***************
2022-09-23 14:23:18,225 - trainer - INFO - {
  "train_loss": 89.60591888427734
}
2022-09-23 14:23:18,226 - trainer - INFO - start training epoch 36
2022-09-23 14:23:18,226 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,226 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,227 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,230 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval training set based on eval_every=2***************
2022-09-23 14:23:18,231 - trainer - INFO - {
  "train_loss": 67.19421768188477
}
2022-09-23 14:23:18,233 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval development set based on eval_every=2***************
2022-09-23 14:23:18,233 - trainer - INFO - {
  "dev_loss": 32.3097038269043,
  "dev_best_score_for_loss": -32.3097038269043
}
2022-09-23 14:23:18,234 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,235 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,236 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,236 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_30
2022-09-23 14:23:18,237 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,239 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,239 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,239 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,240 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,240 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_36
2022-09-23 14:23:18,242 - trainer - INFO - save model to path: model/mlp_pivot\ck_36
2022-09-23 14:23:18,243 - trainer - INFO - 
*****************[epoch: 36, global step: 37] eval training set at end of epoch***************
2022-09-23 14:23:18,243 - trainer - INFO - {
  "train_loss": 44.78251647949219
}
2022-09-23 14:23:18,243 - trainer - INFO - start training epoch 37
2022-09-23 14:23:18,244 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,244 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,244 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,248 - trainer - INFO - 
*****************[epoch: 37, global step: 38] eval training set at end of epoch***************
2022-09-23 14:23:18,248 - trainer - INFO - {
  "train_loss": 32.3097038269043
}
2022-09-23 14:23:18,248 - trainer - INFO - start training epoch 38
2022-09-23 14:23:18,249 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,249 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,249 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,252 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval training set based on eval_every=2***************
2022-09-23 14:23:18,253 - trainer - INFO - {
  "train_loss": 41.51704216003418
}
2022-09-23 14:23:18,255 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval development set based on eval_every=2***************
2022-09-23 14:23:18,255 - trainer - INFO - {
  "dev_loss": 75.26456451416016,
  "dev_best_score_for_loss": -32.3097038269043
}
2022-09-23 14:23:18,256 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,256 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,257 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,257 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,257 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_32
2022-09-23 14:23:18,258 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_38
2022-09-23 14:23:18,261 - trainer - INFO - save model to path: model/mlp_pivot\ck_38
2022-09-23 14:23:18,263 - trainer - INFO - 
*****************[epoch: 38, global step: 39] eval training set at end of epoch***************
2022-09-23 14:23:18,263 - trainer - INFO - {
  "train_loss": 50.72438049316406
}
2022-09-23 14:23:18,264 - trainer - INFO - start training epoch 39
2022-09-23 14:23:18,264 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,264 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,265 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,269 - trainer - INFO - 
*****************[epoch: 39, global step: 40] eval training set at end of epoch***************
2022-09-23 14:23:18,269 - trainer - INFO - {
  "train_loss": 75.26456451416016
}
2022-09-23 14:23:18,270 - trainer - INFO - start training epoch 40
2022-09-23 14:23:18,270 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,270 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,271 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,274 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval training set based on eval_every=2***************
2022-09-23 14:23:18,275 - trainer - INFO - {
  "train_loss": 79.60313415527344
}
2022-09-23 14:23:18,278 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval development set based on eval_every=2***************
2022-09-23 14:23:18,278 - trainer - INFO - {
  "dev_loss": 71.1153793334961,
  "dev_best_score_for_loss": -32.3097038269043
}
2022-09-23 14:23:18,279 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:23:18,279 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,280 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,280 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,281 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_34
2022-09-23 14:23:18,282 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_40
2022-09-23 14:23:18,285 - trainer - INFO - save model to path: model/mlp_pivot\ck_40
2022-09-23 14:23:18,286 - trainer - INFO - 
*****************[epoch: 40, global step: 41] eval training set at end of epoch***************
2022-09-23 14:23:18,286 - trainer - INFO - {
  "train_loss": 83.94170379638672
}
2022-09-23 14:23:18,287 - trainer - INFO - start training epoch 41
2022-09-23 14:23:18,287 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,287 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,288 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,292 - trainer - INFO - 
*****************[epoch: 41, global step: 42] eval training set at end of epoch***************
2022-09-23 14:23:18,293 - trainer - INFO - {
  "train_loss": 71.1153793334961
}
2022-09-23 14:23:18,293 - trainer - INFO - start training epoch 42
2022-09-23 14:23:18,293 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,293 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,294 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,298 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval training set based on eval_every=2***************
2022-09-23 14:23:18,298 - trainer - INFO - {
  "train_loss": 59.295894622802734
}
2022-09-23 14:23:18,301 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval development set based on eval_every=2***************
2022-09-23 14:23:18,301 - trainer - INFO - {
  "dev_loss": 31.028249740600586,
  "dev_best_score_for_loss": -31.028249740600586
}
2022-09-23 14:23:18,302 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,303 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,303 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,303 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_36
2022-09-23 14:23:18,305 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,307 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,307 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,308 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,308 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,309 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_42
2022-09-23 14:23:18,312 - trainer - INFO - save model to path: model/mlp_pivot\ck_42
2022-09-23 14:23:18,312 - trainer - INFO - 
*****************[epoch: 42, global step: 43] eval training set at end of epoch***************
2022-09-23 14:23:18,313 - trainer - INFO - {
  "train_loss": 47.476409912109375
}
2022-09-23 14:23:18,313 - trainer - INFO - start training epoch 43
2022-09-23 14:23:18,313 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,313 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,314 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,318 - trainer - INFO - 
*****************[epoch: 43, global step: 44] eval training set at end of epoch***************
2022-09-23 14:23:18,318 - trainer - INFO - {
  "train_loss": 31.02824592590332
}
2022-09-23 14:23:18,319 - trainer - INFO - start training epoch 44
2022-09-23 14:23:18,319 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,319 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,320 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,323 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval training set based on eval_every=2***************
2022-09-23 14:23:18,323 - trainer - INFO - {
  "train_loss": 32.07186412811279
}
2022-09-23 14:23:18,326 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval development set based on eval_every=2***************
2022-09-23 14:23:18,326 - trainer - INFO - {
  "dev_loss": 47.88785934448242,
  "dev_best_score_for_loss": -31.028249740600586
}
2022-09-23 14:23:18,327 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,327 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,328 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,328 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,329 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_38
2022-09-23 14:23:18,330 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_44
2022-09-23 14:23:18,332 - trainer - INFO - save model to path: model/mlp_pivot\ck_44
2022-09-23 14:23:18,333 - trainer - INFO - 
*****************[epoch: 44, global step: 45] eval training set at end of epoch***************
2022-09-23 14:23:18,333 - trainer - INFO - {
  "train_loss": 33.115482330322266
}
2022-09-23 14:23:18,333 - trainer - INFO - start training epoch 45
2022-09-23 14:23:18,334 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,334 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,334 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,338 - trainer - INFO - 
*****************[epoch: 45, global step: 46] eval training set at end of epoch***************
2022-09-23 14:23:18,338 - trainer - INFO - {
  "train_loss": 47.88785934448242
}
2022-09-23 14:23:18,338 - trainer - INFO - start training epoch 46
2022-09-23 14:23:18,338 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,339 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,339 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,343 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval training set based on eval_every=2***************
2022-09-23 14:23:18,344 - trainer - INFO - {
  "train_loss": 52.64984703063965
}
2022-09-23 14:23:18,346 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval development set based on eval_every=2***************
2022-09-23 14:23:18,347 - trainer - INFO - {
  "dev_loss": 51.08421325683594,
  "dev_best_score_for_loss": -31.028249740600586
}
2022-09-23 14:23:18,347 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:23:18,347 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,348 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,349 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,349 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_40
2022-09-23 14:23:18,350 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_46
2022-09-23 14:23:18,352 - trainer - INFO - save model to path: model/mlp_pivot\ck_46
2022-09-23 14:23:18,353 - trainer - INFO - 
*****************[epoch: 46, global step: 47] eval training set at end of epoch***************
2022-09-23 14:23:18,354 - trainer - INFO - {
  "train_loss": 57.411834716796875
}
2022-09-23 14:23:18,354 - trainer - INFO - start training epoch 47
2022-09-23 14:23:18,354 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,354 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,355 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,358 - trainer - INFO - 
*****************[epoch: 47, global step: 48] eval training set at end of epoch***************
2022-09-23 14:23:18,358 - trainer - INFO - {
  "train_loss": 51.08421325683594
}
2022-09-23 14:23:18,359 - trainer - INFO - start training epoch 48
2022-09-23 14:23:18,360 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,360 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,360 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,365 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval training set based on eval_every=2***************
2022-09-23 14:23:18,365 - trainer - INFO - {
  "train_loss": 43.740102767944336
}
2022-09-23 14:23:18,367 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval development set based on eval_every=2***************
2022-09-23 14:23:18,367 - trainer - INFO - {
  "dev_loss": 27.77332305908203,
  "dev_best_score_for_loss": -27.77332305908203
}
2022-09-23 14:23:18,368 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,369 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,369 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,369 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_42
2022-09-23 14:23:18,370 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,373 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,373 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,373 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,374 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,374 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_48
2022-09-23 14:23:18,376 - trainer - INFO - save model to path: model/mlp_pivot\ck_48
2022-09-23 14:23:18,377 - trainer - INFO - 
*****************[epoch: 48, global step: 49] eval training set at end of epoch***************
2022-09-23 14:23:18,377 - trainer - INFO - {
  "train_loss": 36.395992279052734
}
2022-09-23 14:23:18,378 - trainer - INFO - start training epoch 49
2022-09-23 14:23:18,378 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,379 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,379 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,383 - trainer - INFO - 
*****************[epoch: 49, global step: 50] eval training set at end of epoch***************
2022-09-23 14:23:18,383 - trainer - INFO - {
  "train_loss": 27.7733211517334
}
2022-09-23 14:23:18,383 - trainer - INFO - start training epoch 50
2022-09-23 14:23:18,383 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,384 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,384 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,387 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval training set based on eval_every=2***************
2022-09-23 14:23:18,388 - trainer - INFO - {
  "train_loss": 29.1536808013916
}
2022-09-23 14:23:18,390 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval development set based on eval_every=2***************
2022-09-23 14:23:18,390 - trainer - INFO - {
  "dev_loss": 38.44321060180664,
  "dev_best_score_for_loss": -27.77332305908203
}
2022-09-23 14:23:18,391 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,391 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,392 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,392 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,393 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_44
2022-09-23 14:23:18,394 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_50
2022-09-23 14:23:18,398 - trainer - INFO - save model to path: model/mlp_pivot\ck_50
2022-09-23 14:23:18,398 - trainer - INFO - 
*****************[epoch: 50, global step: 51] eval training set at end of epoch***************
2022-09-23 14:23:18,399 - trainer - INFO - {
  "train_loss": 30.534040451049805
}
2022-09-23 14:23:18,399 - trainer - INFO - start training epoch 51
2022-09-23 14:23:18,399 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,399 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,400 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,403 - trainer - INFO - 
*****************[epoch: 51, global step: 52] eval training set at end of epoch***************
2022-09-23 14:23:18,404 - trainer - INFO - {
  "train_loss": 38.44321060180664
}
2022-09-23 14:23:18,404 - trainer - INFO - start training epoch 52
2022-09-23 14:23:18,405 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,405 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,405 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,408 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval training set based on eval_every=2***************
2022-09-23 14:23:18,408 - trainer - INFO - {
  "train_loss": 40.3988094329834
}
2022-09-23 14:23:18,411 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval development set based on eval_every=2***************
2022-09-23 14:23:18,411 - trainer - INFO - {
  "dev_loss": 38.535701751708984,
  "dev_best_score_for_loss": -27.77332305908203
}
2022-09-23 14:23:18,412 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:23:18,412 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,413 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,413 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,413 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_46
2022-09-23 14:23:18,415 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_52
2022-09-23 14:23:18,417 - trainer - INFO - save model to path: model/mlp_pivot\ck_52
2022-09-23 14:23:18,418 - trainer - INFO - 
*****************[epoch: 52, global step: 53] eval training set at end of epoch***************
2022-09-23 14:23:18,418 - trainer - INFO - {
  "train_loss": 42.354408264160156
}
2022-09-23 14:23:18,418 - trainer - INFO - start training epoch 53
2022-09-23 14:23:18,419 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,419 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,419 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,423 - trainer - INFO - 
*****************[epoch: 53, global step: 54] eval training set at end of epoch***************
2022-09-23 14:23:18,423 - trainer - INFO - {
  "train_loss": 38.535701751708984
}
2022-09-23 14:23:18,424 - trainer - INFO - start training epoch 54
2022-09-23 14:23:18,424 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,424 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,424 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,427 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval training set based on eval_every=2***************
2022-09-23 14:23:18,428 - trainer - INFO - {
  "train_loss": 34.58773326873779
}
2022-09-23 14:23:18,431 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval development set based on eval_every=2***************
2022-09-23 14:23:18,431 - trainer - INFO - {
  "dev_loss": 25.568777084350586,
  "dev_best_score_for_loss": -25.568777084350586
}
2022-09-23 14:23:18,431 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,432 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,433 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,433 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_48
2022-09-23 14:23:18,434 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,436 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,436 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,436 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,437 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,437 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_54
2022-09-23 14:23:18,440 - trainer - INFO - save model to path: model/mlp_pivot\ck_54
2022-09-23 14:23:18,440 - trainer - INFO - 
*****************[epoch: 54, global step: 55] eval training set at end of epoch***************
2022-09-23 14:23:18,440 - trainer - INFO - {
  "train_loss": 30.6397647857666
}
2022-09-23 14:23:18,441 - trainer - INFO - start training epoch 55
2022-09-23 14:23:18,441 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,441 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,441 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,445 - trainer - INFO - 
*****************[epoch: 55, global step: 56] eval training set at end of epoch***************
2022-09-23 14:23:18,445 - trainer - INFO - {
  "train_loss": 25.56877899169922
}
2022-09-23 14:23:18,445 - trainer - INFO - start training epoch 56
2022-09-23 14:23:18,446 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,446 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,447 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,449 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval training set based on eval_every=2***************
2022-09-23 14:23:18,450 - trainer - INFO - {
  "train_loss": 26.23208999633789
}
2022-09-23 14:23:18,452 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval development set based on eval_every=2***************
2022-09-23 14:23:18,452 - trainer - INFO - {
  "dev_loss": 31.48028564453125,
  "dev_best_score_for_loss": -25.568777084350586
}
2022-09-23 14:23:18,453 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,453 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,454 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,454 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,455 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_50
2022-09-23 14:23:18,456 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_56
2022-09-23 14:23:18,459 - trainer - INFO - save model to path: model/mlp_pivot\ck_56
2022-09-23 14:23:18,460 - trainer - INFO - 
*****************[epoch: 56, global step: 57] eval training set at end of epoch***************
2022-09-23 14:23:18,460 - trainer - INFO - {
  "train_loss": 26.895401000976562
}
2022-09-23 14:23:18,461 - trainer - INFO - start training epoch 57
2022-09-23 14:23:18,461 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,461 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,461 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,466 - trainer - INFO - 
*****************[epoch: 57, global step: 58] eval training set at end of epoch***************
2022-09-23 14:23:18,466 - trainer - INFO - {
  "train_loss": 31.480287551879883
}
2022-09-23 14:23:18,467 - trainer - INFO - start training epoch 58
2022-09-23 14:23:18,467 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,467 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,467 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,471 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval training set based on eval_every=2***************
2022-09-23 14:23:18,472 - trainer - INFO - {
  "train_loss": 32.317437171936035
}
2022-09-23 14:23:18,474 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval development set based on eval_every=2***************
2022-09-23 14:23:18,474 - trainer - INFO - {
  "dev_loss": 29.76088523864746,
  "dev_best_score_for_loss": -25.568777084350586
}
2022-09-23 14:23:18,475 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:23:18,475 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,476 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,476 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,477 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_52
2022-09-23 14:23:18,478 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_58
2022-09-23 14:23:18,480 - trainer - INFO - save model to path: model/mlp_pivot\ck_58
2022-09-23 14:23:18,481 - trainer - INFO - 
*****************[epoch: 58, global step: 59] eval training set at end of epoch***************
2022-09-23 14:23:18,481 - trainer - INFO - {
  "train_loss": 33.15458679199219
}
2022-09-23 14:23:18,482 - trainer - INFO - start training epoch 59
2022-09-23 14:23:18,482 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,482 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,482 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,486 - trainer - INFO - 
*****************[epoch: 59, global step: 60] eval training set at end of epoch***************
2022-09-23 14:23:18,486 - trainer - INFO - {
  "train_loss": 29.76088523864746
}
2022-09-23 14:23:18,487 - trainer - INFO - start training epoch 60
2022-09-23 14:23:18,487 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,487 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,487 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,491 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval training set based on eval_every=2***************
2022-09-23 14:23:18,491 - trainer - INFO - {
  "train_loss": 27.37296199798584
}
2022-09-23 14:23:18,493 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval development set based on eval_every=2***************
2022-09-23 14:23:18,493 - trainer - INFO - {
  "dev_loss": 23.28281593322754,
  "dev_best_score_for_loss": -23.28281593322754
}
2022-09-23 14:23:18,494 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,495 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,495 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,495 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_54
2022-09-23 14:23:18,497 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,499 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,499 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,499 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,500 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,500 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_60
2022-09-23 14:23:18,502 - trainer - INFO - save model to path: model/mlp_pivot\ck_60
2022-09-23 14:23:18,503 - trainer - INFO - 
*****************[epoch: 60, global step: 61] eval training set at end of epoch***************
2022-09-23 14:23:18,503 - trainer - INFO - {
  "train_loss": 24.98503875732422
}
2022-09-23 14:23:18,503 - trainer - INFO - start training epoch 61
2022-09-23 14:23:18,504 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,504 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,504 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,508 - trainer - INFO - 
*****************[epoch: 61, global step: 62] eval training set at end of epoch***************
2022-09-23 14:23:18,508 - trainer - INFO - {
  "train_loss": 23.282814025878906
}
2022-09-23 14:23:18,509 - trainer - INFO - start training epoch 62
2022-09-23 14:23:18,509 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,509 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,510 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,513 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval training set based on eval_every=2***************
2022-09-23 14:23:18,513 - trainer - INFO - {
  "train_loss": 24.16287612915039
}
2022-09-23 14:23:18,515 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval development set based on eval_every=2***************
2022-09-23 14:23:18,516 - trainer - INFO - {
  "dev_loss": 27.140371322631836,
  "dev_best_score_for_loss": -23.28281593322754
}
2022-09-23 14:23:18,516 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,517 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,517 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,518 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,518 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_56
2022-09-23 14:23:18,519 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_62
2022-09-23 14:23:18,521 - trainer - INFO - save model to path: model/mlp_pivot\ck_62
2022-09-23 14:23:18,522 - trainer - INFO - 
*****************[epoch: 62, global step: 63] eval training set at end of epoch***************
2022-09-23 14:23:18,522 - trainer - INFO - {
  "train_loss": 25.042938232421875
}
2022-09-23 14:23:18,523 - trainer - INFO - start training epoch 63
2022-09-23 14:23:18,523 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,523 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,523 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,527 - trainer - INFO - 
*****************[epoch: 63, global step: 64] eval training set at end of epoch***************
2022-09-23 14:23:18,527 - trainer - INFO - {
  "train_loss": 27.140371322631836
}
2022-09-23 14:23:18,528 - trainer - INFO - start training epoch 64
2022-09-23 14:23:18,528 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,528 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,529 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,532 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval training set based on eval_every=2***************
2022-09-23 14:23:18,532 - trainer - INFO - {
  "train_loss": 26.961902618408203
}
2022-09-23 14:23:18,534 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval development set based on eval_every=2***************
2022-09-23 14:23:18,535 - trainer - INFO - {
  "dev_loss": 24.137880325317383,
  "dev_best_score_for_loss": -23.28281593322754
}
2022-09-23 14:23:18,535 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:23:18,535 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,537 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,537 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,537 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_58
2022-09-23 14:23:18,538 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_64
2022-09-23 14:23:18,541 - trainer - INFO - save model to path: model/mlp_pivot\ck_64
2022-09-23 14:23:18,542 - trainer - INFO - 
*****************[epoch: 64, global step: 65] eval training set at end of epoch***************
2022-09-23 14:23:18,542 - trainer - INFO - {
  "train_loss": 26.78343391418457
}
2022-09-23 14:23:18,543 - trainer - INFO - start training epoch 65
2022-09-23 14:23:18,543 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,543 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,544 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,547 - trainer - INFO - 
*****************[epoch: 65, global step: 66] eval training set at end of epoch***************
2022-09-23 14:23:18,547 - trainer - INFO - {
  "train_loss": 24.13787841796875
}
2022-09-23 14:23:18,548 - trainer - INFO - start training epoch 66
2022-09-23 14:23:18,548 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,548 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,548 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,552 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval training set based on eval_every=2***************
2022-09-23 14:23:18,552 - trainer - INFO - {
  "train_loss": 22.904624938964844
}
2022-09-23 14:23:18,554 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval development set based on eval_every=2***************
2022-09-23 14:23:18,555 - trainer - INFO - {
  "dev_loss": 21.35016441345215,
  "dev_best_score_for_loss": -21.35016441345215
}
2022-09-23 14:23:18,555 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,556 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,556 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,556 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_60
2022-09-23 14:23:18,557 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,559 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,560 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,560 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,561 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,561 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_66
2022-09-23 14:23:18,563 - trainer - INFO - save model to path: model/mlp_pivot\ck_66
2022-09-23 14:23:18,564 - trainer - INFO - 
*****************[epoch: 66, global step: 67] eval training set at end of epoch***************
2022-09-23 14:23:18,565 - trainer - INFO - {
  "train_loss": 21.671371459960938
}
2022-09-23 14:23:18,565 - trainer - INFO - start training epoch 67
2022-09-23 14:23:18,565 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,565 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,566 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,569 - trainer - INFO - 
*****************[epoch: 67, global step: 68] eval training set at end of epoch***************
2022-09-23 14:23:18,569 - trainer - INFO - {
  "train_loss": 21.35016441345215
}
2022-09-23 14:23:18,570 - trainer - INFO - start training epoch 68
2022-09-23 14:23:18,570 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,570 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,571 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,574 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval training set based on eval_every=2***************
2022-09-23 14:23:18,574 - trainer - INFO - {
  "train_loss": 21.974441528320312
}
2022-09-23 14:23:18,576 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval development set based on eval_every=2***************
2022-09-23 14:23:18,576 - trainer - INFO - {
  "dev_loss": 23.194501876831055,
  "dev_best_score_for_loss": -21.35016441345215
}
2022-09-23 14:23:18,577 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,577 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,578 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,578 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,579 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_62
2022-09-23 14:23:18,580 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_68
2022-09-23 14:23:18,582 - trainer - INFO - save model to path: model/mlp_pivot\ck_68
2022-09-23 14:23:18,583 - trainer - INFO - 
*****************[epoch: 68, global step: 69] eval training set at end of epoch***************
2022-09-23 14:23:18,583 - trainer - INFO - {
  "train_loss": 22.598718643188477
}
2022-09-23 14:23:18,583 - trainer - INFO - start training epoch 69
2022-09-23 14:23:18,584 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,584 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,584 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,588 - trainer - INFO - 
*****************[epoch: 69, global step: 70] eval training set at end of epoch***************
2022-09-23 14:23:18,588 - trainer - INFO - {
  "train_loss": 23.194501876831055
}
2022-09-23 14:23:18,588 - trainer - INFO - start training epoch 70
2022-09-23 14:23:18,589 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,589 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,589 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,592 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval training set based on eval_every=2***************
2022-09-23 14:23:18,593 - trainer - INFO - {
  "train_loss": 22.597341537475586
}
2022-09-23 14:23:18,595 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval development set based on eval_every=2***************
2022-09-23 14:23:18,595 - trainer - INFO - {
  "dev_loss": 20.11904525756836,
  "dev_best_score_for_loss": -20.11904525756836
}
2022-09-23 14:23:18,596 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,597 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,597 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,597 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_64
2022-09-23 14:23:18,599 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,601 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,601 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,601 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,602 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,602 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_70
2022-09-23 14:23:18,604 - trainer - INFO - save model to path: model/mlp_pivot\ck_70
2022-09-23 14:23:18,605 - trainer - INFO - 
*****************[epoch: 70, global step: 71] eval training set at end of epoch***************
2022-09-23 14:23:18,605 - trainer - INFO - {
  "train_loss": 22.000181198120117
}
2022-09-23 14:23:18,605 - trainer - INFO - start training epoch 71
2022-09-23 14:23:18,605 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,606 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,606 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,609 - trainer - INFO - 
*****************[epoch: 71, global step: 72] eval training set at end of epoch***************
2022-09-23 14:23:18,610 - trainer - INFO - {
  "train_loss": 20.119047164916992
}
2022-09-23 14:23:18,610 - trainer - INFO - start training epoch 72
2022-09-23 14:23:18,610 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,610 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,611 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,614 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval training set based on eval_every=2***************
2022-09-23 14:23:18,615 - trainer - INFO - {
  "train_loss": 19.67989730834961
}
2022-09-23 14:23:18,617 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval development set based on eval_every=2***************
2022-09-23 14:23:18,617 - trainer - INFO - {
  "dev_loss": 19.621809005737305,
  "dev_best_score_for_loss": -19.621809005737305
}
2022-09-23 14:23:18,617 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,618 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,619 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,619 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_66
2022-09-23 14:23:18,620 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,622 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,622 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,623 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,623 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,624 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_72
2022-09-23 14:23:18,626 - trainer - INFO - save model to path: model/mlp_pivot\ck_72
2022-09-23 14:23:18,627 - trainer - INFO - 
*****************[epoch: 72, global step: 73] eval training set at end of epoch***************
2022-09-23 14:23:18,627 - trainer - INFO - {
  "train_loss": 19.240747451782227
}
2022-09-23 14:23:18,628 - trainer - INFO - start training epoch 73
2022-09-23 14:23:18,628 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,628 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,629 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,632 - trainer - INFO - 
*****************[epoch: 73, global step: 74] eval training set at end of epoch***************
2022-09-23 14:23:18,633 - trainer - INFO - {
  "train_loss": 19.621809005737305
}
2022-09-23 14:23:18,633 - trainer - INFO - start training epoch 74
2022-09-23 14:23:18,633 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,633 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,634 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,637 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval training set based on eval_every=2***************
2022-09-23 14:23:18,637 - trainer - INFO - {
  "train_loss": 19.85918617248535
}
2022-09-23 14:23:18,639 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval development set based on eval_every=2***************
2022-09-23 14:23:18,640 - trainer - INFO - {
  "dev_loss": 19.630474090576172,
  "dev_best_score_for_loss": -19.621809005737305
}
2022-09-23 14:23:18,640 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,641 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,641 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,642 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,642 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_68
2022-09-23 14:23:18,643 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_74
2022-09-23 14:23:18,646 - trainer - INFO - save model to path: model/mlp_pivot\ck_74
2022-09-23 14:23:18,647 - trainer - INFO - 
*****************[epoch: 74, global step: 75] eval training set at end of epoch***************
2022-09-23 14:23:18,647 - trainer - INFO - {
  "train_loss": 20.0965633392334
}
2022-09-23 14:23:18,647 - trainer - INFO - start training epoch 75
2022-09-23 14:23:18,647 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,648 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,648 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,651 - trainer - INFO - 
*****************[epoch: 75, global step: 76] eval training set at end of epoch***************
2022-09-23 14:23:18,652 - trainer - INFO - {
  "train_loss": 19.63047218322754
}
2022-09-23 14:23:18,652 - trainer - INFO - start training epoch 76
2022-09-23 14:23:18,652 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,652 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,653 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,656 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval training set based on eval_every=2***************
2022-09-23 14:23:18,656 - trainer - INFO - {
  "train_loss": 19.02321434020996
}
2022-09-23 14:23:18,658 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval development set based on eval_every=2***************
2022-09-23 14:23:18,658 - trainer - INFO - {
  "dev_loss": 17.45891571044922,
  "dev_best_score_for_loss": -17.45891571044922
}
2022-09-23 14:23:18,659 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,660 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,660 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,660 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_70
2022-09-23 14:23:18,662 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,664 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,664 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,664 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,665 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,665 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_76
2022-09-23 14:23:18,668 - trainer - INFO - save model to path: model/mlp_pivot\ck_76
2022-09-23 14:23:18,668 - trainer - INFO - 
*****************[epoch: 76, global step: 77] eval training set at end of epoch***************
2022-09-23 14:23:18,669 - trainer - INFO - {
  "train_loss": 18.415956497192383
}
2022-09-23 14:23:18,669 - trainer - INFO - start training epoch 77
2022-09-23 14:23:18,669 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,670 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,670 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,673 - trainer - INFO - 
*****************[epoch: 77, global step: 78] eval training set at end of epoch***************
2022-09-23 14:23:18,674 - trainer - INFO - {
  "train_loss": 17.458913803100586
}
2022-09-23 14:23:18,674 - trainer - INFO - start training epoch 78
2022-09-23 14:23:18,674 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,674 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,675 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,678 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval training set based on eval_every=2***************
2022-09-23 14:23:18,678 - trainer - INFO - {
  "train_loss": 17.394041061401367
}
2022-09-23 14:23:18,681 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval development set based on eval_every=2***************
2022-09-23 14:23:18,681 - trainer - INFO - {
  "dev_loss": 17.55690574645996,
  "dev_best_score_for_loss": -17.45891571044922
}
2022-09-23 14:23:18,681 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:23:18,682 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,683 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,683 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,683 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_72
2022-09-23 14:23:18,684 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_78
2022-09-23 14:23:18,687 - trainer - INFO - save model to path: model/mlp_pivot\ck_78
2022-09-23 14:23:18,688 - trainer - INFO - 
*****************[epoch: 78, global step: 79] eval training set at end of epoch***************
2022-09-23 14:23:18,688 - trainer - INFO - {
  "train_loss": 17.32916831970215
}
2022-09-23 14:23:18,689 - trainer - INFO - start training epoch 79
2022-09-23 14:23:18,689 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,689 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,689 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,693 - trainer - INFO - 
*****************[epoch: 79, global step: 80] eval training set at end of epoch***************
2022-09-23 14:23:18,694 - trainer - INFO - {
  "train_loss": 17.55690574645996
}
2022-09-23 14:23:18,694 - trainer - INFO - start training epoch 80
2022-09-23 14:23:18,694 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,694 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,695 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,698 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval training set based on eval_every=2***************
2022-09-23 14:23:18,698 - trainer - INFO - {
  "train_loss": 17.442143440246582
}
2022-09-23 14:23:18,701 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval development set based on eval_every=2***************
2022-09-23 14:23:18,702 - trainer - INFO - {
  "dev_loss": 16.51629638671875,
  "dev_best_score_for_loss": -16.51629638671875
}
2022-09-23 14:23:18,702 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,704 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,704 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,704 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_74
2022-09-23 14:23:18,705 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,707 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,708 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,708 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,709 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,709 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_80
2022-09-23 14:23:18,712 - trainer - INFO - save model to path: model/mlp_pivot\ck_80
2022-09-23 14:23:18,713 - trainer - INFO - 
*****************[epoch: 80, global step: 81] eval training set at end of epoch***************
2022-09-23 14:23:18,713 - trainer - INFO - {
  "train_loss": 17.327381134033203
}
2022-09-23 14:23:18,714 - trainer - INFO - start training epoch 81
2022-09-23 14:23:18,714 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,714 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,715 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,719 - trainer - INFO - 
*****************[epoch: 81, global step: 82] eval training set at end of epoch***************
2022-09-23 14:23:18,719 - trainer - INFO - {
  "train_loss": 16.51629638671875
}
2022-09-23 14:23:18,720 - trainer - INFO - start training epoch 82
2022-09-23 14:23:18,720 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,720 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,721 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,725 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval training set based on eval_every=2***************
2022-09-23 14:23:18,725 - trainer - INFO - {
  "train_loss": 16.12336540222168
}
2022-09-23 14:23:18,728 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval development set based on eval_every=2***************
2022-09-23 14:23:18,728 - trainer - INFO - {
  "dev_loss": 15.442790031433105,
  "dev_best_score_for_loss": -15.442790031433105
}
2022-09-23 14:23:18,729 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,730 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,731 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,731 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_76
2022-09-23 14:23:18,732 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,735 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,735 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,735 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,736 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,736 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_82
2022-09-23 14:23:18,738 - trainer - INFO - save model to path: model/mlp_pivot\ck_82
2022-09-23 14:23:18,739 - trainer - INFO - 
*****************[epoch: 82, global step: 83] eval training set at end of epoch***************
2022-09-23 14:23:18,739 - trainer - INFO - {
  "train_loss": 15.73043441772461
}
2022-09-23 14:23:18,740 - trainer - INFO - start training epoch 83
2022-09-23 14:23:18,740 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,740 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,740 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,744 - trainer - INFO - 
*****************[epoch: 83, global step: 84] eval training set at end of epoch***************
2022-09-23 14:23:18,744 - trainer - INFO - {
  "train_loss": 15.442790031433105
}
2022-09-23 14:23:18,744 - trainer - INFO - start training epoch 84
2022-09-23 14:23:18,745 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,745 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,745 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,749 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval training set based on eval_every=2***************
2022-09-23 14:23:18,749 - trainer - INFO - {
  "train_loss": 15.447369575500488
}
2022-09-23 14:23:18,752 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval development set based on eval_every=2***************
2022-09-23 14:23:18,752 - trainer - INFO - {
  "dev_loss": 15.247784614562988,
  "dev_best_score_for_loss": -15.247784614562988
}
2022-09-23 14:23:18,753 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,754 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,754 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,754 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_78
2022-09-23 14:23:18,756 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,758 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,758 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,759 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,759 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,760 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_84
2022-09-23 14:23:18,763 - trainer - INFO - save model to path: model/mlp_pivot\ck_84
2022-09-23 14:23:18,763 - trainer - INFO - 
*****************[epoch: 84, global step: 85] eval training set at end of epoch***************
2022-09-23 14:23:18,764 - trainer - INFO - {
  "train_loss": 15.451949119567871
}
2022-09-23 14:23:18,764 - trainer - INFO - start training epoch 85
2022-09-23 14:23:18,764 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,764 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,765 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,768 - trainer - INFO - 
*****************[epoch: 85, global step: 86] eval training set at end of epoch***************
2022-09-23 14:23:18,769 - trainer - INFO - {
  "train_loss": 15.247783660888672
}
2022-09-23 14:23:18,769 - trainer - INFO - start training epoch 86
2022-09-23 14:23:18,769 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,769 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,770 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,773 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval training set based on eval_every=2***************
2022-09-23 14:23:18,773 - trainer - INFO - {
  "train_loss": 14.959957122802734
}
2022-09-23 14:23:18,775 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval development set based on eval_every=2***************
2022-09-23 14:23:18,776 - trainer - INFO - {
  "dev_loss": 14.047467231750488,
  "dev_best_score_for_loss": -14.047467231750488
}
2022-09-23 14:23:18,776 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,777 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,777 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,778 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_80
2022-09-23 14:23:18,779 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,780 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,781 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,781 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,781 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,782 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_86
2022-09-23 14:23:18,784 - trainer - INFO - save model to path: model/mlp_pivot\ck_86
2022-09-23 14:23:18,784 - trainer - INFO - 
*****************[epoch: 86, global step: 87] eval training set at end of epoch***************
2022-09-23 14:23:18,785 - trainer - INFO - {
  "train_loss": 14.672130584716797
}
2022-09-23 14:23:18,785 - trainer - INFO - start training epoch 87
2022-09-23 14:23:18,785 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,785 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,786 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,789 - trainer - INFO - 
*****************[epoch: 87, global step: 88] eval training set at end of epoch***************
2022-09-23 14:23:18,790 - trainer - INFO - {
  "train_loss": 14.047466278076172
}
2022-09-23 14:23:18,790 - trainer - INFO - start training epoch 88
2022-09-23 14:23:18,790 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,790 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,791 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,794 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval training set based on eval_every=2***************
2022-09-23 14:23:18,794 - trainer - INFO - {
  "train_loss": 13.878543853759766
}
2022-09-23 14:23:18,796 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval development set based on eval_every=2***************
2022-09-23 14:23:18,796 - trainer - INFO - {
  "dev_loss": 13.591072082519531,
  "dev_best_score_for_loss": -13.591072082519531
}
2022-09-23 14:23:18,797 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,798 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,798 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,798 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_82
2022-09-23 14:23:18,799 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,801 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,801 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,802 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,802 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,802 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_88
2022-09-23 14:23:18,805 - trainer - INFO - save model to path: model/mlp_pivot\ck_88
2022-09-23 14:23:18,806 - trainer - INFO - 
*****************[epoch: 88, global step: 89] eval training set at end of epoch***************
2022-09-23 14:23:18,806 - trainer - INFO - {
  "train_loss": 13.70962142944336
}
2022-09-23 14:23:18,807 - trainer - INFO - start training epoch 89
2022-09-23 14:23:18,807 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,807 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,807 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,811 - trainer - INFO - 
*****************[epoch: 89, global step: 90] eval training set at end of epoch***************
2022-09-23 14:23:18,811 - trainer - INFO - {
  "train_loss": 13.591071128845215
}
2022-09-23 14:23:18,811 - trainer - INFO - start training epoch 90
2022-09-23 14:23:18,812 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,812 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,812 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,815 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval training set based on eval_every=2***************
2022-09-23 14:23:18,815 - trainer - INFO - {
  "train_loss": 13.47792387008667
}
2022-09-23 14:23:18,818 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval development set based on eval_every=2***************
2022-09-23 14:23:18,818 - trainer - INFO - {
  "dev_loss": 12.900497436523438,
  "dev_best_score_for_loss": -12.900497436523438
}
2022-09-23 14:23:18,819 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,820 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,820 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,820 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_84
2022-09-23 14:23:18,821 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,823 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,823 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,823 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,824 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,824 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_90
2022-09-23 14:23:18,827 - trainer - INFO - save model to path: model/mlp_pivot\ck_90
2022-09-23 14:23:18,827 - trainer - INFO - 
*****************[epoch: 90, global step: 91] eval training set at end of epoch***************
2022-09-23 14:23:18,827 - trainer - INFO - {
  "train_loss": 13.364776611328125
}
2022-09-23 14:23:18,828 - trainer - INFO - start training epoch 91
2022-09-23 14:23:18,828 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,828 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,829 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,832 - trainer - INFO - 
*****************[epoch: 91, global step: 92] eval training set at end of epoch***************
2022-09-23 14:23:18,832 - trainer - INFO - {
  "train_loss": 12.900500297546387
}
2022-09-23 14:23:18,833 - trainer - INFO - start training epoch 92
2022-09-23 14:23:18,833 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,833 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,833 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,836 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval training set based on eval_every=2***************
2022-09-23 14:23:18,837 - trainer - INFO - {
  "train_loss": 12.650680541992188
}
2022-09-23 14:23:18,839 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval development set based on eval_every=2***************
2022-09-23 14:23:18,839 - trainer - INFO - {
  "dev_loss": 12.083535194396973,
  "dev_best_score_for_loss": -12.083535194396973
}
2022-09-23 14:23:18,840 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,841 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,841 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,841 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_86
2022-09-23 14:23:18,842 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,845 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,845 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,845 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,846 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,846 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_92
2022-09-23 14:23:18,849 - trainer - INFO - save model to path: model/mlp_pivot\ck_92
2022-09-23 14:23:18,850 - trainer - INFO - 
*****************[epoch: 92, global step: 93] eval training set at end of epoch***************
2022-09-23 14:23:18,850 - trainer - INFO - {
  "train_loss": 12.400860786437988
}
2022-09-23 14:23:18,851 - trainer - INFO - start training epoch 93
2022-09-23 14:23:18,851 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,851 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,851 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,855 - trainer - INFO - 
*****************[epoch: 93, global step: 94] eval training set at end of epoch***************
2022-09-23 14:23:18,855 - trainer - INFO - {
  "train_loss": 12.083535194396973
}
2022-09-23 14:23:18,856 - trainer - INFO - start training epoch 94
2022-09-23 14:23:18,856 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,856 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,857 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,860 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval training set based on eval_every=2***************
2022-09-23 14:23:18,860 - trainer - INFO - {
  "train_loss": 11.99155855178833
}
2022-09-23 14:23:18,862 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval development set based on eval_every=2***************
2022-09-23 14:23:18,863 - trainer - INFO - {
  "dev_loss": 11.64211654663086,
  "dev_best_score_for_loss": -11.64211654663086
}
2022-09-23 14:23:18,863 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,864 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,864 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,865 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_88
2022-09-23 14:23:18,866 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,868 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,868 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,868 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,869 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,869 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_94
2022-09-23 14:23:18,872 - trainer - INFO - save model to path: model/mlp_pivot\ck_94
2022-09-23 14:23:18,873 - trainer - INFO - 
*****************[epoch: 94, global step: 95] eval training set at end of epoch***************
2022-09-23 14:23:18,873 - trainer - INFO - {
  "train_loss": 11.899581909179688
}
2022-09-23 14:23:18,873 - trainer - INFO - start training epoch 95
2022-09-23 14:23:18,874 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,874 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,874 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,878 - trainer - INFO - 
*****************[epoch: 95, global step: 96] eval training set at end of epoch***************
2022-09-23 14:23:18,878 - trainer - INFO - {
  "train_loss": 11.642117500305176
}
2022-09-23 14:23:18,879 - trainer - INFO - start training epoch 96
2022-09-23 14:23:18,879 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,879 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,880 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,883 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval training set based on eval_every=2***************
2022-09-23 14:23:18,884 - trainer - INFO - {
  "train_loss": 11.441549301147461
}
2022-09-23 14:23:18,886 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval development set based on eval_every=2***************
2022-09-23 14:23:18,886 - trainer - INFO - {
  "dev_loss": 10.829643249511719,
  "dev_best_score_for_loss": -10.829643249511719
}
2022-09-23 14:23:18,886 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,887 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,888 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,888 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_90
2022-09-23 14:23:18,889 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,891 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,891 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,891 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,892 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,892 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_96
2022-09-23 14:23:18,895 - trainer - INFO - save model to path: model/mlp_pivot\ck_96
2022-09-23 14:23:18,896 - trainer - INFO - 
*****************[epoch: 96, global step: 97] eval training set at end of epoch***************
2022-09-23 14:23:18,896 - trainer - INFO - {
  "train_loss": 11.240981101989746
}
2022-09-23 14:23:18,897 - trainer - INFO - start training epoch 97
2022-09-23 14:23:18,897 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,897 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,898 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,901 - trainer - INFO - 
*****************[epoch: 97, global step: 98] eval training set at end of epoch***************
2022-09-23 14:23:18,902 - trainer - INFO - {
  "train_loss": 10.829643249511719
}
2022-09-23 14:23:18,902 - trainer - INFO - start training epoch 98
2022-09-23 14:23:18,902 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,903 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,903 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,906 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval training set based on eval_every=2***************
2022-09-23 14:23:18,906 - trainer - INFO - {
  "train_loss": 10.684648513793945
}
2022-09-23 14:23:18,908 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval development set based on eval_every=2***************
2022-09-23 14:23:18,909 - trainer - INFO - {
  "dev_loss": 10.325447082519531,
  "dev_best_score_for_loss": -10.325447082519531
}
2022-09-23 14:23:18,909 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,910 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,910 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,911 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_92
2022-09-23 14:23:18,912 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,914 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,914 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,915 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,916 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,916 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_98
2022-09-23 14:23:18,919 - trainer - INFO - save model to path: model/mlp_pivot\ck_98
2022-09-23 14:23:18,920 - trainer - INFO - 
*****************[epoch: 98, global step: 99] eval training set at end of epoch***************
2022-09-23 14:23:18,921 - trainer - INFO - {
  "train_loss": 10.539653778076172
}
2022-09-23 14:23:18,921 - trainer - INFO - start training epoch 99
2022-09-23 14:23:18,922 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,922 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,923 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,927 - trainer - INFO - 
*****************[epoch: 99, global step: 100] eval training set at end of epoch***************
2022-09-23 14:23:18,927 - trainer - INFO - {
  "train_loss": 10.325448036193848
}
2022-09-23 14:23:18,928 - trainer - INFO - start training epoch 100
2022-09-23 14:23:18,928 - trainer - INFO - training using device=cpu
2022-09-23 14:23:18,928 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:23:18,929 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_pivot",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:23:18,933 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval training set based on eval_every=2***************
2022-09-23 14:23:18,933 - trainer - INFO - {
  "train_loss": 10.190714836120605
}
2022-09-23 14:23:18,936 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval development set based on eval_every=2***************
2022-09-23 14:23:18,937 - trainer - INFO - {
  "dev_loss": 9.703498840332031,
  "dev_best_score_for_loss": -9.703498840332031
}
2022-09-23 14:23:18,937 - trainer - INFO -    save the model with best score so far
2022-09-23 14:23:18,939 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:23:18,939 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:23:18,939 - trainer - INFO -   Remove checkpoint model/mlp_pivot\ck_94
2022-09-23 14:23:18,941 - trainer - INFO -   Save checkpoint to model/mlp_pivot
2022-09-23 14:23:18,943 - trainer - INFO - save model to path: model/mlp_pivot
2022-09-23 14:23:18,943 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:23:18,944 - trainer - INFO -   patience: 200
2022-09-23 14:23:18,944 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:23:18,945 - trainer - INFO -   Save checkpoint to model/mlp_pivot\ck_100
2022-09-23 14:23:18,948 - trainer - INFO - save model to path: model/mlp_pivot\ck_100
2022-09-23 14:23:18,949 - trainer - INFO - 
*****************[epoch: 100, global step: 101] eval training set at end of epoch***************
2022-09-23 14:23:18,949 - trainer - INFO - {
  "train_loss": 10.055981636047363
}
