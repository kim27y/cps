2022-09-23 13:51:47,697 - trainer - INFO - MLP(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=32, bias=True)
  )
  (activation_layers): ModuleList(
    (0): ReLU(inplace=True)
    (1): ReLU(inplace=True)
    (2): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (linear_output): Linear(in_features=32, out_features=1, bias=True)
)
2022-09-23 13:51:47,698 - trainer - INFO -   Total params: 10625
2022-09-23 13:51:47,698 - trainer - INFO -   Trainable params: 10625
2022-09-23 13:51:47,699 - trainer - INFO -   Non-trainable params: 0
2022-09-23 13:51:47,699 - trainer - INFO -   There are 9  training examples
2022-09-23 13:51:47,699 - trainer - INFO -   There are 9 examples for development
2022-09-23 13:51:47,700 - trainer - INFO - start training epoch 1
2022-09-23 13:51:47,700 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,701 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,701 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,732 - trainer - INFO - 
*****************[epoch: 1, global step: 2] eval training set at end of epoch***************
2022-09-23 13:51:47,732 - trainer - INFO - {
  "train_loss": 5457.068359375
}
2022-09-23 13:51:47,733 - trainer - INFO - start training epoch 2
2022-09-23 13:51:47,733 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,733 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,734 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,737 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval training set based on eval_every=2***************
2022-09-23 13:51:47,737 - trainer - INFO - {
  "train_loss": 5305.90966796875
}
2022-09-23 13:51:47,740 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval development set based on eval_every=2***************
2022-09-23 13:51:47,741 - trainer - INFO - {
  "dev_loss": 4466.33349609375,
  "dev_best_score_for_loss": -4466.33349609375
}
2022-09-23 13:51:47,741 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:47,742 - trainer - INFO -    Check 0 checkpoints already saved
2022-09-23 13:51:47,742 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:47,745 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:47,745 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:47,745 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,746 - trainer - INFO -    Check 0 checkpoints already saved
2022-09-23 13:51:47,746 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_2
2022-09-23 13:51:47,749 - trainer - INFO - save model to path: model/mlp_e41g\ck_2
2022-09-23 13:51:47,749 - trainer - INFO - 
*****************[epoch: 2, global step: 3] eval training set at end of epoch***************
2022-09-23 13:51:47,750 - trainer - INFO - {
  "train_loss": 5154.7509765625
}
2022-09-23 13:51:47,750 - trainer - INFO - start training epoch 3
2022-09-23 13:51:47,750 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,750 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,751 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,755 - trainer - INFO - 
*****************[epoch: 3, global step: 4] eval training set at end of epoch***************
2022-09-23 13:51:47,755 - trainer - INFO - {
  "train_loss": 4466.33349609375
}
2022-09-23 13:51:47,755 - trainer - INFO - start training epoch 4
2022-09-23 13:51:47,756 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,756 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,756 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,760 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval training set based on eval_every=2***************
2022-09-23 13:51:47,760 - trainer - INFO - {
  "train_loss": 3868.4708251953125
}
2022-09-23 13:51:47,762 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval development set based on eval_every=2***************
2022-09-23 13:51:47,763 - trainer - INFO - {
  "dev_loss": 1612.041015625,
  "dev_best_score_for_loss": -1612.041015625
}
2022-09-23 13:51:47,763 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:47,764 - trainer - INFO -    Check 1 checkpoints already saved
2022-09-23 13:51:47,764 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:47,766 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:47,766 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:47,767 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,767 - trainer - INFO -    Check 1 checkpoints already saved
2022-09-23 13:51:47,767 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_4
2022-09-23 13:51:47,770 - trainer - INFO - save model to path: model/mlp_e41g\ck_4
2022-09-23 13:51:47,770 - trainer - INFO - 
*****************[epoch: 4, global step: 5] eval training set at end of epoch***************
2022-09-23 13:51:47,770 - trainer - INFO - {
  "train_loss": 3270.608154296875
}
2022-09-23 13:51:47,771 - trainer - INFO - start training epoch 5
2022-09-23 13:51:47,771 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,771 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,772 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,775 - trainer - INFO - 
*****************[epoch: 5, global step: 6] eval training set at end of epoch***************
2022-09-23 13:51:47,776 - trainer - INFO - {
  "train_loss": 1612.041015625
}
2022-09-23 13:51:47,776 - trainer - INFO - start training epoch 6
2022-09-23 13:51:47,776 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,776 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,777 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,779 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval training set based on eval_every=2***************
2022-09-23 13:51:47,780 - trainer - INFO - {
  "train_loss": 886.3430786132812
}
2022-09-23 13:51:47,782 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval development set based on eval_every=2***************
2022-09-23 13:51:47,782 - trainer - INFO - {
  "dev_loss": 994.0432739257812,
  "dev_best_score_for_loss": -994.0432739257812
}
2022-09-23 13:51:47,783 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:47,784 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:47,784 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:47,786 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:47,786 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:47,786 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,787 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:47,787 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_6
2022-09-23 13:51:47,806 - trainer - INFO - save model to path: model/mlp_e41g\ck_6
2022-09-23 13:51:47,806 - trainer - INFO - 
*****************[epoch: 6, global step: 7] eval training set at end of epoch***************
2022-09-23 13:51:47,807 - trainer - INFO - {
  "train_loss": 160.6451416015625
}
2022-09-23 13:51:47,807 - trainer - INFO - start training epoch 7
2022-09-23 13:51:47,808 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,808 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,808 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,813 - trainer - INFO - 
*****************[epoch: 7, global step: 8] eval training set at end of epoch***************
2022-09-23 13:51:47,813 - trainer - INFO - {
  "train_loss": 994.0432739257812
}
2022-09-23 13:51:47,814 - trainer - INFO - start training epoch 8
2022-09-23 13:51:47,814 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,814 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,815 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,818 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval training set based on eval_every=2***************
2022-09-23 13:51:47,818 - trainer - INFO - {
  "train_loss": 1243.2889099121094
}
2022-09-23 13:51:47,821 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval development set based on eval_every=2***************
2022-09-23 13:51:47,822 - trainer - INFO - {
  "dev_loss": 589.2159423828125,
  "dev_best_score_for_loss": -589.2159423828125
}
2022-09-23 13:51:47,822 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:47,823 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:47,823 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:47,824 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_2
2022-09-23 13:51:47,825 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:47,827 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:47,828 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:47,828 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,829 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:47,829 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_8
2022-09-23 13:51:47,832 - trainer - INFO - save model to path: model/mlp_e41g\ck_8
2022-09-23 13:51:47,833 - trainer - INFO - 
*****************[epoch: 8, global step: 9] eval training set at end of epoch***************
2022-09-23 13:51:47,833 - trainer - INFO - {
  "train_loss": 1492.5345458984375
}
2022-09-23 13:51:47,833 - trainer - INFO - start training epoch 9
2022-09-23 13:51:47,834 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,834 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,834 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,839 - trainer - INFO - 
*****************[epoch: 9, global step: 10] eval training set at end of epoch***************
2022-09-23 13:51:47,839 - trainer - INFO - {
  "train_loss": 589.2159423828125
}
2022-09-23 13:51:47,839 - trainer - INFO - start training epoch 10
2022-09-23 13:51:47,839 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,840 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,840 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,844 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval training set based on eval_every=2***************
2022-09-23 13:51:47,844 - trainer - INFO - {
  "train_loss": 321.86165046691895
}
2022-09-23 13:51:47,846 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval development set based on eval_every=2***************
2022-09-23 13:51:47,847 - trainer - INFO - {
  "dev_loss": 155.92034912109375,
  "dev_best_score_for_loss": -155.92034912109375
}
2022-09-23 13:51:47,847 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:47,848 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:47,848 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:47,848 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_4
2022-09-23 13:51:47,849 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:47,851 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:47,852 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:47,852 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,852 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:47,853 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_10
2022-09-23 13:51:47,856 - trainer - INFO - save model to path: model/mlp_e41g\ck_10
2022-09-23 13:51:47,856 - trainer - INFO - 
*****************[epoch: 10, global step: 11] eval training set at end of epoch***************
2022-09-23 13:51:47,856 - trainer - INFO - {
  "train_loss": 54.50735855102539
}
2022-09-23 13:51:47,857 - trainer - INFO - start training epoch 11
2022-09-23 13:51:47,857 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,857 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,857 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,862 - trainer - INFO - 
*****************[epoch: 11, global step: 12] eval training set at end of epoch***************
2022-09-23 13:51:47,862 - trainer - INFO - {
  "train_loss": 155.92037963867188
}
2022-09-23 13:51:47,862 - trainer - INFO - start training epoch 12
2022-09-23 13:51:47,862 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,863 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,863 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,866 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval training set based on eval_every=2***************
2022-09-23 13:51:47,866 - trainer - INFO - {
  "train_loss": 311.5836486816406
}
2022-09-23 13:51:47,868 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval development set based on eval_every=2***************
2022-09-23 13:51:47,868 - trainer - INFO - {
  "dev_loss": 686.6651611328125,
  "dev_best_score_for_loss": -155.92034912109375
}
2022-09-23 13:51:47,869 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:47,869 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,870 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:47,871 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:47,871 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_6
2022-09-23 13:51:47,872 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_12
2022-09-23 13:51:47,875 - trainer - INFO - save model to path: model/mlp_e41g\ck_12
2022-09-23 13:51:47,876 - trainer - INFO - 
*****************[epoch: 12, global step: 13] eval training set at end of epoch***************
2022-09-23 13:51:47,876 - trainer - INFO - {
  "train_loss": 467.2469177246094
}
2022-09-23 13:51:47,876 - trainer - INFO - start training epoch 13
2022-09-23 13:51:47,877 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,877 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,877 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,880 - trainer - INFO - 
*****************[epoch: 13, global step: 14] eval training set at end of epoch***************
2022-09-23 13:51:47,881 - trainer - INFO - {
  "train_loss": 686.6651611328125
}
2022-09-23 13:51:47,881 - trainer - INFO - start training epoch 14
2022-09-23 13:51:47,881 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,881 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,882 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,885 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval training set based on eval_every=2***************
2022-09-23 13:51:47,885 - trainer - INFO - {
  "train_loss": 705.8323364257812
}
2022-09-23 13:51:47,887 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval development set based on eval_every=2***************
2022-09-23 13:51:47,888 - trainer - INFO - {
  "dev_loss": 595.965576171875,
  "dev_best_score_for_loss": -155.92034912109375
}
2022-09-23 13:51:47,888 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:47,889 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,889 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:47,890 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:47,890 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_8
2022-09-23 13:51:47,891 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_14
2022-09-23 13:51:47,894 - trainer - INFO - save model to path: model/mlp_e41g\ck_14
2022-09-23 13:51:47,894 - trainer - INFO - 
*****************[epoch: 14, global step: 15] eval training set at end of epoch***************
2022-09-23 13:51:47,895 - trainer - INFO - {
  "train_loss": 724.99951171875
}
2022-09-23 13:51:47,895 - trainer - INFO - start training epoch 15
2022-09-23 13:51:47,895 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,895 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,896 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,899 - trainer - INFO - 
*****************[epoch: 15, global step: 16] eval training set at end of epoch***************
2022-09-23 13:51:47,899 - trainer - INFO - {
  "train_loss": 595.965576171875
}
2022-09-23 13:51:47,900 - trainer - INFO - start training epoch 16
2022-09-23 13:51:47,900 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,900 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,900 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,903 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval training set based on eval_every=2***************
2022-09-23 13:51:47,904 - trainer - INFO - {
  "train_loss": 479.56451416015625
}
2022-09-23 13:51:47,906 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval development set based on eval_every=2***************
2022-09-23 13:51:47,906 - trainer - INFO - {
  "dev_loss": 132.13775634765625,
  "dev_best_score_for_loss": -132.13775634765625
}
2022-09-23 13:51:47,907 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:47,908 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:47,908 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:47,908 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_10
2022-09-23 13:51:47,909 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:47,911 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:47,911 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:47,911 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,912 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:47,912 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_16
2022-09-23 13:51:47,914 - trainer - INFO - save model to path: model/mlp_e41g\ck_16
2022-09-23 13:51:47,915 - trainer - INFO - 
*****************[epoch: 16, global step: 17] eval training set at end of epoch***************
2022-09-23 13:51:47,915 - trainer - INFO - {
  "train_loss": 363.1634521484375
}
2022-09-23 13:51:47,916 - trainer - INFO - start training epoch 17
2022-09-23 13:51:47,916 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,917 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,917 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,921 - trainer - INFO - 
*****************[epoch: 17, global step: 18] eval training set at end of epoch***************
2022-09-23 13:51:47,921 - trainer - INFO - {
  "train_loss": 132.13775634765625
}
2022-09-23 13:51:47,921 - trainer - INFO - start training epoch 18
2022-09-23 13:51:47,922 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,922 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,922 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,925 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval training set based on eval_every=2***************
2022-09-23 13:51:47,926 - trainer - INFO - {
  "train_loss": 82.9450569152832
}
2022-09-23 13:51:47,928 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval development set based on eval_every=2***************
2022-09-23 13:51:47,928 - trainer - INFO - {
  "dev_loss": 139.31405639648438,
  "dev_best_score_for_loss": -132.13775634765625
}
2022-09-23 13:51:47,928 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:47,929 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,929 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:47,930 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:47,930 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_12
2022-09-23 13:51:47,931 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_18
2022-09-23 13:51:47,933 - trainer - INFO - save model to path: model/mlp_e41g\ck_18
2022-09-23 13:51:47,934 - trainer - INFO - 
*****************[epoch: 18, global step: 19] eval training set at end of epoch***************
2022-09-23 13:51:47,934 - trainer - INFO - {
  "train_loss": 33.752357482910156
}
2022-09-23 13:51:47,934 - trainer - INFO - start training epoch 19
2022-09-23 13:51:47,934 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,935 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,935 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,938 - trainer - INFO - 
*****************[epoch: 19, global step: 20] eval training set at end of epoch***************
2022-09-23 13:51:47,938 - trainer - INFO - {
  "train_loss": 139.31405639648438
}
2022-09-23 13:51:47,939 - trainer - INFO - start training epoch 20
2022-09-23 13:51:47,939 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,939 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,940 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,942 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval training set based on eval_every=2***************
2022-09-23 13:51:47,943 - trainer - INFO - {
  "train_loss": 234.0091552734375
}
2022-09-23 13:51:47,945 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval development set based on eval_every=2***************
2022-09-23 13:51:47,945 - trainer - INFO - {
  "dev_loss": 368.3598327636719,
  "dev_best_score_for_loss": -132.13775634765625
}
2022-09-23 13:51:47,946 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:47,946 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,947 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:47,947 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:47,947 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_14
2022-09-23 13:51:47,948 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_20
2022-09-23 13:51:47,951 - trainer - INFO - save model to path: model/mlp_e41g\ck_20
2022-09-23 13:51:47,952 - trainer - INFO - 
*****************[epoch: 20, global step: 21] eval training set at end of epoch***************
2022-09-23 13:51:47,952 - trainer - INFO - {
  "train_loss": 328.7042541503906
}
2022-09-23 13:51:47,952 - trainer - INFO - start training epoch 21
2022-09-23 13:51:47,952 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,953 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,953 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,957 - trainer - INFO - 
*****************[epoch: 21, global step: 22] eval training set at end of epoch***************
2022-09-23 13:51:47,957 - trainer - INFO - {
  "train_loss": 368.3598327636719
}
2022-09-23 13:51:47,957 - trainer - INFO - start training epoch 22
2022-09-23 13:51:47,958 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,958 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,958 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,962 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval training set based on eval_every=2***************
2022-09-23 13:51:47,962 - trainer - INFO - {
  "train_loss": 296.9351501464844
}
2022-09-23 13:51:47,964 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval development set based on eval_every=2***************
2022-09-23 13:51:47,965 - trainer - INFO - {
  "dev_loss": 73.52949523925781,
  "dev_best_score_for_loss": -73.52949523925781
}
2022-09-23 13:51:47,965 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:47,966 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:47,966 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:47,966 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_16
2022-09-23 13:51:47,967 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:47,969 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:47,969 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:47,969 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,970 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:47,970 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_22
2022-09-23 13:51:47,973 - trainer - INFO - save model to path: model/mlp_e41g\ck_22
2022-09-23 13:51:47,974 - trainer - INFO - 
*****************[epoch: 22, global step: 23] eval training set at end of epoch***************
2022-09-23 13:51:47,974 - trainer - INFO - {
  "train_loss": 225.51046752929688
}
2022-09-23 13:51:47,975 - trainer - INFO - start training epoch 23
2022-09-23 13:51:47,975 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,975 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,975 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,979 - trainer - INFO - 
*****************[epoch: 23, global step: 24] eval training set at end of epoch***************
2022-09-23 13:51:47,980 - trainer - INFO - {
  "train_loss": 73.52949523925781
}
2022-09-23 13:51:47,980 - trainer - INFO - start training epoch 24
2022-09-23 13:51:47,980 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,980 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,981 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:47,984 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval training set based on eval_every=2***************
2022-09-23 13:51:47,984 - trainer - INFO - {
  "train_loss": 54.364593505859375
}
2022-09-23 13:51:47,987 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval development set based on eval_every=2***************
2022-09-23 13:51:47,987 - trainer - INFO - {
  "dev_loss": 93.52245330810547,
  "dev_best_score_for_loss": -73.52949523925781
}
2022-09-23 13:51:47,988 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:47,988 - trainer - INFO -   patience: 200
2022-09-23 13:51:47,989 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:47,989 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:47,990 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_18
2022-09-23 13:51:47,991 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_24
2022-09-23 13:51:47,995 - trainer - INFO - save model to path: model/mlp_e41g\ck_24
2022-09-23 13:51:47,996 - trainer - INFO - 
*****************[epoch: 24, global step: 25] eval training set at end of epoch***************
2022-09-23 13:51:47,997 - trainer - INFO - {
  "train_loss": 35.19969177246094
}
2022-09-23 13:51:47,997 - trainer - INFO - start training epoch 25
2022-09-23 13:51:47,997 - trainer - INFO - training using device=cpu
2022-09-23 13:51:47,997 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:47,998 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,003 - trainer - INFO - 
*****************[epoch: 25, global step: 26] eval training set at end of epoch***************
2022-09-23 13:51:48,003 - trainer - INFO - {
  "train_loss": 93.52244567871094
}
2022-09-23 13:51:48,003 - trainer - INFO - start training epoch 26
2022-09-23 13:51:48,004 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,004 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,004 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,008 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval training set based on eval_every=2***************
2022-09-23 13:51:48,008 - trainer - INFO - {
  "train_loss": 132.33682250976562
}
2022-09-23 13:51:48,011 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval development set based on eval_every=2***************
2022-09-23 13:51:48,011 - trainer - INFO - {
  "dev_loss": 208.25306701660156,
  "dev_best_score_for_loss": -73.52949523925781
}
2022-09-23 13:51:48,012 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:48,013 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,014 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,014 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,014 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_20
2022-09-23 13:51:48,015 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_26
2022-09-23 13:51:48,019 - trainer - INFO - save model to path: model/mlp_e41g\ck_26
2022-09-23 13:51:48,019 - trainer - INFO - 
*****************[epoch: 26, global step: 27] eval training set at end of epoch***************
2022-09-23 13:51:48,020 - trainer - INFO - {
  "train_loss": 171.1511993408203
}
2022-09-23 13:51:48,020 - trainer - INFO - start training epoch 27
2022-09-23 13:51:48,021 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,021 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,021 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,026 - trainer - INFO - 
*****************[epoch: 27, global step: 28] eval training set at end of epoch***************
2022-09-23 13:51:48,026 - trainer - INFO - {
  "train_loss": 208.2530975341797
}
2022-09-23 13:51:48,026 - trainer - INFO - start training epoch 28
2022-09-23 13:51:48,027 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,027 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,027 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,031 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval training set based on eval_every=2***************
2022-09-23 13:51:48,031 - trainer - INFO - {
  "train_loss": 197.12554168701172
}
2022-09-23 13:51:48,034 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval development set based on eval_every=2***************
2022-09-23 13:51:48,034 - trainer - INFO - {
  "dev_loss": 121.75214385986328,
  "dev_best_score_for_loss": -73.52949523925781
}
2022-09-23 13:51:48,035 - trainer - INFO -   no_improve_count: 3
2022-09-23 13:51:48,035 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,036 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,036 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,037 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_22
2022-09-23 13:51:48,038 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_28
2022-09-23 13:51:48,041 - trainer - INFO - save model to path: model/mlp_e41g\ck_28
2022-09-23 13:51:48,042 - trainer - INFO - 
*****************[epoch: 28, global step: 29] eval training set at end of epoch***************
2022-09-23 13:51:48,042 - trainer - INFO - {
  "train_loss": 185.99798583984375
}
2022-09-23 13:51:48,043 - trainer - INFO - start training epoch 29
2022-09-23 13:51:48,043 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,043 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,043 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,047 - trainer - INFO - 
*****************[epoch: 29, global step: 30] eval training set at end of epoch***************
2022-09-23 13:51:48,048 - trainer - INFO - {
  "train_loss": 121.75215911865234
}
2022-09-23 13:51:48,048 - trainer - INFO - start training epoch 30
2022-09-23 13:51:48,048 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,048 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,049 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,052 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval training set based on eval_every=2***************
2022-09-23 13:51:48,053 - trainer - INFO - {
  "train_loss": 88.99756622314453
}
2022-09-23 13:51:48,055 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval development set based on eval_every=2***************
2022-09-23 13:51:48,055 - trainer - INFO - {
  "dev_loss": 32.24100112915039,
  "dev_best_score_for_loss": -32.24100112915039
}
2022-09-23 13:51:48,056 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,057 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,057 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,057 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_24
2022-09-23 13:51:48,058 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,060 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,060 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,060 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,061 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,061 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_30
2022-09-23 13:51:48,063 - trainer - INFO - save model to path: model/mlp_e41g\ck_30
2022-09-23 13:51:48,064 - trainer - INFO - 
*****************[epoch: 30, global step: 31] eval training set at end of epoch***************
2022-09-23 13:51:48,064 - trainer - INFO - {
  "train_loss": 56.24297332763672
}
2022-09-23 13:51:48,064 - trainer - INFO - start training epoch 31
2022-09-23 13:51:48,065 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,065 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,065 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,069 - trainer - INFO - 
*****************[epoch: 31, global step: 32] eval training set at end of epoch***************
2022-09-23 13:51:48,069 - trainer - INFO - {
  "train_loss": 32.24098205566406
}
2022-09-23 13:51:48,069 - trainer - INFO - start training epoch 32
2022-09-23 13:51:48,070 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,070 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,070 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,073 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval training set based on eval_every=2***************
2022-09-23 13:51:48,074 - trainer - INFO - {
  "train_loss": 47.4235954284668
}
2022-09-23 13:51:48,076 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval development set based on eval_every=2***************
2022-09-23 13:51:48,076 - trainer - INFO - {
  "dev_loss": 110.50503540039062,
  "dev_best_score_for_loss": -32.24100112915039
}
2022-09-23 13:51:48,076 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:48,077 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,077 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,077 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,078 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_26
2022-09-23 13:51:48,079 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_32
2022-09-23 13:51:48,081 - trainer - INFO - save model to path: model/mlp_e41g\ck_32
2022-09-23 13:51:48,082 - trainer - INFO - 
*****************[epoch: 32, global step: 33] eval training set at end of epoch***************
2022-09-23 13:51:48,082 - trainer - INFO - {
  "train_loss": 62.60620880126953
}
2022-09-23 13:51:48,083 - trainer - INFO - start training epoch 33
2022-09-23 13:51:48,083 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,083 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,083 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,087 - trainer - INFO - 
*****************[epoch: 33, global step: 34] eval training set at end of epoch***************
2022-09-23 13:51:48,087 - trainer - INFO - {
  "train_loss": 110.50503540039062
}
2022-09-23 13:51:48,087 - trainer - INFO - start training epoch 34
2022-09-23 13:51:48,088 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,088 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,088 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,092 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval training set based on eval_every=2***************
2022-09-23 13:51:48,092 - trainer - INFO - {
  "train_loss": 116.11272048950195
}
2022-09-23 13:51:48,094 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval development set based on eval_every=2***************
2022-09-23 13:51:48,094 - trainer - INFO - {
  "dev_loss": 86.38174438476562,
  "dev_best_score_for_loss": -32.24100112915039
}
2022-09-23 13:51:48,095 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:48,095 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,096 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,096 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,096 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_28
2022-09-23 13:51:48,097 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_34
2022-09-23 13:51:48,100 - trainer - INFO - save model to path: model/mlp_e41g\ck_34
2022-09-23 13:51:48,100 - trainer - INFO - 
*****************[epoch: 34, global step: 35] eval training set at end of epoch***************
2022-09-23 13:51:48,100 - trainer - INFO - {
  "train_loss": 121.72040557861328
}
2022-09-23 13:51:48,101 - trainer - INFO - start training epoch 35
2022-09-23 13:51:48,101 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,101 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,101 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,105 - trainer - INFO - 
*****************[epoch: 35, global step: 36] eval training set at end of epoch***************
2022-09-23 13:51:48,105 - trainer - INFO - {
  "train_loss": 86.38174438476562
}
2022-09-23 13:51:48,105 - trainer - INFO - start training epoch 36
2022-09-23 13:51:48,106 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,106 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,106 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,109 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval training set based on eval_every=2***************
2022-09-23 13:51:48,109 - trainer - INFO - {
  "train_loss": 65.27590751647949
}
2022-09-23 13:51:48,112 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval development set based on eval_every=2***************
2022-09-23 13:51:48,112 - trainer - INFO - {
  "dev_loss": 31.43499755859375,
  "dev_best_score_for_loss": -31.43499755859375
}
2022-09-23 13:51:48,112 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,113 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,114 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,114 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_30
2022-09-23 13:51:48,115 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,117 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,117 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,117 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,118 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,118 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_36
2022-09-23 13:51:48,120 - trainer - INFO - save model to path: model/mlp_e41g\ck_36
2022-09-23 13:51:48,121 - trainer - INFO - 
*****************[epoch: 36, global step: 37] eval training set at end of epoch***************
2022-09-23 13:51:48,121 - trainer - INFO - {
  "train_loss": 44.17007064819336
}
2022-09-23 13:51:48,122 - trainer - INFO - start training epoch 37
2022-09-23 13:51:48,122 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,122 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,123 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,128 - trainer - INFO - 
*****************[epoch: 37, global step: 38] eval training set at end of epoch***************
2022-09-23 13:51:48,128 - trainer - INFO - {
  "train_loss": 31.43499755859375
}
2022-09-23 13:51:48,128 - trainer - INFO - start training epoch 38
2022-09-23 13:51:48,129 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,129 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,129 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,133 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval training set based on eval_every=2***************
2022-09-23 13:51:48,133 - trainer - INFO - {
  "train_loss": 39.93021011352539
}
2022-09-23 13:51:48,136 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval development set based on eval_every=2***************
2022-09-23 13:51:48,137 - trainer - INFO - {
  "dev_loss": 71.68346405029297,
  "dev_best_score_for_loss": -31.43499755859375
}
2022-09-23 13:51:48,137 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:48,137 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,139 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,139 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,139 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_32
2022-09-23 13:51:48,140 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_38
2022-09-23 13:51:48,144 - trainer - INFO - save model to path: model/mlp_e41g\ck_38
2022-09-23 13:51:48,145 - trainer - INFO - 
*****************[epoch: 38, global step: 39] eval training set at end of epoch***************
2022-09-23 13:51:48,145 - trainer - INFO - {
  "train_loss": 48.42542266845703
}
2022-09-23 13:51:48,145 - trainer - INFO - start training epoch 39
2022-09-23 13:51:48,146 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,146 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,146 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,151 - trainer - INFO - 
*****************[epoch: 39, global step: 40] eval training set at end of epoch***************
2022-09-23 13:51:48,151 - trainer - INFO - {
  "train_loss": 71.6834716796875
}
2022-09-23 13:51:48,152 - trainer - INFO - start training epoch 40
2022-09-23 13:51:48,152 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,152 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,153 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,157 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval training set based on eval_every=2***************
2022-09-23 13:51:48,157 - trainer - INFO - {
  "train_loss": 75.70147705078125
}
2022-09-23 13:51:48,159 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval development set based on eval_every=2***************
2022-09-23 13:51:48,159 - trainer - INFO - {
  "dev_loss": 67.18463897705078,
  "dev_best_score_for_loss": -31.43499755859375
}
2022-09-23 13:51:48,160 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:48,160 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,161 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,161 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,161 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_34
2022-09-23 13:51:48,163 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_40
2022-09-23 13:51:48,166 - trainer - INFO - save model to path: model/mlp_e41g\ck_40
2022-09-23 13:51:48,167 - trainer - INFO - 
*****************[epoch: 40, global step: 41] eval training set at end of epoch***************
2022-09-23 13:51:48,167 - trainer - INFO - {
  "train_loss": 79.719482421875
}
2022-09-23 13:51:48,167 - trainer - INFO - start training epoch 41
2022-09-23 13:51:48,167 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,168 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,168 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,172 - trainer - INFO - 
*****************[epoch: 41, global step: 42] eval training set at end of epoch***************
2022-09-23 13:51:48,172 - trainer - INFO - {
  "train_loss": 67.18462371826172
}
2022-09-23 13:51:48,173 - trainer - INFO - start training epoch 42
2022-09-23 13:51:48,173 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,173 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,174 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,177 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval training set based on eval_every=2***************
2022-09-23 13:51:48,178 - trainer - INFO - {
  "train_loss": 56.08273696899414
}
2022-09-23 13:51:48,180 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval development set based on eval_every=2***************
2022-09-23 13:51:48,180 - trainer - INFO - {
  "dev_loss": 30.72337532043457,
  "dev_best_score_for_loss": -30.72337532043457
}
2022-09-23 13:51:48,181 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,182 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,182 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,182 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_36
2022-09-23 13:51:48,183 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,185 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,185 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,185 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,186 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,186 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_42
2022-09-23 13:51:48,189 - trainer - INFO - save model to path: model/mlp_e41g\ck_42
2022-09-23 13:51:48,190 - trainer - INFO - 
*****************[epoch: 42, global step: 43] eval training set at end of epoch***************
2022-09-23 13:51:48,191 - trainer - INFO - {
  "train_loss": 44.98085021972656
}
2022-09-23 13:51:48,191 - trainer - INFO - start training epoch 43
2022-09-23 13:51:48,191 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,191 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,192 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,195 - trainer - INFO - 
*****************[epoch: 43, global step: 44] eval training set at end of epoch***************
2022-09-23 13:51:48,196 - trainer - INFO - {
  "train_loss": 30.723379135131836
}
2022-09-23 13:51:48,196 - trainer - INFO - start training epoch 44
2022-09-23 13:51:48,196 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,196 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,197 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,200 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval training set based on eval_every=2***************
2022-09-23 13:51:48,200 - trainer - INFO - {
  "train_loss": 32.56688213348389
}
2022-09-23 13:51:48,202 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval development set based on eval_every=2***************
2022-09-23 13:51:48,202 - trainer - INFO - {
  "dev_loss": 48.8126106262207,
  "dev_best_score_for_loss": -30.72337532043457
}
2022-09-23 13:51:48,203 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:48,203 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,204 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,204 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,205 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_38
2022-09-23 13:51:48,206 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_44
2022-09-23 13:51:48,208 - trainer - INFO - save model to path: model/mlp_e41g\ck_44
2022-09-23 13:51:48,209 - trainer - INFO - 
*****************[epoch: 44, global step: 45] eval training set at end of epoch***************
2022-09-23 13:51:48,209 - trainer - INFO - {
  "train_loss": 34.41038513183594
}
2022-09-23 13:51:48,210 - trainer - INFO - start training epoch 45
2022-09-23 13:51:48,210 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,210 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,210 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,214 - trainer - INFO - 
*****************[epoch: 45, global step: 46] eval training set at end of epoch***************
2022-09-23 13:51:48,214 - trainer - INFO - {
  "train_loss": 48.81261444091797
}
2022-09-23 13:51:48,214 - trainer - INFO - start training epoch 46
2022-09-23 13:51:48,214 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,215 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,215 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,219 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval training set based on eval_every=2***************
2022-09-23 13:51:48,219 - trainer - INFO - {
  "train_loss": 52.620649337768555
}
2022-09-23 13:51:48,222 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval development set based on eval_every=2***************
2022-09-23 13:51:48,222 - trainer - INFO - {
  "dev_loss": 48.92019271850586,
  "dev_best_score_for_loss": -30.72337532043457
}
2022-09-23 13:51:48,223 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:48,223 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,224 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,224 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,224 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_40
2022-09-23 13:51:48,226 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_46
2022-09-23 13:51:48,228 - trainer - INFO - save model to path: model/mlp_e41g\ck_46
2022-09-23 13:51:48,229 - trainer - INFO - 
*****************[epoch: 46, global step: 47] eval training set at end of epoch***************
2022-09-23 13:51:48,229 - trainer - INFO - {
  "train_loss": 56.42868423461914
}
2022-09-23 13:51:48,230 - trainer - INFO - start training epoch 47
2022-09-23 13:51:48,231 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,231 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,231 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,235 - trainer - INFO - 
*****************[epoch: 47, global step: 48] eval training set at end of epoch***************
2022-09-23 13:51:48,236 - trainer - INFO - {
  "train_loss": 48.92019271850586
}
2022-09-23 13:51:48,236 - trainer - INFO - start training epoch 48
2022-09-23 13:51:48,236 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,236 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,237 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,240 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval training set based on eval_every=2***************
2022-09-23 13:51:48,241 - trainer - INFO - {
  "train_loss": 42.08285331726074
}
2022-09-23 13:51:48,242 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval development set based on eval_every=2***************
2022-09-23 13:51:48,243 - trainer - INFO - {
  "dev_loss": 28.85781478881836,
  "dev_best_score_for_loss": -28.85781478881836
}
2022-09-23 13:51:48,243 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,244 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,244 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,245 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_42
2022-09-23 13:51:48,246 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,248 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,248 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,249 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,249 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,249 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_48
2022-09-23 13:51:48,252 - trainer - INFO - save model to path: model/mlp_e41g\ck_48
2022-09-23 13:51:48,253 - trainer - INFO - 
*****************[epoch: 48, global step: 49] eval training set at end of epoch***************
2022-09-23 13:51:48,253 - trainer - INFO - {
  "train_loss": 35.245513916015625
}
2022-09-23 13:51:48,253 - trainer - INFO - start training epoch 49
2022-09-23 13:51:48,253 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,254 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,254 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,257 - trainer - INFO - 
*****************[epoch: 49, global step: 50] eval training set at end of epoch***************
2022-09-23 13:51:48,258 - trainer - INFO - {
  "train_loss": 28.85781478881836
}
2022-09-23 13:51:48,258 - trainer - INFO - start training epoch 50
2022-09-23 13:51:48,258 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,259 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,259 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,262 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval training set based on eval_every=2***************
2022-09-23 13:51:48,262 - trainer - INFO - {
  "train_loss": 30.957460403442383
}
2022-09-23 13:51:48,264 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval development set based on eval_every=2***************
2022-09-23 13:51:48,264 - trainer - INFO - {
  "dev_loss": 40.636688232421875,
  "dev_best_score_for_loss": -28.85781478881836
}
2022-09-23 13:51:48,265 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:48,265 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,266 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,266 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,266 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_44
2022-09-23 13:51:48,267 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_50
2022-09-23 13:51:48,270 - trainer - INFO - save model to path: model/mlp_e41g\ck_50
2022-09-23 13:51:48,271 - trainer - INFO - 
*****************[epoch: 50, global step: 51] eval training set at end of epoch***************
2022-09-23 13:51:48,271 - trainer - INFO - {
  "train_loss": 33.057106018066406
}
2022-09-23 13:51:48,271 - trainer - INFO - start training epoch 51
2022-09-23 13:51:48,272 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,272 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,272 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,276 - trainer - INFO - 
*****************[epoch: 51, global step: 52] eval training set at end of epoch***************
2022-09-23 13:51:48,276 - trainer - INFO - {
  "train_loss": 40.636695861816406
}
2022-09-23 13:51:48,277 - trainer - INFO - start training epoch 52
2022-09-23 13:51:48,277 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,277 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,277 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,281 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval training set based on eval_every=2***************
2022-09-23 13:51:48,281 - trainer - INFO - {
  "train_loss": 41.880210876464844
}
2022-09-23 13:51:48,283 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval development set based on eval_every=2***************
2022-09-23 13:51:48,284 - trainer - INFO - {
  "dev_loss": 38.31715393066406,
  "dev_best_score_for_loss": -28.85781478881836
}
2022-09-23 13:51:48,284 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:48,284 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,285 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,285 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,285 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_46
2022-09-23 13:51:48,286 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_52
2022-09-23 13:51:48,289 - trainer - INFO - save model to path: model/mlp_e41g\ck_52
2022-09-23 13:51:48,290 - trainer - INFO - 
*****************[epoch: 52, global step: 53] eval training set at end of epoch***************
2022-09-23 13:51:48,290 - trainer - INFO - {
  "train_loss": 43.12372589111328
}
2022-09-23 13:51:48,291 - trainer - INFO - start training epoch 53
2022-09-23 13:51:48,291 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,291 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,291 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,295 - trainer - INFO - 
*****************[epoch: 53, global step: 54] eval training set at end of epoch***************
2022-09-23 13:51:48,295 - trainer - INFO - {
  "train_loss": 38.31715774536133
}
2022-09-23 13:51:48,296 - trainer - INFO - start training epoch 54
2022-09-23 13:51:48,296 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,296 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,296 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,299 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval training set based on eval_every=2***************
2022-09-23 13:51:48,300 - trainer - INFO - {
  "train_loss": 34.64722442626953
}
2022-09-23 13:51:48,302 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval development set based on eval_every=2***************
2022-09-23 13:51:48,302 - trainer - INFO - {
  "dev_loss": 27.694307327270508,
  "dev_best_score_for_loss": -27.694307327270508
}
2022-09-23 13:51:48,302 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,303 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,303 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,304 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_48
2022-09-23 13:51:48,305 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,307 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,307 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,307 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,308 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,308 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_54
2022-09-23 13:51:48,311 - trainer - INFO - save model to path: model/mlp_e41g\ck_54
2022-09-23 13:51:48,312 - trainer - INFO - 
*****************[epoch: 54, global step: 55] eval training set at end of epoch***************
2022-09-23 13:51:48,312 - trainer - INFO - {
  "train_loss": 30.977291107177734
}
2022-09-23 13:51:48,313 - trainer - INFO - start training epoch 55
2022-09-23 13:51:48,313 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,313 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,313 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,317 - trainer - INFO - 
*****************[epoch: 55, global step: 56] eval training set at end of epoch***************
2022-09-23 13:51:48,317 - trainer - INFO - {
  "train_loss": 27.694307327270508
}
2022-09-23 13:51:48,317 - trainer - INFO - start training epoch 56
2022-09-23 13:51:48,317 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,318 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,318 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,321 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval training set based on eval_every=2***************
2022-09-23 13:51:48,321 - trainer - INFO - {
  "train_loss": 29.06468963623047
}
2022-09-23 13:51:48,324 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval development set based on eval_every=2***************
2022-09-23 13:51:48,325 - trainer - INFO - {
  "dev_loss": 34.75825881958008,
  "dev_best_score_for_loss": -27.694307327270508
}
2022-09-23 13:51:48,325 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:48,326 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,327 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,327 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,327 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_50
2022-09-23 13:51:48,329 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_56
2022-09-23 13:51:48,332 - trainer - INFO - save model to path: model/mlp_e41g\ck_56
2022-09-23 13:51:48,332 - trainer - INFO - 
*****************[epoch: 56, global step: 57] eval training set at end of epoch***************
2022-09-23 13:51:48,333 - trainer - INFO - {
  "train_loss": 30.43507194519043
}
2022-09-23 13:51:48,333 - trainer - INFO - start training epoch 57
2022-09-23 13:51:48,334 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,334 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,334 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,339 - trainer - INFO - 
*****************[epoch: 57, global step: 58] eval training set at end of epoch***************
2022-09-23 13:51:48,340 - trainer - INFO - {
  "train_loss": 34.75825500488281
}
2022-09-23 13:51:48,340 - trainer - INFO - start training epoch 58
2022-09-23 13:51:48,340 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,341 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,341 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,345 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval training set based on eval_every=2***************
2022-09-23 13:51:48,345 - trainer - INFO - {
  "train_loss": 34.92224311828613
}
2022-09-23 13:51:48,348 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval development set based on eval_every=2***************
2022-09-23 13:51:48,348 - trainer - INFO - {
  "dev_loss": 31.080718994140625,
  "dev_best_score_for_loss": -27.694307327270508
}
2022-09-23 13:51:48,349 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:48,349 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,350 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,350 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,351 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_52
2022-09-23 13:51:48,352 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_58
2022-09-23 13:51:48,355 - trainer - INFO - save model to path: model/mlp_e41g\ck_58
2022-09-23 13:51:48,356 - trainer - INFO - 
*****************[epoch: 58, global step: 59] eval training set at end of epoch***************
2022-09-23 13:51:48,356 - trainer - INFO - {
  "train_loss": 35.08623123168945
}
2022-09-23 13:51:48,356 - trainer - INFO - start training epoch 59
2022-09-23 13:51:48,357 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,357 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,357 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,361 - trainer - INFO - 
*****************[epoch: 59, global step: 60] eval training set at end of epoch***************
2022-09-23 13:51:48,361 - trainer - INFO - {
  "train_loss": 31.08072280883789
}
2022-09-23 13:51:48,361 - trainer - INFO - start training epoch 60
2022-09-23 13:51:48,362 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,362 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,362 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,366 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval training set based on eval_every=2***************
2022-09-23 13:51:48,366 - trainer - INFO - {
  "train_loss": 29.15133762359619
}
2022-09-23 13:51:48,369 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval development set based on eval_every=2***************
2022-09-23 13:51:48,369 - trainer - INFO - {
  "dev_loss": 26.964052200317383,
  "dev_best_score_for_loss": -26.964052200317383
}
2022-09-23 13:51:48,370 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,371 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,371 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,371 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_54
2022-09-23 13:51:48,373 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,375 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,375 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,375 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,376 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,376 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_60
2022-09-23 13:51:48,379 - trainer - INFO - save model to path: model/mlp_e41g\ck_60
2022-09-23 13:51:48,380 - trainer - INFO - 
*****************[epoch: 60, global step: 61] eval training set at end of epoch***************
2022-09-23 13:51:48,380 - trainer - INFO - {
  "train_loss": 27.221952438354492
}
2022-09-23 13:51:48,380 - trainer - INFO - start training epoch 61
2022-09-23 13:51:48,381 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,381 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,381 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,385 - trainer - INFO - 
*****************[epoch: 61, global step: 62] eval training set at end of epoch***************
2022-09-23 13:51:48,385 - trainer - INFO - {
  "train_loss": 26.964052200317383
}
2022-09-23 13:51:48,385 - trainer - INFO - start training epoch 62
2022-09-23 13:51:48,385 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,386 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,386 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,390 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval training set based on eval_every=2***************
2022-09-23 13:51:48,390 - trainer - INFO - {
  "train_loss": 28.138644218444824
}
2022-09-23 13:51:48,392 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval development set based on eval_every=2***************
2022-09-23 13:51:48,393 - trainer - INFO - {
  "dev_loss": 30.865989685058594,
  "dev_best_score_for_loss": -26.964052200317383
}
2022-09-23 13:51:48,393 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:48,393 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,394 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,394 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,394 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_56
2022-09-23 13:51:48,395 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_62
2022-09-23 13:51:48,398 - trainer - INFO - save model to path: model/mlp_e41g\ck_62
2022-09-23 13:51:48,399 - trainer - INFO - 
*****************[epoch: 62, global step: 63] eval training set at end of epoch***************
2022-09-23 13:51:48,399 - trainer - INFO - {
  "train_loss": 29.313236236572266
}
2022-09-23 13:51:48,399 - trainer - INFO - start training epoch 63
2022-09-23 13:51:48,399 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,400 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,400 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,403 - trainer - INFO - 
*****************[epoch: 63, global step: 64] eval training set at end of epoch***************
2022-09-23 13:51:48,404 - trainer - INFO - {
  "train_loss": 30.865989685058594
}
2022-09-23 13:51:48,404 - trainer - INFO - start training epoch 64
2022-09-23 13:51:48,404 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,404 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,405 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,408 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval training set based on eval_every=2***************
2022-09-23 13:51:48,408 - trainer - INFO - {
  "train_loss": 30.29217529296875
}
2022-09-23 13:51:48,410 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval development set based on eval_every=2***************
2022-09-23 13:51:48,411 - trainer - INFO - {
  "dev_loss": 27.064531326293945,
  "dev_best_score_for_loss": -26.964052200317383
}
2022-09-23 13:51:48,411 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:48,412 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,413 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,413 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,413 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_58
2022-09-23 13:51:48,414 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_64
2022-09-23 13:51:48,416 - trainer - INFO - save model to path: model/mlp_e41g\ck_64
2022-09-23 13:51:48,417 - trainer - INFO - 
*****************[epoch: 64, global step: 65] eval training set at end of epoch***************
2022-09-23 13:51:48,417 - trainer - INFO - {
  "train_loss": 29.718360900878906
}
2022-09-23 13:51:48,418 - trainer - INFO - start training epoch 65
2022-09-23 13:51:48,418 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,418 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,418 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,422 - trainer - INFO - 
*****************[epoch: 65, global step: 66] eval training set at end of epoch***************
2022-09-23 13:51:48,422 - trainer - INFO - {
  "train_loss": 27.064529418945312
}
2022-09-23 13:51:48,423 - trainer - INFO - start training epoch 66
2022-09-23 13:51:48,423 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,423 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,423 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,426 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval training set based on eval_every=2***************
2022-09-23 13:51:48,427 - trainer - INFO - {
  "train_loss": 26.28596591949463
}
2022-09-23 13:51:48,428 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval development set based on eval_every=2***************
2022-09-23 13:51:48,429 - trainer - INFO - {
  "dev_loss": 26.14174461364746,
  "dev_best_score_for_loss": -26.14174461364746
}
2022-09-23 13:51:48,429 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,430 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,430 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,430 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_60
2022-09-23 13:51:48,431 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,433 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,433 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,434 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,434 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,434 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_66
2022-09-23 13:51:48,437 - trainer - INFO - save model to path: model/mlp_e41g\ck_66
2022-09-23 13:51:48,438 - trainer - INFO - 
*****************[epoch: 66, global step: 67] eval training set at end of epoch***************
2022-09-23 13:51:48,438 - trainer - INFO - {
  "train_loss": 25.507402420043945
}
2022-09-23 13:51:48,439 - trainer - INFO - start training epoch 67
2022-09-23 13:51:48,439 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,439 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,440 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,444 - trainer - INFO - 
*****************[epoch: 67, global step: 68] eval training set at end of epoch***************
2022-09-23 13:51:48,444 - trainer - INFO - {
  "train_loss": 26.141746520996094
}
2022-09-23 13:51:48,444 - trainer - INFO - start training epoch 68
2022-09-23 13:51:48,444 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,445 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,445 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,448 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval training set based on eval_every=2***************
2022-09-23 13:51:48,448 - trainer - INFO - {
  "train_loss": 26.822853088378906
}
2022-09-23 13:51:48,450 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval development set based on eval_every=2***************
2022-09-23 13:51:48,450 - trainer - INFO - {
  "dev_loss": 27.52409553527832,
  "dev_best_score_for_loss": -26.14174461364746
}
2022-09-23 13:51:48,451 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:48,451 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,452 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,452 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,452 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_62
2022-09-23 13:51:48,453 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_68
2022-09-23 13:51:48,457 - trainer - INFO - save model to path: model/mlp_e41g\ck_68
2022-09-23 13:51:48,457 - trainer - INFO - 
*****************[epoch: 68, global step: 69] eval training set at end of epoch***************
2022-09-23 13:51:48,457 - trainer - INFO - {
  "train_loss": 27.50395965576172
}
2022-09-23 13:51:48,458 - trainer - INFO - start training epoch 69
2022-09-23 13:51:48,458 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,458 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,458 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,462 - trainer - INFO - 
*****************[epoch: 69, global step: 70] eval training set at end of epoch***************
2022-09-23 13:51:48,462 - trainer - INFO - {
  "train_loss": 27.524097442626953
}
2022-09-23 13:51:48,462 - trainer - INFO - start training epoch 70
2022-09-23 13:51:48,463 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,463 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,463 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,466 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval training set based on eval_every=2***************
2022-09-23 13:51:48,466 - trainer - INFO - {
  "train_loss": 26.772340774536133
}
2022-09-23 13:51:48,469 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval development set based on eval_every=2***************
2022-09-23 13:51:48,469 - trainer - INFO - {
  "dev_loss": 24.613265991210938,
  "dev_best_score_for_loss": -24.613265991210938
}
2022-09-23 13:51:48,469 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,470 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,470 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,471 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_64
2022-09-23 13:51:48,472 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,474 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,474 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,474 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,475 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,475 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_70
2022-09-23 13:51:48,477 - trainer - INFO - save model to path: model/mlp_e41g\ck_70
2022-09-23 13:51:48,478 - trainer - INFO - 
*****************[epoch: 70, global step: 71] eval training set at end of epoch***************
2022-09-23 13:51:48,478 - trainer - INFO - {
  "train_loss": 26.020584106445312
}
2022-09-23 13:51:48,478 - trainer - INFO - start training epoch 71
2022-09-23 13:51:48,478 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,479 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,479 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,482 - trainer - INFO - 
*****************[epoch: 71, global step: 72] eval training set at end of epoch***************
2022-09-23 13:51:48,482 - trainer - INFO - {
  "train_loss": 24.613277435302734
}
2022-09-23 13:51:48,483 - trainer - INFO - start training epoch 72
2022-09-23 13:51:48,483 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,483 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,484 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,487 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval training set based on eval_every=2***************
2022-09-23 13:51:48,487 - trainer - INFO - {
  "train_loss": 24.56473445892334
}
2022-09-23 13:51:48,490 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval development set based on eval_every=2***************
2022-09-23 13:51:48,490 - trainer - INFO - {
  "dev_loss": 25.241559982299805,
  "dev_best_score_for_loss": -24.613265991210938
}
2022-09-23 13:51:48,491 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:48,491 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,492 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,492 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,492 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_66
2022-09-23 13:51:48,493 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_72
2022-09-23 13:51:48,496 - trainer - INFO - save model to path: model/mlp_e41g\ck_72
2022-09-23 13:51:48,496 - trainer - INFO - 
*****************[epoch: 72, global step: 73] eval training set at end of epoch***************
2022-09-23 13:51:48,497 - trainer - INFO - {
  "train_loss": 24.516191482543945
}
2022-09-23 13:51:48,497 - trainer - INFO - start training epoch 73
2022-09-23 13:51:48,497 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,497 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,498 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,502 - trainer - INFO - 
*****************[epoch: 73, global step: 74] eval training set at end of epoch***************
2022-09-23 13:51:48,502 - trainer - INFO - {
  "train_loss": 25.241559982299805
}
2022-09-23 13:51:48,502 - trainer - INFO - start training epoch 74
2022-09-23 13:51:48,503 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,503 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,503 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,507 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval training set based on eval_every=2***************
2022-09-23 13:51:48,508 - trainer - INFO - {
  "train_loss": 25.366894721984863
}
2022-09-23 13:51:48,511 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval development set based on eval_every=2***************
2022-09-23 13:51:48,512 - trainer - INFO - {
  "dev_loss": 24.749839782714844,
  "dev_best_score_for_loss": -24.613265991210938
}
2022-09-23 13:51:48,512 - trainer - INFO -   no_improve_count: 2
2022-09-23 13:51:48,513 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,514 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,514 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,514 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_68
2022-09-23 13:51:48,516 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_74
2022-09-23 13:51:48,519 - trainer - INFO - save model to path: model/mlp_e41g\ck_74
2022-09-23 13:51:48,520 - trainer - INFO - 
*****************[epoch: 74, global step: 75] eval training set at end of epoch***************
2022-09-23 13:51:48,520 - trainer - INFO - {
  "train_loss": 25.492229461669922
}
2022-09-23 13:51:48,521 - trainer - INFO - start training epoch 75
2022-09-23 13:51:48,521 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,521 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,522 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,526 - trainer - INFO - 
*****************[epoch: 75, global step: 76] eval training set at end of epoch***************
2022-09-23 13:51:48,527 - trainer - INFO - {
  "train_loss": 24.74985122680664
}
2022-09-23 13:51:48,527 - trainer - INFO - start training epoch 76
2022-09-23 13:51:48,527 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,527 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,528 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,532 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval training set based on eval_every=2***************
2022-09-23 13:51:48,532 - trainer - INFO - {
  "train_loss": 24.229557991027832
}
2022-09-23 13:51:48,534 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval development set based on eval_every=2***************
2022-09-23 13:51:48,535 - trainer - INFO - {
  "dev_loss": 23.30533790588379,
  "dev_best_score_for_loss": -23.30533790588379
}
2022-09-23 13:51:48,535 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,536 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,537 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,537 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_70
2022-09-23 13:51:48,538 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,540 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,541 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,541 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,542 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,542 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_76
2022-09-23 13:51:48,545 - trainer - INFO - save model to path: model/mlp_e41g\ck_76
2022-09-23 13:51:48,546 - trainer - INFO - 
*****************[epoch: 76, global step: 77] eval training set at end of epoch***************
2022-09-23 13:51:48,546 - trainer - INFO - {
  "train_loss": 23.709264755249023
}
2022-09-23 13:51:48,547 - trainer - INFO - start training epoch 77
2022-09-23 13:51:48,547 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,547 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,547 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,552 - trainer - INFO - 
*****************[epoch: 77, global step: 78] eval training set at end of epoch***************
2022-09-23 13:51:48,552 - trainer - INFO - {
  "train_loss": 23.305334091186523
}
2022-09-23 13:51:48,552 - trainer - INFO - start training epoch 78
2022-09-23 13:51:48,552 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,553 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,553 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,557 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval training set based on eval_every=2***************
2022-09-23 13:51:48,558 - trainer - INFO - {
  "train_loss": 23.45071315765381
}
2022-09-23 13:51:48,560 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval development set based on eval_every=2***************
2022-09-23 13:51:48,560 - trainer - INFO - {
  "dev_loss": 23.809032440185547,
  "dev_best_score_for_loss": -23.30533790588379
}
2022-09-23 13:51:48,561 - trainer - INFO -   no_improve_count: 1
2022-09-23 13:51:48,561 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,562 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,562 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,562 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_72
2022-09-23 13:51:48,563 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_78
2022-09-23 13:51:48,566 - trainer - INFO - save model to path: model/mlp_e41g\ck_78
2022-09-23 13:51:48,566 - trainer - INFO - 
*****************[epoch: 78, global step: 79] eval training set at end of epoch***************
2022-09-23 13:51:48,567 - trainer - INFO - {
  "train_loss": 23.596092224121094
}
2022-09-23 13:51:48,567 - trainer - INFO - start training epoch 79
2022-09-23 13:51:48,567 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,567 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,568 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,571 - trainer - INFO - 
*****************[epoch: 79, global step: 80] eval training set at end of epoch***************
2022-09-23 13:51:48,571 - trainer - INFO - {
  "train_loss": 23.809032440185547
}
2022-09-23 13:51:48,572 - trainer - INFO - start training epoch 80
2022-09-23 13:51:48,572 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,572 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,572 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,575 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval training set based on eval_every=2***************
2022-09-23 13:51:48,575 - trainer - INFO - {
  "train_loss": 23.60175323486328
}
2022-09-23 13:51:48,578 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval development set based on eval_every=2***************
2022-09-23 13:51:48,578 - trainer - INFO - {
  "dev_loss": 22.66566276550293,
  "dev_best_score_for_loss": -22.66566276550293
}
2022-09-23 13:51:48,578 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,579 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,579 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,580 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_74
2022-09-23 13:51:48,581 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,582 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,583 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,583 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,583 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,584 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_80
2022-09-23 13:51:48,586 - trainer - INFO - save model to path: model/mlp_e41g\ck_80
2022-09-23 13:51:48,587 - trainer - INFO - 
*****************[epoch: 80, global step: 81] eval training set at end of epoch***************
2022-09-23 13:51:48,587 - trainer - INFO - {
  "train_loss": 23.394474029541016
}
2022-09-23 13:51:48,587 - trainer - INFO - start training epoch 81
2022-09-23 13:51:48,587 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,588 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,588 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,591 - trainer - INFO - 
*****************[epoch: 81, global step: 82] eval training set at end of epoch***************
2022-09-23 13:51:48,592 - trainer - INFO - {
  "train_loss": 22.66566276550293
}
2022-09-23 13:51:48,592 - trainer - INFO - start training epoch 82
2022-09-23 13:51:48,592 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,592 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,593 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,596 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval training set based on eval_every=2***************
2022-09-23 13:51:48,596 - trainer - INFO - {
  "train_loss": 22.46619987487793
}
2022-09-23 13:51:48,598 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval development set based on eval_every=2***************
2022-09-23 13:51:48,598 - trainer - INFO - {
  "dev_loss": 22.327293395996094,
  "dev_best_score_for_loss": -22.327293395996094
}
2022-09-23 13:51:48,599 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,600 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,600 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,600 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_76
2022-09-23 13:51:48,601 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,603 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,603 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,603 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,604 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,604 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_82
2022-09-23 13:51:48,607 - trainer - INFO - save model to path: model/mlp_e41g\ck_82
2022-09-23 13:51:48,608 - trainer - INFO - 
*****************[epoch: 82, global step: 83] eval training set at end of epoch***************
2022-09-23 13:51:48,608 - trainer - INFO - {
  "train_loss": 22.26673698425293
}
2022-09-23 13:51:48,608 - trainer - INFO - start training epoch 83
2022-09-23 13:51:48,608 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,609 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,609 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,612 - trainer - INFO - 
*****************[epoch: 83, global step: 84] eval training set at end of epoch***************
2022-09-23 13:51:48,613 - trainer - INFO - {
  "train_loss": 22.327293395996094
}
2022-09-23 13:51:48,613 - trainer - INFO - start training epoch 84
2022-09-23 13:51:48,613 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,614 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,614 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,617 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval training set based on eval_every=2***************
2022-09-23 13:51:48,617 - trainer - INFO - {
  "train_loss": 22.365985870361328
}
2022-09-23 13:51:48,619 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval development set based on eval_every=2***************
2022-09-23 13:51:48,619 - trainer - INFO - {
  "dev_loss": 22.12189483642578,
  "dev_best_score_for_loss": -22.12189483642578
}
2022-09-23 13:51:48,620 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,621 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,621 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,621 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_78
2022-09-23 13:51:48,622 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,624 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,624 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,624 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,625 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,625 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_84
2022-09-23 13:51:48,628 - trainer - INFO - save model to path: model/mlp_e41g\ck_84
2022-09-23 13:51:48,628 - trainer - INFO - 
*****************[epoch: 84, global step: 85] eval training set at end of epoch***************
2022-09-23 13:51:48,629 - trainer - INFO - {
  "train_loss": 22.404678344726562
}
2022-09-23 13:51:48,629 - trainer - INFO - start training epoch 85
2022-09-23 13:51:48,629 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,630 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,630 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,633 - trainer - INFO - 
*****************[epoch: 85, global step: 86] eval training set at end of epoch***************
2022-09-23 13:51:48,634 - trainer - INFO - {
  "train_loss": 22.12189483642578
}
2022-09-23 13:51:48,634 - trainer - INFO - start training epoch 86
2022-09-23 13:51:48,634 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,634 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,635 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,638 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval training set based on eval_every=2***************
2022-09-23 13:51:48,639 - trainer - INFO - {
  "train_loss": 21.864593505859375
}
2022-09-23 13:51:48,641 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval development set based on eval_every=2***************
2022-09-23 13:51:48,641 - trainer - INFO - {
  "dev_loss": 21.25460433959961,
  "dev_best_score_for_loss": -21.25460433959961
}
2022-09-23 13:51:48,642 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,643 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,643 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,643 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_80
2022-09-23 13:51:48,644 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,647 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,647 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,647 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,647 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,648 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_86
2022-09-23 13:51:48,650 - trainer - INFO - save model to path: model/mlp_e41g\ck_86
2022-09-23 13:51:48,651 - trainer - INFO - 
*****************[epoch: 86, global step: 87] eval training set at end of epoch***************
2022-09-23 13:51:48,651 - trainer - INFO - {
  "train_loss": 21.60729217529297
}
2022-09-23 13:51:48,652 - trainer - INFO - start training epoch 87
2022-09-23 13:51:48,652 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,652 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,652 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,656 - trainer - INFO - 
*****************[epoch: 87, global step: 88] eval training set at end of epoch***************
2022-09-23 13:51:48,656 - trainer - INFO - {
  "train_loss": 21.25459098815918
}
2022-09-23 13:51:48,657 - trainer - INFO - start training epoch 88
2022-09-23 13:51:48,657 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,657 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,657 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,660 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval training set based on eval_every=2***************
2022-09-23 13:51:48,660 - trainer - INFO - {
  "train_loss": 21.225546836853027
}
2022-09-23 13:51:48,663 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval development set based on eval_every=2***************
2022-09-23 13:51:48,663 - trainer - INFO - {
  "dev_loss": 21.176816940307617,
  "dev_best_score_for_loss": -21.176816940307617
}
2022-09-23 13:51:48,663 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,664 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,664 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,665 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_82
2022-09-23 13:51:48,666 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,667 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,668 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,668 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,668 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,669 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_88
2022-09-23 13:51:48,671 - trainer - INFO - save model to path: model/mlp_e41g\ck_88
2022-09-23 13:51:48,672 - trainer - INFO - 
*****************[epoch: 88, global step: 89] eval training set at end of epoch***************
2022-09-23 13:51:48,672 - trainer - INFO - {
  "train_loss": 21.196502685546875
}
2022-09-23 13:51:48,672 - trainer - INFO - start training epoch 89
2022-09-23 13:51:48,672 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,673 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,673 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,676 - trainer - INFO - 
*****************[epoch: 89, global step: 90] eval training set at end of epoch***************
2022-09-23 13:51:48,677 - trainer - INFO - {
  "train_loss": 21.176822662353516
}
2022-09-23 13:51:48,677 - trainer - INFO - start training epoch 90
2022-09-23 13:51:48,677 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,677 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,678 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,681 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval training set based on eval_every=2***************
2022-09-23 13:51:48,681 - trainer - INFO - {
  "train_loss": 21.05604362487793
}
2022-09-23 13:51:48,684 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval development set based on eval_every=2***************
2022-09-23 13:51:48,684 - trainer - INFO - {
  "dev_loss": 20.542875289916992,
  "dev_best_score_for_loss": -20.542875289916992
}
2022-09-23 13:51:48,685 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,686 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,686 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,686 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_84
2022-09-23 13:51:48,688 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,690 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,690 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,690 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,691 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,691 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_90
2022-09-23 13:51:48,694 - trainer - INFO - save model to path: model/mlp_e41g\ck_90
2022-09-23 13:51:48,694 - trainer - INFO - 
*****************[epoch: 90, global step: 91] eval training set at end of epoch***************
2022-09-23 13:51:48,695 - trainer - INFO - {
  "train_loss": 20.935264587402344
}
2022-09-23 13:51:48,695 - trainer - INFO - start training epoch 91
2022-09-23 13:51:48,695 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,696 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,696 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,700 - trainer - INFO - 
*****************[epoch: 91, global step: 92] eval training set at end of epoch***************
2022-09-23 13:51:48,700 - trainer - INFO - {
  "train_loss": 20.54288101196289
}
2022-09-23 13:51:48,701 - trainer - INFO - start training epoch 92
2022-09-23 13:51:48,701 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,701 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,701 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,705 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval training set based on eval_every=2***************
2022-09-23 13:51:48,705 - trainer - INFO - {
  "train_loss": 20.39840316772461
}
2022-09-23 13:51:48,708 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval development set based on eval_every=2***************
2022-09-23 13:51:48,709 - trainer - INFO - {
  "dev_loss": 20.145980834960938,
  "dev_best_score_for_loss": -20.145980834960938
}
2022-09-23 13:51:48,709 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,711 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,711 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,711 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_86
2022-09-23 13:51:48,712 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,715 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,715 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,716 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,716 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,717 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_92
2022-09-23 13:51:48,719 - trainer - INFO - save model to path: model/mlp_e41g\ck_92
2022-09-23 13:51:48,720 - trainer - INFO - 
*****************[epoch: 92, global step: 93] eval training set at end of epoch***************
2022-09-23 13:51:48,720 - trainer - INFO - {
  "train_loss": 20.253925323486328
}
2022-09-23 13:51:48,721 - trainer - INFO - start training epoch 93
2022-09-23 13:51:48,721 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,721 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,722 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,726 - trainer - INFO - 
*****************[epoch: 93, global step: 94] eval training set at end of epoch***************
2022-09-23 13:51:48,727 - trainer - INFO - {
  "train_loss": 20.14598846435547
}
2022-09-23 13:51:48,727 - trainer - INFO - start training epoch 94
2022-09-23 13:51:48,727 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,728 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,728 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,732 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval training set based on eval_every=2***************
2022-09-23 13:51:48,733 - trainer - INFO - {
  "train_loss": 20.09943675994873
}
2022-09-23 13:51:48,736 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval development set based on eval_every=2***************
2022-09-23 13:51:48,736 - trainer - INFO - {
  "dev_loss": 19.821645736694336,
  "dev_best_score_for_loss": -19.821645736694336
}
2022-09-23 13:51:48,737 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,738 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,738 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,738 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_88
2022-09-23 13:51:48,740 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,742 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,743 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,743 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,743 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,744 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_94
2022-09-23 13:51:48,746 - trainer - INFO - save model to path: model/mlp_e41g\ck_94
2022-09-23 13:51:48,747 - trainer - INFO - 
*****************[epoch: 94, global step: 95] eval training set at end of epoch***************
2022-09-23 13:51:48,747 - trainer - INFO - {
  "train_loss": 20.052885055541992
}
2022-09-23 13:51:48,747 - trainer - INFO - start training epoch 95
2022-09-23 13:51:48,748 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,748 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,748 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,753 - trainer - INFO - 
*****************[epoch: 95, global step: 96] eval training set at end of epoch***************
2022-09-23 13:51:48,753 - trainer - INFO - {
  "train_loss": 19.821645736694336
}
2022-09-23 13:51:48,753 - trainer - INFO - start training epoch 96
2022-09-23 13:51:48,754 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,754 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,754 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,758 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval training set based on eval_every=2***************
2022-09-23 13:51:48,758 - trainer - INFO - {
  "train_loss": 19.66377544403076
}
2022-09-23 13:51:48,761 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval development set based on eval_every=2***************
2022-09-23 13:51:48,761 - trainer - INFO - {
  "dev_loss": 19.261754989624023,
  "dev_best_score_for_loss": -19.261754989624023
}
2022-09-23 13:51:48,762 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,763 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,763 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,763 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_90
2022-09-23 13:51:48,764 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,766 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,767 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,767 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,768 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,768 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_96
2022-09-23 13:51:48,771 - trainer - INFO - save model to path: model/mlp_e41g\ck_96
2022-09-23 13:51:48,771 - trainer - INFO - 
*****************[epoch: 96, global step: 97] eval training set at end of epoch***************
2022-09-23 13:51:48,772 - trainer - INFO - {
  "train_loss": 19.505905151367188
}
2022-09-23 13:51:48,772 - trainer - INFO - start training epoch 97
2022-09-23 13:51:48,773 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,773 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,773 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,777 - trainer - INFO - 
*****************[epoch: 97, global step: 98] eval training set at end of epoch***************
2022-09-23 13:51:48,777 - trainer - INFO - {
  "train_loss": 19.261760711669922
}
2022-09-23 13:51:48,777 - trainer - INFO - start training epoch 98
2022-09-23 13:51:48,778 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,778 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,778 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,781 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval training set based on eval_every=2***************
2022-09-23 13:51:48,781 - trainer - INFO - {
  "train_loss": 19.194272994995117
}
2022-09-23 13:51:48,783 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval development set based on eval_every=2***************
2022-09-23 13:51:48,784 - trainer - INFO - {
  "dev_loss": 18.988895416259766,
  "dev_best_score_for_loss": -18.988895416259766
}
2022-09-23 13:51:48,784 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,785 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,785 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,786 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_92
2022-09-23 13:51:48,787 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,789 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,789 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,789 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,790 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,790 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_98
2022-09-23 13:51:48,793 - trainer - INFO - save model to path: model/mlp_e41g\ck_98
2022-09-23 13:51:48,793 - trainer - INFO - 
*****************[epoch: 98, global step: 99] eval training set at end of epoch***************
2022-09-23 13:51:48,794 - trainer - INFO - {
  "train_loss": 19.126785278320312
}
2022-09-23 13:51:48,794 - trainer - INFO - start training epoch 99
2022-09-23 13:51:48,794 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,794 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,795 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,798 - trainer - INFO - 
*****************[epoch: 99, global step: 100] eval training set at end of epoch***************
2022-09-23 13:51:48,798 - trainer - INFO - {
  "train_loss": 18.988893508911133
}
2022-09-23 13:51:48,799 - trainer - INFO - start training epoch 100
2022-09-23 13:51:48,799 - trainer - INFO - training using device=cpu
2022-09-23 13:51:48,799 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 13:51:48,800 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_e41g",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 13:51:48,803 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval training set based on eval_every=2***************
2022-09-23 13:51:48,803 - trainer - INFO - {
  "train_loss": 18.87467384338379
}
2022-09-23 13:51:48,805 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval development set based on eval_every=2***************
2022-09-23 13:51:48,806 - trainer - INFO - {
  "dev_loss": 18.492237091064453,
  "dev_best_score_for_loss": -18.492237091064453
}
2022-09-23 13:51:48,806 - trainer - INFO -    save the model with best score so far
2022-09-23 13:51:48,807 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 13:51:48,807 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 13:51:48,808 - trainer - INFO -   Remove checkpoint model/mlp_e41g\ck_94
2022-09-23 13:51:48,809 - trainer - INFO -   Save checkpoint to model/mlp_e41g
2022-09-23 13:51:48,811 - trainer - INFO - save model to path: model/mlp_e41g
2022-09-23 13:51:48,811 - trainer - INFO -   no_improve_count: 0
2022-09-23 13:51:48,811 - trainer - INFO -   patience: 200
2022-09-23 13:51:48,812 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 13:51:48,812 - trainer - INFO -   Save checkpoint to model/mlp_e41g\ck_100
2022-09-23 13:51:48,814 - trainer - INFO - save model to path: model/mlp_e41g\ck_100
2022-09-23 13:51:48,815 - trainer - INFO - 
*****************[epoch: 100, global step: 101] eval training set at end of epoch***************
2022-09-23 13:51:48,815 - trainer - INFO - {
  "train_loss": 18.760454177856445
}
