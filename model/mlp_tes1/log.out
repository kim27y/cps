2022-09-23 14:24:32,603 - trainer - INFO - MLP(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=64, bias=True)
    (2): Linear(in_features=64, out_features=32, bias=True)
  )
  (activation_layers): ModuleList(
    (0): ReLU(inplace=True)
    (1): ReLU(inplace=True)
    (2): ReLU(inplace=True)
  )
  (dropout): Dropout(p=0.0, inplace=False)
  (linear_output): Linear(in_features=32, out_features=1, bias=True)
)
2022-09-23 14:24:32,604 - trainer - INFO -   Total params: 10625
2022-09-23 14:24:32,604 - trainer - INFO -   Trainable params: 10625
2022-09-23 14:24:32,605 - trainer - INFO -   Non-trainable params: 0
2022-09-23 14:24:32,605 - trainer - INFO -   There are 6  training examples
2022-09-23 14:24:32,605 - trainer - INFO -   There are 6 examples for development
2022-09-23 14:24:32,606 - trainer - INFO - start training epoch 1
2022-09-23 14:24:32,606 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,606 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,607 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,639 - trainer - INFO - 
*****************[epoch: 1, global step: 2] eval training set at end of epoch***************
2022-09-23 14:24:32,640 - trainer - INFO - {
  "train_loss": 3855.812255859375
}
2022-09-23 14:24:32,640 - trainer - INFO - start training epoch 2
2022-09-23 14:24:32,640 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,641 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,641 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,644 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval training set based on eval_every=2***************
2022-09-23 14:24:32,644 - trainer - INFO - {
  "train_loss": 3716.483154296875
}
2022-09-23 14:24:32,647 - trainer - INFO - 
*****************[epoch: 2, global step: 2] eval development set based on eval_every=2***************
2022-09-23 14:24:32,648 - trainer - INFO - {
  "dev_loss": 2948.6474609375,
  "dev_best_score_for_loss": -2948.6474609375
}
2022-09-23 14:24:32,648 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:32,649 - trainer - INFO -    Check 0 checkpoints already saved
2022-09-23 14:24:32,649 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:32,652 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:32,652 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:32,653 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,653 - trainer - INFO -    Check 0 checkpoints already saved
2022-09-23 14:24:32,653 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_2
2022-09-23 14:24:32,656 - trainer - INFO - save model to path: model/mlp_tes1\ck_2
2022-09-23 14:24:32,657 - trainer - INFO - 
*****************[epoch: 2, global step: 3] eval training set at end of epoch***************
2022-09-23 14:24:32,657 - trainer - INFO - {
  "train_loss": 3577.154052734375
}
2022-09-23 14:24:32,658 - trainer - INFO - start training epoch 3
2022-09-23 14:24:32,658 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,658 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,658 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,662 - trainer - INFO - 
*****************[epoch: 3, global step: 4] eval training set at end of epoch***************
2022-09-23 14:24:32,662 - trainer - INFO - {
  "train_loss": 2948.6474609375
}
2022-09-23 14:24:32,663 - trainer - INFO - start training epoch 4
2022-09-23 14:24:32,663 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,663 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,663 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,667 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval training set based on eval_every=2***************
2022-09-23 14:24:32,667 - trainer - INFO - {
  "train_loss": 2416.8434448242188
}
2022-09-23 14:24:32,669 - trainer - INFO - 
*****************[epoch: 4, global step: 4] eval development set based on eval_every=2***************
2022-09-23 14:24:32,670 - trainer - INFO - {
  "dev_loss": 584.0354614257812,
  "dev_best_score_for_loss": -584.0354614257812
}
2022-09-23 14:24:32,670 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:32,671 - trainer - INFO -    Check 1 checkpoints already saved
2022-09-23 14:24:32,671 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:32,673 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:32,674 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:32,674 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,674 - trainer - INFO -    Check 1 checkpoints already saved
2022-09-23 14:24:32,675 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_4
2022-09-23 14:24:32,677 - trainer - INFO - save model to path: model/mlp_tes1\ck_4
2022-09-23 14:24:32,678 - trainer - INFO - 
*****************[epoch: 4, global step: 5] eval training set at end of epoch***************
2022-09-23 14:24:32,678 - trainer - INFO - {
  "train_loss": 1885.0394287109375
}
2022-09-23 14:24:32,679 - trainer - INFO - start training epoch 5
2022-09-23 14:24:32,679 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,679 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,680 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,684 - trainer - INFO - 
*****************[epoch: 5, global step: 6] eval training set at end of epoch***************
2022-09-23 14:24:32,684 - trainer - INFO - {
  "train_loss": 584.035400390625
}
2022-09-23 14:24:32,684 - trainer - INFO - start training epoch 6
2022-09-23 14:24:32,685 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,685 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,685 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,688 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval training set based on eval_every=2***************
2022-09-23 14:24:32,688 - trainer - INFO - {
  "train_loss": 328.72204971313477
}
2022-09-23 14:24:32,691 - trainer - INFO - 
*****************[epoch: 6, global step: 6] eval development set based on eval_every=2***************
2022-09-23 14:24:32,691 - trainer - INFO - {
  "dev_loss": 1396.935546875,
  "dev_best_score_for_loss": -584.0354614257812
}
2022-09-23 14:24:32,692 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:32,692 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,693 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:32,693 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_6
2022-09-23 14:24:32,696 - trainer - INFO - save model to path: model/mlp_tes1\ck_6
2022-09-23 14:24:32,697 - trainer - INFO - 
*****************[epoch: 6, global step: 7] eval training set at end of epoch***************
2022-09-23 14:24:32,697 - trainer - INFO - {
  "train_loss": 73.40869903564453
}
2022-09-23 14:24:32,697 - trainer - INFO - start training epoch 7
2022-09-23 14:24:32,698 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,698 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,698 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,702 - trainer - INFO - 
*****************[epoch: 7, global step: 8] eval training set at end of epoch***************
2022-09-23 14:24:32,702 - trainer - INFO - {
  "train_loss": 1396.9356689453125
}
2022-09-23 14:24:32,702 - trainer - INFO - start training epoch 8
2022-09-23 14:24:32,702 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,703 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,703 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,706 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval training set based on eval_every=2***************
2022-09-23 14:24:32,706 - trainer - INFO - {
  "train_loss": 1099.5194702148438
}
2022-09-23 14:24:32,708 - trainer - INFO - 
*****************[epoch: 8, global step: 8] eval development set based on eval_every=2***************
2022-09-23 14:24:32,708 - trainer - INFO - {
  "dev_loss": 111.57137298583984,
  "dev_best_score_for_loss": -111.57137298583984
}
2022-09-23 14:24:32,709 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:32,710 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,710 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,710 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_2
2022-09-23 14:24:32,711 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:32,713 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:32,713 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:32,713 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,714 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:32,714 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_8
2022-09-23 14:24:32,717 - trainer - INFO - save model to path: model/mlp_tes1\ck_8
2022-09-23 14:24:32,718 - trainer - INFO - 
*****************[epoch: 8, global step: 9] eval training set at end of epoch***************
2022-09-23 14:24:32,718 - trainer - INFO - {
  "train_loss": 802.103271484375
}
2022-09-23 14:24:32,719 - trainer - INFO - start training epoch 9
2022-09-23 14:24:32,719 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,719 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,720 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,723 - trainer - INFO - 
*****************[epoch: 9, global step: 10] eval training set at end of epoch***************
2022-09-23 14:24:32,724 - trainer - INFO - {
  "train_loss": 111.57137298583984
}
2022-09-23 14:24:32,724 - trainer - INFO - start training epoch 10
2022-09-23 14:24:32,724 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,724 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,725 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,728 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval training set based on eval_every=2***************
2022-09-23 14:24:32,728 - trainer - INFO - {
  "train_loss": 77.6648063659668
}
2022-09-23 14:24:32,731 - trainer - INFO - 
*****************[epoch: 10, global step: 10] eval development set based on eval_every=2***************
2022-09-23 14:24:32,731 - trainer - INFO - {
  "dev_loss": 289.0508728027344,
  "dev_best_score_for_loss": -111.57137298583984
}
2022-09-23 14:24:32,732 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:32,732 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,733 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,733 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,733 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_4
2022-09-23 14:24:32,734 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_10
2022-09-23 14:24:32,737 - trainer - INFO - save model to path: model/mlp_tes1\ck_10
2022-09-23 14:24:32,737 - trainer - INFO - 
*****************[epoch: 10, global step: 11] eval training set at end of epoch***************
2022-09-23 14:24:32,737 - trainer - INFO - {
  "train_loss": 43.75823974609375
}
2022-09-23 14:24:32,738 - trainer - INFO - start training epoch 11
2022-09-23 14:24:32,738 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,738 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,739 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,742 - trainer - INFO - 
*****************[epoch: 11, global step: 12] eval training set at end of epoch***************
2022-09-23 14:24:32,742 - trainer - INFO - {
  "train_loss": 289.0509338378906
}
2022-09-23 14:24:32,743 - trainer - INFO - start training epoch 12
2022-09-23 14:24:32,743 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,743 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,744 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,747 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval training set based on eval_every=2***************
2022-09-23 14:24:32,747 - trainer - INFO - {
  "train_loss": 403.2878875732422
}
2022-09-23 14:24:32,749 - trainer - INFO - 
*****************[epoch: 12, global step: 12] eval development set based on eval_every=2***************
2022-09-23 14:24:32,749 - trainer - INFO - {
  "dev_loss": 611.4832763671875,
  "dev_best_score_for_loss": -111.57137298583984
}
2022-09-23 14:24:32,750 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:32,750 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,751 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,751 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,751 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_6
2022-09-23 14:24:32,752 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_12
2022-09-23 14:24:32,755 - trainer - INFO - save model to path: model/mlp_tes1\ck_12
2022-09-23 14:24:32,756 - trainer - INFO - 
*****************[epoch: 12, global step: 13] eval training set at end of epoch***************
2022-09-23 14:24:32,756 - trainer - INFO - {
  "train_loss": 517.5248413085938
}
2022-09-23 14:24:32,757 - trainer - INFO - start training epoch 13
2022-09-23 14:24:32,757 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,757 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,757 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,761 - trainer - INFO - 
*****************[epoch: 13, global step: 14] eval training set at end of epoch***************
2022-09-23 14:24:32,761 - trainer - INFO - {
  "train_loss": 611.4832763671875
}
2022-09-23 14:24:32,761 - trainer - INFO - start training epoch 14
2022-09-23 14:24:32,762 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,762 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,762 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,765 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval training set based on eval_every=2***************
2022-09-23 14:24:32,766 - trainer - INFO - {
  "train_loss": 587.0485534667969
}
2022-09-23 14:24:32,768 - trainer - INFO - 
*****************[epoch: 14, global step: 14] eval development set based on eval_every=2***************
2022-09-23 14:24:32,768 - trainer - INFO - {
  "dev_loss": 404.0779113769531,
  "dev_best_score_for_loss": -111.57137298583984
}
2022-09-23 14:24:32,769 - trainer - INFO -   no_improve_count: 3
2022-09-23 14:24:32,769 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,770 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,770 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,771 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_8
2022-09-23 14:24:32,772 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_14
2022-09-23 14:24:32,775 - trainer - INFO - save model to path: model/mlp_tes1\ck_14
2022-09-23 14:24:32,775 - trainer - INFO - 
*****************[epoch: 14, global step: 15] eval training set at end of epoch***************
2022-09-23 14:24:32,776 - trainer - INFO - {
  "train_loss": 562.6138305664062
}
2022-09-23 14:24:32,776 - trainer - INFO - start training epoch 15
2022-09-23 14:24:32,776 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,776 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,777 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,780 - trainer - INFO - 
*****************[epoch: 15, global step: 16] eval training set at end of epoch***************
2022-09-23 14:24:32,780 - trainer - INFO - {
  "train_loss": 404.0779113769531
}
2022-09-23 14:24:32,780 - trainer - INFO - start training epoch 16
2022-09-23 14:24:32,781 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,781 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,781 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,784 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval training set based on eval_every=2***************
2022-09-23 14:24:32,784 - trainer - INFO - {
  "train_loss": 301.03125762939453
}
2022-09-23 14:24:32,786 - trainer - INFO - 
*****************[epoch: 16, global step: 16] eval development set based on eval_every=2***************
2022-09-23 14:24:32,787 - trainer - INFO - {
  "dev_loss": 38.13106918334961,
  "dev_best_score_for_loss": -38.13106918334961
}
2022-09-23 14:24:32,787 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:32,788 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,788 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,788 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_10
2022-09-23 14:24:32,790 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:32,792 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:32,792 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:32,792 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,793 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:32,793 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_16
2022-09-23 14:24:32,796 - trainer - INFO - save model to path: model/mlp_tes1\ck_16
2022-09-23 14:24:32,796 - trainer - INFO - 
*****************[epoch: 16, global step: 17] eval training set at end of epoch***************
2022-09-23 14:24:32,797 - trainer - INFO - {
  "train_loss": 197.98460388183594
}
2022-09-23 14:24:32,797 - trainer - INFO - start training epoch 17
2022-09-23 14:24:32,797 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,797 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,798 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,801 - trainer - INFO - 
*****************[epoch: 17, global step: 18] eval training set at end of epoch***************
2022-09-23 14:24:32,802 - trainer - INFO - {
  "train_loss": 38.13106918334961
}
2022-09-23 14:24:32,802 - trainer - INFO - start training epoch 18
2022-09-23 14:24:32,802 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,803 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,803 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,806 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval training set based on eval_every=2***************
2022-09-23 14:24:32,806 - trainer - INFO - {
  "train_loss": 31.001072883605957
}
2022-09-23 14:24:32,808 - trainer - INFO - 
*****************[epoch: 18, global step: 18] eval development set based on eval_every=2***************
2022-09-23 14:24:32,809 - trainer - INFO - {
  "dev_loss": 163.74026489257812,
  "dev_best_score_for_loss": -38.13106918334961
}
2022-09-23 14:24:32,809 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:32,809 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,810 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,811 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,811 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_12
2022-09-23 14:24:32,812 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_18
2022-09-23 14:24:32,815 - trainer - INFO - save model to path: model/mlp_tes1\ck_18
2022-09-23 14:24:32,815 - trainer - INFO - 
*****************[epoch: 18, global step: 19] eval training set at end of epoch***************
2022-09-23 14:24:32,816 - trainer - INFO - {
  "train_loss": 23.871076583862305
}
2022-09-23 14:24:32,816 - trainer - INFO - start training epoch 19
2022-09-23 14:24:32,816 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,817 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,817 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,820 - trainer - INFO - 
*****************[epoch: 19, global step: 20] eval training set at end of epoch***************
2022-09-23 14:24:32,821 - trainer - INFO - {
  "train_loss": 163.7403106689453
}
2022-09-23 14:24:32,821 - trainer - INFO - start training epoch 20
2022-09-23 14:24:32,822 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,822 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,822 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,825 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval training set based on eval_every=2***************
2022-09-23 14:24:32,826 - trainer - INFO - {
  "train_loss": 229.2475357055664
}
2022-09-23 14:24:32,828 - trainer - INFO - 
*****************[epoch: 20, global step: 20] eval development set based on eval_every=2***************
2022-09-23 14:24:32,828 - trainer - INFO - {
  "dev_loss": 257.75543212890625,
  "dev_best_score_for_loss": -38.13106918334961
}
2022-09-23 14:24:32,829 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:32,829 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,830 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,830 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,831 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_14
2022-09-23 14:24:32,832 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_20
2022-09-23 14:24:32,835 - trainer - INFO - save model to path: model/mlp_tes1\ck_20
2022-09-23 14:24:32,835 - trainer - INFO - 
*****************[epoch: 20, global step: 21] eval training set at end of epoch***************
2022-09-23 14:24:32,836 - trainer - INFO - {
  "train_loss": 294.7547607421875
}
2022-09-23 14:24:32,836 - trainer - INFO - start training epoch 21
2022-09-23 14:24:32,836 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,836 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,837 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,840 - trainer - INFO - 
*****************[epoch: 21, global step: 22] eval training set at end of epoch***************
2022-09-23 14:24:32,840 - trainer - INFO - {
  "train_loss": 257.75537109375
}
2022-09-23 14:24:32,841 - trainer - INFO - start training epoch 22
2022-09-23 14:24:32,841 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,841 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,842 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,845 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval training set based on eval_every=2***************
2022-09-23 14:24:32,846 - trainer - INFO - {
  "train_loss": 186.2249412536621
}
2022-09-23 14:24:32,848 - trainer - INFO - 
*****************[epoch: 22, global step: 22] eval development set based on eval_every=2***************
2022-09-23 14:24:32,849 - trainer - INFO - {
  "dev_loss": 17.570087432861328,
  "dev_best_score_for_loss": -17.570087432861328
}
2022-09-23 14:24:32,849 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:32,850 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,850 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,851 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_16
2022-09-23 14:24:32,852 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:32,854 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:32,854 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:32,854 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,855 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:32,855 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_22
2022-09-23 14:24:32,857 - trainer - INFO - save model to path: model/mlp_tes1\ck_22
2022-09-23 14:24:32,858 - trainer - INFO - 
*****************[epoch: 22, global step: 23] eval training set at end of epoch***************
2022-09-23 14:24:32,858 - trainer - INFO - {
  "train_loss": 114.69451141357422
}
2022-09-23 14:24:32,859 - trainer - INFO - start training epoch 23
2022-09-23 14:24:32,859 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,859 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,859 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,863 - trainer - INFO - 
*****************[epoch: 23, global step: 24] eval training set at end of epoch***************
2022-09-23 14:24:32,863 - trainer - INFO - {
  "train_loss": 17.570087432861328
}
2022-09-23 14:24:32,864 - trainer - INFO - start training epoch 24
2022-09-23 14:24:32,864 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,864 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,865 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,868 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval training set based on eval_every=2***************
2022-09-23 14:24:32,869 - trainer - INFO - {
  "train_loss": 19.18544292449951
}
2022-09-23 14:24:32,871 - trainer - INFO - 
*****************[epoch: 24, global step: 24] eval development set based on eval_every=2***************
2022-09-23 14:24:32,871 - trainer - INFO - {
  "dev_loss": 81.58914184570312,
  "dev_best_score_for_loss": -17.570087432861328
}
2022-09-23 14:24:32,872 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:32,872 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,873 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,873 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,874 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_18
2022-09-23 14:24:32,875 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_24
2022-09-23 14:24:32,878 - trainer - INFO - save model to path: model/mlp_tes1\ck_24
2022-09-23 14:24:32,878 - trainer - INFO - 
*****************[epoch: 24, global step: 25] eval training set at end of epoch***************
2022-09-23 14:24:32,879 - trainer - INFO - {
  "train_loss": 20.800798416137695
}
2022-09-23 14:24:32,879 - trainer - INFO - start training epoch 25
2022-09-23 14:24:32,879 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,879 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,880 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,884 - trainer - INFO - 
*****************[epoch: 25, global step: 26] eval training set at end of epoch***************
2022-09-23 14:24:32,884 - trainer - INFO - {
  "train_loss": 81.58914184570312
}
2022-09-23 14:24:32,884 - trainer - INFO - start training epoch 26
2022-09-23 14:24:32,885 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,885 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,885 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,888 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval training set based on eval_every=2***************
2022-09-23 14:24:32,888 - trainer - INFO - {
  "train_loss": 109.96315002441406
}
2022-09-23 14:24:32,891 - trainer - INFO - 
*****************[epoch: 26, global step: 26] eval development set based on eval_every=2***************
2022-09-23 14:24:32,891 - trainer - INFO - {
  "dev_loss": 154.72840881347656,
  "dev_best_score_for_loss": -17.570087432861328
}
2022-09-23 14:24:32,891 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:32,892 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,893 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,893 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,893 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_20
2022-09-23 14:24:32,894 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_26
2022-09-23 14:24:32,897 - trainer - INFO - save model to path: model/mlp_tes1\ck_26
2022-09-23 14:24:32,898 - trainer - INFO - 
*****************[epoch: 26, global step: 27] eval training set at end of epoch***************
2022-09-23 14:24:32,898 - trainer - INFO - {
  "train_loss": 138.337158203125
}
2022-09-23 14:24:32,898 - trainer - INFO - start training epoch 27
2022-09-23 14:24:32,899 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,899 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,899 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,902 - trainer - INFO - 
*****************[epoch: 27, global step: 28] eval training set at end of epoch***************
2022-09-23 14:24:32,903 - trainer - INFO - {
  "train_loss": 154.7284393310547
}
2022-09-23 14:24:32,903 - trainer - INFO - start training epoch 28
2022-09-23 14:24:32,903 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,904 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,904 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,907 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval training set based on eval_every=2***************
2022-09-23 14:24:32,907 - trainer - INFO - {
  "train_loss": 140.12170028686523
}
2022-09-23 14:24:32,909 - trainer - INFO - 
*****************[epoch: 28, global step: 28] eval development set based on eval_every=2***************
2022-09-23 14:24:32,909 - trainer - INFO - {
  "dev_loss": 69.70352172851562,
  "dev_best_score_for_loss": -17.570087432861328
}
2022-09-23 14:24:32,910 - trainer - INFO -   no_improve_count: 3
2022-09-23 14:24:32,910 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,911 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,911 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,912 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_22
2022-09-23 14:24:32,913 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_28
2022-09-23 14:24:32,915 - trainer - INFO - save model to path: model/mlp_tes1\ck_28
2022-09-23 14:24:32,916 - trainer - INFO - 
*****************[epoch: 28, global step: 29] eval training set at end of epoch***************
2022-09-23 14:24:32,916 - trainer - INFO - {
  "train_loss": 125.51496124267578
}
2022-09-23 14:24:32,916 - trainer - INFO - start training epoch 29
2022-09-23 14:24:32,917 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,917 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,917 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,920 - trainer - INFO - 
*****************[epoch: 29, global step: 30] eval training set at end of epoch***************
2022-09-23 14:24:32,921 - trainer - INFO - {
  "train_loss": 69.7035140991211
}
2022-09-23 14:24:32,921 - trainer - INFO - start training epoch 30
2022-09-23 14:24:32,921 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,921 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,922 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,925 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval training set based on eval_every=2***************
2022-09-23 14:24:32,925 - trainer - INFO - {
  "train_loss": 45.18945503234863
}
2022-09-23 14:24:32,927 - trainer - INFO - 
*****************[epoch: 30, global step: 30] eval development set based on eval_every=2***************
2022-09-23 14:24:32,928 - trainer - INFO - {
  "dev_loss": 9.326598167419434,
  "dev_best_score_for_loss": -9.326598167419434
}
2022-09-23 14:24:32,928 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:32,929 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,929 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,930 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_24
2022-09-23 14:24:32,931 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:32,933 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:32,933 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:32,933 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,934 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:32,934 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_30
2022-09-23 14:24:32,937 - trainer - INFO - save model to path: model/mlp_tes1\ck_30
2022-09-23 14:24:32,938 - trainer - INFO - 
*****************[epoch: 30, global step: 31] eval training set at end of epoch***************
2022-09-23 14:24:32,938 - trainer - INFO - {
  "train_loss": 20.675395965576172
}
2022-09-23 14:24:32,939 - trainer - INFO - start training epoch 31
2022-09-23 14:24:32,939 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,939 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,939 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,943 - trainer - INFO - 
*****************[epoch: 31, global step: 32] eval training set at end of epoch***************
2022-09-23 14:24:32,943 - trainer - INFO - {
  "train_loss": 9.326598167419434
}
2022-09-23 14:24:32,943 - trainer - INFO - start training epoch 32
2022-09-23 14:24:32,944 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,944 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,944 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,947 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval training set based on eval_every=2***************
2022-09-23 14:24:32,947 - trainer - INFO - {
  "train_loss": 24.39413595199585
}
2022-09-23 14:24:32,950 - trainer - INFO - 
*****************[epoch: 32, global step: 32] eval development set based on eval_every=2***************
2022-09-23 14:24:32,950 - trainer - INFO - {
  "dev_loss": 76.73255920410156,
  "dev_best_score_for_loss": -9.326598167419434
}
2022-09-23 14:24:32,950 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:32,951 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,952 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,952 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,952 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_26
2022-09-23 14:24:32,953 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_32
2022-09-23 14:24:32,956 - trainer - INFO - save model to path: model/mlp_tes1\ck_32
2022-09-23 14:24:32,956 - trainer - INFO - 
*****************[epoch: 32, global step: 33] eval training set at end of epoch***************
2022-09-23 14:24:32,956 - trainer - INFO - {
  "train_loss": 39.461673736572266
}
2022-09-23 14:24:32,957 - trainer - INFO - start training epoch 33
2022-09-23 14:24:32,957 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,957 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,957 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,961 - trainer - INFO - 
*****************[epoch: 33, global step: 34] eval training set at end of epoch***************
2022-09-23 14:24:32,961 - trainer - INFO - {
  "train_loss": 76.73258209228516
}
2022-09-23 14:24:32,961 - trainer - INFO - start training epoch 34
2022-09-23 14:24:32,961 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,962 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,962 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,966 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval training set based on eval_every=2***************
2022-09-23 14:24:32,966 - trainer - INFO - {
  "train_loss": 78.4756088256836
}
2022-09-23 14:24:32,969 - trainer - INFO - 
*****************[epoch: 34, global step: 34] eval development set based on eval_every=2***************
2022-09-23 14:24:32,969 - trainer - INFO - {
  "dev_loss": 48.669193267822266,
  "dev_best_score_for_loss": -9.326598167419434
}
2022-09-23 14:24:32,970 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:32,970 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,971 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,972 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,972 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_28
2022-09-23 14:24:32,973 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_34
2022-09-23 14:24:32,976 - trainer - INFO - save model to path: model/mlp_tes1\ck_34
2022-09-23 14:24:32,977 - trainer - INFO - 
*****************[epoch: 34, global step: 35] eval training set at end of epoch***************
2022-09-23 14:24:32,977 - trainer - INFO - {
  "train_loss": 80.21863555908203
}
2022-09-23 14:24:32,978 - trainer - INFO - start training epoch 35
2022-09-23 14:24:32,978 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,978 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,979 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,983 - trainer - INFO - 
*****************[epoch: 35, global step: 36] eval training set at end of epoch***************
2022-09-23 14:24:32,983 - trainer - INFO - {
  "train_loss": 48.66923141479492
}
2022-09-23 14:24:32,984 - trainer - INFO - start training epoch 36
2022-09-23 14:24:32,984 - trainer - INFO - training using device=cpu
2022-09-23 14:24:32,984 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:32,985 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:32,989 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval training set based on eval_every=2***************
2022-09-23 14:24:32,989 - trainer - INFO - {
  "train_loss": 32.34751224517822
}
2022-09-23 14:24:32,992 - trainer - INFO - 
*****************[epoch: 36, global step: 36] eval development set based on eval_every=2***************
2022-09-23 14:24:32,992 - trainer - INFO - {
  "dev_loss": 8.392193794250488,
  "dev_best_score_for_loss": -8.392193794250488
}
2022-09-23 14:24:32,993 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:32,994 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:32,994 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:32,995 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_30
2022-09-23 14:24:32,996 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:32,998 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:32,998 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:32,999 - trainer - INFO -   patience: 200
2022-09-23 14:24:32,999 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,000 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_36
2022-09-23 14:24:33,002 - trainer - INFO - save model to path: model/mlp_tes1\ck_36
2022-09-23 14:24:33,003 - trainer - INFO - 
*****************[epoch: 36, global step: 37] eval training set at end of epoch***************
2022-09-23 14:24:33,003 - trainer - INFO - {
  "train_loss": 16.025793075561523
}
2022-09-23 14:24:33,003 - trainer - INFO - start training epoch 37
2022-09-23 14:24:33,004 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,004 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,004 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,009 - trainer - INFO - 
*****************[epoch: 37, global step: 38] eval training set at end of epoch***************
2022-09-23 14:24:33,009 - trainer - INFO - {
  "train_loss": 8.39218807220459
}
2022-09-23 14:24:33,010 - trainer - INFO - start training epoch 38
2022-09-23 14:24:33,010 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,010 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,011 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,014 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval training set based on eval_every=2***************
2022-09-23 14:24:33,015 - trainer - INFO - {
  "train_loss": 15.800184726715088
}
2022-09-23 14:24:33,017 - trainer - INFO - 
*****************[epoch: 38, global step: 38] eval development set based on eval_every=2***************
2022-09-23 14:24:33,018 - trainer - INFO - {
  "dev_loss": 41.640438079833984,
  "dev_best_score_for_loss": -8.392193794250488
}
2022-09-23 14:24:33,018 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,019 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,019 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,020 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,020 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_32
2022-09-23 14:24:33,021 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_38
2022-09-23 14:24:33,024 - trainer - INFO - save model to path: model/mlp_tes1\ck_38
2022-09-23 14:24:33,025 - trainer - INFO - 
*****************[epoch: 38, global step: 39] eval training set at end of epoch***************
2022-09-23 14:24:33,025 - trainer - INFO - {
  "train_loss": 23.208181381225586
}
2022-09-23 14:24:33,026 - trainer - INFO - start training epoch 39
2022-09-23 14:24:33,026 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,026 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,027 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,030 - trainer - INFO - 
*****************[epoch: 39, global step: 40] eval training set at end of epoch***************
2022-09-23 14:24:33,031 - trainer - INFO - {
  "train_loss": 41.640445709228516
}
2022-09-23 14:24:33,031 - trainer - INFO - start training epoch 40
2022-09-23 14:24:33,031 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,031 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,032 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,035 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval training set based on eval_every=2***************
2022-09-23 14:24:33,035 - trainer - INFO - {
  "train_loss": 44.674049377441406
}
2022-09-23 14:24:33,038 - trainer - INFO - 
*****************[epoch: 40, global step: 40] eval development set based on eval_every=2***************
2022-09-23 14:24:33,038 - trainer - INFO - {
  "dev_loss": 37.78586959838867,
  "dev_best_score_for_loss": -8.392193794250488
}
2022-09-23 14:24:33,039 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:33,039 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,040 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,040 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,041 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_34
2022-09-23 14:24:33,042 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_40
2022-09-23 14:24:33,044 - trainer - INFO - save model to path: model/mlp_tes1\ck_40
2022-09-23 14:24:33,045 - trainer - INFO - 
*****************[epoch: 40, global step: 41] eval training set at end of epoch***************
2022-09-23 14:24:33,045 - trainer - INFO - {
  "train_loss": 47.7076530456543
}
2022-09-23 14:24:33,046 - trainer - INFO - start training epoch 41
2022-09-23 14:24:33,046 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,047 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,047 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,050 - trainer - INFO - 
*****************[epoch: 41, global step: 42] eval training set at end of epoch***************
2022-09-23 14:24:33,051 - trainer - INFO - {
  "train_loss": 37.7858772277832
}
2022-09-23 14:24:33,051 - trainer - INFO - start training epoch 42
2022-09-23 14:24:33,051 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,051 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,052 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,055 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval training set based on eval_every=2***************
2022-09-23 14:24:33,055 - trainer - INFO - {
  "train_loss": 29.002824783325195
}
2022-09-23 14:24:33,057 - trainer - INFO - 
*****************[epoch: 42, global step: 42] eval development set based on eval_every=2***************
2022-09-23 14:24:33,057 - trainer - INFO - {
  "dev_loss": 8.436923027038574,
  "dev_best_score_for_loss": -8.392193794250488
}
2022-09-23 14:24:33,058 - trainer - INFO -   no_improve_count: 3
2022-09-23 14:24:33,058 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,059 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,059 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,059 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_36
2022-09-23 14:24:33,060 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_42
2022-09-23 14:24:33,063 - trainer - INFO - save model to path: model/mlp_tes1\ck_42
2022-09-23 14:24:33,064 - trainer - INFO - 
*****************[epoch: 42, global step: 43] eval training set at end of epoch***************
2022-09-23 14:24:33,064 - trainer - INFO - {
  "train_loss": 20.219772338867188
}
2022-09-23 14:24:33,064 - trainer - INFO - start training epoch 43
2022-09-23 14:24:33,065 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,065 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,065 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,069 - trainer - INFO - 
*****************[epoch: 43, global step: 44] eval training set at end of epoch***************
2022-09-23 14:24:33,069 - trainer - INFO - {
  "train_loss": 8.436924934387207
}
2022-09-23 14:24:33,069 - trainer - INFO - start training epoch 44
2022-09-23 14:24:33,070 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,070 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,070 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,073 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval training set based on eval_every=2***************
2022-09-23 14:24:33,074 - trainer - INFO - {
  "train_loss": 9.507709503173828
}
2022-09-23 14:24:33,076 - trainer - INFO - 
*****************[epoch: 44, global step: 44] eval development set based on eval_every=2***************
2022-09-23 14:24:33,076 - trainer - INFO - {
  "dev_loss": 21.951126098632812,
  "dev_best_score_for_loss": -8.392193794250488
}
2022-09-23 14:24:33,077 - trainer - INFO -   no_improve_count: 4
2022-09-23 14:24:33,077 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,078 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,078 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,078 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_38
2022-09-23 14:24:33,079 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_44
2022-09-23 14:24:33,082 - trainer - INFO - save model to path: model/mlp_tes1\ck_44
2022-09-23 14:24:33,082 - trainer - INFO - 
*****************[epoch: 44, global step: 45] eval training set at end of epoch***************
2022-09-23 14:24:33,083 - trainer - INFO - {
  "train_loss": 10.57849407196045
}
2022-09-23 14:24:33,083 - trainer - INFO - start training epoch 45
2022-09-23 14:24:33,083 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,083 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,084 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,087 - trainer - INFO - 
*****************[epoch: 45, global step: 46] eval training set at end of epoch***************
2022-09-23 14:24:33,087 - trainer - INFO - {
  "train_loss": 21.951128005981445
}
2022-09-23 14:24:33,088 - trainer - INFO - start training epoch 46
2022-09-23 14:24:33,088 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,088 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,089 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,092 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval training set based on eval_every=2***************
2022-09-23 14:24:33,092 - trainer - INFO - {
  "train_loss": 25.56142807006836
}
2022-09-23 14:24:33,095 - trainer - INFO - 
*****************[epoch: 46, global step: 46] eval development set based on eval_every=2***************
2022-09-23 14:24:33,095 - trainer - INFO - {
  "dev_loss": 24.561689376831055,
  "dev_best_score_for_loss": -8.392193794250488
}
2022-09-23 14:24:33,096 - trainer - INFO -   no_improve_count: 5
2022-09-23 14:24:33,096 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,097 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,097 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,098 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_40
2022-09-23 14:24:33,099 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_46
2022-09-23 14:24:33,103 - trainer - INFO - save model to path: model/mlp_tes1\ck_46
2022-09-23 14:24:33,104 - trainer - INFO - 
*****************[epoch: 46, global step: 47] eval training set at end of epoch***************
2022-09-23 14:24:33,104 - trainer - INFO - {
  "train_loss": 29.171728134155273
}
2022-09-23 14:24:33,104 - trainer - INFO - start training epoch 47
2022-09-23 14:24:33,105 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,105 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,105 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,110 - trainer - INFO - 
*****************[epoch: 47, global step: 48] eval training set at end of epoch***************
2022-09-23 14:24:33,110 - trainer - INFO - {
  "train_loss": 24.561689376831055
}
2022-09-23 14:24:33,111 - trainer - INFO - start training epoch 48
2022-09-23 14:24:33,111 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,111 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,112 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,116 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval training set based on eval_every=2***************
2022-09-23 14:24:33,116 - trainer - INFO - {
  "train_loss": 19.18851327896118
}
2022-09-23 14:24:33,119 - trainer - INFO - 
*****************[epoch: 48, global step: 48] eval development set based on eval_every=2***************
2022-09-23 14:24:33,119 - trainer - INFO - {
  "dev_loss": 7.617179870605469,
  "dev_best_score_for_loss": -7.617179870605469
}
2022-09-23 14:24:33,120 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,121 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,122 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,122 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_42
2022-09-23 14:24:33,123 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,126 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,126 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,126 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,127 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,127 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_48
2022-09-23 14:24:33,131 - trainer - INFO - save model to path: model/mlp_tes1\ck_48
2022-09-23 14:24:33,132 - trainer - INFO - 
*****************[epoch: 48, global step: 49] eval training set at end of epoch***************
2022-09-23 14:24:33,132 - trainer - INFO - {
  "train_loss": 13.815337181091309
}
2022-09-23 14:24:33,132 - trainer - INFO - start training epoch 49
2022-09-23 14:24:33,133 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,133 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,133 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,138 - trainer - INFO - 
*****************[epoch: 49, global step: 50] eval training set at end of epoch***************
2022-09-23 14:24:33,138 - trainer - INFO - {
  "train_loss": 7.617191314697266
}
2022-09-23 14:24:33,138 - trainer - INFO - start training epoch 50
2022-09-23 14:24:33,139 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,139 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,139 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,143 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval training set based on eval_every=2***************
2022-09-23 14:24:33,144 - trainer - INFO - {
  "train_loss": 8.752296447753906
}
2022-09-23 14:24:33,147 - trainer - INFO - 
*****************[epoch: 50, global step: 50] eval development set based on eval_every=2***************
2022-09-23 14:24:33,147 - trainer - INFO - {
  "dev_loss": 16.030771255493164,
  "dev_best_score_for_loss": -7.617179870605469
}
2022-09-23 14:24:33,148 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,148 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,149 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,150 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,150 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_44
2022-09-23 14:24:33,151 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_50
2022-09-23 14:24:33,155 - trainer - INFO - save model to path: model/mlp_tes1\ck_50
2022-09-23 14:24:33,156 - trainer - INFO - 
*****************[epoch: 50, global step: 51] eval training set at end of epoch***************
2022-09-23 14:24:33,156 - trainer - INFO - {
  "train_loss": 9.887401580810547
}
2022-09-23 14:24:33,157 - trainer - INFO - start training epoch 51
2022-09-23 14:24:33,157 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,157 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,157 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,161 - trainer - INFO - 
*****************[epoch: 51, global step: 52] eval training set at end of epoch***************
2022-09-23 14:24:33,161 - trainer - INFO - {
  "train_loss": 16.030763626098633
}
2022-09-23 14:24:33,162 - trainer - INFO - start training epoch 52
2022-09-23 14:24:33,162 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,162 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,163 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,166 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval training set based on eval_every=2***************
2022-09-23 14:24:33,167 - trainer - INFO - {
  "train_loss": 17.652862548828125
}
2022-09-23 14:24:33,169 - trainer - INFO - 
*****************[epoch: 52, global step: 52] eval development set based on eval_every=2***************
2022-09-23 14:24:33,169 - trainer - INFO - {
  "dev_loss": 16.788503646850586,
  "dev_best_score_for_loss": -7.617179870605469
}
2022-09-23 14:24:33,170 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:33,170 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,171 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,172 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,172 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_46
2022-09-23 14:24:33,173 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_52
2022-09-23 14:24:33,175 - trainer - INFO - save model to path: model/mlp_tes1\ck_52
2022-09-23 14:24:33,176 - trainer - INFO - 
*****************[epoch: 52, global step: 53] eval training set at end of epoch***************
2022-09-23 14:24:33,176 - trainer - INFO - {
  "train_loss": 19.274961471557617
}
2022-09-23 14:24:33,177 - trainer - INFO - start training epoch 53
2022-09-23 14:24:33,177 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,177 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,177 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,181 - trainer - INFO - 
*****************[epoch: 53, global step: 54] eval training set at end of epoch***************
2022-09-23 14:24:33,182 - trainer - INFO - {
  "train_loss": 16.788497924804688
}
2022-09-23 14:24:33,182 - trainer - INFO - start training epoch 54
2022-09-23 14:24:33,182 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,182 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,183 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,186 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval training set based on eval_every=2***************
2022-09-23 14:24:33,186 - trainer - INFO - {
  "train_loss": 13.981843948364258
}
2022-09-23 14:24:33,189 - trainer - INFO - 
*****************[epoch: 54, global step: 54] eval development set based on eval_every=2***************
2022-09-23 14:24:33,189 - trainer - INFO - {
  "dev_loss": 7.514245986938477,
  "dev_best_score_for_loss": -7.514245986938477
}
2022-09-23 14:24:33,190 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,191 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,191 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,191 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_48
2022-09-23 14:24:33,192 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,194 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,195 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,195 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,195 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,196 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_54
2022-09-23 14:24:33,199 - trainer - INFO - save model to path: model/mlp_tes1\ck_54
2022-09-23 14:24:33,199 - trainer - INFO - 
*****************[epoch: 54, global step: 55] eval training set at end of epoch***************
2022-09-23 14:24:33,200 - trainer - INFO - {
  "train_loss": 11.175189971923828
}
2022-09-23 14:24:33,200 - trainer - INFO - start training epoch 55
2022-09-23 14:24:33,200 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,200 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,201 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,204 - trainer - INFO - 
*****************[epoch: 55, global step: 56] eval training set at end of epoch***************
2022-09-23 14:24:33,204 - trainer - INFO - {
  "train_loss": 7.514245510101318
}
2022-09-23 14:24:33,205 - trainer - INFO - start training epoch 56
2022-09-23 14:24:33,205 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,205 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,206 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,209 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval training set based on eval_every=2***************
2022-09-23 14:24:33,209 - trainer - INFO - {
  "train_loss": 8.041902780532837
}
2022-09-23 14:24:33,211 - trainer - INFO - 
*****************[epoch: 56, global step: 56] eval development set based on eval_every=2***************
2022-09-23 14:24:33,212 - trainer - INFO - {
  "dev_loss": 12.208231925964355,
  "dev_best_score_for_loss": -7.514245986938477
}
2022-09-23 14:24:33,212 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,213 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,213 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,214 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,214 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_50
2022-09-23 14:24:33,215 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_56
2022-09-23 14:24:33,218 - trainer - INFO - save model to path: model/mlp_tes1\ck_56
2022-09-23 14:24:33,218 - trainer - INFO - 
*****************[epoch: 56, global step: 57] eval training set at end of epoch***************
2022-09-23 14:24:33,219 - trainer - INFO - {
  "train_loss": 8.569560050964355
}
2022-09-23 14:24:33,219 - trainer - INFO - start training epoch 57
2022-09-23 14:24:33,219 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,219 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,220 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,223 - trainer - INFO - 
*****************[epoch: 57, global step: 58] eval training set at end of epoch***************
2022-09-23 14:24:33,224 - trainer - INFO - {
  "train_loss": 12.208230018615723
}
2022-09-23 14:24:33,224 - trainer - INFO - start training epoch 58
2022-09-23 14:24:33,224 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,224 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,225 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,228 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval training set based on eval_every=2***************
2022-09-23 14:24:33,228 - trainer - INFO - {
  "train_loss": 13.051568984985352
}
2022-09-23 14:24:33,230 - trainer - INFO - 
*****************[epoch: 58, global step: 58] eval development set based on eval_every=2***************
2022-09-23 14:24:33,230 - trainer - INFO - {
  "dev_loss": 11.81405258178711,
  "dev_best_score_for_loss": -7.514245986938477
}
2022-09-23 14:24:33,231 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:33,231 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,232 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,232 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,232 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_52
2022-09-23 14:24:33,233 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_58
2022-09-23 14:24:33,236 - trainer - INFO - save model to path: model/mlp_tes1\ck_58
2022-09-23 14:24:33,236 - trainer - INFO - 
*****************[epoch: 58, global step: 59] eval training set at end of epoch***************
2022-09-23 14:24:33,237 - trainer - INFO - {
  "train_loss": 13.89490795135498
}
2022-09-23 14:24:33,237 - trainer - INFO - start training epoch 59
2022-09-23 14:24:33,237 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,238 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,238 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,241 - trainer - INFO - 
*****************[epoch: 59, global step: 60] eval training set at end of epoch***************
2022-09-23 14:24:33,242 - trainer - INFO - {
  "train_loss": 11.814051628112793
}
2022-09-23 14:24:33,242 - trainer - INFO - start training epoch 60
2022-09-23 14:24:33,242 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,242 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,243 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,246 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval training set based on eval_every=2***************
2022-09-23 14:24:33,247 - trainer - INFO - {
  "train_loss": 10.14606761932373
}
2022-09-23 14:24:33,249 - trainer - INFO - 
*****************[epoch: 60, global step: 60] eval development set based on eval_every=2***************
2022-09-23 14:24:33,249 - trainer - INFO - {
  "dev_loss": 7.240874767303467,
  "dev_best_score_for_loss": -7.240874767303467
}
2022-09-23 14:24:33,250 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,251 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,251 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,251 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_54
2022-09-23 14:24:33,252 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,254 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,254 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,255 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,255 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,256 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_60
2022-09-23 14:24:33,258 - trainer - INFO - save model to path: model/mlp_tes1\ck_60
2022-09-23 14:24:33,258 - trainer - INFO - 
*****************[epoch: 60, global step: 61] eval training set at end of epoch***************
2022-09-23 14:24:33,259 - trainer - INFO - {
  "train_loss": 8.478083610534668
}
2022-09-23 14:24:33,259 - trainer - INFO - start training epoch 61
2022-09-23 14:24:33,259 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,260 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,260 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,263 - trainer - INFO - 
*****************[epoch: 61, global step: 62] eval training set at end of epoch***************
2022-09-23 14:24:33,264 - trainer - INFO - {
  "train_loss": 7.240873336791992
}
2022-09-23 14:24:33,264 - trainer - INFO - start training epoch 62
2022-09-23 14:24:33,264 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,264 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,265 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,268 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval training set based on eval_every=2***************
2022-09-23 14:24:33,268 - trainer - INFO - {
  "train_loss": 7.9399638175964355
}
2022-09-23 14:24:33,270 - trainer - INFO - 
*****************[epoch: 62, global step: 62] eval development set based on eval_every=2***************
2022-09-23 14:24:33,270 - trainer - INFO - {
  "dev_loss": 10.514837265014648,
  "dev_best_score_for_loss": -7.240874767303467
}
2022-09-23 14:24:33,271 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,271 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,272 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,272 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,272 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_56
2022-09-23 14:24:33,273 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_62
2022-09-23 14:24:33,276 - trainer - INFO - save model to path: model/mlp_tes1\ck_62
2022-09-23 14:24:33,277 - trainer - INFO - 
*****************[epoch: 62, global step: 63] eval training set at end of epoch***************
2022-09-23 14:24:33,277 - trainer - INFO - {
  "train_loss": 8.639054298400879
}
2022-09-23 14:24:33,277 - trainer - INFO - start training epoch 63
2022-09-23 14:24:33,278 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,278 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,278 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,282 - trainer - INFO - 
*****************[epoch: 63, global step: 64] eval training set at end of epoch***************
2022-09-23 14:24:33,282 - trainer - INFO - {
  "train_loss": 10.514838218688965
}
2022-09-23 14:24:33,282 - trainer - INFO - start training epoch 64
2022-09-23 14:24:33,282 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,283 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,283 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,286 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval training set based on eval_every=2***************
2022-09-23 14:24:33,287 - trainer - INFO - {
  "train_loss": 10.614247798919678
}
2022-09-23 14:24:33,289 - trainer - INFO - 
*****************[epoch: 64, global step: 64] eval development set based on eval_every=2***************
2022-09-23 14:24:33,289 - trainer - INFO - {
  "dev_loss": 9.128746032714844,
  "dev_best_score_for_loss": -7.240874767303467
}
2022-09-23 14:24:33,290 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:33,290 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,291 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,291 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,291 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_58
2022-09-23 14:24:33,292 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_64
2022-09-23 14:24:33,295 - trainer - INFO - save model to path: model/mlp_tes1\ck_64
2022-09-23 14:24:33,295 - trainer - INFO - 
*****************[epoch: 64, global step: 65] eval training set at end of epoch***************
2022-09-23 14:24:33,296 - trainer - INFO - {
  "train_loss": 10.71365737915039
}
2022-09-23 14:24:33,296 - trainer - INFO - start training epoch 65
2022-09-23 14:24:33,296 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,296 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,297 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,301 - trainer - INFO - 
*****************[epoch: 65, global step: 66] eval training set at end of epoch***************
2022-09-23 14:24:33,301 - trainer - INFO - {
  "train_loss": 9.128746032714844
}
2022-09-23 14:24:33,302 - trainer - INFO - start training epoch 66
2022-09-23 14:24:33,302 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,302 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,303 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,306 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval training set based on eval_every=2***************
2022-09-23 14:24:33,307 - trainer - INFO - {
  "train_loss": 8.289159059524536
}
2022-09-23 14:24:33,309 - trainer - INFO - 
*****************[epoch: 66, global step: 66] eval development set based on eval_every=2***************
2022-09-23 14:24:33,310 - trainer - INFO - {
  "dev_loss": 7.239826202392578,
  "dev_best_score_for_loss": -7.239826202392578
}
2022-09-23 14:24:33,310 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,312 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,312 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,312 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_60
2022-09-23 14:24:33,314 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,316 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,316 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,316 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,317 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,317 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_66
2022-09-23 14:24:33,320 - trainer - INFO - save model to path: model/mlp_tes1\ck_66
2022-09-23 14:24:33,321 - trainer - INFO - 
*****************[epoch: 66, global step: 67] eval training set at end of epoch***************
2022-09-23 14:24:33,321 - trainer - INFO - {
  "train_loss": 7.4495720863342285
}
2022-09-23 14:24:33,322 - trainer - INFO - start training epoch 67
2022-09-23 14:24:33,322 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,322 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,323 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,327 - trainer - INFO - 
*****************[epoch: 67, global step: 68] eval training set at end of epoch***************
2022-09-23 14:24:33,328 - trainer - INFO - {
  "train_loss": 7.239826679229736
}
2022-09-23 14:24:33,328 - trainer - INFO - start training epoch 68
2022-09-23 14:24:33,329 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,329 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,329 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,333 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval training set based on eval_every=2***************
2022-09-23 14:24:33,333 - trainer - INFO - {
  "train_loss": 7.782858610153198
}
2022-09-23 14:24:33,335 - trainer - INFO - 
*****************[epoch: 68, global step: 68] eval development set based on eval_every=2***************
2022-09-23 14:24:33,336 - trainer - INFO - {
  "dev_loss": 9.164008140563965,
  "dev_best_score_for_loss": -7.239826202392578
}
2022-09-23 14:24:33,336 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,336 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,337 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,337 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,338 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_62
2022-09-23 14:24:33,339 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_68
2022-09-23 14:24:33,341 - trainer - INFO - save model to path: model/mlp_tes1\ck_68
2022-09-23 14:24:33,342 - trainer - INFO - 
*****************[epoch: 68, global step: 69] eval training set at end of epoch***************
2022-09-23 14:24:33,342 - trainer - INFO - {
  "train_loss": 8.32589054107666
}
2022-09-23 14:24:33,343 - trainer - INFO - start training epoch 69
2022-09-23 14:24:33,343 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,343 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,344 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,348 - trainer - INFO - 
*****************[epoch: 69, global step: 70] eval training set at end of epoch***************
2022-09-23 14:24:33,348 - trainer - INFO - {
  "train_loss": 9.164009094238281
}
2022-09-23 14:24:33,349 - trainer - INFO - start training epoch 70
2022-09-23 14:24:33,349 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,349 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,350 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,353 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval training set based on eval_every=2***************
2022-09-23 14:24:33,353 - trainer - INFO - {
  "train_loss": 8.944623470306396
}
2022-09-23 14:24:33,355 - trainer - INFO - 
*****************[epoch: 70, global step: 70] eval development set based on eval_every=2***************
2022-09-23 14:24:33,356 - trainer - INFO - {
  "dev_loss": 7.58579683303833,
  "dev_best_score_for_loss": -7.239826202392578
}
2022-09-23 14:24:33,356 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:33,356 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,357 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,357 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,358 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_64
2022-09-23 14:24:33,359 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_70
2022-09-23 14:24:33,362 - trainer - INFO - save model to path: model/mlp_tes1\ck_70
2022-09-23 14:24:33,363 - trainer - INFO - 
*****************[epoch: 70, global step: 71] eval training set at end of epoch***************
2022-09-23 14:24:33,363 - trainer - INFO - {
  "train_loss": 8.725237846374512
}
2022-09-23 14:24:33,363 - trainer - INFO - start training epoch 71
2022-09-23 14:24:33,364 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,364 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,364 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,368 - trainer - INFO - 
*****************[epoch: 71, global step: 72] eval training set at end of epoch***************
2022-09-23 14:24:33,368 - trainer - INFO - {
  "train_loss": 7.585796356201172
}
2022-09-23 14:24:33,369 - trainer - INFO - start training epoch 72
2022-09-23 14:24:33,369 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,369 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,369 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,372 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval training set based on eval_every=2***************
2022-09-23 14:24:33,373 - trainer - INFO - {
  "train_loss": 7.29389762878418
}
2022-09-23 14:24:33,375 - trainer - INFO - 
*****************[epoch: 72, global step: 72] eval development set based on eval_every=2***************
2022-09-23 14:24:33,375 - trainer - INFO - {
  "dev_loss": 7.3855414390563965,
  "dev_best_score_for_loss": -7.239826202392578
}
2022-09-23 14:24:33,376 - trainer - INFO -   no_improve_count: 3
2022-09-23 14:24:33,376 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,377 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,377 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,377 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_66
2022-09-23 14:24:33,378 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_72
2022-09-23 14:24:33,381 - trainer - INFO - save model to path: model/mlp_tes1\ck_72
2022-09-23 14:24:33,382 - trainer - INFO - 
*****************[epoch: 72, global step: 73] eval training set at end of epoch***************
2022-09-23 14:24:33,382 - trainer - INFO - {
  "train_loss": 7.0019989013671875
}
2022-09-23 14:24:33,383 - trainer - INFO - start training epoch 73
2022-09-23 14:24:33,383 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,383 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,383 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,387 - trainer - INFO - 
*****************[epoch: 73, global step: 74] eval training set at end of epoch***************
2022-09-23 14:24:33,387 - trainer - INFO - {
  "train_loss": 7.385534763336182
}
2022-09-23 14:24:33,387 - trainer - INFO - start training epoch 74
2022-09-23 14:24:33,388 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,388 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,388 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,391 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval training set based on eval_every=2***************
2022-09-23 14:24:33,392 - trainer - INFO - {
  "train_loss": 7.708585500717163
}
2022-09-23 14:24:33,393 - trainer - INFO - 
*****************[epoch: 74, global step: 74] eval development set based on eval_every=2***************
2022-09-23 14:24:33,394 - trainer - INFO - {
  "dev_loss": 8.099513053894043,
  "dev_best_score_for_loss": -7.239826202392578
}
2022-09-23 14:24:33,394 - trainer - INFO -   no_improve_count: 4
2022-09-23 14:24:33,394 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,395 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,395 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,396 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_68
2022-09-23 14:24:33,397 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_74
2022-09-23 14:24:33,399 - trainer - INFO - save model to path: model/mlp_tes1\ck_74
2022-09-23 14:24:33,400 - trainer - INFO - 
*****************[epoch: 74, global step: 75] eval training set at end of epoch***************
2022-09-23 14:24:33,400 - trainer - INFO - {
  "train_loss": 8.031636238098145
}
2022-09-23 14:24:33,401 - trainer - INFO - start training epoch 75
2022-09-23 14:24:33,401 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,401 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,402 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,405 - trainer - INFO - 
*****************[epoch: 75, global step: 76] eval training set at end of epoch***************
2022-09-23 14:24:33,405 - trainer - INFO - {
  "train_loss": 8.099506378173828
}
2022-09-23 14:24:33,406 - trainer - INFO - start training epoch 76
2022-09-23 14:24:33,406 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,406 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,406 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,409 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval training set based on eval_every=2***************
2022-09-23 14:24:33,409 - trainer - INFO - {
  "train_loss": 7.813794374465942
}
2022-09-23 14:24:33,411 - trainer - INFO - 
*****************[epoch: 76, global step: 76] eval development set based on eval_every=2***************
2022-09-23 14:24:33,412 - trainer - INFO - {
  "dev_loss": 6.966395854949951,
  "dev_best_score_for_loss": -6.966395854949951
}
2022-09-23 14:24:33,412 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,413 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,413 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,414 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_70
2022-09-23 14:24:33,415 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,416 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,417 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,417 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,417 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,418 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_76
2022-09-23 14:24:33,420 - trainer - INFO - save model to path: model/mlp_tes1\ck_76
2022-09-23 14:24:33,421 - trainer - INFO - 
*****************[epoch: 76, global step: 77] eval training set at end of epoch***************
2022-09-23 14:24:33,421 - trainer - INFO - {
  "train_loss": 7.528082370758057
}
2022-09-23 14:24:33,422 - trainer - INFO - start training epoch 77
2022-09-23 14:24:33,422 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,422 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,422 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,425 - trainer - INFO - 
*****************[epoch: 77, global step: 78] eval training set at end of epoch***************
2022-09-23 14:24:33,426 - trainer - INFO - {
  "train_loss": 6.966404438018799
}
2022-09-23 14:24:33,426 - trainer - INFO - start training epoch 78
2022-09-23 14:24:33,426 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,426 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,427 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,430 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval training set based on eval_every=2***************
2022-09-23 14:24:33,430 - trainer - INFO - {
  "train_loss": 6.962111949920654
}
2022-09-23 14:24:33,432 - trainer - INFO - 
*****************[epoch: 78, global step: 78] eval development set based on eval_every=2***************
2022-09-23 14:24:33,432 - trainer - INFO - {
  "dev_loss": 7.344029903411865,
  "dev_best_score_for_loss": -6.966395854949951
}
2022-09-23 14:24:33,433 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,433 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,435 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,435 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,435 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_72
2022-09-23 14:24:33,436 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_78
2022-09-23 14:24:33,440 - trainer - INFO - save model to path: model/mlp_tes1\ck_78
2022-09-23 14:24:33,441 - trainer - INFO - 
*****************[epoch: 78, global step: 79] eval training set at end of epoch***************
2022-09-23 14:24:33,441 - trainer - INFO - {
  "train_loss": 6.95781946182251
}
2022-09-23 14:24:33,442 - trainer - INFO - start training epoch 79
2022-09-23 14:24:33,442 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,442 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,443 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,447 - trainer - INFO - 
*****************[epoch: 79, global step: 80] eval training set at end of epoch***************
2022-09-23 14:24:33,447 - trainer - INFO - {
  "train_loss": 7.344034194946289
}
2022-09-23 14:24:33,447 - trainer - INFO - start training epoch 80
2022-09-23 14:24:33,448 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,448 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,448 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,452 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval training set based on eval_every=2***************
2022-09-23 14:24:33,453 - trainer - INFO - {
  "train_loss": 7.442307472229004
}
2022-09-23 14:24:33,456 - trainer - INFO - 
*****************[epoch: 80, global step: 80] eval development set based on eval_every=2***************
2022-09-23 14:24:33,456 - trainer - INFO - {
  "dev_loss": 7.283097743988037,
  "dev_best_score_for_loss": -6.966395854949951
}
2022-09-23 14:24:33,457 - trainer - INFO -   no_improve_count: 2
2022-09-23 14:24:33,457 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,458 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,458 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,459 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_74
2022-09-23 14:24:33,460 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_80
2022-09-23 14:24:33,463 - trainer - INFO - save model to path: model/mlp_tes1\ck_80
2022-09-23 14:24:33,464 - trainer - INFO - 
*****************[epoch: 80, global step: 81] eval training set at end of epoch***************
2022-09-23 14:24:33,464 - trainer - INFO - {
  "train_loss": 7.540580749511719
}
2022-09-23 14:24:33,465 - trainer - INFO - start training epoch 81
2022-09-23 14:24:33,465 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,465 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,466 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,469 - trainer - INFO - 
*****************[epoch: 81, global step: 82] eval training set at end of epoch***************
2022-09-23 14:24:33,470 - trainer - INFO - {
  "train_loss": 7.283092975616455
}
2022-09-23 14:24:33,470 - trainer - INFO - start training epoch 82
2022-09-23 14:24:33,470 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,471 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,471 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,474 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval training set based on eval_every=2***************
2022-09-23 14:24:33,475 - trainer - INFO - {
  "train_loss": 7.085971355438232
}
2022-09-23 14:24:33,478 - trainer - INFO - 
*****************[epoch: 82, global step: 82] eval development set based on eval_every=2***************
2022-09-23 14:24:33,478 - trainer - INFO - {
  "dev_loss": 6.780283451080322,
  "dev_best_score_for_loss": -6.780283451080322
}
2022-09-23 14:24:33,479 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,480 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,480 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,481 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_76
2022-09-23 14:24:33,482 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,484 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,485 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,485 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,486 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,486 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_82
2022-09-23 14:24:33,489 - trainer - INFO - save model to path: model/mlp_tes1\ck_82
2022-09-23 14:24:33,489 - trainer - INFO - 
*****************[epoch: 82, global step: 83] eval training set at end of epoch***************
2022-09-23 14:24:33,489 - trainer - INFO - {
  "train_loss": 6.88884973526001
}
2022-09-23 14:24:33,490 - trainer - INFO - start training epoch 83
2022-09-23 14:24:33,490 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,490 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,490 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,494 - trainer - INFO - 
*****************[epoch: 83, global step: 84] eval training set at end of epoch***************
2022-09-23 14:24:33,494 - trainer - INFO - {
  "train_loss": 6.780279636383057
}
2022-09-23 14:24:33,495 - trainer - INFO - start training epoch 84
2022-09-23 14:24:33,495 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,495 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,496 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,499 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval training set based on eval_every=2***************
2022-09-23 14:24:33,500 - trainer - INFO - {
  "train_loss": 6.8759541511535645
}
2022-09-23 14:24:33,502 - trainer - INFO - 
*****************[epoch: 84, global step: 84] eval development set based on eval_every=2***************
2022-09-23 14:24:33,503 - trainer - INFO - {
  "dev_loss": 7.1313652992248535,
  "dev_best_score_for_loss": -6.780283451080322
}
2022-09-23 14:24:33,503 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,504 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,505 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,505 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,505 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_78
2022-09-23 14:24:33,507 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_84
2022-09-23 14:24:33,510 - trainer - INFO - save model to path: model/mlp_tes1\ck_84
2022-09-23 14:24:33,511 - trainer - INFO - 
*****************[epoch: 84, global step: 85] eval training set at end of epoch***************
2022-09-23 14:24:33,511 - trainer - INFO - {
  "train_loss": 6.971628665924072
}
2022-09-23 14:24:33,511 - trainer - INFO - start training epoch 85
2022-09-23 14:24:33,512 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,512 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,512 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,516 - trainer - INFO - 
*****************[epoch: 85, global step: 86] eval training set at end of epoch***************
2022-09-23 14:24:33,517 - trainer - INFO - {
  "train_loss": 7.131368160247803
}
2022-09-23 14:24:33,517 - trainer - INFO - start training epoch 86
2022-09-23 14:24:33,517 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,517 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,518 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,521 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval training set based on eval_every=2***************
2022-09-23 14:24:33,521 - trainer - INFO - {
  "train_loss": 7.079896688461304
}
2022-09-23 14:24:33,523 - trainer - INFO - 
*****************[epoch: 86, global step: 86] eval development set based on eval_every=2***************
2022-09-23 14:24:33,523 - trainer - INFO - {
  "dev_loss": 6.7796478271484375,
  "dev_best_score_for_loss": -6.7796478271484375
}
2022-09-23 14:24:33,524 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,525 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,525 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,525 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_80
2022-09-23 14:24:33,526 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,528 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,528 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,529 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,529 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,530 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_86
2022-09-23 14:24:33,532 - trainer - INFO - save model to path: model/mlp_tes1\ck_86
2022-09-23 14:24:33,532 - trainer - INFO - 
*****************[epoch: 86, global step: 87] eval training set at end of epoch***************
2022-09-23 14:24:33,533 - trainer - INFO - {
  "train_loss": 7.028425216674805
}
2022-09-23 14:24:33,533 - trainer - INFO - start training epoch 87
2022-09-23 14:24:33,533 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,533 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,534 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,537 - trainer - INFO - 
*****************[epoch: 87, global step: 88] eval training set at end of epoch***************
2022-09-23 14:24:33,538 - trainer - INFO - {
  "train_loss": 6.779648303985596
}
2022-09-23 14:24:33,538 - trainer - INFO - start training epoch 88
2022-09-23 14:24:33,538 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,538 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,539 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,542 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval training set based on eval_every=2***************
2022-09-23 14:24:33,543 - trainer - INFO - {
  "train_loss": 6.717569589614868
}
2022-09-23 14:24:33,545 - trainer - INFO - 
*****************[epoch: 88, global step: 88] eval development set based on eval_every=2***************
2022-09-23 14:24:33,545 - trainer - INFO - {
  "dev_loss": 6.736316680908203,
  "dev_best_score_for_loss": -6.736316680908203
}
2022-09-23 14:24:33,546 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,547 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,547 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,547 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_82
2022-09-23 14:24:33,548 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,550 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,550 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,550 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,551 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,551 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_88
2022-09-23 14:24:33,554 - trainer - INFO - save model to path: model/mlp_tes1\ck_88
2022-09-23 14:24:33,554 - trainer - INFO - 
*****************[epoch: 88, global step: 89] eval training set at end of epoch***************
2022-09-23 14:24:33,554 - trainer - INFO - {
  "train_loss": 6.655490875244141
}
2022-09-23 14:24:33,555 - trainer - INFO - start training epoch 89
2022-09-23 14:24:33,555 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,555 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,556 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,559 - trainer - INFO - 
*****************[epoch: 89, global step: 90] eval training set at end of epoch***************
2022-09-23 14:24:33,559 - trainer - INFO - {
  "train_loss": 6.736318588256836
}
2022-09-23 14:24:33,560 - trainer - INFO - start training epoch 90
2022-09-23 14:24:33,560 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,560 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,560 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,564 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval training set based on eval_every=2***************
2022-09-23 14:24:33,564 - trainer - INFO - {
  "train_loss": 6.7900800704956055
}
2022-09-23 14:24:33,567 - trainer - INFO - 
*****************[epoch: 90, global step: 90] eval development set based on eval_every=2***************
2022-09-23 14:24:33,567 - trainer - INFO - {
  "dev_loss": 6.798341751098633,
  "dev_best_score_for_loss": -6.736316680908203
}
2022-09-23 14:24:33,568 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,568 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,569 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,569 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,569 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_84
2022-09-23 14:24:33,571 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_90
2022-09-23 14:24:33,573 - trainer - INFO - save model to path: model/mlp_tes1\ck_90
2022-09-23 14:24:33,574 - trainer - INFO - 
*****************[epoch: 90, global step: 91] eval training set at end of epoch***************
2022-09-23 14:24:33,574 - trainer - INFO - {
  "train_loss": 6.843841552734375
}
2022-09-23 14:24:33,575 - trainer - INFO - start training epoch 91
2022-09-23 14:24:33,575 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,575 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,576 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,579 - trainer - INFO - 
*****************[epoch: 91, global step: 92] eval training set at end of epoch***************
2022-09-23 14:24:33,579 - trainer - INFO - {
  "train_loss": 6.798341751098633
}
2022-09-23 14:24:33,580 - trainer - INFO - start training epoch 92
2022-09-23 14:24:33,580 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,580 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,581 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,584 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval training set based on eval_every=2***************
2022-09-23 14:24:33,584 - trainer - INFO - {
  "train_loss": 6.7194108963012695
}
2022-09-23 14:24:33,586 - trainer - INFO - 
*****************[epoch: 92, global step: 92] eval development set based on eval_every=2***************
2022-09-23 14:24:33,587 - trainer - INFO - {
  "dev_loss": 6.543131351470947,
  "dev_best_score_for_loss": -6.543131351470947
}
2022-09-23 14:24:33,587 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,588 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,588 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,588 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_86
2022-09-23 14:24:33,589 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,591 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,591 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,591 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,592 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,592 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_92
2022-09-23 14:24:33,595 - trainer - INFO - save model to path: model/mlp_tes1\ck_92
2022-09-23 14:24:33,595 - trainer - INFO - 
*****************[epoch: 92, global step: 93] eval training set at end of epoch***************
2022-09-23 14:24:33,595 - trainer - INFO - {
  "train_loss": 6.640480041503906
}
2022-09-23 14:24:33,596 - trainer - INFO - start training epoch 93
2022-09-23 14:24:33,596 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,596 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,596 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,600 - trainer - INFO - 
*****************[epoch: 93, global step: 94] eval training set at end of epoch***************
2022-09-23 14:24:33,600 - trainer - INFO - {
  "train_loss": 6.543131351470947
}
2022-09-23 14:24:33,600 - trainer - INFO - start training epoch 94
2022-09-23 14:24:33,601 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,601 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,601 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,604 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval training set based on eval_every=2***************
2022-09-23 14:24:33,605 - trainer - INFO - {
  "train_loss": 6.558750152587891
}
2022-09-23 14:24:33,607 - trainer - INFO - 
*****************[epoch: 94, global step: 94] eval development set based on eval_every=2***************
2022-09-23 14:24:33,607 - trainer - INFO - {
  "dev_loss": 6.633251667022705,
  "dev_best_score_for_loss": -6.543131351470947
}
2022-09-23 14:24:33,608 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,608 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,609 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,609 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,609 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_88
2022-09-23 14:24:33,610 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_94
2022-09-23 14:24:33,613 - trainer - INFO - save model to path: model/mlp_tes1\ck_94
2022-09-23 14:24:33,613 - trainer - INFO - 
*****************[epoch: 94, global step: 95] eval training set at end of epoch***************
2022-09-23 14:24:33,614 - trainer - INFO - {
  "train_loss": 6.574368953704834
}
2022-09-23 14:24:33,614 - trainer - INFO - start training epoch 95
2022-09-23 14:24:33,614 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,615 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,615 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,618 - trainer - INFO - 
*****************[epoch: 95, global step: 96] eval training set at end of epoch***************
2022-09-23 14:24:33,619 - trainer - INFO - {
  "train_loss": 6.633251667022705
}
2022-09-23 14:24:33,619 - trainer - INFO - start training epoch 96
2022-09-23 14:24:33,619 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,619 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,620 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,623 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval training set based on eval_every=2***************
2022-09-23 14:24:33,623 - trainer - INFO - {
  "train_loss": 6.618865489959717
}
2022-09-23 14:24:33,625 - trainer - INFO - 
*****************[epoch: 96, global step: 96] eval development set based on eval_every=2***************
2022-09-23 14:24:33,625 - trainer - INFO - {
  "dev_loss": 6.502115249633789,
  "dev_best_score_for_loss": -6.502115249633789
}
2022-09-23 14:24:33,626 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,627 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,627 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,627 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_90
2022-09-23 14:24:33,628 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,630 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,630 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,630 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,631 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,631 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_96
2022-09-23 14:24:33,634 - trainer - INFO - save model to path: model/mlp_tes1\ck_96
2022-09-23 14:24:33,634 - trainer - INFO - 
*****************[epoch: 96, global step: 97] eval training set at end of epoch***************
2022-09-23 14:24:33,634 - trainer - INFO - {
  "train_loss": 6.6044793128967285
}
2022-09-23 14:24:33,635 - trainer - INFO - start training epoch 97
2022-09-23 14:24:33,635 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,635 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,636 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,639 - trainer - INFO - 
*****************[epoch: 97, global step: 98] eval training set at end of epoch***************
2022-09-23 14:24:33,639 - trainer - INFO - {
  "train_loss": 6.502120494842529
}
2022-09-23 14:24:33,639 - trainer - INFO - start training epoch 98
2022-09-23 14:24:33,640 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,640 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,640 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,643 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval training set based on eval_every=2***************
2022-09-23 14:24:33,643 - trainer - INFO - {
  "train_loss": 6.465993881225586
}
2022-09-23 14:24:33,645 - trainer - INFO - 
*****************[epoch: 98, global step: 98] eval development set based on eval_every=2***************
2022-09-23 14:24:33,646 - trainer - INFO - {
  "dev_loss": 6.436891078948975,
  "dev_best_score_for_loss": -6.436891078948975
}
2022-09-23 14:24:33,646 - trainer - INFO -    save the model with best score so far
2022-09-23 14:24:33,647 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,647 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,647 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_92
2022-09-23 14:24:33,648 - trainer - INFO -   Save checkpoint to model/mlp_tes1
2022-09-23 14:24:33,650 - trainer - INFO - save model to path: model/mlp_tes1
2022-09-23 14:24:33,650 - trainer - INFO -   no_improve_count: 0
2022-09-23 14:24:33,651 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,651 - trainer - INFO -    Check 2 checkpoints already saved
2022-09-23 14:24:33,651 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_98
2022-09-23 14:24:33,655 - trainer - INFO - save model to path: model/mlp_tes1\ck_98
2022-09-23 14:24:33,655 - trainer - INFO - 
*****************[epoch: 98, global step: 99] eval training set at end of epoch***************
2022-09-23 14:24:33,655 - trainer - INFO - {
  "train_loss": 6.429867267608643
}
2022-09-23 14:24:33,656 - trainer - INFO - start training epoch 99
2022-09-23 14:24:33,656 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,656 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,656 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,660 - trainer - INFO - 
*****************[epoch: 99, global step: 100] eval training set at end of epoch***************
2022-09-23 14:24:33,660 - trainer - INFO - {
  "train_loss": 6.436891078948975
}
2022-09-23 14:24:33,661 - trainer - INFO - start training epoch 100
2022-09-23 14:24:33,661 - trainer - INFO - training using device=cpu
2022-09-23 14:24:33,661 - trainer - INFO - 
*************hyperparam_dict**********

2022-09-23 14:24:33,662 - trainer - INFO - {
  "train_epochs": 100,
  "eval_batch_size": 64,
  "train_batch_size": 130,
  "no_improve_count": 0,
  "device": "cpu",
  "patience": 200,
  "save_path": "model/mlp_tes1",
  "eval_on": "loss",
  "eval_every": 2,
  "use_wandb": false,
  "loss_fn": "mse",
  "keep_ck_num": 3,
  "lr": 0.03
}
2022-09-23 14:24:33,665 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval training set based on eval_every=2***************
2022-09-23 14:24:33,666 - trainer - INFO - {
  "train_loss": 6.450681447982788
}
2022-09-23 14:24:33,668 - trainer - INFO - 
*****************[epoch: 100, global step: 100] eval development set based on eval_every=2***************
2022-09-23 14:24:33,668 - trainer - INFO - {
  "dev_loss": 6.4381866455078125,
  "dev_best_score_for_loss": -6.436891078948975
}
2022-09-23 14:24:33,669 - trainer - INFO -   no_improve_count: 1
2022-09-23 14:24:33,669 - trainer - INFO -   patience: 200
2022-09-23 14:24:33,670 - trainer - INFO -    Check 3 checkpoints already saved
2022-09-23 14:24:33,670 - trainer - INFO -    There are more than keep_ck_num as specified, then remove the oldest saved checkpoint
2022-09-23 14:24:33,670 - trainer - INFO -   Remove checkpoint model/mlp_tes1\ck_94
2022-09-23 14:24:33,671 - trainer - INFO -   Save checkpoint to model/mlp_tes1\ck_100
2022-09-23 14:24:33,674 - trainer - INFO - save model to path: model/mlp_tes1\ck_100
2022-09-23 14:24:33,674 - trainer - INFO - 
*****************[epoch: 100, global step: 101] eval training set at end of epoch***************
2022-09-23 14:24:33,674 - trainer - INFO - {
  "train_loss": 6.464471817016602
}
